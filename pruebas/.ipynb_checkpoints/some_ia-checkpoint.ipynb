{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fde30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf38fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"export\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall protobuf \n",
    "\n",
    "#!pip install protobuf==3.19.4\n",
    "#! pip install tensorflow --upgrade --user\n",
    "\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "!python3 -m pip install tensorflow\n",
    "# Verify install:\n",
    "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb319110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621bd326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8356704398876053814\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d004683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0670cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['clase_pri', 'clase_otr','clases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330e9b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categorias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Actor-Critic Algorithm for Sequence Prediction</td>\n",
       "      <td>We present an approach to training neural ne...</td>\n",
       "      <td>['cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Playing Flappy Bird via Asynchronous Advantage...</td>\n",
       "      <td>Flappy Bird, which has a very high popularit...</td>\n",
       "      <td>['cs.LG', 'cs.NE']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Distributional Advantage Actor-Critic</td>\n",
       "      <td>In traditional reinforcement learning, an ag...</td>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Policy Actor-Critic</td>\n",
       "      <td>This paper presents the first actor-critic a...</td>\n",
       "      <td>['cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asymmetric Actor Critic for Image-Based Robot ...</td>\n",
       "      <td>Deep reinforcement learning (RL) has proven ...</td>\n",
       "      <td>['cs.RO', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36859</th>\n",
       "      <td>A Ray-based Approach for Boundary Estimation o...</td>\n",
       "      <td>Diffusion Tensor Imaging (DTI) is a non-invasi...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36860</th>\n",
       "      <td>Statistical Denoising for single molecule fluo...</td>\n",
       "      <td>Single molecule fluorescence microscopy is a p...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36861</th>\n",
       "      <td>Blinking Molecule Tracking</td>\n",
       "      <td>We discuss a method for tracking individual mo...</td>\n",
       "      <td>['cs.CV', 'cs.DM']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36862</th>\n",
       "      <td>Towards a Mathematical Foundation of Immunolog...</td>\n",
       "      <td>We attempt to set a mathematical foundation of...</td>\n",
       "      <td>['stat.ML', 'cs.LG', 'q-bio.GN']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36863</th>\n",
       "      <td>A Semi-Automatic Graph-Based Approach for Dete...</td>\n",
       "      <td>Diffusion Tensor Imaging (DTI) allows estimati...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36864 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titulo  \\\n",
       "0      An Actor-Critic Algorithm for Sequence Prediction   \n",
       "1      Playing Flappy Bird via Asynchronous Advantage...   \n",
       "2                  Distributional Advantage Actor-Critic   \n",
       "3                                Off-Policy Actor-Critic   \n",
       "4      Asymmetric Actor Critic for Image-Based Robot ...   \n",
       "...                                                  ...   \n",
       "36859  A Ray-based Approach for Boundary Estimation o...   \n",
       "36860  Statistical Denoising for single molecule fluo...   \n",
       "36861                         Blinking Molecule Tracking   \n",
       "36862  Towards a Mathematical Foundation of Immunolog...   \n",
       "36863  A Semi-Automatic Graph-Based Approach for Dete...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0        We present an approach to training neural ne...   \n",
       "1        Flappy Bird, which has a very high popularit...   \n",
       "2        In traditional reinforcement learning, an ag...   \n",
       "3        This paper presents the first actor-critic a...   \n",
       "4        Deep reinforcement learning (RL) has proven ...   \n",
       "...                                                  ...   \n",
       "36859  Diffusion Tensor Imaging (DTI) is a non-invasi...   \n",
       "36860  Single molecule fluorescence microscopy is a p...   \n",
       "36861  We discuss a method for tracking individual mo...   \n",
       "36862  We attempt to set a mathematical foundation of...   \n",
       "36863  Diffusion Tensor Imaging (DTI) allows estimati...   \n",
       "\n",
       "                             categorias  \n",
       "0                             ['cs.LG']  \n",
       "1                    ['cs.LG', 'cs.NE']  \n",
       "2         ['cs.LG', 'cs.AI', 'stat.ML']  \n",
       "3                             ['cs.LG']  \n",
       "4           ['cs.RO', 'cs.AI', 'cs.LG']  \n",
       "...                                 ...  \n",
       "36859                         ['cs.CV']  \n",
       "36860                         ['cs.CV']  \n",
       "36861                ['cs.CV', 'cs.DM']  \n",
       "36862  ['stat.ML', 'cs.LG', 'q-bio.GN']  \n",
       "36863                         ['cs.CV']  \n",
       "\n",
       "[36864 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fced2d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.NE']),\n",
       "       list(['cs.LG', 'cs.AI', 'stat.ML']), list(['cs.LG']),\n",
       "       list(['cs.RO', 'cs.AI', 'cs.LG'])], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"categorias\"] = data[\"categorias\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "data[\"categorias\"].values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8e8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "train_df, test_df = train_test_split(\n",
    "    data,\n",
    "    test_size=test_split,\n",
    "    stratify=data[\"categorias\"].values,\n",
    ")\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af927fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.CR', 'math.OC', 'eess.SP', 'cs.GR', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'cs.MA', 'eess.SY', 'cs.HC', 'cs.CY', 'cs.DC', 'math.IT', 'cs.IT', 'stat.AP', 'stat.TH', 'math.ST', 'stat.ME', 'eess.AS', 'cs.SD', 'q-bio.QM', 'q-bio.NC', 'cs.DS', 'cs.GT', 'cs.NI', 'cs.SE', 'cs.CG', 'I.2.6', 'stat.CO', 'math.NA', 'cs.NA', 'cs.DB', 'physics.chem-ph', 'quant-ph', 'q-bio.BM', 'cs.PL', 'cs.LO', 'cond-mat.dis-nn', '68T45', 'math.PR', 'physics.comp-ph', 'cs.AR', 'cs.CE', 'I.2.10', 'q-fin.ST', 'cond-mat.stat-mech', '68T05', 'physics.data-an', 'math.DS', 'cs.CC', 'I.4.6', 'physics.soc-ph', 'astro-ph.IM', 'physics.ao-ph', 'econ.EM', 'cs.DM', 'q-bio.GN', 'physics.med-ph', 'math.AT', 'I.4.8', 'cs.PF', 'cond-mat.str-el', 'cond-mat.mtrl-sci', 'cond-mat.mes-hall', 'I.4', 'q-fin.TR', 'cs.FL', 'I.5.4', 'I.2', '68U10', 'physics.geo-ph', 'hep-ex', '68T10', 'q-fin.GN', 'q-fin.CP', 'physics.optics', 'physics.flu-dyn', 'math.CO', 'math.AP', 'I.4; I.5', 'I.4.9', 'I.2.6; I.2.8', '68T01', '65D19', 'nlin.CD', 'cs.MS', 'cond-mat.supr-con', 'I.2.6; I.5.1', 'I.2.10; I.4; I.5', 'I.2.0; I.2.6', '68T07', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68U01', '68T30', '68', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.5', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.ins-det', 'physics.bio-ph', 'nucl-th', 'nlin.AO', 'math.SP', 'math.LO', 'math.FA', 'math-ph', 'hep-ph', 'cond-mat.soft', 'astro-ph.HE', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.0', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "terms = tf.ragged.constant(train_df[\"categorias\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(terms)\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b32d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML']\n",
      "Label-binarized representation: [[0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"categorias\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36df5615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    33177.000000\n",
       "mean       156.432529\n",
       "std         41.689557\n",
       "min          5.000000\n",
       "25%        128.000000\n",
       "50%        154.000000\n",
       "75%        183.000000\n",
       "max        462.000000\n",
       "Name: abstract, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"abstract\"].apply(lambda x: len(x.split(\" \"))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516e752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 155\n",
    "batch_size = 128\n",
    "padding_token = \"<pad>\"\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe[\"categorias\"].values)\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"abstract\"].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d542146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b6bf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'RGB-D salient object detection (SOD) is usually formulated as a problem of\\nclassification or regression over two modalities, i.e., RGB and depth. Hence,\\neffective RGBD feature modeling and multi-modal feature fusion both play a\\nvital role in RGB-D SOD. In this paper, we propose a depth-sensitive RGB\\nfeature modeling scheme using the depth-wise geometric prior of salient\\nobjects. In principle, the feature modeling scheme is carried out in a\\ndepth-sensitive attention module, which leads to the RGB feature enhancement as\\nwell as the background distraction reduction by capturing the depth geometry\\nprior. Moreover, to perform effective multi-modal feature fusion, we further\\npresent an automatic architecture search approach for RGB-D SOD, which does\\nwell in finding out a feasible architecture from our specially designed\\nmulti-modal multi-scale search space. Extensive experiments on seven standard\\nbenchmarks demonstrate the effectiveness of the proposed approach against the\\nstate-of-the-art.'\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b\"Transfer learning using pre-trained Convolutional Neural Networks (CNNs) has\\nbeen successfully applied to images for different classification tasks. In this\\npaper, we propose a new pipeline for pain expression recognition in neonates\\nusing transfer learning. Specifically, we propose to exploit a pre-trained CNN\\nthat was originally trained on a relatively similar dataset for face\\nrecognition (VGG Face) as well as CNNs that were pre-trained on a relatively\\ndifferent dataset for image classification (iVGG F,M, and S) to extract deep\\nfeatures from neonates' faces. In the final stage, several supervised machine\\nlearning classifiers are trained to classify neonates' facial expression into\\npain or no pain expression. The proposed pipeline achieved, on a testing\\ndataset, 0.841 AUC and 90.34 accuracy, which is approx. 7 higher than the\\naccuracy of handcrafted traditional features. We also propose to combine deep\\nfeatures with traditional features and hypothesize that the mixed features\\nwould improve pain classification performance. Combining deep features with\\ntraditional features achieved 92.71 accuracy and 0.948 AUC. These results show\\nthat transfer learning, which is a faster and more practical option than\\ntraining CNN from the scratch, can be used to extract useful features for pain\\nexpression recognition in neonates. It also shows that combining deep features\\nwith traditional handcrafted features is a good practice to improve the\\nperformance of pain expression recognition and possibly the performance of\\nsimilar applications.\"\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition,\\nrequiring only scene-level class tags as supervision. WyPR jointly addresses\\nthree core 3D recognition tasks: point-level semantic segmentation, 3D proposal\\ngeneration, and 3D object detection, coupling their predictions through self\\nand cross-task consistency losses. We show that in conjunction with standard\\nmultiple-instance learning objectives, WyPR can detect and segment objects in\\npoint cloud data without access to any spatial labels at training time. We\\ndemonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming\\nprior state of the art on weakly-supervised segmentation by more than 6% mIoU.\\nIn addition, we set up the first benchmark for weakly-supervised 3D object\\ndetection on both datasets, where WyPR outperforms standard approaches and\\nestablishes strong baselines for future work.'\n",
      "Label(s): ['cs.CV' 'cs.LG' 'cs.AI' 'cs.MM']\n",
      " \n",
      "Abstract: b\"Recently, Deep Learning (DL) methods have shown an excellent performance in\\nimage captioning and visual question answering. However, despite their\\nperformance, DL methods do not learn the semantics of the words that are being\\nused to describe a scene, making it difficult to spot incorrect words used in\\ncaptions or to interchange words that have similar meanings. This work proposes\\na combination of DL methods for object detection and natural language\\nprocessing to validate image's captions. We test our method in the FOIL-COCO\\ndata set, since it provides correct and incorrect captions for various images\\nusing only objects represented in the MS-COCO image data set. Results show that\\nour method has a good overall performance, in some cases similar to the human\\nperformance.\"\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'This study investigates the use of reinforcement learning to guide a general\\npurpose cache manager decisions. Cache managers directly impact the overall\\nperformance of computer systems. They govern decisions about which objects\\nshould be cached, the duration they should be cached for, and decides on which\\nobjects to evict from the cache if it is full. These three decisions impact\\nboth the cache hit rate and size of the storage that is needed to achieve that\\ncache hit rate. An optimal cache manager will avoid unnecessary operations,\\nmaximise the cache hit rate which results in fewer round trips to a slower\\nbackend storage system, and minimise the size of storage needed to achieve a\\nhigh hit-rate.\\n  This project investigates using reinforcement learning in cache management by\\ndesigning three separate agents for each of the cache manager tasks.\\nFurthermore, the project investigates two advanced reinforcement learning\\narchitectures for multi-decision problems: a single multi-task agent and a\\nmulti-agent. We also introduce a framework to simplify the modelling of\\ncomputer systems problems as a reinforcement learning task. The framework\\nabstracts delayed experiences observations and reward assignment in computer\\nsystems while providing a flexible way to scale to multiple agents.\\n  Simulation results based on an established database benchmark system show\\nthat reinforcement learning agents can achieve a higher cache hit rate over\\nheuristic driven algorithms while minimising the needed space. They are also\\nable to adapt to a changing workload and dynamically adjust their caching\\nstrategy accordingly. The proposed cache manager model is generic and\\napplicable to other types of caches, such as file system caches. This project\\nis the first, to our knowledge, to model cache manager decisions as a\\nmulti-task control problem.'\n",
      "Label(s): ['cs.LG' 'stat.ML' 'cs.DC']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7da1cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154093\n"
     ]
    }
   ],
   "source": [
    "# Source: https://stackoverflow.com/a/18937309/7636462\n",
    "vocabulary = set()\n",
    "train_df[\"abstract\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cce1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yo\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
    ")\n",
    "\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827201ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "        ]  # More on why \"sigmoid\" has been used here in a moment.\n",
    "    )\n",
    "    return shallow_mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c00d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "260/260 [==============================] - 171s 654ms/step - loss: 0.0318 - binary_accuracy: 0.9894 - val_loss: 0.0176 - val_binary_accuracy: 0.9945\n",
      "Epoch 2/20\n",
      "260/260 [==============================] - 179s 686ms/step - loss: 0.0032 - binary_accuracy: 0.9991 - val_loss: 0.0239 - val_binary_accuracy: 0.9941\n",
      "Epoch 3/20\n",
      "260/260 [==============================] - 178s 685ms/step - loss: 8.2720e-04 - binary_accuracy: 0.9998 - val_loss: 0.0286 - val_binary_accuracy: 0.9944\n",
      "Epoch 4/20\n",
      "260/260 [==============================] - 180s 689ms/step - loss: 3.2408e-04 - binary_accuracy: 1.0000 - val_loss: 0.0334 - val_binary_accuracy: 0.9944\n",
      "Epoch 5/20\n",
      "260/260 [==============================] - 180s 691ms/step - loss: 1.5356e-04 - binary_accuracy: 1.0000 - val_loss: 0.0364 - val_binary_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "  5/260 [..............................] - ETA: 3:50 - loss: 3.7202e-05 - binary_accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_accuracy\"]\n",
    ")\n",
    "\n",
    "history = shallow_mlp_model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(shallow_mlp_model,'radEd_model_v1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde01777",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, binary_acc = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f\"Categorical accuracy on the test set: {round(binary_acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02897c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for inference.\n",
    "model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n",
    "\n",
    "# Create a small dataset just for demoing inference.\n",
    "inference_dataset = make_dataset(test_df.sample(100), is_train=False)\n",
    "text_batch, label_batch = next(iter(inference_dataset))\n",
    "predicted_probabilities = model_for_inference.predict(text_batch)\n",
    "\n",
    "# Perform inference.\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    predicted_proba = [proba for proba in predicted_probabilities[i]]\n",
    "    top_3_labels = [\n",
    "        x\n",
    "        for _, x in sorted(\n",
    "            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n",
    "            key=lambda pair: pair[0],\n",
    "            reverse=True,\n",
    "        )\n",
    "    ][:3]\n",
    "    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
