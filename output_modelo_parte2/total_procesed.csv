Unnamed: 0,_id,fuente,titulo,autores,abstract,clase_primaria,pag_espec,n_citaciones,citado,abstract_limpio,keywords
0,{'ObjectId': '639d9f0d6d67832e75895c46'},arxiv,['Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU'],"['Mohammad Babaeizadeh', 'Iuri Frosio', 'Stephen Tyree', 'Jason Clemons', 'Jan Kautz']","['\n      ', ""  We introduce a hybrid CPU/GPU version of the Asynchronous Advantage\nActor-Critic (A3C) algorithm, currently the state-of-the-art method in\nreinforcement learning for various gaming tasks. We analyze its computational\ntraits and concentrate on aspects critical to leveraging the GPU's\ncomputational power. We introduce a system of queues and a dynamic scheduling\nstrategy, potentially helpful for other asynchronous algorithms as well. Our\nhybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant\nspeed up compared to a CPU implementation; we make it publicly available to\nother researchers at "", ' .\n\n    ']",['Machine Learning (cs.LG)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 222'],"['/scholar?cites=8757115672331028243&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']","[\n       ""  we introduce a hybrid cpugpu version of the asynchronous advantage\nactorcritic ac algorithm currently the stateoftheart method in\nreinforcement learning for various gaming tasks we analyze its computational\ntraits and concentrate on aspects critical to leveraging the gpus\ncomputational power we introduce a system of queues and a dynamic scheduling\nstrategy potentially helpful for other asynchronous algorithms as well our\nhybrid cpugpu version of ac based on tensorflow achieves a significant\nspeed up compared to a cpu implementation; we make it publicly available to\nother researchers at ""  \n\n    ]","['##chronous advantage \\' '##oft' '##s' '\\ nreinforcement learning'
 'computational \\ ntraits' 'cpu implementation'
 'dynamic scheduling \\ nstrategy' 'gaming' 'gpus \\' 'hybrid cpugpu'
 'method' 'nactorcritic ac algorithm' 'tensorflow']"
1,{'ObjectId': '639d9f0d6d67832e75895c47'},arxiv,['Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic'],"['Behrad Toghi', 'Rodolfo Valiente', 'Dorsa Sadigh', 'Ramtin Pedarsani', 'Yaser P. Fallah']","['\n      ', ""  With the adoption of autonomous vehicles on our roads, we will witness a\nmixed-autonomy environment where autonomous and human-driven vehicles must\nlearn to co-exist by sharing the same road infrastructure. To attain\nsocially-desirable behaviors, autonomous vehicles must be instructed to\nconsider the utility of other vehicles around them in their decision-making\nprocess. Particularly, we study the maneuver planning problem for autonomous\nvehicles and investigate how a decentralized reward structure can induce\naltruism in their behavior and incentivize them to account for the interest of\nother autonomous and human-driven vehicles. This is a challenging problem due\nto the ambiguity of a human driver's willingness to cooperate with an\nautonomous vehicle. Thus, in contrast with the existing works which rely on\nbehavior models of human drivers, we take an end-to-end approach and let the\nautonomous agents to implicitly learn the decision-making process of human\ndrivers only from experience. We introduce a multi-agent variant of the\nsynchronous Advantage Actor-Critic (A2C) algorithm and train agents that\ncoordinate with each other and can affect the behavior of human drivers to\nimprove traffic flow and safety.\n\n    ""]",['Robotics (cs.RO)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 11'],"['/scholar?cites=1543039800876013962&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']","[\n       ""  with the adoption of autonomous vehicles on our roads we will witness a\nmixedautonomy environment where autonomous and humandriven vehicles must\nlearn to coexist by sharing the same road infrastructure to attain\nsociallydesirable behaviors autonomous vehicles must be instructed to\nconsider the utility of other vehicles around them in their decisionmaking\nprocess particularly we study the maneuver planning problem for autonomous\nvehicles and investigate how a decentralized reward structure can induce\naltruism in their behavior and incentivize them to account for the interest of\nother autonomous and humandriven vehicles this is a challenging problem due\nto the ambiguity of a human drivers willingness to cooperate with an\nautonomous vehicle thus in contrast with the existing works which rely on\nbehavior models of human drivers we take an endtoend approach and let the\nautonomous agents to implicitly learn the decisionmaking process of human\ndrivers only from experience we introduce a multiagent variant of the\nsynchronous advantage actorcritic ac algorithm and train agents that\ncoordinate with each other and can affect the behavior of human drivers to\nimprove traffic flow and safety\n\n    ""]","['##chronous advantage actorcritic ac algorithm' '##drive'
 '##lydesirable behaviors autonomous vehicles' '##oc' '\\'
 '\\ nbehavior models' 'autonomous vehicles' 'decisionmaking process'
 'endtoend approach' 'human drivers' 'humandriven vehicles'
 'maneuver planning problem' 'multiagent' 'nautonomous agents'
 'road infrastructure' 'train agents']"
2,{'ObjectId': '639d9f0e6d67832e75895c48'},arxiv,['Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup'],"['Han Shen', 'Kaiqing Zhang', 'Mingyi Hong', 'Tianyi Chen']","['\n      ', '  Asynchronous and parallel implementation of standard reinforcement learning\n(RL) algorithms is a key enabler of the tremendous success of modern RL. Among\nmany asynchronous RL algorithms, arguably the most popular and effective one is\nthe asynchronous advantage actor-critic (A3C) algorithm. Although A3C is\nbecoming the workhorse of RL, its theoretical properties are still not\nwell-understood, including its non-asymptotic analysis and the performance gain\nof parallelism (a.k.a. linear speedup). This paper revisits the A3C algorithm\nand establishes its non-asymptotic convergence guarantees. Under both i.i.d.\nand Markovian sampling, we establish the local convergence guarantee for A3C in\nthe general policy approximation case and the global convergence guarantee in\nsoftmax policy parameterization. Under i.i.d. sampling, A3C obtains sample\ncomplexity of $\\mathcal{O}(\\epsilon^{-2.5}/N)$ per worker to achieve $\\epsilon$\naccuracy, where $N$ is the number of workers. Compared to the best-known sample\ncomplexity of $\\mathcal{O}(\\epsilon^{-2.5})$ for two-timescale AC, A3C achieves\n\\emph{linear speedup}, which justifies the advantage of parallelism and\nasynchrony in AC algorithms theoretically for the first time. Numerical tests\non synthetic environment, OpenAI Gym environments and Atari games have been\nprovided to verify our theoretical analysis.\n\n    ']",['Machine Learning (cs.LG)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 10'],"['/scholar?cites=17816434878891582694&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         asynchronous and parallel implementation of standard reinforcement learning\nrl algorithms is a key enabler of the tremendous success of modern rl among\nmany asynchronous rl algorithms arguably the most popular and effective one is\nthe asynchronous advantage actorcritic ac algorithm although ac is\nbecoming the workhorse of rl its theoretical properties are still not\nwellunderstood including its nonasymptotic analysis and the performance gain\nof parallelism aka linear speedup this paper revisits the ac algorithm\nand establishes its nonasymptotic convergence guarantees under both iid\nand markovian sampling we establish the local convergence guarantee for ac in\nthe general policy approximation case and the global convergence guarantee in\nsoftmax policy parameterization under iid sampling ac obtains sample\ncomplexity of $\\mathcal{o}\\epsilon^{}n$ per worker to achieve $\\epsilon$\naccuracy where $n$ is the number of workers compared to the bestknown sample\ncomplexity of $\\mathcal{o}\\epsilon^{}$ for twotimescale ac ac achieves\n\\emph{linear speedup} which justifies the advantage of parallelism and\nasynchrony in ac algorithms theoretically for the first time numerical tests\non synthetic environment openai gym environments and atari games have been\nprovided to verify our theoretical analysis\n\n    ],"['##critic' '##d \\ nand markovian sampling' '##nown sample' '\\ nmany'
 '\\ nrl algorithms' '\\ nthe general policy approximation' 'atari games'
 'best' 'convergence' 'iid sampling ac' 'local convergence guarantee'
 'non synthetic environment' 'nonasymptotic analysis'
 'nonasymptotic convergence' 'nsoftmax policy' 'openai gym environments'
 'parallel implementation' 'reinforcement learning']"
3,{'ObjectId': '639d9f0e6d67832e75895c49'},arxiv,['The Advantage Regret-Matching Actor-Critic'],"['Audrūnas Gruslys', 'Marc Lanctot', 'Rémi Munos', 'Finbarr Timbers', 'Martin Schmid', 'Julien Perolat', 'Dustin Morrill', 'Vinicius Zambaldi', 'Jean-Baptiste Lespiau', 'John Schultz', 'Mohammad Gheshlaghi Azar', 'Michael Bowling', 'Karl Tuyls']","['\n      ', ""  Regret minimization has played a key role in online learning, equilibrium\ncomputation in games, and reinforcement learning (RL). In this paper, we\ndescribe a general model-free RL method for no-regret learning based on\nrepeated reconsideration of past behavior. We propose a model-free RL\nalgorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than\nsaving past state-action data, ARMAC saves a buffer of past policies, replaying\nthrough them to reconstruct hindsight assessments of past behavior. These\nretrospective value estimates are used to predict conditional advantages which,\ncombined with regret matching, produces a new policy. In particular, ARMAC\nlearns from sampled trajectories in a centralized training setting, without\nrequiring the application of importance sampling commonly used in Monte Carlo\ncounterfactual regret (CFR) minimization; hence, it does not suffer from\nexcessive variance in large environments. In the single-agent setting, ARMAC\nshows an interesting form of exploration by keeping past policies intact. In\nthe multiagent setting, ARMAC in self-play approaches Nash equilibria on some\npartially-observable zero-sum benchmarks. We provide exploitability estimates\nin the significantly larger game of betting-abstracted no-limit Texas Hold'em.\n\n    ""]",['Artificial Intelligence (cs.AI)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 14'],"['/scholar?cites=13622395690856603026&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']","[\n       ""  regret minimization has played a key role in online learning equilibrium\ncomputation in games and reinforcement learning rl in this paper we\ndescribe a general modelfree rl method for noregret learning based on\nrepeated reconsideration of past behavior we propose a modelfree rl\nalgorithm the advantageregretmatching actorcritic armac: rather than\nsaving past stateaction data armac saves a buffer of past policies replaying\nthrough them to reconstruct hindsight assessments of past behavior these\nretrospective value estimates are used to predict conditional advantages which\ncombined with regret matching produces a new policy in particular armac\nlearns from sampled trajectories in a centralized training setting without\nrequiring the application of importance sampling commonly used in monte carlo\ncounterfactual regret cfr minimization; hence it does not suffer from\nexcessive variance in large environments in the singleagent setting armac\nshows an interesting form of exploration by keeping past policies intact in\nthe multiagent setting armac in selfplay approaches nash equilibria on some\npartiallyobservable zerosum benchmarks we provide exploitability estimates\nin the significantly larger game of bettingabstracted nolimit texas holdem\n\n    ""]","['##lgorithm' '##regre' '\\' '\\ ncomputation' '\\ nexcessive variance'
 '\\ npartiallyobservable zerosum' '\\ nrepeated reconsideration'
 '\\ nthe multiagent' 'actorcritic armac' 'armac' 'centralized training'
 'conditional advantages' 'general model' 'importance sampling'
 'monte carlo \\ ncounterfactual regret cfr minimization'
 'nash equilibria' 'nolimit' 'noregret learning'
 'nretrospective value estimates' 'online learning equilibrium'
 'past stateaction data' 'regret matching' 'regret minimization'
 'reinforcement learning' 'sampled trajectories' 'selfplay'
 'singleagent setting armac' 'texas holdem']"
4,{'ObjectId': '639d9f0e6d67832e75895c4a'},arxiv,['Actor-Critic Sequence Training for Image Captioning'],"['Li Zhang', 'Flood Sung', 'Feng Liu', 'Tao Xiang', 'Shaogang Gong', 'Yongxin Yang', 'Timothy M. Hospedales']","['\n      ', '  Generating natural language descriptions of images is an important capability\nfor a robot or other visual-intelligence driven AI agent that may need to\ncommunicate with human users about what it is seeing. Such image captioning\nmethods are typically trained by maximising the likelihood of ground-truth\nannotated caption given the image. While simple and easy to implement, this\napproach does not directly maximise the language quality metrics we care about\nsuch as CIDEr. In this paper we investigate training image captioning methods\nbased on actor-critic reinforcement learning in order to directly optimise\nnon-differentiable quality metrics of interest. By formulating a per-token\nadvantage and value computation strategy in this novel reinforcement learning\nbased captioning model, we show that it is possible to achieve the state of the\nart performance on the widely used MSCOCO benchmark.\n\n    ']",['Computer Vision and Pattern Recognition (cs.CV)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 111'],"['/scholar?cites=16998376404483706528&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         generating natural language descriptions of images is an important capability\nfor a robot or other visualintelligence driven ai agent that may need to\ncommunicate with human users about what it is seeing such image captioning\nmethods are typically trained by maximising the likelihood of groundtruth\nannotated caption given the image while simple and easy to implement this\napproach does not directly maximise the language quality metrics we care about\nsuch as cider in this paper we investigate training image captioning methods\nbased on actorcritic reinforcement learning in order to directly optimise\nnondifferentiable quality metrics of interest by formulating a pertoken\nadvantage and value computation strategy in this novel reinforcement learning\nbased captioning model we show that it is possible to achieve the state of the\nart performance on the widely used mscoco benchmark\n\n    ],"['##entiable quality' '##s' '\\ nappro' '\\ nbased captioning model'
 'actorcritic reinforcement learning' 'groundtruth \\ nannotated caption'
 'image captioning \\ nmethods' 'image captioning methods'
 'language quality metrics' 'mscoco benchmark' 'reinforcement learning'
 'training' 'users' 'value computation strategy'
 'visualintelligence driven ai agent']"
5,{'ObjectId': '639d9f0f6d67832e75895c4b'},arxiv,['Sample Efficient Actor-Critic with Experience Replay'],"['Ziyu Wang', 'Victor Bapst', 'Nicolas Heess', 'Volodymyr Mnih', 'Remi Munos', 'Koray Kavukcuoglu', 'Nando de Freitas']","['\n      ', '  This paper presents an actor-critic deep reinforcement learning agent with\nexperience replay that is stable, sample efficient, and performs remarkably\nwell on challenging environments, including the discrete 57-game Atari domain\nand several continuous control problems. To achieve this, the paper introduces\nseveral innovations, including truncated importance sampling with bias\ncorrection, stochastic dueling network architectures, and a new trust region\npolicy optimization method.\n\n    ']",['Machine Learning (cs.LG)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 786'],"['/scholar?cites=8369222693188103740&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         this paper presents an actorcritic deep reinforcement learning agent with\nexperience replay that is stable sample efficient and performs remarkably\nwell on challenging environments including the discrete game atari domain\nand several continuous control problems to achieve this the paper introduces\nseveral innovations including truncated importance sampling with bias\ncorrection stochastic dueling network architectures and a new trust region\npolicy optimization method\n\n    ],"['##olicy' '\\ nexperience replay' 'actorcritic'
 'bias \\ ncorrection stochastic dueling network'
 'continuous control problems' 'deep reinforcement learning'
 'discrete game' 'truncated importance sampling']"
6,{'ObjectId': '639d9f0f6d67832e75895c4c'},arxiv,['Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)'],"['Zhimin Hou', 'Kuangen Zhang', 'Yi Wan', 'Dongyu Li', 'Chenglong Fu', 'Haoyong Yu']","['\n      ', '  The optimal policy of a reinforcement learning problem is often discontinuous\nand non-smooth. I.e., for two states with similar representations, their\noptimal policies can be significantly different. In this case, representing the\nentire policy with a function approximator (FA) with shared parameters for all\nstates maybe not desirable, as the generalization ability of parameters sharing\nmakes representing discontinuous, non-smooth policies difficult. A common way\nto solve this problem, known as Mixture-of-Experts, is to represent the policy\nas the weighted sum of multiple components, where different components perform\nwell on different parts of the state space. Following this idea and inspired by\na recent work called advantage-weighted information maximization, we propose to\nlearn for each state weights of these components, so that they entail the\ninformation of the state itself and also the preferred action learned so far\nfor the state. The action preference is characterized via the advantage\nfunction. In this case, the weight of each component would only be large for\ncertain groups of states whose representations are similar and preferred action\nrepresentations are also similar. Therefore each component is easy to be\nrepresented. We call a policy parameterized in this way an Advantage Weighted\nMixture Policy (AWMP) and apply this idea to improve soft-actor-critic (SAC),\none of the most competitive continuous control algorithm. Experimental results\ndemonstrate that SAC with AWMP clearly outperforms SAC in four commonly used\ncontinuous control tasks and achieve stable performance across different random\nseeds.\n\n    ']",['Machine Learning (cs.LG)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 10'],"['/scholar?cites=9691472150620828666&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         the optimal policy of a reinforcement learning problem is often discontinuous\nand nonsmooth ie for two states with similar representations their\noptimal policies can be significantly different in this case representing the\nentire policy with a function approximator fa with shared parameters for all\nstates maybe not desirable as the generalization ability of parameters sharing\nmakes representing discontinuous nonsmooth policies difficult a common way\nto solve this problem known as mixtureofexperts is to represent the policy\nas the weighted sum of multiple components where different components perform\nwell on different parts of the state space following this idea and inspired by\na recent work called advantageweighted information maximization we propose to\nlearn for each state weights of these components so that they entail the\ninformation of the state itself and also the preferred action learned so far\nfor the state the action preference is characterized via the advantage\nfunction in this case the weight of each component would only be large for\ncertain groups of states whose representations are similar and preferred action\nrepresentations are also similar therefore each component is easy to be\nrepresented we call a policy parameterized in this way an advantage weighted\nmixture policy awmp and apply this idea to improve softactorcritic sac\none of the most competitive continuous control algorithm experimental results\ndemonstrate that sac with awmp clearly outperforms sac in four commonly used\ncontinuous control tasks and achieve stable performance across different random\nseeds\n\n    ],"['##actorcritic' '##ontin' '##smo' '##smooth policies' '\\' '\\ nand'
 '\\ ncertain groups' '\\ nentire policy' '\\ nlearn' '\\ nmakes'
 '\\ nstates' 'advantageweighted information maximization' 'control'
 'control tasks' 'experimental results' 'function' 'mixtureofexperts'
 'nmixture policy' 'reinforcement learning' 'weighted \\' 'weighted sum']"
7,{'ObjectId': '639d9f0f6d67832e75895c4d'},arxiv,['Asynchronous Advantage Actor-Critic Agent for Starcraft II'],"['Basel Alghanem', 'Keerthana P G']","['\n      ', '  Deep reinforcement learning, and especially the Asynchronous Advantage\nActor-Critic algorithm, has been successfully used to achieve super-human\nperformance in a variety of video games. Starcraft II is a new challenge for\nthe reinforcement learning community with the release of pysc2 learning\nenvironment proposed by Google Deepmind and Blizzard Entertainment. Despite\nbeing a target for several AI developers, few have achieved human level\nperformance. In this project we explain the complexities of this environment\nand discuss the results from our experiments on the environment. We have\ncompared various architectures and have proved that transfer learning can be an\neffective paradigm in reinforcement learning research for complex scenarios\nrequiring skill transfer.\n\n    ']",['Artificial Intelligence (cs.AI)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 4'],"['/scholar?cites=11859479095225713207&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         deep reinforcement learning and especially the asynchronous advantage\nactorcritic algorithm has been successfully used to achieve superhuman\nperformance in a variety of video games starcraft ii is a new challenge for\nthe reinforcement learning community with the release of pysc learning\nenvironment proposed by google deepmind and blizzard entertainment despite\nbeing a target for several ai developers few have achieved human level\nperformance in this project we explain the complexities of this environment\nand discuss the results from our experiments on the environment we have\ncompared various architectures and have proved that transfer learning can be an\neffective paradigm in reinforcement learning research for complex scenarios\nrequiring skill transfer\n\n    ],"['##nvironment' '\\' '\\ nper' '\\ nthe reinforcement learning'
 'blizzard entertainment' 'deep' 'google deepmind'
 'nactorcritic algorithm' 'pysc learning' 'reinforcement learning'
 'starcraft' 'transfer learning' 'video games']"
8,{'ObjectId': '639d9f0f6d67832e75895c4e'},arxiv,['An advantage actor-critic algorithm for robotic motion planning in dense and dynamic scenarios'],"['Chengmin Zhou', 'Bingding Huang', 'Pasi Fränti']","['\n      ', '  Intelligent robots provide a new insight into efficiency improvement in\nindustrial and service scenarios to replace human labor. However, these\nscenarios include dense and dynamic obstacles that make motion planning of\nrobots challenging. Traditional algorithms like A* can plan collision-free\ntrajectories in static environment, but their performance degrades and\ncomputational cost increases steeply in dense and dynamic scenarios.\nOptimal-value reinforcement learning algorithms (RL) can address these problems\nbut suffer slow speed and instability in network convergence. Network of policy\ngradient RL converge fast in Atari games where action is discrete and finite,\nbut few works have been done to address problems where continuous actions and\nlarge action space are required. In this paper, we modify existing advantage\nactor-critic algorithm and suit it to complex motion planning, therefore\noptimal speeds and directions of robot are generated. Experimental results\ndemonstrate that our algorithm converges faster and stable than optimal-value\nRL. It achieves higher success rate in motion planning with lesser processing\ntime for robot to reach its goal.\n\n    ']",['Robotics (cs.RO)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 2'],"['/scholar?cites=16945176442469715803&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         intelligent robots provide a new insight into efficiency improvement in\nindustrial and service scenarios to replace human labor however these\nscenarios include dense and dynamic obstacles that make motion planning of\nrobots challenging traditional algorithms like a can plan collisionfree\ntrajectories in static environment but their performance degrades and\ncomputational cost increases steeply in dense and dynamic scenarios\noptimalvalue reinforcement learning algorithms rl can address these problems\nbut suffer slow speed and instability in network convergence network of policy\ngradient rl converge fast in atari games where action is discrete and finite\nbut few works have been done to address problems where continuous actions and\nlarge action space are required in this paper we modify existing advantage\nactorcritic algorithm and suit it to complex motion planning therefore\noptimal speeds and directions of robot are generated experimental results\ndemonstrate that our algorithm converges faster and stable than optimalvalue\nrl it achieves higher success rate in motion planning with lesser processing\ntime for robot to reach its goal\n\n    ],"['\\ ncomputational cost' '\\ ntrajectories' 'collisionfree'
 'complex motion planning' 'human labor' 'motion planning'
 'nactorcritic algorithm' 'ndemonstrate' 'network convergence network'
 'ngradient' 'noptimal' 'noptimalvalue reinforcement learning algorithms'
 'policy' 'service scenarios' 'success']"
9,{'ObjectId': '639d9f106d67832e75895c4f'},arxiv,['Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space'],"['Zhou Fan', 'Rui Su', 'Weinan Zhang', 'Yong Yu']","['\n      ', '  In this paper we propose a hybrid architecture of actor-critic algorithms for\nreinforcement learning in parameterized action space, which consists of\nmultiple parallel sub-actor networks to decompose the structured action space\ninto simpler action spaces along with a critic network to guide the training of\nall sub-actor networks. While this paper is mainly focused on parameterized\naction space, the proposed architecture, which we call hybrid actor-critic, can\nbe extended for more general action spaces which has a hierarchical structure.\nWe present an instance of the hybrid actor-critic architecture based on\nproximal policy optimization (PPO), which we refer to as hybrid proximal policy\noptimization (H-PPO). Our experiments test H-PPO on a collection of tasks with\nparameterized action space, where H-PPO demonstrates superior performance over\nprevious methods of parameterized action reinforcement learning.\n\n    ']",['Machine Learning (cs.LG)'],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=,['Citado por 47'],"['/scholar?cites=18098031806703639264&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[\n         in this paper we propose a hybrid architecture of actorcritic algorithms for\nreinforcement learning in parameterized action space which consists of\nmultiple parallel subactor networks to decompose the structured action space\ninto simpler action spaces along with a critic network to guide the training of\nall subactor networks while this paper is mainly focused on parameterized\naction space the proposed architecture which we call hybrid actorcritic can\nbe extended for more general action spaces which has a hierarchical structure\nwe present an instance of the hybrid actorcritic architecture based on\nproximal policy optimization ppo which we refer to as hybrid proximal policy\noptimization hppo our experiments test hppo on a collection of tasks with\nparameterized action space where hppo demonstrates superior performance over\nprevious methods of parameterized action reinforcement learning\n\n    ],"['##ev' '##oxima' '\\' '\\ nall subactor networks'
 '\\ nmultiple parallel subactor networks' 'actorcritic'
 'actorcritic algorithms' 'hybrid' 'hybrid actorcritic architecture'
 'hybrid proximal policy' 'nparameterized action space'
 'nreinforcement learning' 'parameterized action reinforcement learning'
 'parameterized action space' 'structured action space']"
10,{'ObjectId': '639d9f106d67832e75895c50'},springer,['Design and application of adaptive PID controller based on asynchronous advantage actor–critic learning method'],"['Qifeng Sun', 'Chengze Du', 'Hongqiang Li']","['To address the problems of the slow convergence and inefficiency in the existing adaptive PID controllers, we propose a new adaptive PID controller using the asynchronous advantage actor–critic (A3C) algorithm. Firstly, the controller can train the multiple agents of the actor–critic structures in parallel exploiting the multi-thread asynchronous learning characteristics of the A3C structure. Secondly, in order to achieve the best control effect, each agent uses a multilayer neural network to approach the strategy function and value function to search the best parameter-tuning strategy in continuous action space. The simulation results indicate that our proposed controller can achieve the fast convergence and strong adaptability compared with conventional controllers.']","['Reinforcement learning', 'Asynchronous advantage actor–critic', 'Adaptive PID control', 'Stepping motor']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 13'],"['/scholar?cites=7313712638896097429&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[to address the problems of the slow convergence and inefficiency in the existing adaptive pid controllers we propose a new adaptive pid controller using the asynchronous advantage actor–critic ac algorithm firstly the controller can train the multiple agents of the actor–critic structures in parallel exploiting the multithread asynchronous learning characteristics of the ac structure secondly in order to achieve the best control effect each agent uses a multilayer neural network to approach the strategy function and value function to search the best parametertuning strategy in continuous action space the simulation results indicate that our proposed controller can achieve the fast convergence and strong adaptability compared with conventional controllers],"['adaptive pid controller' 'adaptive pid controllers'
 'asynchronous advantage actor' 'continuous action space' 'control effect'
 'convergence' 'multilayer neural network' 'multithread' 'value function']"
11,{'ObjectId': '639d9f106d67832e75895c51'},springer,['Balance Control for the First-order Inverted Pendulum Based on the Advantage Actor-critic Algorithm'],"['Yan Zheng', 'Xutong Li', 'Long Xu']","['In this paper, a control algorithm based on Advantage Actor-Critic for the classical inverted pendulum system has been proposed. To enrich the observed states which are used to control, a CNN feature-based state is proposed. The direct control and the indirect control algorithms are introduced to address different control situations, such as the situation which only physical states like angle, velocity, etc. provided or the situation which only the indirect states provided like images, etc. A comparison experiment between the direct control and the indirect control algorithms based on the Advantage Actor-Critic has been evaluated. Besides, the comparison experiment with the Deep Q-Network algorithm has been performed. The experiment results show that the proposed method achieves comparable performance with the PID control algorithm and better than the Deep Q-Network based algorithm.']","['Actor critic', 'deep Q network(DQN)', 'inverted pendulum', 'PID', 'reinforcement learning']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 6'],"['/scholar?cites=4536347427942814130&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[in this paper a control algorithm based on advantage actorcritic for the classical inverted pendulum system has been proposed to enrich the observed states which are used to control a cnn featurebased state is proposed the direct control and the indirect control algorithms are introduced to address different control situations such as the situation which only physical states like angle velocity etc provided or the situation which only the indirect states provided like images etc a comparison experiment between the direct control and the indirect control algorithms based on the advantage actorcritic has been evaluated besides the comparison experiment with the deep qnetwork algorithm has been performed the experiment results show that the proposed method achieves comparable performance with the pid control algorithm and better than the deep qnetwork based algorithm],"['advantage actorcritic' 'angle velocity'
 'classical inverted pendulum system' 'cnn featurebased state'
 'deep qnetwork' 'deep qnetwork algorithm' 'pid control algorithm']"
12,{'ObjectId': '639d9f116d67832e75895c52'},springer,['Attention-based advantage actor-critic algorithm with prioritized experience replay for complex 2-D robotic motion planning'],"['Chengmin Zhou', 'Bingding Huang', 'Pasi Fränti']","['Robotic motion planning in dense and dynamic indoor scenarios constantly challenges the researchers because of the motion unpredictability of obstacles. Recent progress in reinforcement learning enables robots to better cope with the dense and unpredictable obstacles by encoding complex features of the robot and obstacles into the encoders like the ', ' (LSTM). Then these features are learned by the robot using reinforcement learning algorithms, such as the deep Q network and asynchronous advantage actor critic algorithm. However, existing methods depend heavily on expert experiences to enhance the convergence speed of the networks by initializing them via imitation learning. Moreover, those approaches based on LSTM to encode the obstacle features are not always efficient and robust enough, therefore sometimes causing the network overfitting in training. This paper focuses on the advantage actor critic algorithm and introduces an ', ' to improve the performance of existing algorithm from two perspectives. First, LSTM encoder is replaced by a robust encoder ', ' to better interpret the complex features of the robot and obstacles. Second, the robot learns from its past prioritized experiences to initialize the networks of the advantage actor-critic algorithm. This is achieved by applying the ', ' method, which makes the best of past useful experiences to improve the convergence speed. As results, the network based on our algorithm takes only around 15% and 30% experiences to get rid of the early-stage training without the expert experiences in cases with five and ten obstacles, respectively. Then it converges faster to a better reward with less experiences (near 45% and 65% of experiences in cases with ten and five obstacles respectively) when comparing with the baseline LSTM-based advantage actor critic algorithm. Our source code is freely available at the GitHub (', ').']","['Motion planning', 'Path planning', 'Reinforcement learning', 'Intelligent robot', 'Deep learning']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,[],[],[robotic motion planning in dense and dynamic indoor scenarios constantly challenges the researchers because of the motion unpredictability of obstacles recent progress in reinforcement learning enables robots to better cope with the dense and unpredictable obstacles by encoding complex features of the robot and obstacles into the encoders like the   lstm then these features are learned by the robot using reinforcement learning algorithms such as the deep q network and asynchronous advantage actor critic algorithm however existing methods depend heavily on expert experiences to enhance the convergence speed of the networks by initializing them via imitation learning moreover those approaches based on lstm to encode the obstacle features are not always efficient and robust enough therefore sometimes causing the network overfitting in training this paper focuses on the advantage actor critic algorithm and introduces an   to improve the performance of existing algorithm from two perspectives first lstm encoder is replaced by a robust encoder   to better interpret the complex features of the robot and obstacles second the robot learns from its past prioritized experiences to initialize the networks of the advantage actorcritic algorithm this is achieved by applying the   method which makes the best of past useful experiences to improve the convergence speed as results the network based on our algorithm takes only around  and  experiences to get rid of the earlystage training without the expert experiences in cases with five and ten obstacles respectively then it converges faster to a better reward with less experiences near  and  of experiences in cases with ten and five obstacles respectively when comparing with the baseline lstmbased advantage actor critic algorithm our source code is freely available at the github  ],"['advantage actor critic algorithm'
 'asynchronous advantage actor critic algorithm' 'convergence speed'
 'deep q network' 'dynamic indoor scenarios' 'encoder'
 'expert experiences' 'github' 'lstm' 'motion unpredictability'
 'reinforcement learning' 'reinforcement learning algorithms'
 'robotic motion planning']"
13,{'ObjectId': '639d9f116d67832e75895c53'},springer,['Robustness Assessment of Asynchronous Advantage Actor-Critic Based on Dynamic Skewness and Sparseness Computation: A Parallel Computing View'],"['Tong Chen', 'Ji-Qiang Liu', 'Gang Li']","[""Reinforcement learning as autonomous learning is greatly driving artificial intelligence (AI) development to practical applications. Having demonstrated the potential to significantly improve synchronously parallel learning, the parallel computing based asynchronous advantage actor-critic (A3C) opens a new door for reinforcement learning. Unfortunately, the acceleration's inuence on A3C robustness has been largely overlooked. In this paper, we perform the first robustness assessment of A3C based on parallel computing. By perceiving the policy’s action, we construct a global matrix of action probability deviation and define two novel measures of skewness and sparseness to form an integral robustness measure. Based on such static assessment, we then develop a dynamic robustness assessing algorithm through situational whole-space state sampling of changing episodes. Extensive experiments with different combinations of agent number and learning rate are implemented on an A3C-based pathfinding application, demonstrating that our proposed robustness assessment can effectively measure the robustness of A3C, which can achieve an accuracy of 83.3%.""]","['robustness assessment', 'skewness', 'sparseness', 'asynchronous advantage actor-critic', 'reinforcement learning']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 2'],"['/scholar?cites=16266676036675648691&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']","[""reinforcement learning as autonomous learning is greatly driving artificial intelligence ai development to practical applications having demonstrated the potential to significantly improve synchronously parallel learning the parallel computing based asynchronous advantage actorcritic ac opens a new door for reinforcement learning unfortunately the accelerations inuence on ac robustness has been largely overlooked in this paper we perform the first robustness assessment of ac based on parallel computing by perceiving the policys action we construct a global matrix of action probability deviation and define two novel measures of skewness and sparseness to form an integral robustness measure based on such static assessment we then develop a dynamic robustness assessing algorithm through situational wholespace state sampling of changing episodes extensive experiments with different combinations of agent number and learning rate are implemented on an acbased pathfinding application demonstrating that our proposed robustness assessment can effectively measure the robustness of ac which can achieve an accuracy of ""]","['##ynchronous advantage actorcritic' 'ac' 'ac robustness' 'acbase'
 'action probability deviation' 'agent number' 'artificial intelligence'
 'autonomous learning' 'dynamic robustness' 'global matrix'
 'integral robustness measure' 'parallel computing' 'parallel learning'
 'reinforcement learning' 'robustness'
 'situational wholespace state sampling' 'skewness' 'sparseness'
 'static assessment']"
14,{'ObjectId': '639d9f116d67832e75895c54'},springer,['A Prioritized objective actor-critic method for deep reinforcement learning'],"['Ngoc Duy Nguyen', 'Thanh Thi Nguyen', 'Saeid Nahavandi']","['An increasing number of complex problems have naturally posed significant challenges in decision-making theory and reinforcement learning practices. These problems often involve multiple conflicting reward signals that inherently cause agents’ poor exploration in seeking a specific goal. In extreme cases, the agent gets stuck in a sub-optimal solution and starts behaving harmfully. To overcome such obstacles, we introduce two actor-critic deep reinforcement learning methods, namely ', ' (MCSP) and ', ' (SCMP), which can adjust agent behaviors to efficiently achieve a designated goal by adopting a weighted-sum scalarization of different objective functions. In particular, MCSP creates a human-centric policy that corresponds to a predefined priority weight of different objectives. Whereas, SCMP is capable of generating a mixed policy based on a set of priority weights, ', ', the generated policy uses the knowledge of different policies (each policy corresponds to a priority weight) to dynamically prioritize objectives in real time. We examine our methods by using the ', ' (A3C) algorithm to utilize the multithreading mechanism for dynamically balancing training intensity of different policies into a single network. Finally, simulation results show that MCSP and SCMP significantly outperform A3C with respect to the mean of total rewards in two complex problems: Food Collector and Seaquest.']","['Deep learning', 'Reinforcement learning', 'Learning systems', 'Multi-objective optimization', 'Actor-critic architecture']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 9'],"['/scholar?cites=5780159474260208219&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[an increasing number of complex problems have naturally posed significant challenges in decisionmaking theory and reinforcement learning practices these problems often involve multiple conflicting reward signals that inherently cause agents poor exploration in seeking a specific goal in extreme cases the agent gets stuck in a suboptimal solution and starts behaving harmfully to overcome such obstacles we introduce two actorcritic deep reinforcement learning methods namely   mcsp and   scmp which can adjust agent behaviors to efficiently achieve a designated goal by adopting a weightedsum scalarization of different objective functions in particular mcsp creates a humancentric policy that corresponds to a predefined priority weight of different objectives whereas scmp is capable of generating a mixed policy based on a set of priority weights   the generated policy uses the knowledge of different policies each policy corresponds to a priority weight to dynamically prioritize objectives in real time we examine our methods by using the   ac algorithm to utilize the multithreading mechanism for dynamically balancing training intensity of different policies into a single network finally simulation results show that mcsp and scmp significantly outperform ac with respect to the mean of total rewards in two complex problems: food collector and seaquest],"['##itic' 'conflicting reward signals' 'decisionmaking theory'
 'deep reinforcement learning methods'
 'dynamically balancing training intensity' 'food collector'
 'humancentric policy' 'mcsp' 'multithreading mechanism' 'priority weight'
 'priority weights' 'reinforcement learning practices' 'scmp' 'seaquest ]'
 'suboptimal solution']"
15,{'ObjectId': '639d9f116d67832e75895c55'},springer,['Deep Reinforcement Learning in VizDoom via DQN and Actor-Critic Agents'],"['Maria Bakhanova', 'Ilya Makarov']","['In this work, we study the problem of learning reinforcement learning-based agents in a first-person shooter environment VizDoom. We compare several well-known architectures, such as DQN, DDQN, A3C, and Curiosity-driven model, while highlighting the main differences in learned policies of agents trained via these models.\n']","['Deep reinforcement learning', 'DQN', 'A3C', 'A2C', 'VizDoom']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 6'],"['/scholar?cites=7478719658550434890&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[in this work we study the problem of learning reinforcement learningbased agents in a firstperson shooter environment vizdoom we compare several wellknown architectures such as dqn ddqn ac and curiositydriven model while highlighting the main differences in learned policies of agents trained via these models\n],"['##based agents' 'curiositydriven model' 'dqn ddqn'
 'firstperson shooter environment' 'reinforcement learning' 'vizdoom'
 'wellknown architectures']"
16,{'ObjectId': '639d9f116d67832e75895c56'},springer,['Application of the asynchronous advantage actor–critic machine learning algorithm to real-time accelerator tuning'],"['Yun Zou', 'Qing-Zi Xing', 'Xue-Wu Wang']","['This paper describes a real-time beam tuning method with an improved asynchronous advantage actor–critic (A3C) algorithm for accelerator systems. The operating parameters of devices are usually inconsistent with the predictions of physical designs because of errors in mechanical matching and installation. Therefore, parameter optimization methods such as pointwise scanning, evolutionary algorithms (EAs), and robust conjugate direction search are widely used in beam tuning to compensate for this inconsistency. However, it is difficult for them to deal with a large number of discrete local optima. The A3C algorithm, which has been applied in the automated control field, provides an approach for improving multi-dimensional optimization. The A3C algorithm is introduced and improved for the real-time beam tuning code for accelerators. Experiments in which optimization is achieved by using pointwise scanning, the genetic algorithm (one kind of EAs), and the A3C-algorithm are conducted and compared to optimize the currents of four steering magnets and two solenoids in the low-energy beam transport section (LEBT) of the Xi’an Proton Application Facility. Optimal currents are determined when the highest transmission of a radio frequency quadrupole (RFQ) accelerator downstream of the LEBT is achieved. The optimal work points of the tuned accelerator were obtained with currents of 0 A, 0 A, 0 A, and 0.1 A, for the four steering magnets, and 107 A and 96 A for the two solenoids. Furthermore, the highest transmission of the RFQ was 91.2%. Meanwhile, the lower time required for the optimization with the A3C algorithm was successfully verified. Optimization with the A3C algorithm consumed 42% and 78% less time than pointwise scanning with random initialization and pre-trained initialization of weights, respectively.']","['Real-time beam tuning', 'Parameter optimization', 'Asynchronous advantage actor–critic algorithm', 'Low-energy beam transport']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 5'],"['/scholar?cites=51620255185177627&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[this paper describes a realtime beam tuning method with an improved asynchronous advantage actor–critic ac algorithm for accelerator systems the operating parameters of devices are usually inconsistent with the predictions of physical designs because of errors in mechanical matching and installation therefore parameter optimization methods such as pointwise scanning evolutionary algorithms eas and robust conjugate direction search are widely used in beam tuning to compensate for this inconsistency however it is difficult for them to deal with a large number of discrete local optima the ac algorithm which has been applied in the automated control field provides an approach for improving multidimensional optimization the ac algorithm is introduced and improved for the realtime beam tuning code for accelerators experiments in which optimization is achieved by using pointwise scanning the genetic algorithm one kind of eas and the acalgorithm are conducted and compared to optimize the currents of four steering magnets and two solenoids in the lowenergy beam transport section lebt of the xian proton application facility optimal currents are determined when the highest transmission of a radio frequency quadrupole rfq accelerator downstream of the lebt is achieved the optimal work points of the tuned accelerator were obtained with currents of  a  a  a and  a for the four steering magnets and  a and  a for the two solenoids furthermore the highest transmission of the rfq was  meanwhile the lower time required for the optimization with the ac algorithm was successfully verified optimization with the ac algorithm consumed  and  less time than pointwise scanning with random initialization and pretrained initialization of weights respectively],"['ac algorithm' 'acalgorithm' 'accelerator systems'
 'asynchronous advantage' 'beam tuning' 'conjugate direction search'
 'discrete local optima' 'genetic algorithm' 'inconsistency'
 'lowenergy beam transport' 'mechanical matching'
 'multidimensional optimization' 'operating parameters'
 'pointwise scanning' 'pointwise scanning evolutionary algorithms'
 'pretrained initialization'
 'proton application facility optimal currents'
 'radio frequency quadrupole rfq accelerator' 'random initialization'
 'realtime beam tuning' 'realtime beam tuning method' 'robust' 'solenoids'
 'xian']"
17,{'ObjectId': '639d9f116d67832e75895c57'},springer,['Natural Actor-Critic'],"['Jan Peters', 'Sethu Vijayakumar', 'Stefan Schaal']","['This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.']","['Reinforcement Learning', 'Fisher Information Matrix', 'Natural Gradient', 'Imitation Learning', 'Motor Primitive']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 290'],"['/scholar?cites=9477490257951164486&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[this paper investigates a novel modelfree reinforcement learning architecture the natural actorcritic the actor updates are based on stochastic policy gradients employing amaris natural gradient approach while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression we show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation and can be estimated more efficiently than regular policy gradients the critic makes use of a special basis function parameterization motivated by the policygradient compatible function approximation we show that several wellknown reinforcement learning methods such as the original actorcritic and bradtkes linear quadratic qlearning are in fact natural actorcritic algorithms empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods and also demonstrate their applicability for learning control on an anthropomorphic robot arm],"['##ient compatible' '##now' 'actor improvements' 'actorcritic algorithms'
 'anthropomorphic robot arm' 'basis function parameterization'
 'bradtkes linear quadratic qlearning' 'coordinate frame'
 'linear regression' 'modelfree reinforcement learning architecture'
 'natural actorcritic' 'natural gradient approach'
 'natural policy gradient' 'natural policy gradients'
 'reinforcement learning methods' 'stochastic policy gradients'
 'value function']"
18,{'ObjectId': '639d9f126d67832e75895c58'},springer,['Optimal fractional-order PID controller based on fractional-order actor-critic algorithm'],"['Raafat Shalaby', 'Mohammad El-Hossainy', 'Tarek A. Mahmoud']","['In this paper, an online optimization approach of a fractional-order PID controller based on a fractional-order actor-critic algorithm (FOPID-FOAC) is proposed. The proposed FOPID-FOAC scheme exploits the advantages of the FOPID controller and FOAC approaches to improve the performance of nonlinear systems. The proposed FOAC is built by developing a FO-based learning approach for the actor-critic neural network with adaptive learning rates. Moreover, a FO rectified linear unit (RLU) is introduced to enable the AC neural network to define and optimize its own activation function. By the means of the Lyapunov theorem, the convergence and the stability analysis of the proposed algorithm are investigated. The FO operators for the FOAC learning algorithm are obtained using the gray wolf optimization (GWO) algorithm. The effectiveness of the proposed approach is proven by extensive simulations based on the tracking problem of the two degrees of freedom (2-DOF) helicopter system and the stabilization issue of the inverted pendulum (IP) system. Moreover, the performance of the proposed algorithm is compared against optimized FOPID control approaches in different system conditions, namely when the system is subjected to parameter uncertainties and external disturbances. The performance comparison is conducted in terms of two types of performance indices, the error performance indices, and the time response performance indices. The first one includes the integral absolute error (IAE), and the integral squared error (ISE), whereas the second type involves the rising time, the maximum overshoot (Max. OS), and the settling time. The simulation results explicitly indicate the high effectiveness of the proposed FOPID-FOAC controller in terms of the two types of performance measurements under different scenarios compared with the other control algorithms.']","['Fractional-order PID controller', 'Reinforcement learning', 'Actor-critic algorithm', 'Gray wolf optimization', 'Lyapunov theorem']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 1'],"['/scholar?cites=4924380534535514723&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[in this paper an online optimization approach of a fractionalorder pid controller based on a fractionalorder actorcritic algorithm fopidfoac is proposed the proposed fopidfoac scheme exploits the advantages of the fopid controller and foac approaches to improve the performance of nonlinear systems the proposed foac is built by developing a fobased learning approach for the actorcritic neural network with adaptive learning rates moreover a fo rectified linear unit rlu is introduced to enable the ac neural network to define and optimize its own activation function by the means of the lyapunov theorem the convergence and the stability analysis of the proposed algorithm are investigated the fo operators for the foac learning algorithm are obtained using the gray wolf optimization gwo algorithm the effectiveness of the proposed approach is proven by extensive simulations based on the tracking problem of the two degrees of freedom dof helicopter system and the stabilization issue of the inverted pendulum ip system moreover the performance of the proposed algorithm is compared against optimized fopid control approaches in different system conditions namely when the system is subjected to parameter uncertainties and external disturbances the performance comparison is conducted in terms of two types of performance indices the error performance indices and the time response performance indices the first one includes the integral absolute error iae and the integral squared error ise whereas the second type involves the rising time the maximum overshoot max os and the settling time the simulation results explicitly indicate the high effectiveness of the proposed fopidfoac controller in terms of the two types of performance measurements under different scenarios compared with the other control algorithms],"['##f' 'ac neural network' 'activation function'
 'actorcritic neural network' 'adaptive learning rates'
 'error performance indices' 'external disturbances' 'f'
 'fo rectified linear unit' 'foac' 'foac learning'
 'fobased learning approach' 'fop' 'fopid control' 'fopidfoac'
 'fopidfoac scheme' 'fractionalorder actorcritic algorithm'
 'fractionalorder pid controller' 'g' 'gray wolf optimization'
 'helicopter system' 'integral absolute error iae'
 'inverted pendulum ip system' 'lyapunov theorem'
 'maximum overshoot max os' 'measurements' 'of freedom'
 'online optimization approach' 'opt' 'performance indices' 'scenarios'
 'simulation' 'squared error' 'stability analysis' 'systems'
 'time response performance indices' 'tracking problem']"
19,{'ObjectId': '639d9f126d67832e75895c59'},springer,"['Evaluate, explain, and explore the state more exactly: an improved Actor-Critic algorithm for complex environment']","['ZhongYi Zha', 'Bo Wang', 'XueSong Tang']","['This paper proposes an Advanced Actor-Critic algorithm, which is improved based on the conventional Actor-Critic algorithm, to train the agent to play the complex strategy game StarCraft II. A series of advanced features have been incorporated, including the distributional advantage estimation, information entropy-based uncertainty estimation, self-confidence-based exploration, and normal constraint-based update strategy.\n A case study including seven StarCraft II mini-games is investigated to identify the effectiveness of the proposed approach, where the famous A3C algorithm is adopted as the comparative baseline. The results verify the superiority of the improved algorithm in accuracy and training efficacy, in complex environment with high-dimensional and hybrid state and action space.\n']","['Deep reinforcement learning', 'Video game AI', 'Advanced actor-critic', 'Distributional advantage estimation', 'Normal constraint']",https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=,['Citado por 2'],"['/scholar?cites=2846085149721688035&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[this paper proposes an advanced actorcritic algorithm which is improved based on the conventional actorcritic algorithm to train the agent to play the complex strategy game starcraft ii a series of advanced features have been incorporated including the distributional advantage estimation information entropybased uncertainty estimation selfconfidencebased exploration and normal constraintbased update strategy\n a case study including seven starcraft ii minigames is investigated to identify the effectiveness of the proposed approach where the famous ac algorithm is adopted as the comparative baseline the results verify the superiority of the improved algorithm in accuracy and training efficacy in complex environment with highdimensional and hybrid state and action space\n],"['##craft' 'actorcritic algorithm' 'distributional advantage estimation'
 'highdimensional' 'hybrid state' 'information entropybased'
 'normal constraintbased update' 'selfconfidencebased exploration'
 'uncertainty estimation']"
20,{'ObjectId': '639d9f126d67832e75895c5a'},SSRN,['Cooperative Traffic Signal Control Through AÂ\xa0 Counterfactual Multi-Agent Deep Actor Critic Approach'],"['Xiang Song', 'Bin Zhou', 'Dongfang Ma']","['Signal control has been effective to alleviate urban traffic congestion. Massive related works about signal timing optimization have been proposed and led to many signal control methods and systems. In recent years, reinforcement learning (RL) algorithms have attracted the increasing attention of researchers in the area of signal control optimization, since they can learn the optimal timing policy themselves by analyzing changing patterns between the traffic condition and signal timing plans. Multi-intersection traffic signal control problems face more challenges than classical single intersection problems such as the dimensionality issue as the joint action space of multiple agents grows exponentially with the number of intersections. The existing state-of-art multi-agent reinforcement learning algorithms developed for other areas may not suit well with traffic signal control given the complex spatial-temporal nature of traffic flows. In this paper, we propose a novel multi-agent counterfactual actor-critic with scheduler (MACS) framework for multiple interactions. In this method, decentralized actors control the traffic signals and the centralized critic combines recurrent policies with feed-forward critics. Additionally, a scheduler module that exchanges information among agents helps the individual agent better understand the entire environment. The proposed method was evaluated using simulation experiments based on a real-world urban street network in Shenzhen, China. Results showed that the proposed method outperforms the classic model-based method and several existing RL-based methods according to different measures such as queue length, average delay time, and throughput. ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],[signal control has been effective to alleviate urban traffic congestion massive related works about signal timing optimization have been proposed and led to many signal control methods and systems in recent years reinforcement learning rl algorithms have attracted the increasing attention of researchers in the area of signal control optimization since they can learn the optimal timing policy themselves by analyzing changing patterns between the traffic condition and signal timing plans multiintersection traffic signal control problems face more challenges than classical single intersection problems such as the dimensionality issue as the joint action space of multiple agents grows exponentially with the number of intersections the existing stateofart multiagent reinforcement learning algorithms developed for other areas may not suit well with traffic signal control given the complex spatialtemporal nature of traffic flows in this paper we propose a novel multiagent counterfactual actorcritic with scheduler macs framework for multiple interactions in this method decentralized actors control the traffic signals and the centralized critic combines recurrent policies with feedforward critics additionally a scheduler module that exchanges information among agents helps the individual agent better understand the entire environment the proposed method was evaluated using simulation experiments based on a realworld urban street network in shenzhen china results showed that the proposed method outperforms the classic modelbased method and several existing rlbased methods according to different measures such as queue length average delay time and throughput ],"['[ signal control' 'complex spatialtempor' 'decentralized actors control'
 'feedforward' 'multiagent counterfactual actorcritic'
 'multiintersection traffic signal control problems'
 'optimal timing policy' 'queue length average delay time'
 'realworld urban street network' 'recurrent' 'reinforcement learning'
 'rl algorithms' 'scheduler macs framework' 'scheduler module'
 'shenzhen china' 'signal control methods' 'signal control optimization'
 'signal timing optimization' 'signal timing plans' 'simulation'
 'stateofart multiagent reinforcement learning algorithms' 'throughput'
 'traffic condition' 'traffic flows' 'traffic signal control'
 'urban traffic congestion']"
21,{'ObjectId': '639d9f126d67832e75895c5b'},SSRN,['Deep Reinforcement Learning - Masterclass 6 - Actor Critic Methods'],"['Eric Benhamou', 'David Saltiel']","[""Episode 6 of 24 lectures on Deep Reinforcement Learning, that is part of the Syllabus of Dauphine PSL's Master Programm IASD, this lecture presents Actor-Critic Algorithms and their implementations. ""]",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],"[""episode  of  lectures on deep reinforcement learning that is part of the syllabus of dauphine psls master programm iasd this lecture presents actorcritic algorithms and their implementations ""]","['actorcritic algorithms' 'dauphine' 'deep reinforcement learning'
 'psls master programm']"
22,{'ObjectId': '639d9f126d67832e75895c5c'},SSRN,['Efficient Multi-Agent Exploration with Mutual-Guided Actor-Critic'],"['Renlong Chen', 'Ying Tan']","['Multi-agent Reinforcement Learning (MARL) has drawn wide attention as a bunch of real-world complex scenes can be abstracted as Multi-Agent Systems. In order to solve the non-local training objective problem in shared reward environments, value-decomposition-based methods have been proposed. Most of them impose priori Individual-Global-Max (IGM) and value-decomposition constraints. Some attempts tune the value-decomposition constraints to achieve a better performance. However, IGM constraint, as the fundamental assumption of value-decomposition methods, is adopted in most value-decomposition methods, which may leads to poor exploration in certain situations. To deal with this problem, a novel algorithm called Mutual-guided Multi-agent Actor-Critic (MugAC) is proposed in this paper. MugAC imposes a joint-action pool, generated by individual action distribution, from which a joint-action is selected by the critic to interact with the environment and as a training objective of the actor. The training paradigm of MugAC provides an off-policy training for actor-critic, which leads to a higher sample efficiency than traditional actor-critic methods in MARL. We evaluate our method against the current state-of-the-art methods in StarCraft micromanagement, which is one of the most challenging and representative tasks. Experimental results show that MugAC outperforms other methods in various scenarios of widely adopted StarCraft Multi-Agent Challenge (SMAC). ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],[multiagent reinforcement learning marl has drawn wide attention as a bunch of realworld complex scenes can be abstracted as multiagent systems in order to solve the nonlocal training objective problem in shared reward environments valuedecompositionbased methods have been proposed most of them impose priori individualglobalmax igm and valuedecomposition constraints some attempts tune the valuedecomposition constraints to achieve a better performance however igm constraint as the fundamental assumption of valuedecomposition methods is adopted in most valuedecomposition methods which may leads to poor exploration in certain situations to deal with this problem a novel algorithm called mutualguided multiagent actorcritic mugac is proposed in this paper mugac imposes a jointaction pool generated by individual action distribution from which a jointaction is selected by the critic to interact with the environment and as a training objective of the actor the training paradigm of mugac provides an offpolicy training for actorcritic which leads to a higher sample efficiency than traditional actorcritic methods in marl we evaluate our method against the current stateoftheart methods in starcraft micromanagement which is one of the most challenging and representative tasks experimental results show that mugac outperforms other methods in various scenarios of widely adopted starcraft multiagent challenge smac ],"['##ecomposition' '##ecomposition constraints' 'actorcritic' 'igm'
 'igm constraint' 'individual action distribution' 'individualglobalmax'
 'jointaction pool' 'mugac' 'multiagent actorcritic mugac'
 'multiagent reinforcement learning' 'multiagent systems' 'mutualguided'
 'nonlocal training objective problem' 'offpolicy training'
 'realworld complex scenes' 'sample efficiency'
 'shared reward environments' 'starcraft micromanagement'
 'starcraft multiage' 'stateoftheart methods' 'training objective'
 'valuedecomposition constraints' 'valuedecompositionbased methods']"
23,{'ObjectId': '639d9f136d67832e75895c5d'},SSRN,['Stock Market Trading Agent Using On-Policy Reinforcement Learning Algorithms'],"['Shreyas Lele', 'Kavit Gangar', 'Harshal Daftary', 'Dewashish Dharkar']","['Stock market has been a complex system which has been difficult to predict for humans, thereby, making the trading decisions difficult to take. It will be useful for traders if there is a model agent which can learn the stock market trends and suggest trading decisions which in turn maximizes the profits. Inorder to develop this agent we have formulated the problem as a Markov Decision Process (MDP) and created a stock trading environment which serves as a platform for this agent to trade the stocks. In this paper, we introduce a Reinforcement Learning based approach to develop a trading agent which performs trading actions on the environment and learns according to the rewards in terms of profit or loss it receives. We have applied different On-policy Reinforcement Learning Algorithms such as Vanilla Policy Gradient (VPG), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) on the environment to obtain the profits while trading stocks for 3 companies viz. Apple, Microsoft and Nike. The performance of these algorithms in order to maximize the profits have been evaluated and the results and conclusions have been elaborated. ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,['Citado por 4'],"['/scholar?cites=14792290705101612990&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[stock market has been a complex system which has been difficult to predict for humans thereby making the trading decisions difficult to take it will be useful for traders if there is a model agent which can learn the stock market trends and suggest trading decisions which in turn maximizes the profits inorder to develop this agent we have formulated the problem as a markov decision process mdp and created a stock trading environment which serves as a platform for this agent to trade the stocks in this paper we introduce a reinforcement learning based approach to develop a trading agent which performs trading actions on the environment and learns according to the rewards in terms of profit or loss it receives we have applied different onpolicy reinforcement learning algorithms such as vanilla policy gradient vpg trust region policy optimization trpo and proximal policy optimization ppo on the environment to obtain the profits while trading stocks for  companies viz apple microsoft and nike the performance of these algorithms in order to maximize the profits have been evaluated and the results and conclusions have been elaborated ],"['##policy' '[ stock market' 'apple microsoft' 'decisions'
 'markov decision process mdp' 'model agent'
 'proximal policy optimization' 'reinforcement learning'
 'reinforcement learning based approach' 'stock market trends'
 'stock trading environment' 'trading agent' 'trading decisions'
 'trading stocks' 'turn maximize' 'vanilla policy gradient'
 'vpg trust region policy optimization']"
24,{'ObjectId': '639d9f136d67832e75895c5e'},SSRN,['Evaluation and Potential Improvements of a Deep Reinforcement Learning Model for Automated Stock Trading'],['Rainer Andreas Jager'],"['The basis of this analysis is a model presented at the ACM International Conference in New York on AI in Finance in October 2020. ', 'The authors claim that the introduced deep reinforcement learning ensemble model outperforms the Dow Jones Industrial Average Index, and the three individual algorithms that form the ensemble in terms of the risk-adjusted returns measured by the Sharpe ratio. Furthermore, it is claimed that the ensemble is more robust and reliable than the individual agents. ', 'We evaluate these claims for statistical significance. As some weaknesses of the model become evident, we suggest a work-around and show the results with the suggested alteration. Finally, we combine all the findings and present an alternative model. ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],[the basis of this analysis is a model presented at the acm international conference in new york on ai in finance in october   the authors claim that the introduced deep reinforcement learning ensemble model outperforms the dow jones industrial average index and the three individual algorithms that form the ensemble in terms of the riskadjusted returns measured by the sharpe ratio furthermore it is claimed that the ensemble is more robust and reliable than the individual agents  we evaluate these claims for statistical significance as some weaknesses of the model become evident we suggest a workaround and show the results with the suggested alteration finally we combine all the findings and present an alternative model ],"['acm international conference' 'ai'
 'deep reinforcement learning ensemble'
 'dow jones industrial average index' 'finance' 'new'
 'riskadjusted returns' 'sharpe ratio' 'statistical significance']"
25,{'ObjectId': '639d9f146d67832e75895c5f'},SSRN,['Fleet Planning Under Demand and Fuel Price Uncertainty'],"['Izaak  L. Geursen', 'Bruno F. Santos', 'Neil Yorke-Smith']","['Current state-of-the-art airline planning models face computational limitations, restricting the operational applicability to problems of representative sizes. This is particular the case when considering the uncertainty necessarily associated with the long-term plan of an aircraft fleet. Considering the growing interest in the application of machine learning techniques to operations research problems, this article investigates the applicability of these techniques for airline planning. Specifically, an Advantage Actorâ\x80\x93Critic (A2C) reinforcementÂ\xa0learning algorithm is developed for the airline fleet planning problem. The increased computational efficiencyÂ\xa0of using an A2C agent allows us to consider real size problems and account for highly-volatile uncertainty in demand and fuel price. The A2C algorithm is found to outperform a deterministic model and a deep Q-network algorithm. The relative performance of the A2C increases as more complexity is added to the problem. Further, the A2C algorithm can compute multi-stage fleet planning solution within few seconds. ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],[current stateoftheart airline planning models face computational limitations restricting the operational applicability to problems of representative sizes this is particular the case when considering the uncertainty necessarily associated with the longterm plan of an aircraft fleet considering the growing interest in the application of machine learning techniques to operations research problems this article investigates the applicability of these techniques for airline planning specifically an advantage actorâ\x\xcritic ac reinforcementâ\xalearning algorithm is developed for the airline fleet planning problem the increased computational efficiencyâ\xaof using an ac agent allows us to consider real size problems and account for highlyvolatile uncertainty in demand and fuel price the ac algorithm is found to outperform a deterministic model and a deep qnetwork algorithm the relative performance of the ac increases as more complexity is added to the problem further the ac algorithm can compute multistage fleet planning solution within few seconds ],"['##alearning algorithm' 'ac agent' 'airline fleet planning problem'
 'airline planning' 'airline planning models' 'deep qnetwork algorithm'
 'deterministic model' 'face computational limitations' 'fleet'
 'fuel price' 'highlyvolatile uncertainty' 'longterm plan'
 'machine learning' 'multistage fleet planning'
 'operational applicability' 'real size problems' 'representative sizes'
 'uncertainty']"
26,{'ObjectId': '639d9f146d67832e75895c60'},SSRN,['Resource Allocation Based on Deep Reinforcement Learning with High-Dimensional Matrix Diagram in Multi-Modal Optical Networks'],"['Zipiao Zhao', 'Yikai Liu', 'Yongli Zhao', 'Yajie Li', 'Sabidur Rahman', 'Dahai Han', 'Jie Zhang']","[""Optical networks have many multi-dimensional properties (e.g., nodes, links, wavelength, bandwidth, etc.), leading to the emerging research topic on multi-modal optical networks. In this study, we propose a representation method of multi-modal optical networks using high-dimensional matrix diagram. Our study designs an advantage actor critic (A2C) learning framework named deep reinforcement learning based routing and wavelength allocation (DRL-RWA). DRL-RWA learns the correct online routing and wavelength allocation policy by parameterizing the convolutional neural network (CNN) that can sense the state of complex optical networks. We further improve DRL-RWA and propose DRL-RWA with the multi-thread-based training mechanism (DRL-RWA-MT) algorithm using novel A2C and CNNâ\x80\x99s training mechanism. DRL-RWA-MT converts dynamic network resources and request routing to graphs by using the representation method of the high-dimensional matrix diagram. These graphs are input to the CNN for parsing the information in the graphs, and CNN's output is transmitted to A2C for learning the correct online RWA policy. The optimization goal of DRL-RWA-MT in each step of the request is to maximize the cumulative reward in the remaining training. Results with multiple topologies show that the multi-modal representation method can improve the learning efficiency, and the proposed ACRA algorithm can optimize the resource allocation. Compared to the baseline (Dijkstra shortest path and First Fit (DSP-FF)), DRL-RWA-MT can optimize resource allocation while effectively reducing blocking probability in the 9-node and NSFNET topologies. ""]",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],"[""optical networks have many multidimensional properties eg nodes links wavelength bandwidth etc leading to the emerging research topic on multimodal optical networks in this study we propose a representation method of multimodal optical networks using highdimensional matrix diagram our study designs an advantage actor critic ac learning framework named deep reinforcement learning based routing and wavelength allocation drlrwa drlrwa learns the correct online routing and wavelength allocation policy by parameterizing the convolutional neural network cnn that can sense the state of complex optical networks we further improve drlrwa and propose drlrwa with the multithreadbased training mechanism drlrwamt algorithm using novel ac and cnnâ\x\xs training mechanism drlrwamt converts dynamic network resources and request routing to graphs by using the representation method of the highdimensional matrix diagram these graphs are input to the cnn for parsing the information in the graphs and cnns output is transmitted to ac for learning the correct online rwa policy the optimization goal of drlrwamt in each step of the request is to maximize the cumulative reward in the remaining training results with multiple topologies show that the multimodal representation method can improve the learning efficiency and the proposed acra algorithm can optimize the resource allocation compared to the baseline dijkstra shortest path and first fit dspff drlrwamt can optimize resource allocation while effectively reducing blocking probability in the node and nsfnet topologies ""]","['##ens' '##jkstra' 'ac' 'advantage actor critic ac learning framework'
 'cnns' 'convolutional neural network cnn' 'correct online routing'
 'correct online rwa policy' 'deep reinforcement learning based routing'
 'dynamic network resources' 'highdimensional matrix diagram' 'multimo'
 'multimodal optical networks' 'multithreadbased' 'resource allocation'
 'training' 'wavelength allocation' 'wavelength allocation policy'
 'wavelength bandwidth']"
27,{'ObjectId': '639d9f146d67832e75895c61'},SSRN,['Empirical Demonstration of Stock Paper Trading for Financial Reinforcement Learning'],"['Xiao-Yang Liu', 'Ziyi Xia']","['Deep reinforcement learning (DRL) has shown great potential in financial tasks. However, existing works optimistically reported profitable results through backtesting, suffering the \\textit{look-ahead bias} and \\textit{overfitting} issues. Therefore, these promising research results do not necessarily lead to a good performance in real-world markets. In this paper, we provide the first empirical demonstration of the stock paper trading task using deep reinforcement learning. First, we employ a ``training-validation-trading"" pipeline where an agent always learns the latest market daily data; meanwhile, the pipeline helps avoid information leakage. Then, we show that the trained DRL agent outperforms a conventional machine learning method (random forest) and the Dow Jones Industrial Average (DJIA) index during stock paper trading. Our codes are available online at: \\url{https://github.com/AI4Finance-Foundation/FinRL-Meta} ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,[],[],"[deep reinforcement learning drl has shown great potential in financial tasks however existing works optimistically reported profitable results through backtesting suffering the \\textit{lookahead bias} and \\textit{overfitting} issues therefore these promising research results do not necessarily lead to a good performance in realworld markets in this paper we provide the first empirical demonstration of the stock paper trading task using deep reinforcement learning first we employ a ``trainingvalidationtrading"" pipeline where an agent always learns the latest market daily data; meanwhile the pipeline helps avoid information leakage then we show that the trained drl agent outperforms a conventional machine learning method random forest and the dow jones industrial average djia index during stock paper trading our codes are available online at: \\url{https:githubcomaifinancefoundationfinrlmeta} ]","['average' 'backtesting' 'deep reinforcement learning' 'djia index'
 'githubcomaifinancefoundationfinrlmeta' 'information leakage' 'jones'
 'lookahead bias' 'machine learning method' 'random forest'
 'realworld markets' 'research results' 'results' 'stock paper trading'
 'stock paper trading task' 'trainingvalidation']"
28,{'ObjectId': '639d9f146d67832e75895c62'},SSRN,['The Use of Deep Reinforcement Learning in Tactical Asset Allocation'],"['Musonda Katongo', 'Ritabrata Bhattacharyya']","['The Tactical Asset Allocation (TAA) problem is a problem to accurately capture short to medium term market trends and anomalies in order to allocate the assets in a portfolio so as to optimize its performance by increasing the risk adjusted returns. This project seeks to address the Tactical Asset Allocation problem by employing Deep Reinforcement Learning (DRL) Algorithms in a Machine Learning Environment as well as employing Neural Network Autoencoders for selection of portfolio assets. This paper presents the implementation of this proposed methodology applied to 30 stocks of the Dow Jones Industrial Average (DJIA). In (1), the Introduction to the project objectives is done with the Problem Description presented in (2). Part (3) presents the literature review of similar studies in the subject area. The methodology used for our implementation is presented in (4) whilst (5) and (6) presents the benchmark portfolios and the DRL portfolios development respectively. The evaluation of the performance of the models is presented in (7) and we present our conclusions and the future works in (8). ']",[],https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=,['Citado por 3'],"['/scholar?cites=7313009570388408153&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']",[the tactical asset allocation taa problem is a problem to accurately capture short to medium term market trends and anomalies in order to allocate the assets in a portfolio so as to optimize its performance by increasing the risk adjusted returns this project seeks to address the tactical asset allocation problem by employing deep reinforcement learning drl algorithms in a machine learning environment as well as employing neural network autoencoders for selection of portfolio assets this paper presents the implementation of this proposed methodology applied to  stocks of the dow jones industrial average djia in  the introduction to the project objectives is done with the problem description presented in  part  presents the literature review of similar studies in the subject area the methodology used for our implementation is presented in  whilst  and  presents the benchmark portfolios and the drl portfolios development respectively the evaluation of the performance of the models is presented in  and we present our conclusions and the future works in  ],"['benchmark portfolios' 'deep reinforcement learning'
 'dow jones industrial average djia' 'drl algorithms'
 'machine learning environment' 'medium term market trends'
 'neural network autoencoders' 'portfolio assets' 'risk adjusted returns'
 'taa problem' 'tactical asset allocation'
 'tactical asset allocation problem']"
29,{'ObjectId': '639d9f146d67832e75895c63'},cambridge,['Machine learning in mental health: a scoping review of methods and applications'],"['Adrian B. R. Shatte', 'Delyse M. Hutchinson', 'Samantha J. Teague']","['This paper aims to synthesise the literature on machine learning (ML) and big data applications for mental health, highlighting current research and applications in practice.', ""We employed a scoping review methodology to rapidly map the field of ML in mental health. Eight health and information technology research databases were searched for papers covering this domain. Articles were assessed by two reviewers, and data were extracted on the article's mental health application, ML technique, data type, and study results. Articles were then synthesised via narrative review."", ""Three hundred papers focusing on the application of ML to mental health were identified. Four main application domains emerged in the literature, including: (i) detection and diagnosis; (ii) prognosis, treatment and support; (iii) public health, and; (iv) research and clinical administration. The most common mental health conditions addressed included depression, schizophrenia, and Alzheimer's disease. ML techniques used included support vector machines, decision trees, neural networks, latent Dirichlet allocation, and clustering."", 'Overall, the application of ML to mental health has demonstrated a range of benefits across the areas of diagnosis, treatment and support, research, and clinical administration. With the majority of studies identified focusing on the detection and diagnosis of mental health conditions, it is evident that there is significant room for the application of ML to other areas of psychology and mental health. The challenges of using ML techniques are discussed, as well as opportunities to improve and advance the field.']","['Big data', 'health informatics', 'machine learning', 'mental health']",https://scholar.google.com/scholar?q=machine+learning+source%3Acambridge&oq=,['Citado por 388'],"['/scholar?cites=11270166376113115069&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']","[this paper aims to synthesise the literature on machine learning ml and big data applications for mental health highlighting current research and applications in practice ""we employed a scoping review methodology to rapidly map the field of ml in mental health eight health and information technology research databases were searched for papers covering this domain articles were assessed by two reviewers and data were extracted on the articles mental health application ml technique data type and study results articles were then synthesised via narrative review"" ""three hundred papers focusing on the application of ml to mental health were identified four main application domains emerged in the literature including: i detection and diagnosis; ii prognosis treatment and support; iii public health and; iv research and clinical administration the most common mental health conditions addressed included depression schizophrenia and alzheimers disease ml techniques used included support vector machines decision trees neural networks latent dirichlet allocation and clustering"" overall the application of ml to mental health has demonstrated a range of benefits across the areas of diagnosis treatment and support research and clinical administration with the majority of studies identified focusing on the detection and diagnosis of mental health conditions it is evident that there is significant room for the application of ml to other areas of psychology and mental health the challenges of using ml techniques are discussed as well as opportunities to improve and advance the field]","['alzheimers disease' 'articles mental health application' 'clustering'
 'decision trees' 'depression schizophrenia' 'latent dirichlet allocation'
 'machine learning' 'mental health' 'mental health conditions'
 'narrative review' 'neural networks' 'prognosis treatment' 'psychology'
 'public health' 'scoping review methodology' 'support vector machines']"
