{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71810a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yo\\.conda\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    SummarizationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,BigBirdPegasusForConditionalGeneration\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9e06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cleaner():\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        self.word_list = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer() \n",
    "    def __call__(self, text):\n",
    "        res = \"\"\n",
    "        words = re.sub( \"[()-/%\"\"”“?''’]\",\"\" ,re.sub(\"\\d+\", \"\", text.lower().replace(\".\",\"\").replace(\",\",\"\").replace(\"b'\",\"\") )).split(\" \")\n",
    "        palabras = [h for h in words  ] # if h not in self.word_list\n",
    "        frase = ' '.join(palabras)\n",
    "        return frase\n",
    "    def flat_clean(self, text):\n",
    "        res = \"\"\n",
    "        words = re.sub( \"[()-/%\"\"”“?''’]\",\"\" ,re.sub(\"\\d+\", \"\", text.lower().replace(\".\",\"\").replace(\",\",\"\").replace(\"b'\",\"\") )).split(\" \")\n",
    "        palabras = [self.stemmer.stem(h) for h in words if h not in self.word_list ]\n",
    "        frase = ' '.join(palabras)\n",
    "        return frase\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be4767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b85c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class summary(SummarizationPipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\", max_length=50),\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\"),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "        )\n",
    "        res = \"none data\"\n",
    "        if len(results) > 0 :\n",
    "            res = results[0].get(\"summary_text\") \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class keyword_extractor(TokenClassificationPipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(\"ml6team/keyphrase-extraction-distilbert-inspec\"),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\"ml6team/keyphrase-extraction-distilbert-inspec\"),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        results = super().postprocess(\n",
    "            model_outputs=model_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.SIMPLE,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f3b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inicio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaner cargado\n",
      "keyword_extractor cargado\n",
      "summary cargado\n",
      "primera ejecucion terminada\n",
      "segunda ejecucion terminada\n",
      "tercera ejecucion terminada\n",
      "cuarta ejecucion terminada\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"inicio\")\n",
    "    data = pd.read_csv(\"total.csv\", on_bad_lines='skip')\n",
    "    data = data.dropna()\n",
    "    data = data.sample(100)\n",
    "     \n",
    "    limpiador = cleaner()\n",
    "    print(\"cleaner cargado\")\n",
    "    extractor_keywords = keyword_extractor()\n",
    "    print(\"keyword_extractor cargado\")\n",
    "    resumidor = summary()\n",
    "    print(\"summary cargado\")\n",
    "    data[\"abstract_limpio\"] =  data[\"abstract\"].apply(lambda x: limpiador(x))\n",
    "    print(\"primera ejecucion terminada\")\n",
    "    data[\"reumen\"] =  data[\"abstract_limpio\"].apply(lambda x: resumidor(x)) \n",
    "    print(\"segunda ejecucion terminada\")\n",
    "    data[\"keywords\"] =  data[\"abstract_limpio\"].apply(lambda x: extractor_keywords(x))\n",
    "    print(\"tercera ejecucion terminada\")\n",
    "    data[\"reumen_limpio\"] =  data[\"reumen\"].apply(lambda x: limpiador(x)) \n",
    "    print(\"cuarta ejecucion terminada\")\n",
    "    data.to_csv(\"total_procesed.csv\",index=False)  \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad4377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a84a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377e654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
