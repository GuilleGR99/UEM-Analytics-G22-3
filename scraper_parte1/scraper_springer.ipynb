{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Fuente Springer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos bibliotecas\n",
    "import requests\n",
    "from lxml import html\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#macros, se podren pasar como argumentos en futuras versiones\n",
    "#hasta = '2021-12-31'\n",
    "#desde = '2021-01-01'\n",
    "area = 'advantage actor critic'\n",
    "fuente = 'springer'\n",
    "# numero de paginas a revisar (paginas de 10 elementos)\n",
    "num_de_paginas = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url original \n",
    "#https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&btnG="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "pagina = 'https://scholar.google.com/scholar?q='+area.replace(\" \", \"+\")+'+source%3A'+fuente+'&oq='\n",
    "headers = requests.utils.default_headers()\n",
    "\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'Chrome/96.0.4664.93',\n",
    "    }\n",
    ")\n",
    "\n",
    "req =  requests.get(pagina, headers=headers)\n",
    "if req.status_code == 429:\n",
    "    print('wait' +req.headers.get('Retry-After'))\n",
    "    time.sleep(int(req.headers.get('Retry-After'))+1)\n",
    "    req =  requests.get(pagina, headers=headers)\n",
    "print(req)\n",
    "\n",
    "#Convertimos a tree \n",
    "arbol_html = html.fromstring(req.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empiezo vuelta 0 :16:08:25\n",
      "Empiezo vuelta 1 :16:08:41\n",
      "Empiezo vuelta 2 :16:08:56\n",
      "Empiezo vuelta 3 :16:09:12\n"
     ]
    }
   ],
   "source": [
    "# start=10 \n",
    "\n",
    "\n",
    "papers = []\n",
    "\n",
    "\n",
    "for i in range (num_de_paginas):\n",
    "    print('Empiezo vuelta '+str(i)+' :'+time.strftime(\"%H:%M:%S\", time.gmtime()))\n",
    "    req =  requests.get(pagina, headers=headers)\n",
    "    if req.status_code == 429:\n",
    "        print('wait' +req.headers.get('Retry-After'))\n",
    "        time.sleep(int(req.headers.get('Retry-After'))+1)\n",
    "        req =  requests.get(pagina, headers=headers)\n",
    "    #Convertimos a tree \n",
    "    arbol_html = html.fromstring(req.content)\n",
    "    temp_res = arbol_html.xpath('//h3 [@class =\"gs_rt\"]/a/@href')\n",
    "    for i in range(len(temp_res)):\n",
    "        pag = temp_res[i]\n",
    "        time.sleep(random.randint(1,2)*0.5)\n",
    "        paper_res =  requests.get(pag, headers=headers)\n",
    "        paper_dic = {}\n",
    "        if paper_res.status_code == 429:\n",
    "            print('wait' +paper_res.headers.get('Retry-After'))\n",
    "            time.sleep(int(paper_res.headers.get('Retry-After'))+1)\n",
    "            paper_res =  requests.get(pag, headers=headers)\n",
    "        paper_arbol_html = html.fromstring(paper_res.content)\n",
    "        # especifico de la pagina\n",
    "        paper_dic[\"fuente\"] = \"Springer\"\n",
    "        paper_dic[\"titulo\"] = paper_arbol_html.xpath('//h1 [@class =\"c-article-title\"]/text()')\n",
    "        paper_dic[\"autores\"] = paper_arbol_html.xpath('//li [@class =\"c-article-author-list__item\"]/a/text()')\n",
    "        paper_dic[\"abstract\"] = paper_arbol_html.xpath('//div [@class =\"c-article-section__content\"][@id=\"Abs1-content\"]/p/text()')\n",
    "        paper_dic[\"clase_primaria\"] = paper_arbol_html.xpath('//li [@class =\"c-article-subject-list__subject\"]/span/text()')\n",
    "        # paper_dic[\"otras_clases\"] = paper_arbol_html.xpath('//li [@class =\"c-article-subject-list__subject\"][position()>1]/span/text()')\n",
    "        #generico\n",
    "        paper_dic[\"n_citaciones\"] = arbol_html.xpath('//a [@href =\"'+pag+'\"]/parent::*/parent::*/div [@class = \"gs_fl\"]/a [contains(text(),\"Citado\")]/text()')\n",
    "        paper_dic[\"citado\"] = arbol_html.xpath('//a [@href =\"'+pag+'\"]/parent::*/parent::*/div [@class = \"gs_fl\"]/a [contains(text(),\"Citado\")]/@href')\n",
    "        papers.append(paper_dic)\n",
    "        pagina = 'https://scholar.google.com/scholar?start='+str(10*(i+1))+'&q='+area.replace(\" \", \"+\")+'+source%3A'+fuente+'&oq='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Design and application of adaptive PID controller based on asynchronous advantage actor–critic learning method']\n",
      "['Balance Control for the First-order Inverted Pendulum Based on the Advantage Actor-critic Algorithm']\n",
      "['Attention-based advantage actor-critic algorithm with prioritized experience replay for complex 2-D robotic motion planning']\n",
      "['Robustness Assessment of Asynchronous Advantage Actor-Critic Based on Dynamic Skewness and Sparseness Computation: A Parallel Computing View']\n",
      "['A Prioritized objective actor-critic method for deep reinforcement learning']\n",
      "['Deep Reinforcement Learning in VizDoom via DQN and Actor-Critic Agents']\n",
      "['Application of the asynchronous advantage actor–critic machine learning algorithm to real-time accelerator tuning']\n",
      "['Natural Actor-Critic']\n",
      "['Optimal fractional-order PID controller based on fractional-order actor-critic algorithm']\n",
      "['Evaluate, explain, and explore the state more exactly: an improved Actor-Critic algorithm for complex environment']\n",
      "['On Optimizing Operational Efficiency in Storage Systems via Deep Reinforcement Learning']\n",
      "['Availability-aware and energy-aware dynamic SFC placement using reinforcement learning']\n",
      "['Atari Games and Intel Processors']\n",
      "['Renewable prediction-driven service offloading for IoT-enabled energy systems with edge computing']\n",
      "['Editorial: Advance of simulations and techniques for communication networks and information systems']\n",
      "['A Reinforcement Learning Approach to\\xa0Inventory Management']\n",
      "['An improved transformer model with multi-head attention and attention to attention for low-carbon multi-depot vehicle routing problem']\n",
      "['Deep reinforcement learning based QoE-aware actor-learner architectures for video streaming in IoT environments']\n",
      "['Actor-critic learning-based energy optimization for UAV access and backhaul networks']\n",
      "['Policy-Approximation Based Deep Reinforcement Learning Techniques: An Overview']\n",
      "['On Optimizing Operational Efficiency in Storage Systems via Deep Reinforcement Learning']\n",
      "['Availability-aware and energy-aware dynamic SFC placement using reinforcement learning']\n",
      "['Atari Games and Intel Processors']\n",
      "['Renewable prediction-driven service offloading for IoT-enabled energy systems with edge computing']\n",
      "['Editorial: Advance of simulations and techniques for communication networks and information systems']\n",
      "['A Reinforcement Learning Approach to\\xa0Inventory Management']\n",
      "['An improved transformer model with multi-head attention and attention to attention for low-carbon multi-depot vehicle routing problem']\n",
      "['Deep reinforcement learning based QoE-aware actor-learner architectures for video streaming in IoT environments']\n",
      "['Actor-critic learning-based energy optimization for UAV access and backhaul networks']\n",
      "['Policy-Approximation Based Deep Reinforcement Learning Techniques: An Overview']\n",
      "['On Optimizing Operational Efficiency in Storage Systems via Deep Reinforcement Learning']\n",
      "['Availability-aware and energy-aware dynamic SFC placement using reinforcement learning']\n",
      "['Atari Games and Intel Processors']\n",
      "['Renewable prediction-driven service offloading for IoT-enabled energy systems with edge computing']\n",
      "['Editorial: Advance of simulations and techniques for communication networks and information systems']\n",
      "['A Reinforcement Learning Approach to\\xa0Inventory Management']\n",
      "['An improved transformer model with multi-head attention and attention to attention for low-carbon multi-depot vehicle routing problem']\n",
      "['Deep reinforcement learning based QoE-aware actor-learner architectures for video streaming in IoT environments']\n",
      "['Actor-critic learning-based energy optimization for UAV access and backhaul networks']\n",
      "['Policy-Approximation Based Deep Reinforcement Learning Techniques: An Overview']\n"
     ]
    }
   ],
   "source": [
    "for i in papers:\n",
    "    print(i[\"titulo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To address the problems of the slow convergence and inefficiency in the existing adaptive PID controllers, we propose a new adaptive PID controller using the asynchronous advantage actor–critic (A3C) algorithm. Firstly, the controller can train the multiple agents of the actor–critic structures in parallel exploiting the multi-thread asynchronous learning characteristics of the A3C structure. Secondly, in order to achieve the best control effect, each agent uses a multilayer neural network to approach the strategy function and value function to search the best parameter-tuning strategy in continuous action space. The simulation results indicate that our proposed controller can achieve the fast convergence and strong adaptability compared with conventional controllers.']\n",
      "['In this paper, a control algorithm based on Advantage Actor-Critic for the classical inverted pendulum system has been proposed. To enrich the observed states which are used to control, a CNN feature-based state is proposed. The direct control and the indirect control algorithms are introduced to address different control situations, such as the situation which only physical states like angle, velocity, etc. provided or the situation which only the indirect states provided like images, etc. A comparison experiment between the direct control and the indirect control algorithms based on the Advantage Actor-Critic has been evaluated. Besides, the comparison experiment with the Deep Q-Network algorithm has been performed. The experiment results show that the proposed method achieves comparable performance with the PID control algorithm and better than the Deep Q-Network based algorithm.']\n",
      "['Robotic motion planning in dense and dynamic indoor scenarios constantly challenges the researchers because of the motion unpredictability of obstacles. Recent progress in reinforcement learning enables robots to better cope with the dense and unpredictable obstacles by encoding complex features of the robot and obstacles into the encoders like the ', ' (LSTM). Then these features are learned by the robot using reinforcement learning algorithms, such as the deep Q network and asynchronous advantage actor critic algorithm. However, existing methods depend heavily on expert experiences to enhance the convergence speed of the networks by initializing them via imitation learning. Moreover, those approaches based on LSTM to encode the obstacle features are not always efficient and robust enough, therefore sometimes causing the network overfitting in training. This paper focuses on the advantage actor critic algorithm and introduces an ', ' to improve the performance of existing algorithm from two perspectives. First, LSTM encoder is replaced by a robust encoder ', ' to better interpret the complex features of the robot and obstacles. Second, the robot learns from its past prioritized experiences to initialize the networks of the advantage actor-critic algorithm. This is achieved by applying the ', ' method, which makes the best of past useful experiences to improve the convergence speed. As results, the network based on our algorithm takes only around 15% and 30% experiences to get rid of the early-stage training without the expert experiences in cases with five and ten obstacles, respectively. Then it converges faster to a better reward with less experiences (near 45% and 65% of experiences in cases with ten and five obstacles respectively) when comparing with the baseline LSTM-based advantage actor critic algorithm. Our source code is freely available at the GitHub (', ').']\n",
      "[\"Reinforcement learning as autonomous learning is greatly driving artificial intelligence (AI) development to practical applications. Having demonstrated the potential to significantly improve synchronously parallel learning, the parallel computing based asynchronous advantage actor-critic (A3C) opens a new door for reinforcement learning. Unfortunately, the acceleration's inuence on A3C robustness has been largely overlooked. In this paper, we perform the first robustness assessment of A3C based on parallel computing. By perceiving the policy’s action, we construct a global matrix of action probability deviation and define two novel measures of skewness and sparseness to form an integral robustness measure. Based on such static assessment, we then develop a dynamic robustness assessing algorithm through situational whole-space state sampling of changing episodes. Extensive experiments with different combinations of agent number and learning rate are implemented on an A3C-based pathfinding application, demonstrating that our proposed robustness assessment can effectively measure the robustness of A3C, which can achieve an accuracy of 83.3%.\"]\n",
      "['An increasing number of complex problems have naturally posed significant challenges in decision-making theory and reinforcement learning practices. These problems often involve multiple conflicting reward signals that inherently cause agents’ poor exploration in seeking a specific goal. In extreme cases, the agent gets stuck in a sub-optimal solution and starts behaving harmfully. To overcome such obstacles, we introduce two actor-critic deep reinforcement learning methods, namely ', ' (MCSP) and ', ' (SCMP), which can adjust agent behaviors to efficiently achieve a designated goal by adopting a weighted-sum scalarization of different objective functions. In particular, MCSP creates a human-centric policy that corresponds to a predefined priority weight of different objectives. Whereas, SCMP is capable of generating a mixed policy based on a set of priority weights, ', ', the generated policy uses the knowledge of different policies (each policy corresponds to a priority weight) to dynamically prioritize objectives in real time. We examine our methods by using the ', ' (A3C) algorithm to utilize the multithreading mechanism for dynamically balancing training intensity of different policies into a single network. Finally, simulation results show that MCSP and SCMP significantly outperform A3C with respect to the mean of total rewards in two complex problems: Food Collector and Seaquest.']\n",
      "['In this work, we study the problem of learning reinforcement learning-based agents in a first-person shooter environment VizDoom. We compare several well-known architectures, such as DQN, DDQN, A3C, and Curiosity-driven model, while highlighting the main differences in learned policies of agents trained via these models.\\n']\n",
      "['This paper describes a real-time beam tuning method with an improved asynchronous advantage actor–critic (A3C) algorithm for accelerator systems. The operating parameters of devices are usually inconsistent with the predictions of physical designs because of errors in mechanical matching and installation. Therefore, parameter optimization methods such as pointwise scanning, evolutionary algorithms (EAs), and robust conjugate direction search are widely used in beam tuning to compensate for this inconsistency. However, it is difficult for them to deal with a large number of discrete local optima. The A3C algorithm, which has been applied in the automated control field, provides an approach for improving multi-dimensional optimization. The A3C algorithm is introduced and improved for the real-time beam tuning code for accelerators. Experiments in which optimization is achieved by using pointwise scanning, the genetic algorithm (one kind of EAs), and the A3C-algorithm are conducted and compared to optimize the currents of four steering magnets and two solenoids in the low-energy beam transport section (LEBT) of the Xi’an Proton Application Facility. Optimal currents are determined when the highest transmission of a radio frequency quadrupole (RFQ) accelerator downstream of the LEBT is achieved. The optimal work points of the tuned accelerator were obtained with currents of 0 A, 0 A, 0 A, and 0.1 A, for the four steering magnets, and 107 A and 96 A for the two solenoids. Furthermore, the highest transmission of the RFQ was 91.2%. Meanwhile, the lower time required for the optimization with the A3C algorithm was successfully verified. Optimization with the A3C algorithm consumed 42% and 78% less time than pointwise scanning with random initialization and pre-trained initialization of weights, respectively.']\n",
      "['This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.']\n",
      "['In this paper, an online optimization approach of a fractional-order PID controller based on a fractional-order actor-critic algorithm (FOPID-FOAC) is proposed. The proposed FOPID-FOAC scheme exploits the advantages of the FOPID controller and FOAC approaches to improve the performance of nonlinear systems. The proposed FOAC is built by developing a FO-based learning approach for the actor-critic neural network with adaptive learning rates. Moreover, a FO rectified linear unit (RLU) is introduced to enable the AC neural network to define and optimize its own activation function. By the means of the Lyapunov theorem, the convergence and the stability analysis of the proposed algorithm are investigated. The FO operators for the FOAC learning algorithm are obtained using the gray wolf optimization (GWO) algorithm. The effectiveness of the proposed approach is proven by extensive simulations based on the tracking problem of the two degrees of freedom (2-DOF) helicopter system and the stabilization issue of the inverted pendulum (IP) system. Moreover, the performance of the proposed algorithm is compared against optimized FOPID control approaches in different system conditions, namely when the system is subjected to parameter uncertainties and external disturbances. The performance comparison is conducted in terms of two types of performance indices, the error performance indices, and the time response performance indices. The first one includes the integral absolute error (IAE), and the integral squared error (ISE), whereas the second type involves the rising time, the maximum overshoot (Max. OS), and the settling time. The simulation results explicitly indicate the high effectiveness of the proposed FOPID-FOAC controller in terms of the two types of performance measurements under different scenarios compared with the other control algorithms.']\n",
      "['This paper proposes an Advanced Actor-Critic algorithm, which is improved based on the conventional Actor-Critic algorithm, to train the agent to play the complex strategy game StarCraft II. A series of advanced features have been incorporated, including the distributional advantage estimation, information entropy-based uncertainty estimation, self-confidence-based exploration, and normal constraint-based update strategy.\\n A case study including seven StarCraft II mini-games is investigated to identify the effectiveness of the proposed approach, where the famous A3C algorithm is adopted as the comparative baseline. The results verify the superiority of the improved algorithm in accuracy and training efficacy, in complex environment with high-dimensional and hybrid state and action space.\\n']\n",
      "['This paper deals with the application of deep reinforcement learning to optimize the operational efficiency of a solid state storage rack. Specifically, we train an on-policy and model-free policy gradient algorithm called the Advantage Actor-Critic (A2C). We deploy a dueling deep network architecture to extract features from the sensor readings off the rack and devise a novel utility function that is used to control the A2C algorithm. Experiments show performance gains greater than 30% over the default policy for deterministic as well as random data workloads.']\n",
      "['Software-defined networking and network functions virtualisation are making networks programmable and consequently much more flexible and agile. To meet service-level agreements, achieve greater utilisation of legacy networks, faster service deployment, and reduce expenditure, telecommunications operators are deploying increasingly complex service function chains (SFCs). Notwithstanding the benefits of SFCs, increasing heterogeneity and dynamism from the cloud to the edge introduces significant SFC placement challenges, not least adding or removing network functions while maintaining availability, quality of service, and minimising cost. In this paper, an availability- and energy-aware solution based on reinforcement learning (RL) is proposed for dynamic SFC placement. Two policy-aware RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimisation (PPO), are compared using simulations of a ground truth network topology based on the Rede Nacional de Ensino e Pesquisa Network, Brazil’s National Teaching and Research Network backbone. The simulation results show that PPO generally outperformed A2C and a greedy approach in terms of both acceptance rate and energy consumption. The biggest difference in the PPO when compared to the other algorithms relates to the SFC availability requirement of 99.965%; the PPO algorithm median acceptance rate is 67.34% better than the A2C algorithm. A2C outperforms PPO only in the scenario where network servers had a greater number of computing resources. In this case, the A2C is 1% better than the PPO.']\n",
      "['The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions.', 'In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.']\n",
      "['The emerging of the Internet of Things (IoT) enables the interconnection among everything. With edge computing serving low-latency services, IoT makes intelligent energy management become a possibility, thereby enhancing the energy sustainability for energy systems. Currently, renewable energy is widely applied in energy systems to alleviate the carbon footprint. However, the instability and discontinuity of renewable generation decrease the quality of service (QoS) of edge servers. To address the challenge, a renewable prediction-driven service offloading method, named ReSome, is proposed. Technically, a deep-learning-based approach is designed for renewable energy prediction firstly. Next, the service offloading process is abstracted to a Markov decision process (MDP). With the predicted renewable energy amount, asynchronous advantage actor-critic (A3C) is leveraged to determine the optimal service offloading strategy. Finally, by utilizing a real-world solar power generation dataset, the experimental evaluation validates the capability and effectiveness of ReSome.']\n",
      "[]\n",
      "['This paper presents our approach for the control of a centralized distributed inventory management system using reinforcement learning (RL). We propose the application of policy-based reinforcement learning algorithms to tackle this problem in an effective manner. We have formulated the problem as a Markov decision process (MDP) and have created an environment that keeps track of multiple products across multiple warehouses returning a reward signal that directly corresponds to the total revenue across all warehouses at every time step. In this environment, we have applied various policy-based reinforcement learning algorithms such as Advantage Actor-Critic, Trust Region Policy Optimization and Proximal Policy Optimization to decide the amount of each product to be stocked in every warehouse. The performance of these algorithms in maximizing average revenue over time has been evaluated considering various statistical distributions from which we sample demand per time step per episode of training. We also compare these approaches to an existing approach involving a fixed replenishment scheme. In conclusion, we elaborate upon the results of our evaluation and the scope for future work on the topic.']\n",
      "['Low-carbon logistics is an emerging and sustainable development industry in the era of a low-carbon economy. The end-to-end deep reinforcement learning (DRL) method with an encoder-decoder framework has been proven effective for solving logistics problems. However, in most cases, the recurrent neural networks (RNN) and attention mechanisms are used in encoders and decoders, which may result in the long-distance dependence problem and the neglect of the correlation between query vectors. To surround this problem, we propose an improved transformer model (TAOA) with both multi-head attention mechanism (MHA) and attention to attention mechanism (AOA), and apply it to solve the low-carbon multi-depot vehicle routing problem (MDVRP). In this model, the MHA and AOA are implemented to solve the probability of route nodes in the encoder and decoder. The MHA is used to process different parts of the input sequence, which can be calculated in parallel, and the AOA is used to deal with the deficiency problem of correlation between query results and query vectors in the MHA. The actor-critic framework based on strategy gradient is constructed to train model parameters. The 2opt operator is further used to optimize the resulting routes. Finally, extensive numerical studies are carried out to verify the effectiveness and operation efficiency of the proposed TAOA, and the results show that the proposed TAOA performs better in solving the MDVRP than the traditional transformer model (Kools), genetic algorithm (GA), and Google OR-Tools (Ortools).']\n",
      "['The number of connected smart devices enabling multimedia applications has expanded tremendously in Internet-of-Things (IoT) environments. Specifically, the requirement for a high quality of experience (QoE) for video streaming services is a crucial prerequisite for a range of use cases, including smart surveillance, smart healthcare, smart agriculture and many more. However, providing a high QoE for video streaming is challenging due to underlying dynamic network conditions. To address this issue, several adaptive bit rate (ABR) algorithms based on predetermined rules have been developed. However, they do not generalize well to a wide variety of network conditions. ABR algorithms, based on reinforcement learning (RL), have been proven to be more effective at generalizing to varying network conditions but they still have limitations, specifically, constrained exploration and high variance in value estimates. In this paper, we propose asynchronous advantage actor-critic (A3C) based actor-learner architectures for generating the adaptive bit rates for video streaming in IoT environments. To address the existing issues, we propose integrating two advanced A3C algorithms: Follow then Forage Exploration (FFE) and Averaged A3C. We demonstrate their efficacy in improving the QoE over vanilla A3C. Additionally, we also demonstrate the benefits of the proposed architecture for video streaming under different network conditions and for different variants of the QoE metric. We show that advanced A3C methods provide up to 30.70% improvement in QoE over vanilla A3C and a considerably higher QoE over other fixed-rule-based ABR algorithms.']\n",
      "['In unmanned aerial vehicle (UAV)-assisted networks, UAV acts as an aerial base station which acquires the requested data via backhaul link and then serves ground users (GUs) through an access network. In this paper, we investigate an energy minimization problem with a limited power supply for both backhaul and access links. The difficulties for solving such a non-convex and combinatorial problem lie at the high computational complexity/time. In solution development, we consider the approaches from both actor-critic deep reinforcement learning (AC-DRL) and optimization perspectives. First, two offline non-learning algorithms, i.e., an optimal and a heuristic algorithms, based on piecewise linear approximation and relaxation are developed as benchmarks. Second, toward real-time decision-making, we improve the conventional AC-DRL and propose two learning schemes: AC-based user group scheduling and backhaul power allocation (ACGP), and joint AC-based user group scheduling and optimization-based backhaul power allocation (ACGOP). Numerical results show that the computation time of both ACGP and ACGOP is reduced tenfold to hundredfold compared to the offline approaches, and ACGOP is better than ACGP in energy savings. The results also verify the superiority of proposed learning solutions in terms of guaranteeing the feasibility and minimizing the system energy compared to the conventional AC-DRL.']\n",
      "['\\n\\nUntil recently, Deep Reinforcement Learning was restricted to innovations in games like Atari, Dota2. Despite surpassing the benchmarks established by their human counterparts in multiple games, these methods could not scale to real-life and industrial automation tasks. The main reason for this was the essential requirement of complex and continuous action control and sophisticated physics of the domain involved in these tasks. Because of these reasons, most of the incumbent solutions for such applications involved the invent of custom planning algorithms. The design of such sophisticated custom solutions required complete knowledge to the dynamics of the domain and its derivatives and hence were not scalable. Policy-based DRL has democratized this space, as now deep reinforcement learning agents could be trained to learn similar sophisticated policies just by learning from the data generated by interacting with these systems or their respective simulations. This has led to significant innovations in real-life and high-value control automation applications like autonomous vehicles, drones, and industrial robots. Therefore, in this paper, we present an overview of different types of policy-approximation based technique in Deep Reinforcement Learning that are the basis of many advanced control automation systems.']\n",
      "['This paper deals with the application of deep reinforcement learning to optimize the operational efficiency of a solid state storage rack. Specifically, we train an on-policy and model-free policy gradient algorithm called the Advantage Actor-Critic (A2C). We deploy a dueling deep network architecture to extract features from the sensor readings off the rack and devise a novel utility function that is used to control the A2C algorithm. Experiments show performance gains greater than 30% over the default policy for deterministic as well as random data workloads.']\n",
      "['Software-defined networking and network functions virtualisation are making networks programmable and consequently much more flexible and agile. To meet service-level agreements, achieve greater utilisation of legacy networks, faster service deployment, and reduce expenditure, telecommunications operators are deploying increasingly complex service function chains (SFCs). Notwithstanding the benefits of SFCs, increasing heterogeneity and dynamism from the cloud to the edge introduces significant SFC placement challenges, not least adding or removing network functions while maintaining availability, quality of service, and minimising cost. In this paper, an availability- and energy-aware solution based on reinforcement learning (RL) is proposed for dynamic SFC placement. Two policy-aware RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimisation (PPO), are compared using simulations of a ground truth network topology based on the Rede Nacional de Ensino e Pesquisa Network, Brazil’s National Teaching and Research Network backbone. The simulation results show that PPO generally outperformed A2C and a greedy approach in terms of both acceptance rate and energy consumption. The biggest difference in the PPO when compared to the other algorithms relates to the SFC availability requirement of 99.965%; the PPO algorithm median acceptance rate is 67.34% better than the A2C algorithm. A2C outperforms PPO only in the scenario where network servers had a greater number of computing resources. In this case, the A2C is 1% better than the PPO.']\n",
      "['The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions.', 'In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.']\n",
      "['The emerging of the Internet of Things (IoT) enables the interconnection among everything. With edge computing serving low-latency services, IoT makes intelligent energy management become a possibility, thereby enhancing the energy sustainability for energy systems. Currently, renewable energy is widely applied in energy systems to alleviate the carbon footprint. However, the instability and discontinuity of renewable generation decrease the quality of service (QoS) of edge servers. To address the challenge, a renewable prediction-driven service offloading method, named ReSome, is proposed. Technically, a deep-learning-based approach is designed for renewable energy prediction firstly. Next, the service offloading process is abstracted to a Markov decision process (MDP). With the predicted renewable energy amount, asynchronous advantage actor-critic (A3C) is leveraged to determine the optimal service offloading strategy. Finally, by utilizing a real-world solar power generation dataset, the experimental evaluation validates the capability and effectiveness of ReSome.']\n",
      "[]\n",
      "['This paper presents our approach for the control of a centralized distributed inventory management system using reinforcement learning (RL). We propose the application of policy-based reinforcement learning algorithms to tackle this problem in an effective manner. We have formulated the problem as a Markov decision process (MDP) and have created an environment that keeps track of multiple products across multiple warehouses returning a reward signal that directly corresponds to the total revenue across all warehouses at every time step. In this environment, we have applied various policy-based reinforcement learning algorithms such as Advantage Actor-Critic, Trust Region Policy Optimization and Proximal Policy Optimization to decide the amount of each product to be stocked in every warehouse. The performance of these algorithms in maximizing average revenue over time has been evaluated considering various statistical distributions from which we sample demand per time step per episode of training. We also compare these approaches to an existing approach involving a fixed replenishment scheme. In conclusion, we elaborate upon the results of our evaluation and the scope for future work on the topic.']\n",
      "['Low-carbon logistics is an emerging and sustainable development industry in the era of a low-carbon economy. The end-to-end deep reinforcement learning (DRL) method with an encoder-decoder framework has been proven effective for solving logistics problems. However, in most cases, the recurrent neural networks (RNN) and attention mechanisms are used in encoders and decoders, which may result in the long-distance dependence problem and the neglect of the correlation between query vectors. To surround this problem, we propose an improved transformer model (TAOA) with both multi-head attention mechanism (MHA) and attention to attention mechanism (AOA), and apply it to solve the low-carbon multi-depot vehicle routing problem (MDVRP). In this model, the MHA and AOA are implemented to solve the probability of route nodes in the encoder and decoder. The MHA is used to process different parts of the input sequence, which can be calculated in parallel, and the AOA is used to deal with the deficiency problem of correlation between query results and query vectors in the MHA. The actor-critic framework based on strategy gradient is constructed to train model parameters. The 2opt operator is further used to optimize the resulting routes. Finally, extensive numerical studies are carried out to verify the effectiveness and operation efficiency of the proposed TAOA, and the results show that the proposed TAOA performs better in solving the MDVRP than the traditional transformer model (Kools), genetic algorithm (GA), and Google OR-Tools (Ortools).']\n",
      "['The number of connected smart devices enabling multimedia applications has expanded tremendously in Internet-of-Things (IoT) environments. Specifically, the requirement for a high quality of experience (QoE) for video streaming services is a crucial prerequisite for a range of use cases, including smart surveillance, smart healthcare, smart agriculture and many more. However, providing a high QoE for video streaming is challenging due to underlying dynamic network conditions. To address this issue, several adaptive bit rate (ABR) algorithms based on predetermined rules have been developed. However, they do not generalize well to a wide variety of network conditions. ABR algorithms, based on reinforcement learning (RL), have been proven to be more effective at generalizing to varying network conditions but they still have limitations, specifically, constrained exploration and high variance in value estimates. In this paper, we propose asynchronous advantage actor-critic (A3C) based actor-learner architectures for generating the adaptive bit rates for video streaming in IoT environments. To address the existing issues, we propose integrating two advanced A3C algorithms: Follow then Forage Exploration (FFE) and Averaged A3C. We demonstrate their efficacy in improving the QoE over vanilla A3C. Additionally, we also demonstrate the benefits of the proposed architecture for video streaming under different network conditions and for different variants of the QoE metric. We show that advanced A3C methods provide up to 30.70% improvement in QoE over vanilla A3C and a considerably higher QoE over other fixed-rule-based ABR algorithms.']\n",
      "['In unmanned aerial vehicle (UAV)-assisted networks, UAV acts as an aerial base station which acquires the requested data via backhaul link and then serves ground users (GUs) through an access network. In this paper, we investigate an energy minimization problem with a limited power supply for both backhaul and access links. The difficulties for solving such a non-convex and combinatorial problem lie at the high computational complexity/time. In solution development, we consider the approaches from both actor-critic deep reinforcement learning (AC-DRL) and optimization perspectives. First, two offline non-learning algorithms, i.e., an optimal and a heuristic algorithms, based on piecewise linear approximation and relaxation are developed as benchmarks. Second, toward real-time decision-making, we improve the conventional AC-DRL and propose two learning schemes: AC-based user group scheduling and backhaul power allocation (ACGP), and joint AC-based user group scheduling and optimization-based backhaul power allocation (ACGOP). Numerical results show that the computation time of both ACGP and ACGOP is reduced tenfold to hundredfold compared to the offline approaches, and ACGOP is better than ACGP in energy savings. The results also verify the superiority of proposed learning solutions in terms of guaranteeing the feasibility and minimizing the system energy compared to the conventional AC-DRL.']\n",
      "['\\n\\nUntil recently, Deep Reinforcement Learning was restricted to innovations in games like Atari, Dota2. Despite surpassing the benchmarks established by their human counterparts in multiple games, these methods could not scale to real-life and industrial automation tasks. The main reason for this was the essential requirement of complex and continuous action control and sophisticated physics of the domain involved in these tasks. Because of these reasons, most of the incumbent solutions for such applications involved the invent of custom planning algorithms. The design of such sophisticated custom solutions required complete knowledge to the dynamics of the domain and its derivatives and hence were not scalable. Policy-based DRL has democratized this space, as now deep reinforcement learning agents could be trained to learn similar sophisticated policies just by learning from the data generated by interacting with these systems or their respective simulations. This has led to significant innovations in real-life and high-value control automation applications like autonomous vehicles, drones, and industrial robots. Therefore, in this paper, we present an overview of different types of policy-approximation based technique in Deep Reinforcement Learning that are the basis of many advanced control automation systems.']\n",
      "['This paper deals with the application of deep reinforcement learning to optimize the operational efficiency of a solid state storage rack. Specifically, we train an on-policy and model-free policy gradient algorithm called the Advantage Actor-Critic (A2C). We deploy a dueling deep network architecture to extract features from the sensor readings off the rack and devise a novel utility function that is used to control the A2C algorithm. Experiments show performance gains greater than 30% over the default policy for deterministic as well as random data workloads.']\n",
      "['Software-defined networking and network functions virtualisation are making networks programmable and consequently much more flexible and agile. To meet service-level agreements, achieve greater utilisation of legacy networks, faster service deployment, and reduce expenditure, telecommunications operators are deploying increasingly complex service function chains (SFCs). Notwithstanding the benefits of SFCs, increasing heterogeneity and dynamism from the cloud to the edge introduces significant SFC placement challenges, not least adding or removing network functions while maintaining availability, quality of service, and minimising cost. In this paper, an availability- and energy-aware solution based on reinforcement learning (RL) is proposed for dynamic SFC placement. Two policy-aware RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimisation (PPO), are compared using simulations of a ground truth network topology based on the Rede Nacional de Ensino e Pesquisa Network, Brazil’s National Teaching and Research Network backbone. The simulation results show that PPO generally outperformed A2C and a greedy approach in terms of both acceptance rate and energy consumption. The biggest difference in the PPO when compared to the other algorithms relates to the SFC availability requirement of 99.965%; the PPO algorithm median acceptance rate is 67.34% better than the A2C algorithm. A2C outperforms PPO only in the scenario where network servers had a greater number of computing resources. In this case, the A2C is 1% better than the PPO.']\n",
      "['The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions.', 'In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.']\n",
      "['The emerging of the Internet of Things (IoT) enables the interconnection among everything. With edge computing serving low-latency services, IoT makes intelligent energy management become a possibility, thereby enhancing the energy sustainability for energy systems. Currently, renewable energy is widely applied in energy systems to alleviate the carbon footprint. However, the instability and discontinuity of renewable generation decrease the quality of service (QoS) of edge servers. To address the challenge, a renewable prediction-driven service offloading method, named ReSome, is proposed. Technically, a deep-learning-based approach is designed for renewable energy prediction firstly. Next, the service offloading process is abstracted to a Markov decision process (MDP). With the predicted renewable energy amount, asynchronous advantage actor-critic (A3C) is leveraged to determine the optimal service offloading strategy. Finally, by utilizing a real-world solar power generation dataset, the experimental evaluation validates the capability and effectiveness of ReSome.']\n",
      "[]\n",
      "['This paper presents our approach for the control of a centralized distributed inventory management system using reinforcement learning (RL). We propose the application of policy-based reinforcement learning algorithms to tackle this problem in an effective manner. We have formulated the problem as a Markov decision process (MDP) and have created an environment that keeps track of multiple products across multiple warehouses returning a reward signal that directly corresponds to the total revenue across all warehouses at every time step. In this environment, we have applied various policy-based reinforcement learning algorithms such as Advantage Actor-Critic, Trust Region Policy Optimization and Proximal Policy Optimization to decide the amount of each product to be stocked in every warehouse. The performance of these algorithms in maximizing average revenue over time has been evaluated considering various statistical distributions from which we sample demand per time step per episode of training. We also compare these approaches to an existing approach involving a fixed replenishment scheme. In conclusion, we elaborate upon the results of our evaluation and the scope for future work on the topic.']\n",
      "['Low-carbon logistics is an emerging and sustainable development industry in the era of a low-carbon economy. The end-to-end deep reinforcement learning (DRL) method with an encoder-decoder framework has been proven effective for solving logistics problems. However, in most cases, the recurrent neural networks (RNN) and attention mechanisms are used in encoders and decoders, which may result in the long-distance dependence problem and the neglect of the correlation between query vectors. To surround this problem, we propose an improved transformer model (TAOA) with both multi-head attention mechanism (MHA) and attention to attention mechanism (AOA), and apply it to solve the low-carbon multi-depot vehicle routing problem (MDVRP). In this model, the MHA and AOA are implemented to solve the probability of route nodes in the encoder and decoder. The MHA is used to process different parts of the input sequence, which can be calculated in parallel, and the AOA is used to deal with the deficiency problem of correlation between query results and query vectors in the MHA. The actor-critic framework based on strategy gradient is constructed to train model parameters. The 2opt operator is further used to optimize the resulting routes. Finally, extensive numerical studies are carried out to verify the effectiveness and operation efficiency of the proposed TAOA, and the results show that the proposed TAOA performs better in solving the MDVRP than the traditional transformer model (Kools), genetic algorithm (GA), and Google OR-Tools (Ortools).']\n",
      "['The number of connected smart devices enabling multimedia applications has expanded tremendously in Internet-of-Things (IoT) environments. Specifically, the requirement for a high quality of experience (QoE) for video streaming services is a crucial prerequisite for a range of use cases, including smart surveillance, smart healthcare, smart agriculture and many more. However, providing a high QoE for video streaming is challenging due to underlying dynamic network conditions. To address this issue, several adaptive bit rate (ABR) algorithms based on predetermined rules have been developed. However, they do not generalize well to a wide variety of network conditions. ABR algorithms, based on reinforcement learning (RL), have been proven to be more effective at generalizing to varying network conditions but they still have limitations, specifically, constrained exploration and high variance in value estimates. In this paper, we propose asynchronous advantage actor-critic (A3C) based actor-learner architectures for generating the adaptive bit rates for video streaming in IoT environments. To address the existing issues, we propose integrating two advanced A3C algorithms: Follow then Forage Exploration (FFE) and Averaged A3C. We demonstrate their efficacy in improving the QoE over vanilla A3C. Additionally, we also demonstrate the benefits of the proposed architecture for video streaming under different network conditions and for different variants of the QoE metric. We show that advanced A3C methods provide up to 30.70% improvement in QoE over vanilla A3C and a considerably higher QoE over other fixed-rule-based ABR algorithms.']\n",
      "['In unmanned aerial vehicle (UAV)-assisted networks, UAV acts as an aerial base station which acquires the requested data via backhaul link and then serves ground users (GUs) through an access network. In this paper, we investigate an energy minimization problem with a limited power supply for both backhaul and access links. The difficulties for solving such a non-convex and combinatorial problem lie at the high computational complexity/time. In solution development, we consider the approaches from both actor-critic deep reinforcement learning (AC-DRL) and optimization perspectives. First, two offline non-learning algorithms, i.e., an optimal and a heuristic algorithms, based on piecewise linear approximation and relaxation are developed as benchmarks. Second, toward real-time decision-making, we improve the conventional AC-DRL and propose two learning schemes: AC-based user group scheduling and backhaul power allocation (ACGP), and joint AC-based user group scheduling and optimization-based backhaul power allocation (ACGOP). Numerical results show that the computation time of both ACGP and ACGOP is reduced tenfold to hundredfold compared to the offline approaches, and ACGOP is better than ACGP in energy savings. The results also verify the superiority of proposed learning solutions in terms of guaranteeing the feasibility and minimizing the system energy compared to the conventional AC-DRL.']\n",
      "['\\n\\nUntil recently, Deep Reinforcement Learning was restricted to innovations in games like Atari, Dota2. Despite surpassing the benchmarks established by their human counterparts in multiple games, these methods could not scale to real-life and industrial automation tasks. The main reason for this was the essential requirement of complex and continuous action control and sophisticated physics of the domain involved in these tasks. Because of these reasons, most of the incumbent solutions for such applications involved the invent of custom planning algorithms. The design of such sophisticated custom solutions required complete knowledge to the dynamics of the domain and its derivatives and hence were not scalable. Policy-based DRL has democratized this space, as now deep reinforcement learning agents could be trained to learn similar sophisticated policies just by learning from the data generated by interacting with these systems or their respective simulations. This has led to significant innovations in real-life and high-value control automation applications like autonomous vehicles, drones, and industrial robots. Therefore, in this paper, we present an overview of different types of policy-approximation based technique in Deep Reinforcement Learning that are the basis of many advanced control automation systems.']\n"
     ]
    }
   ],
   "source": [
    "for i in papers:\n",
    "    print(i[\"abstract\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "506d569f0b28ea103188d4d9f745b7b2b85d3e31dafde01bdf7899299ac0033b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
