titulo,autores,abstract,clase_pri,clase_otr,id
Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU,Mohammad Babaeizadeh,"  We introduce a hybrid CPU/GPU version of the Asynchronous Advantage
Actor-Critic (A3C) algorithm, currently the state-of-the-art method in
reinforcement learning for various gaming tasks. We analyze its computational
traits and concentrate on aspects critical to leveraging the GPU's
computational power. We introduce a system of queues and a dynamic scheduling
strategy, potentially helpful for other asynchronous algorithms as well. Our
hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant
speed up compared to a CPU implementation; we make it publicly available to
other researchers at ",Machine Learning (cs.LG),,0
Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic,Behrad Toghi,"  With the adoption of autonomous vehicles on our roads, we will witness a
mixed-autonomy environment where autonomous and human-driven vehicles must
learn to co-exist by sharing the same road infrastructure. To attain
socially-desirable behaviors, autonomous vehicles must be instructed to
consider the utility of other vehicles around them in their decision-making
process. Particularly, we study the maneuver planning problem for autonomous
vehicles and investigate how a decentralized reward structure can induce
altruism in their behavior and incentivize them to account for the interest of
other autonomous and human-driven vehicles. This is a challenging problem due
to the ambiguity of a human driver's willingness to cooperate with an
autonomous vehicle. Thus, in contrast with the existing works which rely on
behavior models of human drivers, we take an end-to-end approach and let the
autonomous agents to implicitly learn the decision-making process of human
drivers only from experience. We introduce a multi-agent variant of the
synchronous Advantage Actor-Critic (A2C) algorithm and train agents that
coordinate with each other and can affect the behavior of human drivers to
improve traffic flow and safety.

    ",Robotics (cs.RO),,1
Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup,Han Shen,"  Asynchronous and parallel implementation of standard reinforcement learning
(RL) algorithms is a key enabler of the tremendous success of modern RL. Among
many asynchronous RL algorithms, arguably the most popular and effective one is
the asynchronous advantage actor-critic (A3C) algorithm. Although A3C is
becoming the workhorse of RL, its theoretical properties are still not
well-understood, including its non-asymptotic analysis and the performance gain
of parallelism (a.k.a. linear speedup). This paper revisits the A3C algorithm
and establishes its non-asymptotic convergence guarantees. Under both i.i.d.
and Markovian sampling, we establish the local convergence guarantee for A3C in
the general policy approximation case and the global convergence guarantee in
softmax policy parameterization. Under i.i.d. sampling, A3C obtains sample
complexity of $\mathcal{O}(\epsilon^{-2.5}/N)$ per worker to achieve $\epsilon$
accuracy, where $N$ is the number of workers. Compared to the best-known sample
complexity of $\mathcal{O}(\epsilon^{-2.5})$ for two-timescale AC, A3C achieves
\emph{linear speedup}, which justifies the advantage of parallelism and
asynchrony in AC algorithms theoretically for the first time. Numerical tests
on synthetic environment, OpenAI Gym environments and Atari games have been
provided to verify our theoretical analysis.

    ",Machine Learning (cs.LG),; Optimization and Control (math.OC),2
The Advantage Regret-Matching Actor-Critic,AudrÅ«nas Gruslys,"  Regret minimization has played a key role in online learning, equilibrium
computation in games, and reinforcement learning (RL). In this paper, we
describe a general model-free RL method for no-regret learning based on
repeated reconsideration of past behavior. We propose a model-free RL
algorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than
saving past state-action data, ARMAC saves a buffer of past policies, replaying
through them to reconstruct hindsight assessments of past behavior. These
retrospective value estimates are used to predict conditional advantages which,
combined with regret matching, produces a new policy. In particular, ARMAC
learns from sampled trajectories in a centralized training setting, without
requiring the application of importance sampling commonly used in Monte Carlo
counterfactual regret (CFR) minimization; hence, it does not suffer from
excessive variance in large environments. In the single-agent setting, ARMAC
shows an interesting form of exploration by keeping past policies intact. In
the multiagent setting, ARMAC in self-play approaches Nash equilibria on some
partially-observable zero-sum benchmarks. We provide exploitability estimates
in the significantly larger game of betting-abstracted no-limit Texas Hold'em.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),3
Actor-Critic Sequence Training for Image Captioning,Li Zhang,"  Generating natural language descriptions of images is an important capability
for a robot or other visual-intelligence driven AI agent that may need to
communicate with human users about what it is seeing. Such image captioning
methods are typically trained by maximising the likelihood of ground-truth
annotated caption given the image. While simple and easy to implement, this
approach does not directly maximise the language quality metrics we care about
such as CIDEr. In this paper we investigate training image captioning methods
based on actor-critic reinforcement learning in order to directly optimise
non-differentiable quality metrics of interest. By formulating a per-token
advantage and value computation strategy in this novel reinforcement learning
based captioning model, we show that it is possible to achieve the state of the
art performance on the widely used MSCOCO benchmark.

    ",Computer Vision and Pattern Recognition (cs.CV),,4
Sample Efficient Actor-Critic with Experience Replay,Ziyu Wang,"  This paper presents an actor-critic deep reinforcement learning agent with
experience replay that is stable, sample efficient, and performs remarkably
well on challenging environments, including the discrete 57-game Atari domain
and several continuous control problems. To achieve this, the paper introduces
several innovations, including truncated importance sampling with bias
correction, stochastic dueling network architectures, and a new trust region
policy optimization method.

    ",Machine Learning (cs.LG),,5
Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP),Zhimin Hou,"  The optimal policy of a reinforcement learning problem is often discontinuous
and non-smooth. I.e., for two states with similar representations, their
optimal policies can be significantly different. In this case, representing the
entire policy with a function approximator (FA) with shared parameters for all
states maybe not desirable, as the generalization ability of parameters sharing
makes representing discontinuous, non-smooth policies difficult. A common way
to solve this problem, known as Mixture-of-Experts, is to represent the policy
as the weighted sum of multiple components, where different components perform
well on different parts of the state space. Following this idea and inspired by
a recent work called advantage-weighted information maximization, we propose to
learn for each state weights of these components, so that they entail the
information of the state itself and also the preferred action learned so far
for the state. The action preference is characterized via the advantage
function. In this case, the weight of each component would only be large for
certain groups of states whose representations are similar and preferred action
representations are also similar. Therefore each component is easy to be
represented. We call a policy parameterized in this way an Advantage Weighted
Mixture Policy (AWMP) and apply this idea to improve soft-actor-critic (SAC),
one of the most competitive continuous control algorithm. Experimental results
demonstrate that SAC with AWMP clearly outperforms SAC in four commonly used
continuous control tasks and achieve stable performance across different random
seeds.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),6
Asynchronous Advantage Actor-Critic Agent for Starcraft II,Basel Alghanem,"  Deep reinforcement learning, and especially the Asynchronous Advantage
Actor-Critic algorithm, has been successfully used to achieve super-human
performance in a variety of video games. Starcraft II is a new challenge for
the reinforcement learning community with the release of pysc2 learning
environment proposed by Google Deepmind and Blizzard Entertainment. Despite
being a target for several AI developers, few have achieved human level
performance. In this project we explain the complexities of this environment
and discuss the results from our experiments on the environment. We have
compared various architectures and have proved that transfer learning can be an
effective paradigm in reinforcement learning research for complex scenarios
requiring skill transfer.

    ",Artificial Intelligence (cs.AI),,7
An advantage actor-critic algorithm for robotic motion planning in dense and dynamic scenarios,Chengmin Zhou,"  Intelligent robots provide a new insight into efficiency improvement in
industrial and service scenarios to replace human labor. However, these
scenarios include dense and dynamic obstacles that make motion planning of
robots challenging. Traditional algorithms like A* can plan collision-free
trajectories in static environment, but their performance degrades and
computational cost increases steeply in dense and dynamic scenarios.
Optimal-value reinforcement learning algorithms (RL) can address these problems
but suffer slow speed and instability in network convergence. Network of policy
gradient RL converge fast in Atari games where action is discrete and finite,
but few works have been done to address problems where continuous actions and
large action space are required. In this paper, we modify existing advantage
actor-critic algorithm and suit it to complex motion planning, therefore
optimal speeds and directions of robot are generated. Experimental results
demonstrate that our algorithm converges faster and stable than optimal-value
RL. It achieves higher success rate in motion planning with lesser processing
time for robot to reach its goal.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),8
Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space,Zhou Fan,"  In this paper we propose a hybrid architecture of actor-critic algorithms for
reinforcement learning in parameterized action space, which consists of
multiple parallel sub-actor networks to decompose the structured action space
into simpler action spaces along with a critic network to guide the training of
all sub-actor networks. While this paper is mainly focused on parameterized
action space, the proposed architecture, which we call hybrid actor-critic, can
be extended for more general action spaces which has a hierarchical structure.
We present an instance of the hybrid actor-critic architecture based on
proximal policy optimization (PPO), which we refer to as hybrid proximal policy
optimization (H-PPO). Our experiments test H-PPO on a collection of tasks with
parameterized action space, where H-PPO demonstrates superior performance over
previous methods of parameterized action reinforcement learning.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),9
Multimodal Machine Translation with Reinforcement Learning,Xin Qian,"  Multimodal machine translation is one of the applications that integrates
computer vision and language processing. It is a unique task given that in the
field of machine translation, many state-of-the-arts algorithms still only
employ textual information. In this work, we explore the effectiveness of
reinforcement learning in multimodal machine translation. We present a novel
algorithm based on the Advantage Actor-Critic (A2C) algorithm that specifically
cater to the multimodal machine translation task of the EMNLP 2018 Third
Conference on Machine Translation (WMT18). We experiment our proposed algorithm
on the Multi30K multilingual English-German image description dataset and the
Flickr30K image entity dataset. Our model takes two channels of inputs, image
and text, uses translation evaluation metrics as training rewards, and achieves
better results than supervised learning MLE baseline models. Furthermore, we
discuss the prospects and limitations of using reinforcement learning for
machine translation. Our experiment results suggest a promising reinforcement
learning solution to the general task of multimodal sequence to sequence
learning.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Multiagent Systems (cs.MA); Multimedia (cs.MM),10
On the Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost,Zhuoran Yang,"  Despite the empirical success of the actor-critic algorithm, its theoretical
understanding lags behind. In a broader context, actor-critic can be viewed as
an online alternating update algorithm for bilevel optimization, whose
convergence is known to be fragile. To understand the instability of
actor-critic, we focus on its application to linear quadratic regulators, a
simple yet fundamental setting of reinforcement learning. We establish a
nonasymptotic convergence analysis of actor-critic in this setting. In
particular, we prove that actor-critic finds a globally optimal pair of actor
(policy) and critic (action-value function) at a linear rate of convergence.
Our analysis may serve as a preliminary step towards a complete theoretical
understanding of bilevel optimization with nonconvex subproblems, which is
NP-hard in the worst case and is often solved using heuristics.

    ",Machine Learning (cs.LG),; Optimization and Control (math.OC); Machine Learning (stat.ML),11
Inspiration Learning through Preferences,Nir Baram,"  Current imitation learning techniques are too restrictive because they
require the agent and expert to share the same action space. However,
oftentimes agents that act differently from the expert can solve the task just
as good. For example, a person lifting a box can be imitated by a ceiling
mounted robot or a desktop-based robotic-arm. In both cases, the end goal of
lifting the box is achieved, perhaps using different strategies. We denote this
setup as \textit{Inspiration Learning} - knowledge transfer between agents that
operate in different action spaces. Since state-action expert demonstrations
can no longer be used, Inspiration learning requires novel methods to guide the
agent towards the end goal. In this work, we rely on ideas of Preferential
based Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms
for solving inspiration learning tasks. Unlike classic actor-critic
architectures, the critic we use consists of two parts: a) a state-value
estimation as in common actor-critic algorithms and b) a single step reward
function derived from an expert/agent classifier. We show that our method is
capable of extending the current imitation framework to new horizons. This
includes continuous-to-discrete action imitation, as well as primitive-to-macro
action imitation.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),12
Epersist: A Self Balancing Robot Using PID Controller And Deep Reinforcement Learning,Ghanta Sai Krishna,"  A two-wheeled self-balancing robot is an example of an inverse pendulum and
is an inherently non-linear, unstable system. The fundamental concept of the
proposed framework ""Epersist"" is to overcome the challenge of counterbalancing
an initially unstable system by delivering robust control mechanisms,
Proportional Integral Derivative(PID), and Reinforcement Learning (RL).
Moreover, the micro-controller NodeMCUESP32 and inertial sensor in the Epersist
employ fewer computational procedures to give accurate instruction regarding
the spin of wheels to the motor driver, which helps control the wheels and
balance the robot. This framework also consists of the mathematical model of
the PID controller and a novel self-trained advantage actor-critic algorithm as
the RL agent. After several experiments, control variable calibrations are made
as the benchmark values to attain the angle of static equilibrium. This
""Epersist"" framework proposes PID and RL-assisted functional prototypes and
simulations for better utility.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),13
Towards Practical Credit Assignment for Deep Reinforcement Learning,Vyacheslav Alipov,"  Credit assignment is a fundamental problem in reinforcement learning, the
problem of measuring an action's influence on future rewards. Explicit credit
assignment methods have the potential to boost the performance of RL algorithms
on many tasks, but thus far remain impractical for general use. Recently, a
family of methods called Hindsight Credit Assignment (HCA) was proposed, which
explicitly assign credit to actions in hindsight based on the probability of
the action having led to an observed outcome. This approach has appealing
properties, but remains a largely theoretical idea applicable to a limited set
of tabular RL tasks. Moreover, it is unclear how to extend HCA to deep RL
environments. In this work, we explore the use of HCA-style credit in a deep RL
context. We first describe the limitations of existing HCA algorithms in deep
RL that lead to their poor performance or complete lack of training, then
propose several theoretically-justified modifications to overcome them. We
explore the quantitative and qualitative effects of the resulting algorithm on
the Arcade Learning Environment (ALE) benchmark, and observe that it improves
performance over Advantage Actor-Critic (A2C) on many games where non-trivial
credit assignment is necessary to achieve high scores and where hindsight
probabilities can be accurately estimated.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),14
Intelligent Coordination among Multiple Traffic Intersections Using Multi-Agent Reinforcement Learning,Ujwal Padam Tewari,"  We use Asynchronous Advantage Actor Critic (A3C) for implementing an AI agent
in the controllers that optimize flow of traffic across a single intersection
and then extend it to multiple intersections by considering a multi-agent
setting. We explore three different methodologies to address the multi-agent
problem - (1) use of asynchronous property of A3C to control multiple
intersections using a single agent (2) utilise self/competitive play among
independent agents across multiple intersections and (3) ingest a global reward
function among agents to introduce cooperative behavior between intersections.
We observe that (1) & (2) leads to a reduction in traffic congestion.
Additionally the use of (3) with (1) & (2) led to a further reduction in
congestion.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Multiagent Systems (cs.MA),15
Acceleration of Actor-Critic Deep Reinforcement Learning for Visual Grasping in Clutter by State Representation Learning Based on Disentanglement of a Raw Input Image,Taewon Kim,"  For a robotic grasping task in which diverse unseen target objects exist in a
cluttered environment, some deep learning-based methods have achieved
state-of-the-art results using visual input directly. In contrast, actor-critic
deep reinforcement learning (RL) methods typically perform very poorly when
grasping diverse objects, especially when learning from raw images and sparse
rewards. To make these RL techniques feasible for vision-based grasping tasks,
we employ state representation learning (SRL), where we encode essential
information first for subsequent use in RL. However, typical representation
learning procedures are unsuitable for extracting pertinent information for
learning the grasping skill, because the visual inputs for representation
learning, where a robot attempts to grasp a target object in clutter, are
extremely complex. We found that preprocessing based on the disentanglement of
a raw input image is the key to effectively capturing a compact representation.
This enables deep RL to learn robotic grasping skills from highly varied and
diverse visual inputs. We demonstrate the effectiveness of this approach with
varying levels of disentanglement in a realistic simulated environment.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),16
Cooperative Multi-Agent Actor-Critic for Privacy-Preserving Load Scheduling in a Residential Microgrid,Zhaoming Qin,"  As a scalable data-driven approach, multi-agent reinforcement learning (MARL)
has made remarkable advances in solving the cooperative residential load
scheduling problems. However, the common centralized training strategy of MARL
algorithms raises privacy risks for involved households. In this work, we
propose a privacy-preserving multi-agent actor-critic framework where the
decentralized actors are trained with distributed critics, such that both the
decentralized execution and the distributed training do not require the global
state information. The proposed framework can preserve the privacy of the
households while simultaneously learn the multi-agent credit assignment
mechanism implicitly. The simulation experiments demonstrate that the proposed
framework significantly outperforms the existing privacy-preserving
actor-critic framework, and can achieve comparable performance to the
state-of-the-art actor-critic framework without privacy constraints.

    ",Multiagent Systems (cs.MA),; Cryptography and Security (cs.CR); Machine Learning (cs.LG),17
Expert-augmented actor-critic for ViZDoom and Montezumas Revenge,MichaÅ Garmulewicz,"  We propose an expert-augmented actor-critic algorithm, which we evaluate on
two environments with sparse rewards: Montezumas Revenge and a demanding maze
from the ViZDoom suite. In the case of Montezumas Revenge, an agent trained
with our method achieves very good results consistently scoring above 27,000
points (in many experiments beating the first world). With an appropriate
choice of hyperparameters, our algorithm surpasses the performance of the
expert data. In a number of experiments, we have observed an unreported bug in
Montezumas Revenge which allowed the agent to score more than 800,000 points.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),18
Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,Sahil Sharma,"  Reinforcement Learning algorithms can learn complex behavioral patterns for
sequential decision making tasks wherein an agent interacts with an environment
and acquires feedback in the form of rewards sampled from it. Traditionally,
such algorithms make decisions, i.e., select actions to execute, at every
single time step of the agent-environment interactions. In this paper, we
propose a novel framework, Fine Grained Action Repetition (FiGAR), which
enables the agent to decide the action as well as the time scale of repeating
it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm
which maintains an explicit policy estimate by enabling temporal abstractions
in the action space. We empirically demonstrate the efficacy of our framework
by showing performance improvements on top of three policy search algorithms in
different domains: Asynchronous Advantage Actor Critic in the Atari 2600
domain, Trust Region Policy Optimization in Mujoco domain and Deep
Deterministic Policy Gradients in the TORCS car racing domain.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE),19
Self-Imitation Advantage Learning,Johan Ferret,"  Self-imitation learning is a Reinforcement Learning (RL) method that
encourages actions whose returns were higher than expected, which helps in hard
exploration and sparse reward problems. It was shown to improve the performance
of on-policy actor-critic methods in several discrete control tasks.
Nevertheless, applying self-imitation to the mostly action-value based
off-policy RL methods is not straightforward. We propose SAIL, a novel
generalization of self-imitation learning for off-policy RL, based on a
modification of the Bellman optimality operator that we connect to Advantage
Learning. Crucially, our method mitigates the problem of stale returns by
choosing the most optimistic return estimate between the observed return and
the current action-value for self-imitation. We demonstrate the empirical
effectiveness of SAIL on the Arcade Learning Environment, with a focus on hard
exploration games.

    ",Machine Learning (cs.LG),,20
Microscopic Traffic Simulation by Cooperative Multi-agent Deep Reinforcement Learning,Giulio Bacchiani,"  Expert human drivers perform actions relying on traffic laws and their
previous experience. While traffic laws are easily embedded into an artificial
brain, modeling human complex behaviors which come from past experience is a
more challenging task. One of these behaviors is the capability of
communicating intentions and negotiating the right of way through driving
actions, as when a driver is entering a crowded roundabout and observes other
cars movements to guess the best time to merge in. In addition, each driver has
its own unique driving style, which is conditioned by both its personal
characteristics, such as age and quality of sight, and external factors, such
as being late or in a bad mood. For these reasons, the interaction between
different drivers is not trivial to simulate in a realistic manner. In this
paper, this problem is addressed by developing a microscopic simulator using a
Deep Reinforcement Learning Algorithm based on a combination of visual frames,
representing the perception around the vehicle, and a vector of numerical
parameters. In particular, the algorithm called Asynchronous Advantage
Actor-Critic has been extended to a multi-agent scenario in which every agent
needs to learn to interact with other similar agents. Moreover, the model
includes a novel architecture such that the driving style of each vehicle is
adjustable by tuning some of its input parameters, permitting to simulate
drivers with different levels of aggressiveness and desired cruising speeds.

    ",Multiagent Systems (cs.MA),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),21
Simultaneous Control and Human Feedback in the Training of a Robotic Agent with Actor-Critic Reinforcement Learning,Kory W. Mathewson,"  This paper contributes a preliminary report on the advantages and
disadvantages of incorporating simultaneous human control and feedback signals
in the training of a reinforcement learning robotic agent. While robotic
human-machine interfaces have become increasingly complex in both form and
function, control remains challenging for users. This has resulted in an
increasing gap between user control approaches and the number of robotic motors
which can be controlled. One way to address this gap is to shift some autonomy
to the robot. Semi-autonomous actions of the robotic agent can then be shaped
by human feedback, simplifying user control. Most prior work on agent shaping
by humans has incorporated training with feedback, or has included indirect
control signals. By contrast, in this paper we explore how a human can provide
concurrent feedback signals and real-time myoelectric control signals to train
a robot's actor-critic reinforcement learning control system. Using both a
physical and a simulated robotic system, we compare training performance on a
simple movement task when reward is derived from the environment, when reward
is provided by the human, and combinations of these two approaches. Our results
indicate that some benefit can be gained with the inclusion of human generated
feedback.

    ",Human-Computer Interaction (cs.HC),; Artificial Intelligence (cs.AI); Robotics (cs.RO),22
Variational Quantum Soft Actor-Critic,Qingfeng Lan,"  Quantum computing has a superior advantage in tackling specific problems,
such as integer factorization and Simon's problem. For more general tasks in
machine learning, by applying variational quantum circuits, more and more
quantum algorithms have been proposed recently, especially in supervised
learning and unsupervised learning. However, little work has been done in
reinforcement learning, arguably more important and challenging. Previous work
in quantum reinforcement learning mainly focuses on discrete control tasks
where the action space is discrete. In this work, we develop a quantum
reinforcement learning algorithm based on soft actor-critic -- one of the
state-of-the-art methods for continuous control. Specifically, we use a hybrid
quantum-classical policy network consisting of a variational quantum circuit
and a classical artificial neural network. Tested in a standard reinforcement
learning benchmark, we show that this quantum version of soft actor-critic is
comparable with the original soft actor-critic, using much less adjustable
parameters. Furthermore, we analyze the effect of different hyper-parameters
and policy network architectures, pointing out the importance of architecture
design for quantum reinforcement learning.

    ",Quantum Physics (quant-ph),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),23
An Actor-Critic Contextual Bandit Algorithm for Personalized Mobile Health Interventions,Huitian Lei,"  Increasing technological sophistication and widespread use of smartphones and
wearable devices provide opportunities for innovative and highly personalized
health interventions. A Just-In-Time Adaptive Intervention (JITAI) uses
real-time data collection and communication capabilities of modern mobile
devices to deliver interventions in real-time that are adapted to the
in-the-moment needs of the user. The lack of methodological guidance in
constructing data-based JITAIs remains a hurdle in advancing JITAI research
despite the increasing popularity of JITAIs among clinical scientists. In this
article, we make a first attempt to bridge this methodological gap by
formulating the task of tailoring interventions in real-time as a contextual
bandit problem. Interpretability requirements in the domain of mobile health
lead us to formulate the problem differently from existing formulations
intended for web applications such as ad or news article placement. Under the
assumption of linear reward function, we choose the reward function (the
""critic"") parameterization separately from a lower dimensional parameterization
of stochastic policies (the ""actor""). We provide an online actor-critic
algorithm that guides the construction and refinement of a JITAI. Asymptotic
properties of the actor-critic algorithm are developed and backed up by
numerical experiments. Additional numerical experiments are conducted to test
the robustness of the algorithm when idealized assumptions used in the analysis
of contextual bandit algorithm are breached.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),24
Robust Reinforcement Learning with Wasserstein Constraint,Linfang Hou,"  Robust Reinforcement Learning aims to find the optimal policy with some
extent of robustness to environmental dynamics. Existing learning algorithms
usually enable the robustness through disturbing the current state or
simulating environmental parameters in a heuristic way, which lack quantified
robustness to the system dynamics (i.e. transition probability). To overcome
this issue, we leverage Wasserstein distance to measure the disturbance to the
reference transition kernel. With Wasserstein distance, we are able to connect
transition kernel disturbance to the state disturbance, i.e. reduce an
infinite-dimensional optimization problem to a finite-dimensional risk-aware
problem. Through the derived risk-aware optimal Bellman equation, we show the
existence of optimal robust policies, provide a sensitivity analysis for the
perturbations, and then design a novel robust learning algorithm--Wasserstein
Robust Advantage Actor-Critic algorithm (WRAAC). The effectiveness of the
proposed algorithm is verified in the Cart-Pole environment.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),25
Multimodal Machine Translation with Reinforcement Learning,Xin Qian,"  Multimodal machine translation is one of the applications that integrates
computer vision and language processing. It is a unique task given that in the
field of machine translation, many state-of-the-arts algorithms still only
employ textual information. In this work, we explore the effectiveness of
reinforcement learning in multimodal machine translation. We present a novel
algorithm based on the Advantage Actor-Critic (A2C) algorithm that specifically
cater to the multimodal machine translation task of the EMNLP 2018 Third
Conference on Machine Translation (WMT18). We experiment our proposed algorithm
on the Multi30K multilingual English-German image description dataset and the
Flickr30K image entity dataset. Our model takes two channels of inputs, image
and text, uses translation evaluation metrics as training rewards, and achieves
better results than supervised learning MLE baseline models. Furthermore, we
discuss the prospects and limitations of using reinforcement learning for
machine translation. Our experiment results suggest a promising reinforcement
learning solution to the general task of multimodal sequence to sequence
learning.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Multiagent Systems (cs.MA); Multimedia (cs.MM),26
On the Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost,Zhuoran Yang,"  Despite the empirical success of the actor-critic algorithm, its theoretical
understanding lags behind. In a broader context, actor-critic can be viewed as
an online alternating update algorithm for bilevel optimization, whose
convergence is known to be fragile. To understand the instability of
actor-critic, we focus on its application to linear quadratic regulators, a
simple yet fundamental setting of reinforcement learning. We establish a
nonasymptotic convergence analysis of actor-critic in this setting. In
particular, we prove that actor-critic finds a globally optimal pair of actor
(policy) and critic (action-value function) at a linear rate of convergence.
Our analysis may serve as a preliminary step towards a complete theoretical
understanding of bilevel optimization with nonconvex subproblems, which is
NP-hard in the worst case and is often solved using heuristics.

    ",Machine Learning (cs.LG),; Optimization and Control (math.OC); Machine Learning (stat.ML),27
Inspiration Learning through Preferences,Nir Baram,"  Current imitation learning techniques are too restrictive because they
require the agent and expert to share the same action space. However,
oftentimes agents that act differently from the expert can solve the task just
as good. For example, a person lifting a box can be imitated by a ceiling
mounted robot or a desktop-based robotic-arm. In both cases, the end goal of
lifting the box is achieved, perhaps using different strategies. We denote this
setup as \textit{Inspiration Learning} - knowledge transfer between agents that
operate in different action spaces. Since state-action expert demonstrations
can no longer be used, Inspiration learning requires novel methods to guide the
agent towards the end goal. In this work, we rely on ideas of Preferential
based Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms
for solving inspiration learning tasks. Unlike classic actor-critic
architectures, the critic we use consists of two parts: a) a state-value
estimation as in common actor-critic algorithms and b) a single step reward
function derived from an expert/agent classifier. We show that our method is
capable of extending the current imitation framework to new horizons. This
includes continuous-to-discrete action imitation, as well as primitive-to-macro
action imitation.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),28
Epersist: A Self Balancing Robot Using PID Controller And Deep Reinforcement Learning,Ghanta Sai Krishna,"  A two-wheeled self-balancing robot is an example of an inverse pendulum and
is an inherently non-linear, unstable system. The fundamental concept of the
proposed framework ""Epersist"" is to overcome the challenge of counterbalancing
an initially unstable system by delivering robust control mechanisms,
Proportional Integral Derivative(PID), and Reinforcement Learning (RL).
Moreover, the micro-controller NodeMCUESP32 and inertial sensor in the Epersist
employ fewer computational procedures to give accurate instruction regarding
the spin of wheels to the motor driver, which helps control the wheels and
balance the robot. This framework also consists of the mathematical model of
the PID controller and a novel self-trained advantage actor-critic algorithm as
the RL agent. After several experiments, control variable calibrations are made
as the benchmark values to attain the angle of static equilibrium. This
""Epersist"" framework proposes PID and RL-assisted functional prototypes and
simulations for better utility.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),29
Towards Practical Credit Assignment for Deep Reinforcement Learning,Vyacheslav Alipov,"  Credit assignment is a fundamental problem in reinforcement learning, the
problem of measuring an action's influence on future rewards. Explicit credit
assignment methods have the potential to boost the performance of RL algorithms
on many tasks, but thus far remain impractical for general use. Recently, a
family of methods called Hindsight Credit Assignment (HCA) was proposed, which
explicitly assign credit to actions in hindsight based on the probability of
the action having led to an observed outcome. This approach has appealing
properties, but remains a largely theoretical idea applicable to a limited set
of tabular RL tasks. Moreover, it is unclear how to extend HCA to deep RL
environments. In this work, we explore the use of HCA-style credit in a deep RL
context. We first describe the limitations of existing HCA algorithms in deep
RL that lead to their poor performance or complete lack of training, then
propose several theoretically-justified modifications to overcome them. We
explore the quantitative and qualitative effects of the resulting algorithm on
the Arcade Learning Environment (ALE) benchmark, and observe that it improves
performance over Advantage Actor-Critic (A2C) on many games where non-trivial
credit assignment is necessary to achieve high scores and where hindsight
probabilities can be accurately estimated.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),30
Intelligent Coordination among Multiple Traffic Intersections Using Multi-Agent Reinforcement Learning,Ujwal Padam Tewari,"  We use Asynchronous Advantage Actor Critic (A3C) for implementing an AI agent
in the controllers that optimize flow of traffic across a single intersection
and then extend it to multiple intersections by considering a multi-agent
setting. We explore three different methodologies to address the multi-agent
problem - (1) use of asynchronous property of A3C to control multiple
intersections using a single agent (2) utilise self/competitive play among
independent agents across multiple intersections and (3) ingest a global reward
function among agents to introduce cooperative behavior between intersections.
We observe that (1) & (2) leads to a reduction in traffic congestion.
Additionally the use of (3) with (1) & (2) led to a further reduction in
congestion.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Multiagent Systems (cs.MA),31
Acceleration of Actor-Critic Deep Reinforcement Learning for Visual Grasping in Clutter by State Representation Learning Based on Disentanglement of a Raw Input Image,Taewon Kim,"  For a robotic grasping task in which diverse unseen target objects exist in a
cluttered environment, some deep learning-based methods have achieved
state-of-the-art results using visual input directly. In contrast, actor-critic
deep reinforcement learning (RL) methods typically perform very poorly when
grasping diverse objects, especially when learning from raw images and sparse
rewards. To make these RL techniques feasible for vision-based grasping tasks,
we employ state representation learning (SRL), where we encode essential
information first for subsequent use in RL. However, typical representation
learning procedures are unsuitable for extracting pertinent information for
learning the grasping skill, because the visual inputs for representation
learning, where a robot attempts to grasp a target object in clutter, are
extremely complex. We found that preprocessing based on the disentanglement of
a raw input image is the key to effectively capturing a compact representation.
This enables deep RL to learn robotic grasping skills from highly varied and
diverse visual inputs. We demonstrate the effectiveness of this approach with
varying levels of disentanglement in a realistic simulated environment.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),32
Cooperative Multi-Agent Actor-Critic for Privacy-Preserving Load Scheduling in a Residential Microgrid,Zhaoming Qin,"  As a scalable data-driven approach, multi-agent reinforcement learning (MARL)
has made remarkable advances in solving the cooperative residential load
scheduling problems. However, the common centralized training strategy of MARL
algorithms raises privacy risks for involved households. In this work, we
propose a privacy-preserving multi-agent actor-critic framework where the
decentralized actors are trained with distributed critics, such that both the
decentralized execution and the distributed training do not require the global
state information. The proposed framework can preserve the privacy of the
households while simultaneously learn the multi-agent credit assignment
mechanism implicitly. The simulation experiments demonstrate that the proposed
framework significantly outperforms the existing privacy-preserving
actor-critic framework, and can achieve comparable performance to the
state-of-the-art actor-critic framework without privacy constraints.

    ",Multiagent Systems (cs.MA),; Cryptography and Security (cs.CR); Machine Learning (cs.LG),33
