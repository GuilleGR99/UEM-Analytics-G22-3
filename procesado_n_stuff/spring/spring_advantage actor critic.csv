identifier,language,title,genre,abstract,subjects
doi:10.1007/978-3-031-16203-9_8,en,The Comprehensive Model of Using In-Depth Consolidated Multimodal Learning to Study Trading Strategies in the Securities Market,OriginalPaper,"The paper describes the relevance of machine learning methods, namely training with reinforcement, to the problems of predicting financial time series. An overview of existing applications based on machine learning in the issues of financial market forecasting is presented. The reasons for the popularity of my research topic are highlighted both from the scientific (increasing the number of publications on issues relevant to the research topic over the past five years) and from the practical side. The analysis of scientific works, the subject and purpose of which are related to the issues and objectives of my research and their features are presented. The main problems associated with the problem of predicting stochastic time series are identified. According to the analysis, the purpose of work is defined, and also the list of tasks for the achievement of the set goal is made. The article is devoted to studying the use of the ensemble of neuro-learning networks with the strengthening of the securities trading market. The practical significance of the work is to use the model of efficient distribution of investments in the market. This paper will explore a set of models that use in-depth consolidated learning to explore trading strategies to maximize return on investment. The potential of using acting-critical models as an ensemble has been studied. Models such as Proximal Policy Optimizer (PPO), Advantage Actor-Critic (A2C) and Deep Determinist Police Gradient (DDPG) were used to teach trading strategy. To adapt the model to different situations, analyzes are analyzed according to three algorithms: the Dow Jones average and a portfolio that minimizes fluctuations in the Charpy ratio by balancing risk and return. The article compares ensembles by the method of fixing deep neural networks. To optimize the balance of risk and profit, the indicators of the ensemble model are analyzed. The ensemble and three-component models that apply well to market collapse conditions are considered. The models have learned to use the turbulence index for early stock sales to minimize losses during a stock collapse. The turbulence index threshold is demonstrated using ensemble models to regulate risk avoidance.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-08246-7_7,en,Implementation of Reinforcement-Learning Algorithms in Autonomous Robot Navigation,OriginalPaper,"The problem of autonomous robot navigation in indoor environments must overcome various difficulties such as the dimensionality of the data, the computational cost, and the possible presence of mobile objects. This chapter addresses the implementation of an algorithm for autonomous navigation of robots in indoor environments based on machine learning. It characterizes some strategies that the literature reports and specifies a Deep Q-Network reinforcement-learning algorithm to implement on the Turtlebot robotic platform of the Gazebo simulator. Besides, a series of experiments changing the parameters of algorithm to validate the strategy shows how the robotic platform, through the exploration of the environment and the subsequent exploitation of the information, creates effective route planning.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4960-9_31,en,Reinforcement Learning for Autonomous Driving Scenarios in Indian Roads,OriginalPaper,"The decision-making process for autonomous vehicles comes with numerous challenges that are not easily solved. With the ever-changing traffic situation of the world and the increasing need for autonomous driving technology, there are constant innovations to deal with the increasing number of problems in the complex environments that autonomous driving agents find themselves in. Developing countries like India face even more numerous challenges with existing autonomous driving solutions not being directly transferable. However, with the maturation and advancement of deep learning technology over the years, more and more novel methods in the field of deep reinforcement learning are being proposed to tackle both new challenges and existing challenges. In this study, we explore the contemporary reinforcement learning techniques for autonomous driving tasks and analyze their applicability for the unstructured road environment and also look at some of the less-common scenarios that occur frequently in the Indian context.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-3-031-18461-1_11,en,"A Survey of Reinforcement Learning Toolkits for Gaming: Applications, Challenges and Trends",OriginalPaper,"The gaming industry has become one of the most exciting and creative industries. The annual revenue has crossed $200 billion in recent years and has created a lot of jobs globally. Many games are using Artificial Intelligence (AI) and techniques like Machine Learning (ML), Reinforcement Learning (RL) gained popularity among researchers and game development community to enable smart games involving AI-based agents at a faster rate. Although, many toolkits are available for use, a framework to evaluate, compare and advise on these toolkits is still missing. In this paper, we present a comprehensive overview of ML/RL toolkits for games with an emphasis on their applications, challenges, and trends. We propose a qualitative evaluation methodology, discuss the obtained analysis results, and conclude with future work and perspectives.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-7648-3_12,en,Priority-Aware Computational Resource Allocation,OriginalPaper,"Vehicular fog computing (VFC) has been expected as a promising scheme that can increase the computational capability of vehicles without relying on servers. Comparing with accessing the remote cloud, VFC is suitable for delay-sensitive tasks because of its low-latency vehicle-to-vehicle (V2V) transmission. However, due to the dynamic vehicular environment, how to motivate vehicles to share their idle computing resource while simultaneously evaluating the service availability of vehicles in terms of vehicle mobility and vehicular computational capability in heterogeneous vehicular networks is a main challenge. Meanwhile, tasks with different priorities of a vehicle should be processed with different efficiencies. In this work, we propose a task offloading scheme in the context of VFC, where vehicles are incentivized to share their idle computing resource by dynamic pricing, which comprehensively considers the mobility of vehicles, the task priority, and the service availability of vehicles. Given that the policy of task offloading depends on the state of the dynamic vehicular environment, we formulate the task offloading problem as a Markov decision process (MDP) aiming at maximizing the mean latency-aware utility of tasks in a period. To solve this problem, we develop a soft actor-critic (SAC) based deep reinforcement learning (DRL) algorithm for the sake of maximizing both the expected reward and the entropy of policy. Finally, extensive simulation results validate the effectiveness and superiority of our proposed scheme benchmarked with traditional algorithms.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Communications Engineering, Networks', 'Wireless and Mobile Communication']"
doi:10.1007/978-981-19-4815-2_6,en,A-DDPG: Attention Mechanism-Based Deep Reinforcement Learning for NFV,OriginalPaper,"Chapters 3–5 transform the VNF placement and traffic routing proble into some well-known NP-hard problems, and then a heuristic or approximation method is proposed to solve it, at the expense of ignoring the network state dynamics. To bridge that gap, in this chapter, we formulate the VNF placement and traffic routing problem as a Markov Decision Process model to capture the dynamic network state transitions. In order to jointly minimize the delay and cost of NFV providers and maximize the revenue, we devise a customized Deep Reinforcement Learning (DRL) algorithm, called A-DDPG, for VNF placement and traffic routing in a real-time network. A-DDPG uses the attention mechanism to ascertain smooth network behavior within the general framework of network utility maximization (NUM). The simulation results show that A-DDPG outperforms the state-of-the-art in terms of network utility, delay, and cost.","['Engineering', 'Communications Engineering, Networks', 'Graph Theory', 'Operations Research, Management Science', 'Theory of Computation', 'Algorithm Analysis and Problem Complexity']"
doi:10.1007/978-3-031-11814-2_5,en,Multi-Echelon Inventory Optimization Using Deep Reinforcement Learning,OriginalPaper,"In this chapter, we provide an overview of inventory management within the pharmaceutical industry and how to model and optimize it. Inventory management is a highly relevant topic, as it causes high costs such as holding, shortage, and reordering costs. Especially the event of a stock-out can cause damage that goes beyond monetary damage in the form of lost sales. To minimize those costs is the task of an optimized reorder policy. A reorder policy is optimal when it minimizes the accumulated cost in every situation. However, finding an optimal policy is not trivial. First, the problem is highly stochastic as we need to consider variable demands and lead times. Second, the supply chain consists of several warehouses incl. the factory, global distribution warehouses, and local affiliate warehouses, whereby the reorder policy of each warehouse has an impact on the optimal reorder policy of related warehouses. In this context, we discuss the concept of multi-echelon inventory optimization and a methodology that is capable of capturing both, the stochastic behavior of the environment and how it is impacted by the reorder policy: Markov decision processes (MDPs). On this basis, we introduce the concept, its related benefits and weaknesses of a methodology named Reinforcement Learning (RL). RL is capable of finding (near-) optimal (reorder) policies for MDPs. Furthermore, some simulation-based results and current research directions are presented.","['Economics', 'Health Economics', 'Pharmaceutical Sciences/Technology', 'Economics, general', 'Biomedicine, general', 'Biotechnology', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1007/978-3-031-16075-2_29,en,Towards End-to-End Chase in Urban Autonomous Driving Using Reinforcement Learning,OriginalPaper,"This paper addresses the challenging task of developing an autonomous chase protocol. First, training of an autonomous vehicle capable of driving autonomously from point A to B was developed to proceed with a chase protocol as a second step. A dedicated driving setup, based on a discrete action space and a single RGB camera, was developed through a series of experiments. A dedicated curriculum learning agenda allowed to train the model capable of performing all fundamental road maneuvers. Several reward functions were proposed, which enabled effective training of the agent. In the subsequent experiments, we selected the reward function and model that produced the most significant outcome, guaranteeing that the chasing car was within 25 m of a runaway car for 63% of the episode duration. To the best of our knowledge, this work is the first one that addressed the task of the chase in urban driving using the Reinforcement Learning approach.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2600-6_17,en,Towards Efficient Edge Computing Through Adoption of Reinforcement Learning Strategies: A Review,OriginalPaper,"This paper contributes towards the mapping of the variants of Reinforcement Learning (RL) techniques to solve the key challenges of Edge Computing (EC) through broadly addressing task handling and Quality of Service (QoS) parameters. EC has bolstered ever since the advent of Industry 4.0 with computationally reliable heterogeneous mobile secured dynamic edge devices powered by an array of multifarious sensors designed on multi-edge hierarchical architectures found a strong footing on the backbone of ably equipped communication protocols to manifest their growth powered by the advent of 5G technology. However, with millions of such edge devices finding its way in a plethora of EC applications, with each having its own set of domain specific challenges, devising a suitable agent so as to sense the environment and learn from it has driven RL find its way as one of the significant tools to make the EC framework intelligent. Here we lay a good understanding of how RL has achieved noteworthy success to solve some of the pressing EC challenges, given that EC finds its use in settings of autonomous driving, content delivery, smart grid, healthcare applications and so on.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-11748-0_14,en,A Study on Efficient Reinforcement Learning Through Knowledge Transfer,OriginalPaper,"Although Reinforcement Learning (RL) algorithms have made impressive progress in learning complex tasks over the past years, there are still prevailing short-comings and challenges. Specifically, the sample-inefficiency and limited adaptation across tasks often make classic RL techniques impractical for real-world applications despite the gained representational power when combining deep neural networks with RL, known as Deep Reinforcement Learning (DRL). Recently, a number of approaches to address those issues have emerged. Many of those solutions are based on smart DRL architectures that enhance single task algorithms with the capability to share knowledge between agents and across tasks by introducing Transfer Learning (TL) capabilities. This survey addresses strategies of knowledge transfer from simple parameter sharing to privacy preserving federated learning and aims at providing a general overview of the field of TL in the DRL domain, establishes a classification framework, and briefly describes representative works in the area.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning']"
doi:10.1007/978-3-031-13786-0_1,en,"A Comprehensive Study on 5G: RAN Architecture, Enabling Technologies, Challenges, and Deployment",OriginalPaper,"The fifth-generation (5G) technology promises to provide agile, scalable, and programmable network services in order to respond to the myriad of applications and connected devices of vertical industries. It aims to boost the network capacity, throughput, energy, and spectral efficiencies while reducing latency for sub-milliseconds. In order to fulfill the diverse requirements of industrial Internet of Things (IIoT) applications, drastic changes have been proposed by several telecommunication bodies for the radio access network (RAN) and core. In this chapter, we aim to study comprehensively the 5G architectural frameworks proposed by telecommunication bodies and standards for public and private 5G networks. Furthermore, this chapter provides an in-depth study on the key 5G enabling technologies such as software-defined network (SDN), network functions virtualization (NFV), network slicing, artificial intelligence/machine learning (AI/ML), and multi-access edge computing (MEC). Moreover, 5G simulators and projects are explored and compared considering features, advantages, and limitations.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Computer Communication Networks']"
doi:10.1007/978-3-031-16035-6_4,en,One-Shot Federated Learning-based Model-Free Reinforcement Learning,OriginalPaper,"The Federated Learning (FL) paradigm is emerging as a way to train machine learning (ML) models in distributed systems. A large population of interconnected devices (i.e. Internet of Things (IoT)) acting as local learners optimize the model parameters collectively (e.g., neural networks’ weights), rather than sharing and disclosing the training data set with the server. FL approaches assume each participant has enough training data for the tasks of interest. Realistically, data collected by IoT devices may be insufficient and often unlabeled. In particular, each IoT device may only contain one or a few samples of every relevant data category, and may not have the time or interest to label them. In realistic applications, this severely limits FL’s practicality and usability. In this paper, we propose a One-Shot Federated Learning (OSFL) framework considering a FL scenario wherein the local training is carried out on IoT devices and the global aggregation is done at the level of an edge server. Moreover, we combine model-free reinforcement learning with OSFL to design a more intelligent IoT device to infer whether to label a sample automatically or request the true label for the one-shot learning set-up. We validate our system on the SODA10M dataset. Experiments show that our solution achieves better performance than DQN and RS benchmark approaches.","['Engineering', 'Computational Intelligence', 'Data Engineering']"
doi:10.1007/978-3-031-16078-3_6,en,Training Heterogeneous Features in Sequence to Sequence Tasks: Latent Enhanced Multi-filter Seq2Seq Model,OriginalPaper,"In language processing, training data with extremely large variance may lead to difficulty of language model’s convergence. It is difficult for the network parameters to adapt sentences with largely varied semantics or grammatical structures. To resolve this problem, we introduce a model that concentrates the each of the heterogeneous features in the input sentences. Build upon the encoder-decoder architecture, we design a latent-enhanced multi-filter seq2seq model (LEMS) that analyzes the input representations by introducing a latent space transformation and clustering. The representations are extracted from the final hidden state of the encoder and lie in the latent space. A latent space transformation is applied for enhancing the quality of the representations. Thus the clustering algorithm can easily separate samples based on the features of these representations. Multiple filters are trained by the features from their corresponding clusters, the heterogeneity of the training data can be resolved accordingly. We conduct two sets of comparative experiments on semantic parsing and machine translation, using the Geo-query dataset and Multi30k English-French to demonstrate the enhancement our model has made, respectively.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3951-8_46,en,"A PSO-Optimized Fixed and a PSO-Optimized Neural Network-Adaptive Traffic Signal Controllers for Traffic Improvement in Santo Domingo, Dominican Republic",OriginalPaper,"Satisfying the mobility demand is one of the biggest concerns arising with the increase of urban population. With many people in the road network, traffic congestions are present in most of the cities in the world. The Distrito Nacional in Santo Domingo, capital city of Dominican Republic, is a notorious example of this phenomenon. Unfortunately, all the efforts to improve traffic experience there have had little success. With this work, two models have been developed using Particle Swarm Optimization (PSO): a PSO-Optimized Fixed Traffic Signal Control (PSO-FTSC) and a PSO-Optimized Neural Network-Adaptive Traffic Signal Control (PSO-NN-ATSC) that uses 4 neural networks to predict phase times. The intersection of 27 de Febrero Avenue corner with Winston Churchill Avenue was simulated using Simulation of Urban Mobility (SUMO), minimizing the time loss per vehicle during optimization. These models, PSO-FTSC and PSO-NN-ATSC, present reductions of 17% and 24% of mean time loss, respectively. These promising results may lead to a decrease of fuel consumption, reducing the consequent air pollution, as well as to an improvement of businesses and people’s productivity and mental health.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-15160-6_14,en,Intelligent Load Scheduling in Cognitive Buildings: A Use Case,OriginalPaper,"In the last few years, many appliances are spreading into our houses and are daily used. Such equipment significantly improves the quality of life of people, but their use, when not well regulated, can bring a needless increment in the electricity bill. Such an increment could be mitigated by using intelligent scheduling policies that guide the users toward correct exploitation of electric devices so optimizing their use while, at the same time, saving energy, money, and time. This chapter proposes a case study in which a cognitive scheduling approach is used. Such a case study, implemented in the context of the COGITO project, is devoted to automatically scheduling electric loads in houses according to user preferences, self-produced energy, and variable energy costs.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Computer Applications']"
doi:10.1186/s13677-022-00372-9,en,Task offloading in hybrid-decision-based multi-cloud computing network: a cooperative multi-agent deep reinforcement learning,"['OriginalPaper', 'Research']","Multi-cloud computing is becoming a promising paradigm to provide abundant computation resources for Internet-of-Things (IoT) devices. For a multi-device multi-cloud network, the real-time computing requirements, frequently varied wireless channel gains and changeable network scale, make the system more dynamic. It is critical to satisfy the dynamic nature of network with different constraints of IoT devices in multi-cloud environment. In this paper, we establish a continuous-discrete hybrid decision offloading model, each device should learn to make coordinated actions, including cloud server selection, offloading ratio and local computation capacity. Therefore, both continuous-discrete hybrid decision and coordination among IoT devices are challenging. To this end, we first develop a probabilistic method to relax the discrete action (e.g. cloud server selection) to a continuous set. Then, by leveraging a centralized training and distributed execution strategy, we design a cooperative multi-agent deep reinforcement learning (CMADRL) based framework to minimize the total system cost in terms of the energy consumption of IoT device and the renting charge of cloud servers. Each IoT device acts as an agent, which not only learns efficient decentralized policies, but also relieves IoT devices’ computing pressure. Experimental results demonstrate that the proposed CMADRL could efficiently learn dynamic offloading polices at each IoT device, and significantly outperform the four state-of-the-art DRL based agents and two heuristic algorithms with lower system cost.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s12652-022-04467-8,en,Development of reinforced learning based non-linear controller for unmanned aerial vehicle,"['OriginalPaper', 'Original Research']","Design complexities of trending UAVs and the operational harsh environments necessitates Control Law formulation utilizing intelligent techniques that are both robust, model-free and adaptable. In this research, an intelligent control architecture for an experimental Unmanned Aerial Vehicle (UAV) having an unconventional inverted V-tail design, is presented. Due to unique design of the vehicle strong roll and yaw coupling exists, making the control of vehicle challenging. To handle UAV’s inherent control complexities, while keeping them computationally acceptable, a variant of distinct Deep Reinforcement learning (DRL) algorithm, namely Reformed Deep Deterministic Policy Gradient (R-DDPG) is proposed. Conventional DDPG algorithm after being modified in its learning architecture becomes capable of intelligently handling the continuous state and control space domains besides controlling the platform in its entire flight regime. The paper illustrates the application of modified DDPG algorithm (namely R-DDPG) towards the design, while the performance of the resulting controller is assessed in simulation using dynamic model of the vehicle. Nonlinear simulations were then performed to analyze UAV performance under different environmental and launch conditions. The effectiveness of the proposed strategy is further demonstrated by comparing the results with the linear controller for the same UAV whose feedback loop gains are optimized by employing technique of optimal control theory achieved through application of Linear quadratic regulator (LQR) based control strategy. The efficacy of the results and performance characteristics, demonstrated the ability of the presented algorithm to dynamically adapt to the changing environment, thereby making it suitable for UAV applications.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s13177-022-00334-0,en,Reinforcement Learning Based Control Scheme for Emergency Vehicle Preemption with Edge Computing,OriginalPaper,"This paper proposes a reinforcement learning-based collaborative multi-agent actor and critic scheme (RL-CMAS) under edge computing architecture for emergency vehicle preemption. The RL-CMAS deployed a parallel training process at the cloud side for building knowledge and well accelerating learning. Priority of message and model of message offloading strategy have been developed. The simulation results show that the proposed RL-CMAS is efficient in detecting even complex data. Finally, a comparison was made with other benchmark methods, namely, Regular scheduling algorithm, Alameddine’s DTOS algorithm, and independent multi-agent actor-critic. The result showed the proposed method outperforming the other three bench marking methods. The proposed RL-CMAS provides reduction in message processing delay, total delay, and an increase of message delivery success ratio of 14.22%, 18.21%, and 8.86% respectively.","['Engineering', 'Electrical Engineering', 'Automotive Engineering', 'Robotics and Automation', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Civil Engineering', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s11263-022-01721-6,en,Active Perception for Visual-Language Navigation,OriginalPaper,"Visual-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to take navigation actions at every step. In contrast, when humans face such a challenge, we can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make a more confident navigation decision. This work draws inspiration from human navigation behavior and endows an agent with an active perception ability for more intelligent navigation. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides (i) when and where to explore, (ii) what information is worth gathering during exploration, and (iii) how to adjust the navigation decision after the exploration. In this way, the agent is able to turn its past experiences as well as new explored knowledge to contexts for more confident navigation decision making. In addition, an external memory is used to explicitly store the visited visual environments and thus allows the agent to adopt a late action-taking strategy to avoid duplicate exploration and navigation movements. Our experimental results on two standard benchmark datasets show promising exploration strategies emerged from training, which leads to significant boost in navigation performance.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00521-022-07543-4,en,"To learn or not to learn? Evaluating autonomous, adaptive, automated traders in cryptocurrencies financial bubbles","['OriginalPaper', 'Original Article']","Financial bubbles represent a severe problem for investors. In particular, the cryptocurrency market has witnessed the bursting of different bubbles in the last decade, which in turn have had spillovers on all the markets and real economies of countries. These kinds of markets and their unique characteristics are of great interest to researchers. Generally, investors and financial operators study market trends to understand when bubbles might occur using technical analysis tools. Such tools, which have been historically used, resulted in being precious allies at the basis of more advanced systems. In this regard, different autonomous, adaptive and automated trading agents have been introduced in the literature to study several kinds of markets. Among these, we can distinguish between agents with Zero/Minimal Intelligence (ZI/MI) and Computational Intelligence (CI) -based agents. The first ones typically trade on the market without resorting to complex learning strategies; the second ones usually use (deep) reinforcement learning mechanisms. However, these trading agents have never been tested on the cryptocurrencies market and related financial bubbles, which are still mostly overlooked in the literature. It is unclear how these agents can make profits/losses before, during, and after a bubble to adjust their strategy and avoid critical situations. This paper compares a broad set of trading agents (between ZI/MI and CI ones) and evaluates them with well-known financial indicators (e.g., volatility, returns Sharpe ratio , drawdown, Sortino and Omega ratio ). Among the experiment’s outcomes, ZI/MI agents were more explainable than CI ones. Based on the results obtained above, we introduce GGSMZ , a trading agent relying on a neuro-fuzzy mechanism. The neuro-fuzzy system is able to learn from the trades performed by the agents adopted in the previous stage. GGSMZ ’s performances overcome those of other tested agents. We argue that GGSMZ could be used by investors as a decision support tool.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10586-022-03644-w,en,"Federated recommenders: methods, challenges and future",OriginalPaper,"Abstract Web users are flooded with information on the internet, and they feel overwhelmed by the different choices they have to make online daily. Recommender systems come to their rescue by suggesting products best aligned with their interests. To achieve this, traditional recommenders transfer users’ personal data from the client to the server and dig for information about the user’s interests and tastes. Moving data to the cloud violates the user confidentiality requirement and poses severe threats to user privacy and security. Moreover, with the tremendous increase in data size, it is no longer possible to collect and process massive data in the cloud. With the emergence of federated learning, numerous innovative recommender models are devised to solve these issues. In these models, the user data never leaves the client-side, and only the inferred results are sent back to the server for aggregating and updating the master model. Hence, the federated recommenders preserve user privacy and save the hassle of transferring enormous data to the cloud. This paper meticulously studies the recently proposed federated recommenders and classifies them based on the enhancements introduced in the prediction model, security scheme, or optimization technique. We identify the challenges faced by current federated recommenders and observe that most issues are inherently due to various aspects of federated learning, such as heterogeneous and non-IID data, malicious users, distributed framework, and non-reliable edge devices. While some emerge due to the coupling of the recommendation process in the federated paradigm. This research summarizes the current limitations, highlights the areas that need improvements, and presents future paths. In short, it paves the way for the development of robust federated recommenders that can handle the challenges of federated learning and, at the same time, generate high-quality recommendations.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s13177-022-00321-5,en,Intelligent Traffic Light via Policy-based Deep Reinforcement Learning,OriginalPaper,"Intelligent traffic lights in smart cities can optimally reduce traffic congestion. In this study, we employ reinforcement learning to train the control agent of a traffic light on a simulator of urban mobility. As a difference from existing works, a policy-based deep reinforcement learning method, Proximal Policy Optimization (PPO), is utilized rather than value-based methods such as Deep Q Network (DQN) and Double DQN (DDQN). First, the obtained optimal policy from PPO is compared to those from DQN and DDQN. It is found that the policy from PPO performs better than the others. Next, instead of fixed-interval traffic light phases, we adopt light phases with variable time intervals, which result in a better policy to pass the traffic flow. Then, the effects of environment and action disturbances are studied to demonstrate that the learning-based controller is robust. Finally, we consider unbalanced traffic flows and find that an intelligent traffic light can perform moderately well for the unbalanced traffic scenarios, although it learns the optimal policy from the balanced traffic scenarios only.","['Engineering', 'Electrical Engineering', 'Automotive Engineering', 'Robotics and Automation', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Civil Engineering', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s10489-022-03317-6,en,Improving indoor visual navigation generalization with scene priors and Markov relational reasoning,OriginalPaper,"The problem of visual navigation is the poor generalization to find the given target object in unexplored environment without the help of auxiliary sensors. We propose solving the visual navigation problem by incorporating object spatial scene priors and visible object relational reasoning. To get more accurate ground truth environment priors, we construct specific scene graph priors for indoor navigation, which provides rich object spatial relationships for helping finding the target objects by object relation detection. Furthermore, to imitate human’s reasonability in searching objects, we encode our scene graph priors with Markov model for relational reasoning and fuse them into reinforcement learning policy network, which improves model generalization in novel scenes. Moreover, we perform experiments on the AI2THOR virtual environment and outperform the current most state-of-the-art both in SPL (Success weighted by Path Length) and success rate on average.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s41315-022-00235-1,en,A deep reinforcement learning approach for multi-agent mobile robot patrolling,"['OriginalPaper', 'Regular Paper']","Patrolling strategies primarily deal with minimising the time taken to visit specific locations and cover an area. The use of intelligent agents in patrolling has become beneficial in automation and analysing patterns in patrolling. However, practical scenarios demand these strategies to be adaptive in various conditions and robust against adversaries. Traditional Q-learning based patrolling keeps track of all possible states and actions in a Q-table, making them susceptible to the curse of dimensionality. For multi-agent patrolling to be adaptive in various scenarios represented using graphs, we propose a formulation of the Markov Decision Process (MDP) with state-representations that can be utilised for Deep Reinforcement Learning (DRL) approaches such as Deep Q-Networks (DQN). The implemented DQN can estimate the MDP using a finite length state vector trained with a novel reward function. Proposed state-space representation is independent of the number of nodes in the graph, thereby addressing scalability to graph dimensions. We also propose a reward function to penalise the agents for lack of global coordination while providing immediate local feedback on their actions. As independent policy learners subject to the MDP and reward function, the DRL agents formed a collaborative patrolling strategy. The policies learned by the agents generalise and adapt to multiple behaviours without explicit training or design to do so. We provide empirical analysis that shows the strategy’s adaptive capabilities with changes in agents’ position, non-uniform node visit frequency requirements, changes in a graph structure representing the environment, and induced randomness in the trajectories. DRL patrolling proves to be a promising patrolling strategy for intelligent agents by potentially being scalable, adaptive, and robust against adversaries.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s10586-022-03817-7,en,Edge resource slicing approaches for latency optimization in AI-edge orchestration,OriginalPaper,"Edge service computing is an emerging paradigm for computing, storage, and communication services to optimize edge framework latency and cost based on mobile edge computing (MEC) devices. The devices are battery-enabled and have limited communication and computation resources. X consolidation is a major issue in distributed heterogeneous MEC orchestrations, where X represents the task scheduling/device selection/channel selection/offloading strategy . The network entities need to enhance network performance under uncertain circumstances for such orchestrations. Haphazard X consolidation leads to abnormal resource and energy usage, quality of service (QoS) and latency of the edge framework. However, this study concentrates on analysing the impact of reinforcement learning-based edge resource consolidation models. The models are classified according to functionality, including device resource management, service request allocation, device selection, and offloading types. Finally, the article discusses and highlights some unresolved challenges for further study on MEC orchestration to enhance offloading strategy and resource management, as well as device and channel selection efficiency.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s10586-022-03813-x,en,Security provisions in smart edge computing devices using blockchain and machine learning algorithms: a novel approach,OriginalPaper,"It is difficult to manage massive amounts of data in an overlying environment with a single server. Therefore, it is necessary to comprehend the security provisions for erratic data in a dynamic environment. The authors are concerned about the security risk of vulnerable data in a Mobile Edge based distributive environment. As a result, edge computing appears to be an excellent perspective in which training can be done in an Edge-based environment. The combination of Edge computing and consensus approach of Blockchain in conjunction with machine learning techniques can further improve data security, mitigate the possibility of exposed data, and it reduces the risk of a data breach. As a result, the concept of federated learning provides a path for training the shared data. A dataset was collected that contained several vulnerable, exposed, recovered, and secured data and data security was precepted under the surveillance of two-factor authentication. This paper discusses the evolution of data and security flaws and their corresponding solutions in smart edge computing devices. The proposed model incorporates data security using consensus approach of Blockchain and machine learning techniques that include several classifiers and optimization techniques. Further, the authors applied the proposed algorithms in an edge computing environment by distributing several batches of data to different clients. As a result, the client privacy was maintained by using Blockchain servers. Furthermore, the authors segregated the client data into batches that were trained using the federated learning technique. The results obtained in this paper demonstrate the implementation of a Blockchain-based training model in an edge-based computing environment.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1186/s13677-022-00353-y,en,A systematic review of the purposes of Blockchain and fog computing integration: classification and open issues,"['ReviewPaper', 'Review']","The fog computing concept was proposed to help cloud computing for the data processing of Internet of Things (IoT) applications. However, fog computing faces several challenges such as security, privacy, and storage. One way to address these challenges is to integrate blockchain with fog computing. There are several applications of blockchain-fog computing integration that have been proposed, recently, due to their lucrative benefits such as enhancing security and privacy. There is a need to systematically review and synthesize the literature on this topic of blockchain-fog computing integration. The purposes of integrating blockchain and fog computing were determined using a systematic literature review approach and tailored search criteria established from the research questions. In this research, 181 relevant papers were found and reviewed. The results showed that the authors proposed the combination of blockchain and fog computing for several purposes such as security, privacy, access control, and trust management. A lack of standards and laws may make it difficult for blockchain and fog computing to be integrated in the future, particularly in light of newly developed technologies like quantum computing and artificial intelligence. The findings of this paper serve as a resource for researchers and practitioners of blockchain-fog computing integration for future research and designs.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11227-022-04947-w,en,Virtual network function deployment algorithm based on graph convolution deep reinforcement learning,OriginalPaper,"In Network Function virtualization (NFV), network functions are virtualized as Virtual Network functions (VNFs). A network service consists of a set of VNFs. One of the major challenges in implementing this paradigm is allocating optimal resources to VNFs. Most existing work assumes that services are represented as service functional chains (SFC), which are chains. However, for more complex and diversified network services, a more appropriate representation is Virtual Network Function Forwarding Graph (VNF-FG), namely directed acyclic Graph (DAGs). Previous works failed to take advantage of this special graph structure, which makes them unsuitable for complex and diverse network service scenarios. Aiming at the problem of virtual network function Forwarding Graph mapping (VNF-FGE) represented by DAG, this paper proposes a virtual network function (VNF) deployment algorithm based on graph convolution network (GCN) and deep reinforcement learning (DRL), called GDRL-VNFP. First, we describe the VNF-FGE problem as an integer linear programming (ILP) problem and the virtual network service request (NSR) as a DAG. Secondly, in order to overcome the challenges brought about by the different sizes and dynamic arrival of the DAG representation of NSR, an efficient algorithm based on GCN and DRL is proposed, which can generate deployment solutions in real time under the premise of meeting the quality of service and minimize the total cost of deployment. We use GCN to extract features from the physical network topology and VNF-FG represented by DAG, and in addition, and construct a sequence-to-sequence model based on the attention mechanism for the output mapping scheme. Finally, we trained the model using a policy-based gradient-based reinforcement learning algorithm that could quickly find a near-optimal solution for the problem instance. Simulation results show that our GDRL-VNFP outperforms state-of-the-art solutions in terms of deployment cost, service request reception rate and runtime.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s43684-022-00045-z,en,Multi-agent reinforcement learning for autonomous vehicles: a survey,"['ReviewPaper', 'Review']","In the near future, autonomous vehicles (AVs) may cohabit with human drivers in mixed traffic. This cohabitation raises serious challenges, both in terms of traffic flow and individual mobility, as well as from the road safety point of view. Mixed traffic may fail to fulfill expected security requirements due to the heterogeneity and unpredictability of human drivers, and autonomous cars could then monopolize the traffic. Using multi-agent reinforcement learning (MARL) algorithms, researchers have attempted to design autonomous vehicles for both scenarios, and this paper investigates their recent advances. We focus on articles tackling decision-making problems and identify four paradigms. While some authors address mixed traffic problems with or without social-desirable AVs, others tackle the case of fully-autonomous traffic. While the latter case is essentially a communication problem, most authors addressing the mixed traffic admit some limitations. The current human driver models found in the literature are too simplistic since they do not cover the heterogeneity of the drivers’ behaviors. As a result, they fail to generalize over the wide range of possible behaviors. For each paper investigated, we analyze how the authors formulated the MARL problem in terms of observation, action, and rewards to match the paradigm they apply.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Control and Systems Theory', 'Machine Learning']"
doi:10.1007/s10489-022-04281-x,en,A Q-based policy gradient optimization approach for Doudizhu,OriginalPaper,"Deep reinforcement learning (DRL) has recently been employed in various games, with which superhuman intelligence has been achieved, including Atari, Go, no-limit, and Texas hold’em. However, this technique has not been fully considered for Doudizhu which is a popular poker game in Asia and involves confrontation and cooperation among multiple players with imperfect information. In this paper we present a new deep reinforcement learning approach NV-Dou for the game Doudizhu. It adopts a variant of neural fictitious self-play to approximate the Nash equilibria of the game. The loss functions of the neural network integrate Q-Based policy gradient (mean actor critic) with advantage learning and proximal policy optimization. In addition, parametric noises are adopted for the fully connected layers in the neural network. The experimental results show that it needs only a few hours of training and achieves almost state-of-the-art performance comparing with the well-known open implementations RHCP, CQL, MCTS and others for Doudizhu.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s40295-022-00354-8,en,Space-Based Sensor Tasking Using Deep Reinforcement Learning,"['OriginalPaper', 'Original Article']","To maintain a robust catalog of resident space objects (RSOs), space situational awareness (SSA) mission operators depend on ground- and space-based sensors to repeatedly detect, characterize, and track objects in orbit. Although some space sensors are capable of monitoring large swaths of the sky with wide fields of view (FOVs), others—such as maneuverable optical telescopes, narrow-band imaging radars, or satellite laser-ranging systems—are restricted to relatively narrow FOVs and must slew at a finite rate from object to object during observation. Since there are many objects that a narrow FOV sensor could choose to observe within its field of regard (FOR), it must schedule its pointing direction and duration using some algorithm. This combinatorial optimization problem is known as the sensor-tasking problem. In this paper, we developed a deep reinforcement learning agent to task a space-based narrow-FOV sensor in low Earth orbit (LEO) using the proximal policy optimization algorithm. The sensor’s performance—both as a singular sensor acting alone, but also as a complement to a network of taskable, narrow-FOV ground-based sensors—is compared to the greedy scheduler across several figures of merit, including the cumulative number of RSOs observed and the mean trace of the covariance matrix of all of the observable objects in the scenario. The results of several simulations are presented and discussed. Additionally, the results from an LEO SSA sensor in different orbits are evaluated and discussed, as well as various combinations of space-based sensors.","['Engineering', 'Aerospace Technology and Astronautics', 'Mathematical Applications in the Physical Sciences', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)']"
doi:10.1186/s13677-022-00352-z,en,Reinforcement learning empowered multi-AGV offloading scheduling in edge-cloud IIoT,"['OriginalPaper', 'Research']","The edge-cloud computing architecture has been introduced to industrial circles to ensure the time constraints for industrial computing tasks. Besides the central cloud, various numbers of edge servers (ESes) are deployed in a distributed manner. In the meantime, most large factories currently use auto guided vehicles (AGVs). They usually travel along a given route and can help offload tasks to ESes. An ES maybe accessed by multiple AGVs, thus incurring offloading and processing delays due to resource competition. In this paper, we investigate the offloading scheduling issue for cyclical tasks and put forth the Multi-AGV Cyclical Offloading Optimization (MCOO) algorithm to reduce conflicts. The solution divides the offloading optimization problem into two parts. Firstly, the load balancing algorithm and greedy algorithm are utilized to find the optimal allocation of tasks for a single AGV under limited conditions. Then, multiple AGVs are asynchronously trained by applying the Reinforcement Learning-based A3C algorithm to optimize the offloading scheme. The simulation results show that the MCOO algorithm improves the global offloading performance both in task volume and adaptability compared with the baseline algorithms.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11227-022-04924-3,en,Automated cloud resources provisioning with the use of the proximal policy optimization,OriginalPaper,"Many modern applications, both scientific and commercial, are deployed to cloud environments and often employ multiple types of resources. That allows them to efficiently allocate only the resources which are actually needed to achieve their goals. However, in many workloads the actual usage of the infrastructure varies over time, which results in over-provisioning and unnecessarily high costs. In such cases, automatic resource scaling can provide significant cost savings by provisioning only the amount of resources which are necessary to support the current workload. Unfortunately, due to the complex nature of distributed systems, automatic scaling remains a challenge. Reinforcement learning domain has been recently a very active field of research. Thanks to combining it with Deep Learning, many newly designed algorithms improve the state of the art in many complex domains. In this paper we present the results of our attempt to use the recent advancements in Reinforcement Learning to optimize the cost of running a compute-intensive evolutionary process by automating the scaling of heterogeneous resources in a compute cloud environment. We describe the architecture of our system and present evaluation results. The experiments include autonomous management of a sample workload and a comparison of its performance to the traditional automatic threshold-based management approach. We also provide the details of training of the management policy using the proximal policy optimization algorithm. Finally, we discuss the feasibility to extend the presented approach to further scenarios.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1038/s41598-022-23924-0,en,SimTune: bridging the simulator reality gap for resource management in edge-cloud computing,"['OriginalPaper', 'Article']","Industries and services are undergoing an Internet of Things centric transformation globally, giving rise to an explosion of multi-modal data generated each second. This, with the requirement of low-latency result delivery, has led to the ubiquitous adoption of edge and cloud computing paradigms. Edge computing follows the data gravity principle, wherein the computational devices move closer to the end-users to minimize data transfer and communication times. However, large-scale computation has exacerbated the problem of efficient resource management in hybrid edge-cloud platforms. In this regard, data-driven models such as deep neural networks (DNNs) have gained popularity to give rise to the notion of edge intelligence. However, DNNs face significant problems of data saturation when fed volatile data. Data saturation is when providing more data does not translate to improvements in performance. To address this issue, prior work has leveraged coupled simulators that, akin to digital twins, generate out-of-distribution training data alleviating the data-saturation problem. However, simulators face the reality-gap problem, which is the inaccuracy in the emulation of real computational infrastructure due to the abstractions in such simulators. To combat this, we develop a framework, SimTune, that tackles this challenge by leveraging a low-fidelity surrogate model of the high-fidelity simulator to update the parameters of the latter, so to increase the simulation accuracy. This further helps co-simulated methods to generalize to edge-cloud configurations for which human encoded parameters are not known apriori. Experiments comparing SimTune against state-of-the-art data-driven resource management solutions on a real edge-cloud platform demonstrate that simulator tuning can improve quality of service metrics such as energy consumption and response time by up to 14.7% and 7.6% respectively.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10489-022-04227-3,en,Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent,OriginalPaper,"The industrial application of Deep Reinforcement Learning (DRL) is frequently slowed down due to an inability to generate the experience required to train the models. Collecting data often involves considerable time and financial outlays that can make it unaffordable. Fortunately, devices like robots can be trained with synthetic experience through virtual environments. With this approach, the problems of sample efficiency with artificial agents are mitigated, but another issue arises: the need to efficiently transfer the synthetic experience into the real world (sim-to-real). This paper analyzes the robustness of a state-of-the-art sim-to-real technique known as Progressive Neural Networks (PNNs) and studies how adding diversity to the synthetic experience can complement it. To better understand the drivers that lead to a lack of robustness, the robotic agent is still tested in a virtual environment to ensure total control on the divergence between the simulated and real models. The results show that a PNN-like agent exhibits a substantial decrease in its robustness at the beginning of the real training phase. Randomizing specific variables during simulation-based training significantly mitigates this issue. The average increase in the model’s accuracy is around 25% when diversity is introduced in the training process. This improvement can translate into a decrease in the number of real experiences required for the same final robust performance. Notwithstanding, adding real experience to agents should still be beneficial, regardless of the quality of the virtual experience fed to the agent. The source code is available at: https://gitlab.com/comillas-cic/sim-to-real/pnn-dr.git .","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-04225-5,en,Learning controlled and targeted communication with the centralized critic for the multi-agent system,OriginalPaper,"Multi-agent deep reinforcement learning (MDRL) has attracted attention for solving complex tasks. Two main challenges of MDRL are non-stationarity and partial observability from the perspective of agents, impacting the performance of agents’ learning cooperative policies. In this study, Controlled and Targeted Communication with the Centralized Critic (COTAC) is proposed, thereby constructing the paradigm of centralized learning and decentralized execution with partial communication. It is capable of decoupling how the MAS obtains environmental information during training and execution. Specifically, COTAC can make the environment faced by agents to be stationarity in the training phase and learn partial communication to overcome the limitation of partial observability in the execution phase. Based on this, decentralized actors learn controlled and targeted communication and policies optimized by centralized critics during training. As a result, agents comprehensively learn when to communicate during the sending and how to target information aggregation during the receiving. Apart from that, COTAC is evaluated on two multi-agent scenarios with continuous space. Experimental results demonstrated that partial agents with important information choose to send messages and targeted aggregate received information by identifying the relevant important information, which can still have better cooperation performance while reducing the communication traffic of the system.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10994-022-06243-3,en,Scaling up stochastic gradient descent for non-convex optimisation,OriginalPaper,"Stochastic gradient descent (SGD) is a widely adopted iterative method for optimizing differentiable objective functions. In this paper, we propose and discuss a novel approach to scale up SGD in applications involving non-convex functions and large datasets. We address the bottleneck problem arising when using both shared and distributed memory. Typically, the former is bounded by limited computation resources and bandwidth whereas the latter suffers from communication overheads. We propose a unified distributed and parallel implementation of SGD (named DPSGD) that relies on both asynchronous distribution and lock-free parallelism. By combining two strategies into a unified framework, DPSGD is able to strike a better trade-off between local computation and communication. The convergence properties of DPSGD are studied for non-convex problems such as those arising in statistical modelling and machine learning. Our theoretical analysis shows that DPSGD leads to speed-up with respect to the number of cores and number of workers while guaranteeing an asymptotic convergence rate of $$O(1/\sqrt{T})$$ O ( 1 / T ) given that the number of cores is bounded by $$T^{1/4}$$ T 1 / 4 and the number of workers is bounded by $$T^{1/2}$$ T 1 / 2 where T is the number of iterations. The potential gains that can be achieved by DPSGD are demonstrated empirically on a stochastic variational inference problem (Latent Dirichlet Allocation) and on a deep reinforcement learning (DRL) problem (advantage actor critic - A2C) resulting in two algorithms: DPSVI and HSA2C. Empirical results validate our theoretical findings. Comparative studies are conducted to show the performance of the proposed DPSGD against the state-of-the-art DRL algorithms.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s12652-020-01905-3,en,Multi-step medical image segmentation based on reinforcement learning,"['OriginalPaper', 'Original Research']","Image segmentation technology has made a remarkable effect in medical image analysis and processing, which is used to help physicians get a more accurate diagnosis. Manual segmentation of the medical image requires a lot of effort by professionals, which is also a subjective task. Therefore, developing an advanced segmentation method is an essential demand. We propose an end-to-end segmentation method for medical images, which mimics physicians delineating a region of interest (ROI) on the medical image in a multi-step manner. This multi-step operation improves the performance from a coarse result to a fine result progressively. In this paper, the segmentation process is formulated as a Markov decision process and solved by a deep reinforcement learning (DRL) algorithm, which trains an agent for segmenting ROI in images. The agent performs a serial action to delineate the ROI. We define the action as a set of continuous parameters. Then, we adopted a DRL algorithm called deep deterministic policy gradient to learn the segmentation model in continuous action space. The experimental result shows that the proposed method has 7.24% improved to the state-of-the-art method on three prostate MR data sets and has 3.52% improved on one retinal fundus image data set.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s11227-022-04911-8,en,A federated multi-agent deep reinforcement learning for vehicular fog computing,OriginalPaper,"Vehicular fog computing is an emerging paradigm for delay-sensitive computations. In this highly dynamic resource-sharing environment, optimal offloading decision for effective resource utilization is a challenging task. In recent years, deep reinforcement learning has emerged as an effective approach for dealing with resource allocation problems because of its self-adapting nature in a large state space scenario. However, due to high mobility and rapid changes in the network topology cause fluctuating task arrival rate. Similarly, the data sharing between the vehicles and the fog nodes raises a variety of security and privacy concerns. Therefore, the proposed system is based on local and global model training approaches. In this paper, we propose a federated multi-agent deep reinforcement learning solution that efficiently learns task-offloading decisions at multiple tiers i.e. locally and globally. The proposed work results in fast convergence due to its collaborative learning model among vehicles and fog servers. The local model runs at the vehicular nodes, and the global model runs at the fog servers. To reduce network overhead, the models are learned locally; thus, limited information is shared across the network this reduces the communication overhead and improves the privacy of the agents. The proposed system is compared with the greedy and stochastic approaches in terms of residence times, cost, delivery rate, and utilization ratio. We observed that the proposed approach has significantly reduced the task residence time, end-to-end delay and overall system cost.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s10462-022-10299-x,en,Deep multiagent reinforcement learning: challenges and directions,OriginalPaper,"This paper surveys the field of deep multiagent reinforcement learning (RL). The combination of deep neural networks with RL has gained increased traction in recent years and is slowly shifting the focus from single-agent to multiagent environments. Dealing with multiple agents is inherently more complex as (a) the future rewards depend on multiple players’ joint actions and (b) the computational complexity increases. We present the most common multiagent problem representations and their main challenges, and identify five research areas that address one or more of these challenges: centralised training and decentralised execution, opponent modelling, communication, efficient coordination, and reward shaping. We find that many computational studies rely on unrealistic assumptions or are not generalisable to other settings; they struggle to overcome the curse of dimensionality or nonstationarity. Approaches from psychology and sociology capture promising relevant behaviours, such as communication and coordination, to help agents achieve better performance in multiagent settings. We suggest that, for multiagent RL to be successful, future research should address these challenges with an interdisciplinary approach to open up new possibilities in multiagent RL.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10462-022-10286-2,en,"AI-big data analytics for building automation and management systems: a survey, actual challenges and future perspectives",OriginalPaper,"In theory, building automation and management systems (BAMSs) can provide all the components and functionalities required for analyzing and operating buildings. However, in reality, these systems can only ensure the control of heating ventilation and air conditioning system systems. Therefore, many other tasks are left to the operator, e.g. evaluating buildings’ performance, detecting abnormal energy consumption, identifying the changes needed to improve efficiency, ensuring the security and privacy of end-users, etc. To that end, there has been a movement for developing artificial intelligence (AI) big data analytic tools as they offer various new and tailor-made solutions that are incredibly appropriate for practical buildings’ management. Typically, they can help the operator in (i) analyzing the tons of connected equipment data; and; (ii) making intelligent, efficient, and on-time decisions to improve the buildings’ performance. This paper presents a comprehensive systematic survey on using AI-big data analytics in BAMSs. It covers various AI-based tasks, e.g. load forecasting, water management, indoor environmental quality monitoring, occupancy detection, etc. The first part of this paper adopts a well-designed taxonomy to overview existing frameworks. A comprehensive review is conducted about different aspects, including the learning process, building environment, computing platforms, and application scenario. Moving on, a critical discussion is performed to identify current challenges. The second part aims at providing the reader with insights into the real-world application of AI-big data analytics. Thus, three case studies that demonstrate the use of AI-big data analytics in BAMSs are presented, focusing on energy anomaly detection in residential and office buildings and energy and performance optimization in sports facilities. Lastly, future directions and valuable recommendations are identified to improve the performance and reliability of BAMSs in intelligent buildings.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10489-022-04105-y,en,A review of cooperative multi-agent deep reinforcement learning,OriginalPaper,"Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. The aim of this review article is to provide an overview of recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. Our classification of MARL approaches includes five categories for modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critics, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. We first discuss each of these methods, their potential challenges, and how these challenges were mitigated in the relevant papers. Additionally, we make connections among different papers in each category if applicable. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. In light of MARL’s recent success in real-world applications, we have dedicated a section to reviewing these applications and articles. This survey also provides a list of available environments for MARL research. Finally, the paper is concluded with proposals on possible research directions.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11280-022-01112-4,en,Profit-based deep architecture with integration of reinforced data selector to enhance trend-following strategy,OriginalPaper,"Despite the popularity of trend-following strategies in financial markets, they often lack adaptability to the emerging varied markets. Recently, deep learning (DL) methods demonstrate the effectiveness in stock-market analysis. Thus, the application of DL methods to enhance trend-following strategies has received substantial attention. However, there are two key challenges to be solved before the adoption of DL methods in enhancing trend-following strategies: (1) how to design an effective data selector to include more related data? (2) how to design a profit-based model to enhance strategies? To address these two challenges, this paper contributes to a new framework, namely profit-based deep architecture with the integration of reinforced data selector (PDA-RDS) to improve the effectiveness of DL methods. In particular, profit-based deep architecture (PDA) integrates a dynamic profit weight and a focal loss function to obtain high profits. In addition, reinforced data selector (RDS) is constructed to select high-quality training samples and a training-aware immediate reward is designated to improve the effectiveness of RDS. Extensive experiments on both U.S. and China stock-market datasets demonstrate that PDA-RDS outperforms the state-of-the-art baseline methods in terms of higher cumulative percentage rate and average percentage rate, both of which are crucial to investment strategies.","['Computer Science', 'Information Systems Applications (incl.Internet)', 'Database Management', 'Operating Systems']"
doi:10.1007/s11227-022-04867-9,en,Resource orchestration in network slicing using GAN-based distributional deep Q-network for industrial applications,OriginalPaper,"The Industrial Internet of Things (IIoT) is an emerging and promising concept that allows intelligent manufacturing through the connectivity of 5G/6G and the interaction of industrial production units. The introduction of network slicing in 5G and beyond has made it possible to manage and allocate resources to various applications according to their requirements. In this paper, we study network slicing within a radio access network containing IIoT devices which include base stations that share the same physical infrastructure. We use deep reinforcement learning-based resource orchestration technique to achieve variable service demands of environment state-value and resource allocation as environment action-value. We describe the cognitive decision objectives to maximise the optimal policy for IIoT reward by achieving higher system throughput, spectral efficiency (SE), service level agreement (SLA), transmission packet rate with low power consumption and transmission delay. We use generative adversarial network-based deep distributional noisy Q -networks (GAN–NoisyNet) to learn the action-value distribution. Furthermore, we introduce dueling GAN–NoisyNet, which employs a duel generator that estimates the action advantage function and state-value distribution. Finally, we conduct extensive simulations to verify the performance of the proposed GAN–NoisyNet and dueling GAN–NoisyNet.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s41365-022-01107-w,en,Machine learning-based analyses for total ionizing dose effects in bipolar junction transistors,OriginalPaper,"Machine learning methods have proven to be powerful in various research fields. In this paper, we show that research on radiation effects could benefit from such methods and present a machine learning-based scientific discovery approach. The total ionizing dose (TID) effects usually cause gain degradation of bipolar junction transistors (BJTs), leading to functional failures of bipolar integrated circuits. Currently, many experiments of TID effects on BJTs have been conducted at different laboratories worldwide, producing a large amount of experimental data, which provides a wealth of information. However, it is difficult to utilize these data effectively. In this study, we proposed a new artificial neural network (ANN) approach to analyze the experimental data of TID effects on BJTs. An ANN model was built and trained using data collected from different experiments. The results indicate that the proposed ANN model has advantages in capturing nonlinear correlations and predicting the data. The trained ANN model suggests that the TID hardness of a BJT tends to increase with base current I B0 . A possible cause for this finding was analyzed and confirmed through irradiation experiments.","['Physics', 'Particle and Nuclear Physics', 'Particle Acceleration and Detection, Beam Physics', 'Nuclear Energy']"
doi:10.1038/s41598-022-21125-3,en,Adaptability and sustainability of machine learning approaches to traffic signal control,"['OriginalPaper', 'Article']","This study investigates how adaptable Machine Learning Traffic Signal control methods are to topological variability. We ask how well can these methods generalize to non-Manhattan-like networks with non-uniform distances between intersections. A Machine Learning method that is highly reliable in various topologies is proposed and compared with state-of-the-art alternatives. Lastly, we analyze the sustainability of different traffic signal control methods based on computational efforts required to achieve convergence and perform training and testing. We show that our method achieves an approximately seven-fold improvement in terms of CO $$_2$$ 2 emitted in training over the second-best method.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00170-022-09877-8,en,Deep reinforcement learning applied to an assembly sequence planning problem with user preferences,"['OriginalPaper', 'ORIGINAL ARTICLE']","Deep reinforcement learning (DRL) has demonstrated its potential in solving complex manufacturing decision-making problems, especially in a context where the system learns over time with actual operation in the absence of training data. One interesting and challenging application for such methods is the assembly sequence planning (ASP) problem. In this paper, we propose an approach to the implementation of DRL methods in ASP. The proposed approach introduces in the RL environment parametric actions to improve training time and sample efficiency and uses two different reward signals: (1) user’s preferences and (2) total assembly time duration. The user’s preferences signal addresses the difficulties and non-ergonomic properties of the assembly faced by the human and the total assembly time signal enforces the optimization of the assembly. Three of the most powerful deep RL methods were studied, Advantage Actor-Critic (A2C), Deep Q-Learning (DQN), and Rainbow, in two different scenarios: a stochastic and a deterministic one. Finally, the performance of the DRL algorithms was compared to tabular Q-Learnings performance. After 10,000 episodes, the system achieved near optimal behaviour for the algorithms tabular Q-Learning, A2C, and Rainbow. Though, for more complex scenarios, the algorithm tabular Q-Learning is expected to underperform in comparison to the other 2 algorithms. The results support the potential for the application of deep reinforcement learning in assembly sequence planning problems with human interaction.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s40747-022-00735-4,en,Research on energy saving technology at mobile edge networks of IoTs based on big data analysis,"['OriginalPaper', 'Original Article']","The 5G IoT is very complicated and there are many factors that affect the network performance. Presently, the optimization of network is still the focus of research. Although the existing literature has done a large number of researches in this aspect, there have always been problems, such as complex algorithms. Based on the previous research, we propose a big data mining analysis method, which improves the comprehensive performance of the network by analyzing the relationship of massive data variables so as to optimize the combination of the network. In this paper, according to each of terminal variables at any moment such as power consumption, bandwidth, noise power, subcarrier bandwidth, interference power and coding efficiency, etc. we develop the mathematical modeling of principal component multiple regression. Then we simulate this scheme by edge computing technology and combine it with intelligent algorithms. The research results show that this method can effectively predict the data concerned, and the residual is the smallest. Therefore, the research provides an important basic for application of the approach to the mobile edge network optimization of IoTs.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10462-022-10138-z,en,Design possibilities and challenges of DNN models: a review on the perspective of end devices,OriginalPaper,"Deep Neural Network (DNN) models for both resource-rich environments and resource-constrained devices have become abundant in recent years. As of now, the literature on different available options for the design, development, and deployment of DNN models to resource constrained-end devices is limited and demands extensive further study. This paper reviews vital research efforts for the design of DNN models while deploying them at the end devices such as smart cameras for real-time object detection tasks. The design ideas include the types of DNN models, hardware and software requirements for the development, resource constraints imposed by the computing devices, and the optimization techniques required for the efficient processing of DNN. The study also aims to conduct a systematic literature review on current trends in different real-time applications of DNN models and explores the following four dimensions: (1) DNN model perspective: to associate appropriate DNN models with the proper hardware to achieve optimal throughput. (2) Hardware perspective: to answer different available options in hardware platforms for achieving on-device intelligence. (3) Resources and optimization perspective: to analyze the type of resource limitations in hardware platforms and the use of optimization techniques to overcome the performance issues. (4) Application perspective: to understand the real-time uses of DNN models in different application domains. This work also explores different performance measures that need to be considered for on-device intelligence and provides possible future directions for the challenges reviewed.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11063-022-10796-8,en,Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships,OriginalPaper,"Embodied Artificial Intelligence has become popular in recent years. Its task shifts from focusing on internet images to active settings, involving an embodied agent to perceive and act within 3D environments. In this paper, we study the Target-driven Visual Navigation (TDVN) in 3D indoor scenes using deep reinforcement learning techniques. The generalization of TDVN is a long-standing ill-posed issue, where the agent is expected to transfer intelligent knowledge from training domains to unseen domains. To address this issue, we propose a model that combines visual and relational graph features to learn the navigation policy. Graph convolutional networks are used to obtain graph features, which encodes spatial relations between objects. We also adopt a Target Skill Extension module to generate sub-targets, in order to allow the agent to learn from its failures. For evaluation, we perform experiments in the AI2-THOR. Experimental results show that our proposed model outperforms baselines under various metrics.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s10586-022-03573-8,en,Intelligent energy aware approaches for residential buildings: state-of-the-art review and future directions,OriginalPaper,"In the past decade, the world’s energy consumption is increasing largely, while residential buildings are the primary sector consuming about a quarter of the total energy produced. The researchers have made significant efforts to reduce energy usage in previous years by implementing energy monitoring and prediction techniques. Further, these techniques have been utilized for energy optimization in residential buildings and provide the consumer awareness about the usage patterns. In this paper, intelligent energy aware approaches have been reviewed by focusing on energy monitoring, prediction, optimization and performance evaluation using benchmark energy datasets. This review has been concluded with a discussion on future research directions for improving energy aware approaches in residential buildings.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00521-022-07803-3,en,Deep reinforcement learning for automated search of model parameters: photo-fenton wastewater disinfection case study,"['OriginalPaper', 'Original Article']","Numerical optimization solves problems that are analytically intractable at the cost of arriving at a sufficiently good but rarely optimal solution. To maximize the result, optimization algorithms are run with the guidance and supervision of a human, usually an expert in the problem. Recent advances in deep reinforcement learning motivate interest in an artificial agent capable of learning to do the expert’s task. Specifically, we present a proximal policy optimization agent that learns to optimize in a real case study such as the modeling of the photo-fenton disinfection process, which involves a number of parameters that have to be adjusted to minimize the error of the model with respect to the experimental data collected in several trials. The expert spends an average of 4 h to find a suitable set of parameters. On the other hand, the agent we present does not require a human expert to guide or validate the optimization procedure and achieves similar results in $$2.5\times$$ 2.5 × less time.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1186/s12859-022-04912-7,en,A reinforcement learning approach for protein–ligand binding pose prediction,"['OriginalPaper', 'Research']","Protein ligand docking is an indispensable tool for computational prediction of protein functions and screening drug candidates. Despite significant progress over the past two decades, it is still a challenging problem, characterized by the still limited understanding of the energetics between proteins and ligands, and the vast conformational space that has to be searched to find a satisfactory solution. In this project, we developed a novel reinforcement learning (RL) approach, the asynchronous advantage actor-critic model (A3C), to address the protein ligand docking problem. The overall framework consists of two models. During the search process, the agent takes an action selected by the actor model based on the current location. The critic model then evaluates this action and predict the distance between the current location and true binding site. Experimental results showed that in both single- and multi-atom cases, our model improves binding site prediction substantially compared to a naïve model. For the single-atom ligand, copper ion (Cu 2+ ), the model predicted binding sites have a median root-mean-square-deviation (RMSD) of 2.39 Å to the true binding sites when starting from random starting locations. For the multi-atom ligand, sulfate ion (SO 4 2− ), the predicted binding sites have a median RMSD of 3.82 Å to the true binding sites. The ligand-specific models built in this study can be used in solvent mapping studies and the RL framework can be readily scaled up to larger and more diverse sets of ligands.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s13042-022-01641-4,en,Modular transfer learning with transition mismatch compensation for excessive disturbance rejection,"['OriginalPaper', 'Original Article']","Underwater robots in shallow waters usually suffer from strong wave forces, which may frequently exceed robot’s control constraints. Learning-based controllers are suitable for disturbance rejection control, but the excessive disturbances heavily affect the state transition in Markov Decision Process (MDP) or Partially Observable Markov Decision Process (POMDP). This issue is amplified by training-test model mismatch. In this paper, we propose a transfer reinforcement learning algorithm using Transition Mismatch Compensation (TMC), that learns an additional compensatory policy through minimizing mismatch of transitions predicted by the two dynamics models of the source and target tasks. A modular network of learning policies is applied, composed of a Generalized Control Policy (GCP) and an Online Disturbance Identification Model (ODI). GCP is first trained over a wide array of disturbance waveforms. ODI then learns to use past states and actions of the system to predict the disturbance waveforms which are provided as input to GCP (along with the system state). We demonstrated on a pose regulation task in simulation that TMC is able to successfully reject the disturbances and stabilize the robot under an empirical model of the robot system, meanwhile improve sample efficiency.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s00521-022-07065-z,en,Learning from data streams for automation and orchestration of 6G industrial IoT: toward a semantic communication framework,"['OriginalPaper', 'S.I.: LSNC & OUA']","Established methods of communication are based mainly on Shannon’s theory of information, which purposefully overlooks semantic elements of communication. The future wireless technology should promise to facilitate many services, based on content, needs, and semantics, precisely customized to network capabilities. This gave rise to significant concern for Semantic Communication (SC), a novel paradigm considering the message’s meaning during transmission. Federated learning (FL) and Asynchronous Advantage Actor Critic (A3C) are the two emerging distributed and artificially intelligent approaches that provide diverse and possibly massive network coverage for data-driven SC solutions of industry 4.0 automation. Although SC is still in an early development stage, FL-empowered architecture has been recognized as one of the most promising solutions to meet the ubiquitous intelligence in the anticipated sixth-generation (6G) networks. This paper identifies industry 4.0 automation needs that drive the convergence of artificial intelligence and 6G for learning from data streams. We develop a novel SC framework based on the FL and A3C networks and discuss its potential along with transfer learning to address most of the new difficulties anticipated in 6G for industrial communication networks. Our proposed framework has been evaluated with extensive simulation results.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-021-03084-w,en,Reinforcement learning based connectivity restoration in wireless sensor networks,OriginalPaper,"Connectivity is a critical prerequisite for the effective operation of data gathering and forwarding procedures in Wireless Sensor Networks (WSNs). Failure of several sensor nodes in the network makes the base station incapable of receiving data from all the portions of the target area. Multiple partitions that are unable to communicate with one another are formed. Such networks are repaired using additional mobile relays. These relays cooperatively work together to create links among the partitions. In order to complete their task, they require a quick, communication-efficient, and machine learning-based approach. Reinforcement learning has evolved as a very efficient approach with long-term solutions in solving such problems. In this work, we propose a Reinforcement Learning-based Connectivity Restoration (RLCR) method that applies an intelligent machine learning algorithm for collaborative movement and connection establishment among partitions using relay nodes. It takes into account the actions of other agents and is capable of learning complicated multi-agent coordinating strategies. In a subsequent step, relay selection and connectivity maintenance have also been included. In RLCR, relays search for isolated partitions while maintaining communication with one another. Besides, we use Python to simulate the procedure and compare the results to various current methodologies. The experimental analysis illustrates that the proposed RLCR method performs better than the standard algorithms.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11227-022-04478-4,en,Enhancing gas detection-based swarming through deep reinforcement learning,OriginalPaper,"Swarm-Intelligence (SI), the collective behavior of decentralized and self-organized system, is used to efficiently carry out practical missions in various environments. To guarantee the performance of swarm, it is highly important that each object operates as an individual system while the devices are organized as simple as possible. This paper proposes an efficient, scalable, and practical swarming system using gas detection device. Each object of the proposed system has multiple sensors and detects gas in real time. To let the objects move toward gas rich spot, we propose two approaches for system design, vector-sum based, and Reinforcement Learning (RL) based. We firstly introduce our deterministic vector-sum-based approach and address the RL-based approach to extend the applicability and flexibility of the system. Through system performance evaluation, we validated that each object with a simple device configuration performs its mission perfectly in various environments.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s13042-022-01549-z,en,A sequential attention interface with a dense reward function for mitosis detection,"['OriginalPaper', 'Original Article']","The work aims to develop a fast detection method for instances of mitosis in breast cell sections, which needs time-consuming and labor-intensive searches. The system consists of two sequential processes. The first involves data pre-processing to avoid confusing images transferring to the successive detection procedure from wasteful computations. The input data is filtered using the blue ratio threshold to remove unnecessary background information and increase the color difference between the target and the non-target. Cropped images of suspicious candidates are classified as mitotic or non-mitotic employing a hard attention model, which only grapes the fine trained features locally and detailly instead of the entire picture. There is less computational complexity in terms of efficiency and performance because there are fewer parameters and smaller image sizes, so the proposed classification system outperforms traditional models, such as LEnet-5 and VGG-19, for the benchmarked data set provided in the TPAC2016 competition data sets. The proposed method is also compared to other methods listed on a ranking table for the ICPR2012 competition using its official test data set.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s00521-021-06738-5,en,Pipeline risk big data intelligent decision-making system based on machine learning and situation awareness,"['OriginalPaper', 'S.I: LSNC & OUAI']","Underground pipelines are an indispensable part of urban public facilities. However, the frequent occurrence of pipeline accidents in recent years has not only brought great inconvenience to people’s lives, but also affected people’s lives and property safety to a certain extent. Therefore, timely treatment and treatment are very important. Preventing sudden underground pipeline accidents plays an important role in improving urban livability. This article studies pipeline risk big data intelligent decision-making systems based on machine learning and situational awareness. In this paper, by analyzing the application scope of gas leakage and diffusion models under different modes, leakage, diffusion, fire and explosion models are determined, and a combined model framework of leakage accident consequence system analysis is formed. The system uses the pipeline failure probability model and the pipeline failure consequence analysis model to determine the pipeline failure probability, the probability and the consequences of each accident; it uses the spatial analysis ability of GIS technology to determine the accident impact area and displays the impact area in graphics form. Through the effect verification of the test set, the prediction result of the SVR model based on the grid search parameter, the relative percentage error of the predicted value of each sample and the true value fluctuate is in the range of 4%-36%, and the amplitude is not very large. Most of the error values are approximately 13.56% of the MAPE value. The results show that the optimization method using grid search parameters can have better prediction performances.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11227-022-04493-5,en,Resource management and switch migration in SDN-based multi-access edge computing environments,OriginalPaper,"As the growing of data volumes due to the successive development of new mobile devices and the creation of new applications, the emergence of multi-access edge computing can successfully improve quality of service based on reduced latency and lower system energy consumption. The introduction of software-defined networking technologies in multi-access edge computing environments supports access to more network devices and enhances the scalability and service management flexibility of mobile edge computing environments. The limited nature of computing resources in mobile edge computing environments makes resource management a critical issue. Therefore, to minimize the energy consumption and latency of task execution in mobile edge computing environment, and to ensure reasonable resource allocation during task execution, a resource management strategy based on multi-objective optimization in edge computing environment is proposed. In this strategy, the overall energy consumption weighting and minimization problem is solved by optimizing the management of communication and computing resources, and an improved NSGA-II algorithm is proposed to rationally allocate communication and computational resources for each task. To deal with load imbalance caused by large traffic fluctuations in multi-access edge computing environments based on software-defined networks, in this paper, a load-balancing-oriented switch migration strategy is proposed in which a switch migration algorithm based on an improved ant colony algorithm is proposed to optimally select the switch migration process so that the static deployment of the controller adapts to the changing needs of dynamic flows in the network. Experimental results demonstrate that the proposed resource management strategy minimizes the latency and energy consumption during task execution and increases resource utilization and average throughput of servers. The proposed switch migration strategy can effectively achieve load balancing and reduce the response time.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s12124-020-09531-6,en,There Can Be no Other Reason for this Behavior: Issues in the Ascription of Knowledge to Humans and AI,"['OriginalPaper', 'Regular Article']","While machine learning techniques have been used to model categorization/decision making tasks that are beyond the capabilities of traditional AI, these new models are typically uninterpretable, i.e., the reasons for their decisions are not clear. Some have argued that, in developing machines that can report the reasons for their decisions, developers should take, as a guide, human explanations for behavior, which make reference to mental states (e.g., knowledge/belief). This proposal is correct, but unattainable given certain characteristics of current AI. To explain, this article draws on recent discourse-analytic research showing that ascriptions of knowledge/belief presume behavioral performances to instantiate particular sorts of broader dispositions. This is reflected by the possibility of ascribing knowledge/belief to an agent on the basis that there can be no other explanation for their observed behavior . The behavior of AI trained through machine learning is unpredictable in ways that precludes such certainty. Consequently, while it is certainly possible to program machines to report mental states of knowledge/belief to account for their decisions, the failure of current AI to engage in typically human forms of life means that such ascribed mental states are inevitably meaningless.","['Psychology', 'Psychology, general', 'Sociology, general', 'Anthropology']"
doi:10.1007/s00521-022-07710-7,en,Optimal fractional-order PID controller based on fractional-order actor-critic algorithm,"['OriginalPaper', 'Original Article']","In this paper, an online optimization approach of a fractional-order PID controller based on a fractional-order actor-critic algorithm (FOPID-FOAC) is proposed. The proposed FOPID-FOAC scheme exploits the advantages of the FOPID controller and FOAC approaches to improve the performance of nonlinear systems. The proposed FOAC is built by developing a FO-based learning approach for the actor-critic neural network with adaptive learning rates. Moreover, a FO rectified linear unit (RLU) is introduced to enable the AC neural network to define and optimize its own activation function. By the means of the Lyapunov theorem, the convergence and the stability analysis of the proposed algorithm are investigated. The FO operators for the FOAC learning algorithm are obtained using the gray wolf optimization (GWO) algorithm. The effectiveness of the proposed approach is proven by extensive simulations based on the tracking problem of the two degrees of freedom (2-DOF) helicopter system and the stabilization issue of the inverted pendulum (IP) system. Moreover, the performance of the proposed algorithm is compared against optimized FOPID control approaches in different system conditions, namely when the system is subjected to parameter uncertainties and external disturbances. The performance comparison is conducted in terms of two types of performance indices, the error performance indices, and the time response performance indices. The first one includes the integral absolute error (IAE), and the integral squared error (ISE), whereas the second type involves the rising time, the maximum overshoot (Max. OS), and the settling time. The simulation results explicitly indicate the high effectiveness of the proposed FOPID-FOAC controller in terms of the two types of performance measurements under different scenarios compared with the other control algorithms.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07628-0,en,Proxemic behavior in navigation tasks using reinforcement learning,"['ReviewPaper', 'S.I. : Human-aligned Reinforcement Learning for Autonomous Agents and Robots']","Human interaction starts with a person approaching another one, respecting their personal space to prevent uncomfortable feelings. Spatial behavior, called proxemics, allows defining an acceptable distance so that the interaction process begins appropriately. In recent decades, human-agent interaction has been an area of interest for researchers, where it is proposed that artificial agents naturally interact with people. Thus, new alternatives are needed to allow optimal communication, avoiding humans feeling uncomfortable. Several works consider proxemic behavior with cognitive agents, where human-robot interaction techniques and machine learning are implemented. However, it is assumed that the personal space is fixed and known in advance, and the agent is only expected to make an optimal trajectory toward the person. In this work, we focus on studying the behavior of a reinforcement learning agent in a proxemic-based environment. Experiments were carried out implementing a grid-world problem and a continuous simulated robotic approaching environment. These environments assume that there is an issuer agent that provides non-conformity information. Our results suggest that the agent can identify regions where the issuer feels uncomfortable and find the best path to approach the issuer. The results obtained highlight the usefulness of reinforcement learning in order to identify proxemic regions.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11227-022-04747-2,en,Optimized task scheduling and preemption for distributed resource management in fog-assisted IoT environment,OriginalPaper,"The fog-assisted cloud computing gives better quality of service (QoS) to Internet of things (IoT) applications. However, the large quantity of data transmitted by the IoT devices results in the overhead of bandwidth and increased delay. Moreover, large amounts of data transmission generate resource management issues and decrease the system’s throughput. This paper proposes the o ptimized task s c heduling a nd p r eemption (OSCAR) model to overcome the limitations and improve the QoS. The dataset used for the study is a real-time crowd-based dataset which provides task information. The processes involved in this paper are as follows: (i) Initially, the tasks from the IoT devices are clustered based on the priority and deadline by implementing expectation–maximization (EM) clustering to decrease the computational complexity and bandwidth overhead. (ii) The clustered tasks are then scheduled by implementing a modified heap-based optimizer based on the QoS and service level agreement (SLA) constraints. (iii) Distributed resource management is performed by allocating resources to the tasks based on multiple constraints. The categorical deep Q network is the deep reinforcement learning model is implemented for this purpose. The dynamic nature of tasks from the IoT devices is addressed by performing preemption of tasks using the ranking method, where the tasks with higher priority, with a short deadline replaces less priority task by moving it into the waiting queue. The proposed model is experimented with in the iFogsim simulation tool and evaluated in terms of average response time, loss ratio, resource utilization, average makespan time, queuing waiting time, percentage of tasks satisfying the deadline and throughput. The proposed OSCAR model outperforms the existing model in achieving the QoS and SLA with maximal throughput and reduced response time.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s10845-022-01988-z,en,Attention-based advantage actor-critic algorithm with prioritized experience replay for complex 2-D robotic motion planning,OriginalPaper,"Robotic motion planning in dense and dynamic indoor scenarios constantly challenges the researchers because of the motion unpredictability of obstacles. Recent progress in reinforcement learning enables robots to better cope with the dense and unpredictable obstacles by encoding complex features of the robot and obstacles into the encoders like the long-short term memory (LSTM). Then these features are learned by the robot using reinforcement learning algorithms, such as the deep Q network and asynchronous advantage actor critic algorithm. However, existing methods depend heavily on expert experiences to enhance the convergence speed of the networks by initializing them via imitation learning. Moreover, those approaches based on LSTM to encode the obstacle features are not always efficient and robust enough, therefore sometimes causing the network overfitting in training. This paper focuses on the advantage actor critic algorithm and introduces an attention-based actor critic algorithm with experience replay algorithm to improve the performance of existing algorithm from two perspectives. First, LSTM encoder is replaced by a robust encoder attention weight to better interpret the complex features of the robot and obstacles. Second, the robot learns from its past prioritized experiences to initialize the networks of the advantage actor-critic algorithm. This is achieved by applying the prioritized experience replay method, which makes the best of past useful experiences to improve the convergence speed. As results, the network based on our algorithm takes only around 15% and 30% experiences to get rid of the early-stage training without the expert experiences in cases with five and ten obstacles, respectively. Then it converges faster to a better reward with less experiences (near 45% and 65% of experiences in cases with ten and five obstacles respectively) when comparing with the baseline LSTM-based advantage actor critic algorithm. Our source code is freely available at the GitHub ( https://github.com/CHUENGMINCHOU/AW-PER-A2C ).","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11036-022-01977-9,en,Robust and Cost-effective Resource Allocation for Complex IoT Applications in Edge-Cloud Collaboration,OriginalPaper,"The rapid increasing of the Internet-of-Things (IoT) applications make it convenient to sense and collect real-world information in our daily life. To ensure the performance of these IoT applications, researchers established an edge-cloud collaboration application system based on the multi-access edge computing (MEC) paradigm where the IoT data can be processed not only on the cloud but also on nearby edge servers. However, as the edge servers are resource-limited, we should be more careful in allocating the edge resource to the application, especially when it is composed by several micro-services. In this paper, we considered how edge-cloud cooperation can help running these service composition based IoT applications and proposed an efficient resource allocation approach to balance performance, robustness, and cost-effectiveness of IoT applications in MEC environments. We mathematically modeled the cost-effective performance optimization problem in robust edge-cloud application systems and proved the convexity of the approximated problem so that they can be solved in tractable ways with existing solvers to generate the resource allocation strategies. Meanwhile, we carried out a series of experiments to evaluate our approach. The experiment results showed that our approach was powerful in managing the performance, cost and robustness compared with representative baselines.","['Engineering', 'Communications Engineering, Networks', 'Computer Communication Networks', 'Electrical Engineering', 'IT in Business']"
doi:10.1007/s11227-022-04728-5,en,AdaInNet: an adaptive inference engine for distributed deep neural networks offloading in IoT-FOG applications based on reinforcement learning,OriginalPaper,"The increasing expansion of Internet-of-Things (IoT) in the world requires Big Data analytic infrastructures to produce valuable knowledge in IoT applications. IoT includes devices with limited resources, whereby it requires efficient platforms to process massive data obtained from sensors. Nowadays, many IoT applications such as audio and video recognition depend on state-of-the-art Deep Neural Networks (DNNs). Therefore, we need to execute DNNs on IoT devices. DNNs offer excellent recognition accuracy but they suffer from high computational and memory resource demands. Due to these constraints, currently, IoT applications that depend on deep learning are mostly offloaded to cloudlets and clouds. Offloading imposes extra network bandwidth consumption costs in addition to delayed response for IoT devices. In this paper, we propose a method that instead of using all layers of DNN for inference, only selects a subset of layers that provide sufficient accuracy for each task. We propose AdaInNet, a method to significantly reduce computational cost and network latency in DNN-based IoT applications while maintaining prediction accuracy based on Distributed DNNs (DDNNs). The method uses modified Distributed DNNs with early exits in order to minimize computation costs and network latency by selecting sub-layers or exit branches of DDNNs with early exits. We also proposed a hybrid Classifier-Wise (CW)—Interactive learning method for the training of DDNNs and Agent’s networks. Furthermore, we create a custom agent model for the Advantage Actor-Critic Deep Reinforcement Learning method in order to preserve recognition accuracy while utilizing a minimum number of layers. Finally, we execute the extensive numerical simulation, in order to evaluate and compare our proposed AdaInNet method with rival methods under standard CIFAR 100 and CIFAR 10 datasets and ResNet-110 and ResNet-32 DNNs which are used in IoT applications in previous works. The results provide strong quantitative evidence that the AdaInNet method not only accelerates inference but also reduces computational cost and latency.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s00521-022-07604-8,en,Method for solving constrained 0-1 quadratic programming problems based on pointer network and reinforcement learning,"['OriginalPaper', 'S.I.: Interpretation of Deep Learning']","The constrained 0-1 quadratic programming problem (CBQP) is an important problem of integer programming, and many combinatorial optimization problems can be converted to CBQP problem. Because BQP is NP-hard problem, the solving time and accuracy of traditional optimization algorithm are very dependent on the size of the problem, and the local optimal solution obtained by the heuristic algorithm is unstable. Deep learning algorithm has great advantages in solving such problems. In this paper, for the CBQP problem with linear constraints, we creatively apply two algorithms and models to solve it: the graph pointer network model (GPN) trained by hierarchical reinforcement learning (HRL), and the multi-head attention-based pointer network model trained by Advantage Actor-Critic (A2C), which greatly improves the solving speed, accuracy and constraint satisfaction rate of CBQP problems of different scales. At the same time, the bidirectional mask mechanism is innovatively introduced into the network so that the constraint satisfaction rate of the solution is very high. For the two algorithms, this paper solved the 0-1 knapsack (BKP) problem and the quadratic knapsack (QKP) problem, which are equivalent to the CBQP problem, and compared the results of the CBQP problem with different data distribution and scales. The experiment shows that no matter the objective function of the CBQP problem is linear or nonlinear, different data set distribution, or the scale, the pointer network trained by reinforcement learning in this paper has better results than traditional optimization algorithms in solving time, accuracy, stability and constraint satisfaction rate, and with the increase in the size of the problem, this advantage becomes more obvious.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10664-022-10158-x,en,GBGallery : A benchmark and framework for game testing,OriginalPaper,"Software bug database and benchmark are the wheels of advancing automated software testing. In practice, real bugs often occur sparsely relative to the amount of software code, the extraction and curation of which are quite labor-intensive but can be essential to facilitate the innovation of testing techniques. Over the past decade, several milestones have been made to construct bug databases, pushing the progress of automated software testing research. However, up to the present, it still lacks a real bug database and benchmark for game software, making current game testing research mostly stagnant. The missing of bug database and framework greatly limits the development of automated game testing techniques. To bridge this gap, we first perform large-scale real bug collection and manual analysis from 5 large commercial games, with a total of more than 250,000 lines of code. Based on this, we propose GBGallery , a game bug database and an extensible framework, to enable automated game testing research. In its initial version, GBGallery contains 76 real bugs from 5 games and incorporates 5 state-of-the-art testing techniques for comparative study as a baseline for further research. With GBGallery , we perform large-scale empirical studies and find that the current automated game testing is still at an early stage, where new testing techniques for game software should be extensively investigated. We make GBGallery publicly available, hoping to facilitate the game testing research.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Programming Languages, Compilers, Interpreters']"
doi:10.1007/s10796-022-10314-0,en,How to Maximize Clicks for Display Advertisement in Digital Marketing? A Reinforcement Learning Approach,OriginalPaper,"One of the core challenges in digital marketing is that the business conditions continuously change, which impacts the reception of campaigns. A winning campaign strategy can become unfavored over time, while an old strategy can gain new traction. In data driven digital marketing and web analytics, A/B testing is the prevalent method of comparing digital campaigns, choosing the winning ad, and deciding targeting strategy. A/B testing is suitable when testing variations on similar solutions and having one or more metrics that are clear indicators of success or failure. However, when faced with a complex problem or working on future topics, A/B testing fails to deliver and achieving long-term impact from experimentation is demanding and resource intensive. This study proposes a reinforcement learning based model and demonstrates its application to digital marketing campaigns. We argue and validate with actual-world data that reinforcement learning can help overcome some of the critical challenges that A/B testing, and popular Machine Learning methods currently used in digital marketing campaigns face. We demonstrate the effectiveness of the proposed technique on real actual data for a digital marketing campaign collected from a firm.","['Business and Management', 'IT in Business', 'Management of Computing and Information Systems', 'Systems Theory, Control', 'Operations Research/Decision Theory']"
doi:10.1007/s41365-022-01069-z,en,A non-invasive diagnostic method of cavity detuning based on a convolutional neural network,OriginalPaper,"As modern accelerator technologies advance toward more compact sizes, conventional invasive diagnostic methods of cavity detuning introduce negligible interference in measurements and run the risk of harming structural surfaces. To overcome these difficulties, this study developed a non-invasive diagnostic method using knowledge of scattering parameters with a convolutional neural network and the interior point method. Meticulous construction and training of the neural network led to remarkable results on three typical acceleration structures: a 13-cell S-band standing-wave linac, a 12-cell X-band traveling-wave linac, and a 3-cell X-band RF gun. The trained networks significantly reduced the burden of the tuning process, freed researchers from tedious tuning tasks, and provided a new perspective for the tuning of side-coupling, semi-enclosed, and total-enclosed structures.","['Physics', 'Particle and Nuclear Physics', 'Particle Acceleration and Detection, Beam Physics', 'Nuclear Energy']"
doi:10.1007/s10489-022-03208-w,en,Graph cooperation deep reinforcement learning for ecological urban traffic signal control,OriginalPaper,"Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10462-022-10224-2,en,A survey on deep reinforcement learning for audio-based applications,OriginalPaper,"Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields including computer vision, natural language processing, healthcare, robotics, to name a few. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising applications in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together research studies across different but related areas in speech and music. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting important challenges faced by audio-based DRL agents and by highlighting open areas for future research and investigation. The findings of this paper will guide researchers interested in DRL for the audio domain.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s12065-022-00740-z,en,Optimization for reinforcement learning based 3D animation exercise,"['OriginalPaper', 'Special Issue']","3D animation makes art “live” and gives works of art with vitality, thus enabling artistic charm to be better presented and artistic practice to create more value. Due to the superior 3D model creation of 3D animation, it has important practical prospects in the field of art. At the same time, 3D animation is an important part of the animation industry, and its artistic style affects the quality of animation works and derivatives. Against the backdrop of computer technology, computer has a great influence on 3D animation art, which greatly enhances the artistic expression of 3D animation and promotes new animation forms. The artistic expression of 3D animation is mainly reflected in the performance of light and film, space and motion, lens, details and texture, and traditional artistic expression of 3D technology. However, when viewing 3D animation, we are often affected by the network bandwidth which leads to lag in 3D animation video and affects the user Quality of experience (QoE). According to the duration of 3D animation videos, this paper divides them into three types and obtains better user QoE through reinforcement learning (RL). The datasets of 3D animation videos of different durations are used to train the RL model, and the corresponding reward function parameters of different 3D animation video durations are obtained. Therefore, duration-sensitive Asynchronous Advantage Actor Critic (A3C)—RL algorithm is presented. The experimental results show that the A3C-RL algorithm has a much better user QoE for 3D animation video than that of state-of-the-art algorithms.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s12652-022-04117-z,en,FPGA implementation of Proximal Policy Optimization algorithm for Edge devices with application to Agriculture Technology,"['OriginalPaper', 'Original Research']","Reinforcement Learning (RL) is a technique where an agent learns to accomplish an assigned task on the basis of reward phenomenon. RL algorithm when implemented with embedded - Field Programmable Gate Array (FPGA) hardware, is capable of influencing future applications and automation to a much greater extent than other implementation approaches. This work discusses an important RL algorithm called the Proximal Policy Optimization (PPO) applied to the example of Cart-Pole a well know benchmark from the control theory domain. It presents a novel hardware architecture designed, implemented and verified for the benchmark using the PPO based RL algorithm. The hardware implementation uses the Xilinx Avnet Ultra96v2 platform consisting of the Xilinx Zynq Ultrascale + MPSoC (ZU3EG). The synthesis platform uses the Xilinx Vivado HLS 2019.2v and Xilinx Vivado 2019.2v along with Xilinx’s PYNQ framework. The results from Matlab/Simulink are used as a golden reference to verify the results from the hardware implementation. It also enables a better understanding of the dynamics of the Cart-Pole benchmark problem. A comparative analysis of the proposed hardware architecture with the state of art implementations in the literature is done. This along with the illustration of application framework enables us to establish the novelty of the proposed approach and its usefulness for applications from the domain of agriculture intended to be executed on edge devices.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s10922-022-09667-3,en,An Efficient and Decentralized Fuzzy Reinforcement Learning Bandwidth Controller for Multitenant Data Centers,OriginalPaper,"Cloud service providers rely on bandwidth overprovisioning to avoid Service Level Agreements’ violation (SLAs) when allocating tenants’ resources in multitenant cloud environments. Tenants’ network usage is usually dynamic, but the shared resources are often allocated statically and in batches, causing resource idleness. This paper envisions an opportunity for optimizing cloud service networks. As such, we propose an autonomous bandwidth allocation mechanism based on Fuzzy Reinforcement Learning (FRL) to reduce the idleness of cloud network resources. Our mechanism dynamically allocates resources, prioritizing tenants and allowing them to exceed the contracted bandwidth temporarily without violating the SLAs. We assess our mechanism by comparing FRL usage against pure Fuzzy Inference System (FIS) and pure Reinforcement Learning (RL). The evaluation scenario is an emulation in which tenants share resources from a cloud provider and generate traffic based on real HTTP traffic. The results show that our mechanism increases tenant’s cloud network utilization by 30% compared to FIS while maintaining the cloud traffic load within a healthy threshold and more stable than RL.","['Computer Science', 'Computer Communication Networks', 'Computer Systems Organization and Communication Networks', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Operations Research/Decision Theory']"
doi:10.1007/s00607-021-01046-1,en,Deep reinforcement learning based QoE-aware actor-learner architectures for video streaming in IoT environments,"['OriginalPaper', 'Special Issue Article']","The number of connected smart devices enabling multimedia applications has expanded tremendously in Internet-of-Things (IoT) environments. Specifically, the requirement for a high quality of experience (QoE) for video streaming services is a crucial prerequisite for a range of use cases, including smart surveillance, smart healthcare, smart agriculture and many more. However, providing a high QoE for video streaming is challenging due to underlying dynamic network conditions. To address this issue, several adaptive bit rate (ABR) algorithms based on predetermined rules have been developed. However, they do not generalize well to a wide variety of network conditions. ABR algorithms, based on reinforcement learning (RL), have been proven to be more effective at generalizing to varying network conditions but they still have limitations, specifically, constrained exploration and high variance in value estimates. In this paper, we propose asynchronous advantage actor-critic (A3C) based actor-learner architectures for generating the adaptive bit rates for video streaming in IoT environments. To address the existing issues, we propose integrating two advanced A3C algorithms: Follow then Forage Exploration (FFE) and Averaged A3C. We demonstrate their efficacy in improving the QoE over vanilla A3C. Additionally, we also demonstrate the benefits of the proposed architecture for video streaming under different network conditions and for different variants of the QoE metric. We show that advanced A3C methods provide up to 30.70% improvement in QoE over vanilla A3C and a considerably higher QoE over other fixed-rule-based ABR algorithms.","['Computer Science', 'Computer Science, general', 'Information Systems Applications (incl.Internet)', 'Computer Communication Networks', 'Software Engineering', 'Artificial Intelligence', 'Computer Appl. in Administrative Data Processing']"
doi:10.1007/s10668-021-01836-9,en,Collaborative multi-agents in dynamic industrial internet of things using deep reinforcement learning,OriginalPaper,"Sustainable cities are envisioned to have economic and industrial steps toward reducing pollution. Many real-world applications such as autonomous vehicles, transportation, traffic signals, and industrial automation can now be trained using deep reinforcement learning (DRL) techniques. These applications are designed to take benefit of DRL in order to improve the monitoring as well as measurements in industrial internet of things for automation identification system. The complexity of these environments means that it is more appropriate to use multi-agent systems rather than a single-agent. However, in non-stationary environments multi-agent systems can suffer from increased number of observations, limiting the scalability of algorithms. This study proposes a model to tackle the problem of scalability in DRL algorithms in transportation domain. A partition-based approach is used in the proposed model to reduce the complexity of the environment. This partition-based approach helps agents to stay in their working area. This reduces the complexity of the learning environment and the number of observations for each agent. The proposed model uses generative adversarial imitation learning and behavior cloning, combined with a proximal policy optimization algorithm, for training multiple agents in a dynamic environment. We present a comparison of PPO, soft actor-critic, and our model in reward gathering. Our simulation results show that our model outperforms SAC and PPO in cumulative reward gathering and dramatically improved training multiple agents.","['Environment', 'Sustainable Development', 'Environmental Management', 'Environmental Economics', 'Ecology', 'Economic Growth', 'Economic Geology']"
doi:10.1007/s40815-022-01293-0,en,Fuzzy Deep Deterministic Policy Gradient-Based Motion Controller for Humanoid Robot,OriginalPaper,"In conventional robot arm control, inverse kinematics (IK) is used as the basis for computing arm joint angles. However, IK can be used to compute joint angles only after the terminal point has been reached, and it cannot optimize arm movements. Furthermore, the singularity problem is sometimes encountered when using IK. For example, if a robot arm in motion passes through a singularity, the next step’s movement is incomputable, which results in errors. Therefore, this study did not use IK for computing the joint angles of humanoid robot arms. Instead, this paper proposes a motion controller based on machine learning and fuzzy logic for the aforementioned purpose. Conventional reinforcement learning can provide satisfactory results for a single state but cannot be used to perform optimized calculations for infinite states. To solve this problem, this study used the deep deterministic policy gradient (DDPG) algorithm and allowed a humanoid robot to self-learn and autonomously plan the movement of and joint angles in its arm. A state and its action can be calculated in a hyperplane by using the developed neural network. A continuous mapping relationship exists between the state and its action in this hyperplane. Thus, the humanoid robot obtained optimal learning experiences in multiple self-learning processes. Finally, the concept was incorporated into a visual feedback system to achieve object grasping by the humanoid robot. The humanoid robot exhibited satisfactory learning outcomes—as well as satisfactory motion control performance—in experiments when combining fuzzy logic with the DDPG algorithm.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Operations Research, Management Science']"
doi:10.1007/s10479-022-04788-z,en,An improved transformer model with multi-head attention and attention to attention for low-carbon multi-depot vehicle routing problem,"['OriginalPaper', 'Original Research']","Low-carbon logistics is an emerging and sustainable development industry in the era of a low-carbon economy. The end-to-end deep reinforcement learning (DRL) method with an encoder-decoder framework has been proven effective for solving logistics problems. However, in most cases, the recurrent neural networks (RNN) and attention mechanisms are used in encoders and decoders, which may result in the long-distance dependence problem and the neglect of the correlation between query vectors. To surround this problem, we propose an improved transformer model (TAOA) with both multi-head attention mechanism (MHA) and attention to attention mechanism (AOA), and apply it to solve the low-carbon multi-depot vehicle routing problem (MDVRP). In this model, the MHA and AOA are implemented to solve the probability of route nodes in the encoder and decoder. The MHA is used to process different parts of the input sequence, which can be calculated in parallel, and the AOA is used to deal with the deficiency problem of correlation between query results and query vectors in the MHA. The actor-critic framework based on strategy gradient is constructed to train model parameters. The 2opt operator is further used to optimize the resulting routes. Finally, extensive numerical studies are carried out to verify the effectiveness and operation efficiency of the proposed TAOA, and the results show that the proposed TAOA performs better in solving the MDVRP than the traditional transformer model (Kools), genetic algorithm (GA), and Google OR-Tools (Ortools).","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1186/s13638-022-02124-4,en,Dynamic spectrum access and sharing through actor-critic deep reinforcement learning,"['OriginalPaper', 'Research']","When primary users of the spectrum use frequency channels intermittently, secondary users can selectively transmit without interfering with the primary users. The secondary users adjust the transmission power allocation on the frequency channels to maximize their information rate while reducing channel conflicts with the primary users. In this paper, the secondary users do not know the spectrum usage by the primary users or the channel gains of the secondary users. Based on the conflict warnings from the primary users and the signal-to-interference-plus-noise ratio measurement at the receiver, the secondary users adapt and improve spectrum utilization through deep reinforcement learning. The secondary users adopt the actor-critic deep deterministic policy gradient algorithm to overcome the challenges of large state space and large action space in reinforcement learning with continuous-valued actions. In addition, multiple secondary users implement multi-agent deep reinforcement learning under certain coordination. Numerical results show that the secondary users can successfully adapt to the spectrum environment and learn effective transmission policies.","['Engineering', 'Signal,Image and Speech Processing', 'Communications Engineering, Networks', 'Information Systems Applications (incl.Internet)']"
doi:10.1007/s10514-022-10039-8,en,Motion planning and control for mobile robot navigation using machine learning: a survey,OriginalPaper,"Moving in complex environments is an essential capability of intelligent mobile robots. Decades of research and engineering have been dedicated to developing sophisticated navigation systems to move mobile robots from one point to another. Despite their overall success, a recently emerging research thrust is devoted to developing machine learning techniques to address the same problem, based in large part on the success of deep learning. However, to date, there has not been much direct comparison between the classical and emerging paradigms to this problem. In this article, we survey recent works that apply machine learning for motion planning and control in mobile robot navigation, within the context of classical navigation systems. The surveyed works are classified into different categories, which delineate the relationship of the learning approaches to classical methods. Based on this classification, we identify common challenges and promising future directions.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10489-022-03550-z,en,Towards designing a generic and comprehensive deep reinforcement learning framework,OriginalPaper,"Reinforcement learning (RL) has emerged as an effective approach for building an intelligent system, which involves multiple self-operated agents to collectively accomplish a designated task. More importantly, there has been a renewed focus on RL since the introduction of deep learning that essentially makes RL feasible to operate in high-dimensional environments. However, there are many diversified research directions in the current literature, such as multi-agent and multi-objective learning, and human-machine interactions. Therefore, in this paper, we propose a comprehensive software architecture that not only plays a vital role in designing a connect-the-dots deep RL architecture but also provides a guideline to develop a realistic RL application in a short time span. By inheriting the proposed architecture, software managers can foresee any challenges when designing a deep RL-based system. As a result, they can expedite the design process and actively control every stage of software development, which is especially critical in agile development environments. For this reason, we design a deep RL-based framework that strictly ensures flexibility, robustness, and scalability. To enforce generalization, the proposed architecture also does not depend on a specific RL algorithm, a network configuration, the number of agents, or the type of agents.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03606-0,en,Dynamic stock-decision ensemble strategy based on deep reinforcement learning,OriginalPaper,"In a complex and changeable stock market, it is very important to design a trading agent that can benefit investors. In this paper, we propose two stock trading decision-making methods. First, we propose a nested reinforcement learning (Nested RL) method based on three deep reinforcement learning models (the Advantage Actor Critic, Deep Deterministic Policy Gradient, and Soft Actor Critic models) that adopts an integration strategy by nesting reinforcement learning on the basic decision-maker. Thus, this strategy can dynamically select agents according to the current situation to generate trading decisions made under different market environments. Second, to inherit the advantages of three basic decision-makers, we consider confidence and propose a weight random selection with confidence (WRSC) strategy. In this way, investors can gain more profits by integrating the advantages of all agents. All the algorithms are validated for the U.S., Japanese and British stocks and evaluated by different performance indicators. The experimental results show that the annualized return, cumulative return, and Sharpe ratio values of our ensemble strategy are higher than those of the baselines, which indicates that our nested RL and WRSC methods can assist investors in their portfolio management with more profits under the same level of investment risk.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03453-z,en,Solving uncapacitated P-Median problem with reinforcement learning assisted by graph attention networks,OriginalPaper,"The P-Median Problem is one of the basic cases of facility location problems and has been studied for many years. Most methods of solving it are based on classical heuristics or meta-heuristic and they don’t perform well on large-scale problems according to time cost. In this paper, we propose the first reinforcement learning-based method which uses Multi-Talking-Heads Graph Attention Networks to learn representations and design a learnable attention mechanism to solve the uncapacitated P-Median Problem. We train the model using REINFORCE algorithm and show that it has good performance on uncapacitated P-Median Problem according to solution quality and time consumption. We also apply our model to the realistic dataset and empirically figure out that the difference between data distributions is one of the most important factors to influence the final performances.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03468-6,en,Improving environmental awareness for autonomous vehicles,OriginalPaper,"Autonomous vehicles (AVs) have multiple tasks with different priorities and safety levels where classic supervised learning techniques are no longer applicable. Thus, reinforcement learning (RL) algorithms become increasingly appropriate for this domain as the RL algorithms can act on complex problems and adapt their responses in the face of unforeseen situations and environments. The RL agent aims to perform the action that guarantees the optimal reward with the best score. The problem with this approach is if the agent finds a possible optimal action with a reasonable premium and gets stuck in this mediocre strategy, which at the same time is neither the best nor the worst solution. Therefore, the agent avoids performing a more extensive exploration to find new paths and learn alternatives to generate a higher reward. To alleviate this problem, we research the behavior of two types of noise in AVs training. We analyze the results and point out the noise method that most stimulates exploration. A vast exploration of the environment is highly relevant to AVs because they know more about the environment and learn alternative ways of acting in the face of uncertainties. With that, AVs can expect more reliable actions in front of sudden changes in the environment. According to our experiments’ results in a simulator, we can see that noise allows the autonomous vehicle to improve its exploration and increase the reward.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-12412-2,en,Three dimensional objects recognition & pattern recognition technique; related challenges: A review,OriginalPaper,"3D object recognition and pattern recognition are active and fast-growing research areas in the field of computer vision. It is mandatory to define the pattern class, feature extraction, design classifiers, clustering, and selection of test datasets and evaluate performance for any pattern recognition system. The pattern recognition system recognizes the object, so it is required to extract the features in such a way that it will be suitable for a particular recognition method. Features can be retrieved either locally or globally. The object recognition technique is divided into two parts: the local feature extraction method and the global feature extraction method. Many researchers have done admirable work in the field of local and global feature extraction. Local feature-based techniques are more suitable for the real-world scene. The Global feature-based methods are more suitable for retrieving the 3D model & identifying the object’s shape when the object’s geometric structure is fragile. A lot of research has been done on pattern recognition in the last 50 years. Still, no single technique can be used for all types of applications, such as bioinformatics, data mining, speech recognition, remote sensing, multimedia applications, text detection, localization, etc. The main agenda of this paper is to summarize the 3D object recognition methodologies. This paper provides a complete study of 3D object recognition based on local and global feature-based methods and different techniques of pattern recognition. We have tried to summarize the results of different technologies and the future scope of this paper’s particular technique. We enlisted the accessible online 3D database and their attributes, evaluation parameters of the 3D datasets. This paper will immensely help the researchers to Identify the research gap and limitations in pattern recognition and object recognition so that the researchers will be motivated to do something new in this field.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s43684-022-00025-3,en,Machine learning techniques for robotic and autonomous inspection of mechanical systems and civil infrastructure,"['ReviewPaper', 'Review']","Machine learning and in particular deep learning techniques have demonstrated the most efficacy in training, learning, analyzing, and modelling large complex structured and unstructured datasets. These techniques have recently been commonly deployed in different industries to support robotic and autonomous system (RAS) requirements and applications ranging from planning and navigation to machine vision and robot manipulation in complex environments. This paper reviews the state-of-the-art with regard to RAS technologies (including unmanned marine robot systems, unmanned ground robot systems, climbing and crawler robots, unmanned aerial vehicles, and space robot systems) and their application for the inspection and monitoring of mechanical systems and civil infrastructure. We explore various types of data provided by such systems and the analytical techniques being adopted to process and analyze these data. This paper provides a brief overview of machine learning and deep learning techniques, and more importantly, a classification of the literature which have reported the deployment of such techniques for RAS-based inspection and monitoring of utility pipelines, wind turbines, aircrafts, power lines, pressure vessels, bridges, etc. Our research provides documented information on the use of advanced data-driven technologies in the analysis of critical assets and examines the main challenges to the applications of such technologies in the industry.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Control and Systems Theory', 'Machine Learning']"
doi:10.1007/s10723-022-09603-4,en,Scalable Virtual Machine Migration using Reinforcement Learning,OriginalPaper,"Heuristic approaches require fixed knowledge of how resource allocation should be carried out, and this can be limiting when managing variable cloud workloads. Solutions based on Reinforcement Learning (RL) have been presented to manage cloud infrastructure, however, these tend to be centralized and suffer in their ability to maintain Quality of Service (QoS) for data centres with thousands of nodes. To address this, we propose a reinforcement learning management policy, which can run decentralized, and achieve fast convergence towards efficient resource allocation, resulting in lower SLA violations compared to centralized architectures. To address some of the common challenges in applying RL to cloud resource management, such as slow learning and state/action management, we use parallel learning and reduction of the state/action space. We apply a decision making approach to optimize the migration of a VM and choose a target node to host the VM in such a way that brings response time within SLA level. We have also demonstrate unique, multi-level reinforcement learning cooperation, that further reduces SLA violations. We use simulation to evaluate and demonstrate our proposal in practice, and compare the results obtained with an established heuristic, demonstrating significant improvement to SLA violations and higher scalability.","['Computer Science', 'Processor Architectures', 'Management of Computing and Information Systems', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s10489-022-03470-y,en,Modeling multi-scale sub-group context for group activity recognition,OriginalPaper,"Group activity recognition is a challenging task for complex motion and relation between actors. To utilize similar action of actors, this paper proposes a novel multi-scale Sub-group Context Block (SCB) for group Activity Recognition. Node embedding matrix and adjacent matrix are constructed and fed into SCB. In SCB, we use an assignment matrix to learn the mapping from actors to sub-groups, so the representation and interaction of sub-group can be learned automatically. Then Graph Convolution is used for further feature representation refine. In order to emphasize effect of different sub-groups, a reinforcement learning based module Sub-group Attention Block (SAB) is designed, which models it as a Markov decision process and gives each sub-group an importance value for further procedure. Multi-scale context for group activity in different levels is adopted by fusing features obtained with various clustering numbers. Finally, temporal information is integrated by multiple frames merging. Extensive experiments are performed on two standard group activity recognition datasets: the Volleyball and the Collective Activity. Our proposed method gets outstanding performance. The results also validate that SCB and SAB are effective for group activity recognition.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03456-w,en,Deep reinforcement learning for the dynamic and uncertain vehicle routing problem,OriginalPaper,"Accurate and real-time tracking for real-world urban logistics has become a popular research topic in the field of intelligent transportation. While the routing of urban logistic service is usually accomplished via complex mathematical and analytical methods. However, the nature and scope of real-world urban logistics are highly dynamic, and the existing optimization technique cannot precisely formulate the dynamic characteristics of the route. To ensure customers’ demands are met, planners need to respond to these changes quickly (sometimes instantaneously). This paper proposes the formulation of a novel deep reinforcement learning framework to solve a dynamic and uncertain vehicle routing problem (DU-VRP), whose objective is to meet the uncertain servicing needs of customers in a dynamic environment. Considering uncertain information about the demands of customers in this problem, the partial observation Markov decision process is designed to frequently observe the changes in customers’ demands in a real-time decision support system that consists of a deep neural network with a dynamic attention mechanism. Besides, a cutting-edge reinforcement learning algorithm is presented to control the value function of the DU-VRP for better training the routing process’s dynamics and uncertainty. Computational experiments are conducted considering different data sources to obtain satisfactory solutions of the DU-VRP.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10462-021-10061-9,en,Deep reinforcement learning in computer vision: a comprehensive survey,OriginalPaper,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations . In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10462-021-10085-1,en,Model-free reinforcement learning from expert demonstrations: a survey,OriginalPaper,"Reinforcement learning from expert demonstrations (RLED) is the intersection of imitation learning with reinforcement learning that seeks to take advantage of these two learning approaches. RLED uses demonstration trajectories to improve sample efficiency in high-dimensional spaces. RLED is a new promising approach to behavioral learning through demonstrations from an expert teacher. RLED considers two possible knowledge sources to guide the reinforcement learning process: prior knowledge and online knowledge. This survey focuses on novel methods for model-free reinforcement learning guided through demonstrations, commonly but not necessarily provided by humans. The methods are analyzed and classified according to the impact of the demonstrations. Challenges, applications, and promising approaches to improve the discussed methods are also discussed.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s40747-021-00577-6,en,Missile guidance with assisted deep reinforcement learning for head-on interception of maneuvering target,"['OriginalPaper', 'Original Article']","In missile guidance, pursuit performance is seriously degraded due to the uncertainty and randomness in target maneuverability, detection delay, and environmental noise. In many methods, accurately estimating the acceleration of the target or the time-to-go is needed to intercept the maneuvering target, which is hard in an environment with uncertainty. In this paper, we propose an assisted deep reinforcement learning (ARL) algorithm to optimize the neural network-based missile guidance controller for head-on interception. Based on the relative velocity, distance, and angle, ARL can control the missile to intercept the maneuvering target and achieve large terminal intercept angle. To reduce the influence of environmental uncertainty, ARL predicts the target’s acceleration as an auxiliary supervised task. The supervised learning task improves the ability of the agent to extract information from observations. To exploit the agent’s good trajectories, ARL presents the Gaussian self-imitation learning to make the mean of action distribution approach the agent’s good actions. Compared with vanilla self-imitation learning, Gaussian self-imitation learning improves the exploration in continuous control. Simulation results validate that ARL outperforms traditional methods and proximal policy optimization algorithm with higher hit rate and larger terminal intercept angle in the simulation environment with noise, delay, and maneuverable target.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10207-021-00554-7,en,The Agent Web Model: modeling web hacking for reinforcement learning,"['OriginalPaper', 'Regular contribution']","Website hacking is a frequent attack type used by malicious actors to obtain confidential information, modify the integrity of web pages or make websites unavailable. The tools used by attackers are becoming more and more automated and sophisticated, and malicious machine learning agents seem to be the next development in this line. In order to provide ethical hackers with similar tools, and to understand the impact and the limitations of artificial agents, we present in this paper a model that formalizes web hacking tasks for reinforcement learning agents. Our model, named Agent Web Model , considers web hacking as a capture-the-flag style challenge, and it defines reinforcement learning problems at seven different levels of abstraction. We discuss the complexity of these problems in terms of actions and states an agent has to deal with, and we show that such a model allows to represent most of the relevant web vulnerabilities. Aware that the driver of advances in reinforcement learning is the availability of standardized challenges, we provide an implementation for the first three abstraction layers, in the hope that the community would consider these challenges in order to develop intelligent web hacking agents.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Coding and Information Theory', 'Management of Computing and Information Systems', 'Communications Engineering, Networks']"
doi:10.1007/s10922-022-09654-8,en,On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement,OriginalPaper,"The evaluation of the impact of using Machine Learning in the management of softwarized networks is considered in multiple research works. In this paper, we propose to evaluate the robustness of online learning for optimal network slice placement. A major assumption in this study is to consider that slice request arrivals are non-stationary. We precisely simulate unpredictable network load variations and compare two Deep Reinforcement Learning (DRL) algorithms: a pure DRL-based algorithm and a heuristically controlled DRL as a hybrid DRL-heuristic algorithm, in order to assess the impact of these unpredictable changes of traffic load on the algorithms performance. We conduct extensive simulations of a large-scale operator infrastructure. The evaluation results show that the proposed hybrid DRL-heuristic approach is more robust and reliable than pure DRL in real network scenarios.","['Computer Science', 'Computer Communication Networks', 'Computer Systems Organization and Communication Networks', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Operations Research/Decision Theory']"
doi:10.1007/s43684-022-00023-5,en,Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed traffic,"['OriginalPaper', 'Original Article']","Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL) has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision-making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic (MA2C) method is proposed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is designed to incorporate fuel efficiency, driving comfort, and the safety of autonomous driving. A comprehensive experimental study is made that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety, and driver comfort.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Control and Systems Theory', 'Machine Learning']"
doi:10.1007/s10479-022-04612-8,en,A deep reinforcement learning assisted simulated annealing algorithm for a maintenance planning problem,"['OriginalPaper', 'Original Research']","Maintenance planning aims to improve the reliability of assets, prevent the occurrence of asset failures, and reduce maintenance costs associated with downtime of assets and maintenance resources (such as spare parts and workforce). Thus, effective maintenance planning is instrumental in ensuring high asset availability with the minimum cost. Nevertheless, to find such optimal planning is a nontrivial task due to the (i) complex and usually nonlinear inter-relationship between different planning decisions (e.g., inventory level and workforce capacity), and (ii) stochastic nature of the system (e.g., random failures of parts installed in assets). To alleviate these challenges, we study a joint maintenance planning problem by considering several decisions simultaneously, including workforce planning, workforce training, and spare parts inventory management. We develop a hybrid solution algorithm ( $$\mathcal {DRLSA}$$ DRLSA ) that is a combination of Double Deep Q-Network based Deep Reinforcement Learning (DRL) and Simulated Annealing (SA) algorithms. In each episode of the proposed algorithm, the best solution found by DRL is delivered to SA to be used as an initial solution, and the best solution of SA is delivered to DRL to be used as the initial state. Different from the traditional SA algorithms where neighborhood structures are selected only randomly, the DRL part of $$\mathcal {DRLSA}$$ DRLSA learns to choose the best neighborhood structure to use based on experience gained from previous episodes. We compare the performance of the proposed solution algorithm with several well-known meta-heuristic algorithms, including Simulated Annealing, Genetic Algorithm (GA), and Variable Neighborhood Search (VNS). Further, we also develop a Machine Learning (ML) algorithm (i.e., K-Median) as another benchmark in which different properties of spare parts (e.g., failure rates, holding costs, and repair rates) are used as clustering features for the ML algorithm. Our study reveals that the $$\mathcal {DRLSA}$$ DRLSA finds the optimal solutions for relatively small-size instances, and it has the potential to outperform traditional meta-heuristic and ML algorithms.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1038/s41598-022-07264-7,en,A target-driven visual navigation method based on intrinsic motivation exploration and space topological cognition,"['OriginalPaper', 'Article']","Target-driven visual navigation is essential for many applications in robotics, and it has gained increasing interest in recent years. In this work, inspired by animal cognitive mechanisms, we propose a novel navigation architecture that simultaneously learns exploration policy and encodes environmental structure. First, to learn exploration policy directly from raw visual input, we use deep reinforcement learning as the basic framework and allow agents to create rewards for themselves as learning signals. In our approach, the reward for the current observation is driven by curiosity and calculated by a count-based approach and temporal distance. While agents learn exploration policy, we use temporal distance to find waypoints in observation sequences and incrementally describe the structure of the environment in a way that integrates episodic memory. Finally, space topological cognition is integrated into the model as a path planning module and combined with a locomotion network to obtain a more generalized approach to navigation. We test our approach in the DMlab, a visually rich 3D environment, and validate its exploration efficiency and navigation performance through extensive experiments. The experimental results show that our approach can explore and encode the environment more efficiently and has better capability in dealing with stochastic objects. In navigation tasks, agents can use space topological cognition to effectively reach the target and guide detour behaviour when a path is unavailable, exhibiting good environmental adaptability.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-021-05737-w,en,Cross-domain recommendation based on latent factor alignment,"['OriginalPaper', 'Special issue on Multi-modal Information Learning and Analytics on Big Data']","Recently, various cross-domain recommendation (CDR) models are proposed to overcome the sparsity problem, which leverage relatively abundant rating data from the auxiliary domain to improve recommendation performance of target domain. Though matrix factorization-based collaborative filtering algorithms gain dominance in single-domain recommendation systems, they cannot be used directly in cross-domain cases as the obtained latent factors of the target and auxiliary domains may not be aligned, which will lead to inaccurate knowledge transfer from the auxiliary domain to the target one. A CDR model named CDCFLFA is presented in this paper to solve this problem. In CDCFLFA, firstly latent factors between the two domains are aligned based on pattern matching. Then, user preferences of the auxiliary domain are transferred to update the original user latent vectors of target domain. Finally, a linear least square problem is solved to compute the item latent vectors of target domain and thus unknown ratings are obtained according to the updated user and item latent vectors. CDCFLFA does not require the same user or item sets between the two domains. Extensive experiments are conducted, and the results show that CDCFLFA achieves smaller MAE and RMSE values and larger precision and recall than the previous single- and cross-domain recommendation methods. Hence, CDCFLFA can be regarded as an effective cross-domain extension of single-domain matrix factorization algorithm.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12083-021-01273-5,en,Energy-effective artificial internet-of-things application deployment in edge-cloud systems,OriginalPaper,"Recently, the Internet-of-Things technique is believed to play an important role as the foundation of the coming Artificial Intelligence age for its capability to sense and collect real-time context information of the world, and the concept Artificial Intelligence of Things (AIoT) is developed to summarize this vision. However, in typical centralized architecture, the increasing of device links and massive data will bring huge congestion to the network, so that the latency brought by unstable and time-consuming long-distance network transmission limits its development. The multi-access edge computing (MEC) technique is now regarded as the key tool to solve this problem. By establishing a MEC-based AIoT service system at the edge of the network, the latency can be reduced with the help of corresponding AIoT services deployed on nearby edge servers. However, as the edge servers are resource-constrained and energy-intensive, we should be more careful in deploying the related AIoT services, especially when they can be composed to make complex applications. In this paper, we modeled complex AIoT applications using directed acyclic graphs (DAGs), and investigated the relationship between the AIoT application performance and the energy cost in the MEC-based service system by translating it into a multi-objective optimization problem, namely the CA $$^3$$ 3 D problem — the optimization problem was efficiently solved with the help of heuristic algorithm. Besides, with the actual simple or complex workflow data set like the Alibaba Cloud and the Montage project, we conducted comprehensive experiments to evaluate the results of our approach. The results showed that the proposed approach can effectively obtain balanced solutions, and the factors that may impact the results were also adequately explored.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1038/s42256-022-00446-y,en,Optimizing quantum annealing schedules with Monte Carlo tree search enhanced with neural networks,"['OriginalPaper', 'Article']","Quantum annealing is a practical approach to approximately implement the adiabatic quantum computational model in a real-world setting. The goal of an adiabatic algorithm is to prepare the ground state of a problem-encoded Hamiltonian at the end of an annealing path. This is typically achieved by driving the dynamical evolution of a quantum system slowly to enforce adiabaticity. Properly optimized annealing schedules often considerably accelerate the computational process. Inspired by the recent success of deep reinforcement learning such as DeepMind’s AlphaZero, we propose a Monte Carlo tree search (MCTS) algorithm and its enhanced version boosted by neural networks—which we name QuantumZero (QZero)—to automate the design of annealing schedules in a hybrid quantum–classical framework. Both the MCTS and QZero algorithms perform remarkably well in discovering effective annealing schedules even when the annealing time is short for the 3-SAT examples considered in this study. Furthermore, the flexibility of neural networks allows us to apply transfer-learning techniques to boost QZero’s performance. We demonstrate in benchmark studies that MCTS and QZero perform more efficiently than other reinforcement learning algorithms in designing annealing schedules. Quantum annealers are computational models implemented on quantum hardware that can efficiently solve combinatorial optimization problems. Annealing schedules with enhanced performance can be discovered with a Monte Carlo tree search algorithm and an enhanced version incorporating value and policy neural networks—as inspired by DeepMind’s AlphaZero.","['Engineering', 'Engineering, general']"
doi:10.1007/s10479-022-04572-z,en,Strategic bidding in freight transport using deep reinforcement learning,"['OriginalPaper', 'Original Research']","This paper presents a multi-agent reinforcement learning algorithm to represent strategic bidding behavior by carriers and shippers in freight transport markets. We investigate whether feasible market equilibriums arise without central control or communication between agents. Observed behavior in such environments serves as a stepping stone towards self-organizing logistics systems like the Physical Internet, while also offering valuable insights for the design of contemporary transport brokerage platforms. We model an agent-based environment in which shipper and carrier actively learn bidding strategies using policy gradient methods, posing bid- and ask prices at the individual container level. Both agents aim to learn the best response given the expected behavior of the opposing agent. Inspired by financial markets, a neutral broker allocates jobs based on bid-ask spreads. Our game-theoretical analysis and numerical experiments focus on behavioral insights. To evaluate system performance, we measure adherence to Nash equilibria, fairness of reward division and utilization of transport capacity. We observe good performance both in predictable, deterministic settings ( $$\sim $$ ∼  95% adherence to Nash equilibria) and highly stochastic environments ( $$\sim $$ ∼  85% adherence). Risk-seeking behavior may increase an agent’s reward share, yet overly aggressive strategies destabilize the system. The results suggest a potential for full automation and decentralization of freight transport markets. These insights ease the design of real-world market platforms, suggesting an innate tendency of markets to reach equilibria without behavioral models, information sharing or explicit incentives.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s10846-022-01577-5,en,Autonomous Learning in a Pseudo-Episodic Physical Environment,"['OriginalPaper', 'Regular paper']","Forpractical considerations reinforcement learning has proven to be a difficult task outside of simulation when applied to a physical experiment. Here we derive an optional approach to model free reinforcement learning, achieved entirely online, through careful experimental design and algorithmic decision making. We design a reinforcement learning scheme to implement traditionally episodic algorithms for an unstable 1-dimensional mechanical environment. The training scheme is completely autonomous, requiring no human to be present throughout the learning process. We show that the pseudo-episodic technique allows for additional learning updates with off-policy actor-critic and experience replay methods. We show that including these additional updates between periods of traditional training episodes can improve speed and consistency of learning. Furthermore, we validate the procedure in experimental hardware. In the physical environment, several algorithm variants learned rapidly, each surpassing baseline maximum reward. The algorithms in this research are model free and use only information obtained by an onboard sensor during training.","['Engineering', 'Control, Robotics, Mechatronics', 'Electrical Engineering', 'Artificial Intelligence', 'Mechanical Engineering']"
doi:10.1007/s10845-021-01867-z,en,A review of motion planning algorithms for intelligent robots,ReviewPaper,"Principles of typical motion planning algorithms are investigated and analyzed in this paper. These algorithms include traditional planning algorithms, classical machine learning algorithms, optimal value reinforcement learning, and policy gradient reinforcement learning. Traditional planning algorithms investigated include graph search algorithms , sampling-based algorithms , interpolating curve algorithms , and reaction-based algorithms . Classical machine learning algorithms include multiclass support vector machine , long short-term memory , Monte-Carlo tree search and convolutional neural network . Optimal value reinforcement learning algorithms include Q learning , deep Q-learning network , double deep Q-learning network , dueling deep Q-learning network . Policy gradient algorithms include policy gradient method , actor-critic algorithm , asynchronous advantage actor-critic , advantage actor-critic , deterministic policy gradient , deep deterministic policy gradient , trust region policy optimization and proximal policy optimization . New general criteria are also introduced to evaluate the performance and application of motion planning algorithms by analytical comparisons. The convergence speed and stability of optimal value and policy gradient algorithms are specially analyzed. Future directions are presented analytically according to principles and analytical comparisons of motion planning algorithms. This paper provides researchers with a clear and comprehensive understanding about advantages, disadvantages, relationships, and future of motion planning algorithms in robots, and paves ways for better motion planning algorithms in academia, engineering, and manufacturing.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00521-021-06270-6,en,Discrete-to-deep reinforcement learning methods,"['OriginalPaper', 'S.I. : Adaptive and Learning Agents 2020']","Neural networks are effective function approximators, but hard to train in the reinforcement learning (RL) context mainly because samples are correlated. In complex problems, a neural RL approach is often able to learn a better solution than tabular RL, but generally takes longer. This paper proposes two methods, Discrete-to-Deep Supervised Policy Learning (D2D-SPL) and Discrete-to-Deep Supervised Q-value Learning (D2D-SQL), whose objective is to acquire the generalisability of a neural network at a cost nearer to that of a tabular method. Both methods combine RL and supervised learning (SL) and are based on the idea that a fast-learning tabular method can generate off-policy data to accelerate learning in neural RL. D2D-SPL uses the data to train a classifier which is then used as a controller for the RL problem. D2D-SQL uses the data to initialise a neural network which is then allowed to continue learning using another RL method. We demonstrate the viability of our algorithms with Cartpole, Lunar Lander and an aircraft manoeuvring problem, three continuous-space environments with low-dimensional state variables. Both methods learn at least 38% faster than baseline methods and yield policies that outperform them.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-021-06104-5,en,Lucid dreaming for experience replay: refreshing past states with the current policy,"['OriginalPaper', 'S.I. : Adaptive and Learning Agents 2020']","Experience replay (ER) improves the data efficiency of off-policy reinforcement learning (RL) algorithms by allowing an agent to store and reuse its past experiences in a replay buffer. While many techniques have been proposed to enhance ER by biasing how experiences are sampled from the buffer, thus far they have not considered strategies for refreshing experiences inside the buffer. In this work, we introduce L uc i d D reaming for E xperience R eplay (LiDER) , a conceptually new framework that allows replay experiences to be refreshed by leveraging the agent’s current policy. LiDER consists of three steps: First, LiDER moves an agent back to a past state. Second, from that state, LiDER then lets the agent execute a sequence of actions by following its current policy—as if the agent were “dreaming” about the past and can try out different behaviors to encounter new experiences in the dream. Third, LiDER stores and reuses the new experience if it turned out better than what the agent previously experienced, i.e., to refresh its memories. LiDER is designed to be easily incorporated into off-policy, multi-worker RL algorithms that use ER; we present in this work a case study of applying LiDER to an actor–critic-based algorithm. Results show LiDER consistently improves performance over the baseline in six Atari 2600 games. Our open-source implementation of LiDER and the data used to generate all plots in this work are available at https://github.com/duyunshu/lucid-dreaming-for-exp-replay .","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.3758/s13428-021-01602-9,en,Building an intelligent recommendation system for personalized test scheduling in computerized assessments: A reinforcement learning approach,OriginalPaper,"The introduction of computerized formative assessments in the classroom has opened a new area of effective progress monitoring with more accessible test administrations. With computerized formative assessments, all students could be tested at the same time and with the same number of test administrations within a school year. Alternatively, the decision for the number and frequency of such tests could be made by teachers based on their observations and personal judgments about students. However, this often results in rigid test scheduling that fails to take into account the pace at which students acquire knowledge. To administer computerized formative assessments efficiently, teachers should be provided with systematic guidance regarding effective test scheduling based on each student’s level of progress. In this study, we introduce an intelligent recommendation system that can gauge the optimal number and timing of testing for each student. We discuss how to build an intelligent recommendation system using a reinforcement learning approach. Then, we present a case study with a large sample of students’ test results in a computerized formative assessment. We show that the intelligent recommendation system can significantly reduce the number of testing for the students by eliminating unnecessary test administrations where students do not show significant progress (i.e., growth). Also, the proposed recommendation system is capable of identifying the optimal test time for students to demonstrate adequate progress from one test administration to another. Implications for future research on personalized assessment scheduling are discussed.","['Psychology', 'Cognitive Psychology']"
doi:10.1007/s00521-021-05933-8,en,A model-based collaborate filtering algorithm based on stacked AutoEncoder,"['OriginalPaper', 'S.I: Cognitive-inspired Computing and Applications']","Recently, recommender systems are widely used on various platforms in real world to provide personalized recommendations. However, sparsity is a tough problem in a Collaborate Filtering (CF) recommender system as it always leads to the over-fitting problem. This paper proposes a Model-based Collaborate Filtering Algorithm Based on Stacked AutoEncoder (MCFSAE) to overcome the sparsity problem. In the MCFSAE model, we first convert the rating matrix into a high-dimensional classification dataset with a size equal to the number of ratings. As the number of ratings is usually large scale, the classification performance can be guaranteed. Since the obtained classification dataset is high dimensional, we then utilize Stacked AutoEncoder, which is a good nonlinear feature reduction model, to obtain a high-level low-dimensional feature presentation. Finally, a softmax classification model is used to predict the unknown ratings based on the high-level features. Extensive experiments on EachMovie and MovieLens datasets are conducted to compare the proposed MCFSAE model with other SOTA CF models. Experimental results show that MCFSAE performs better than other CF models, especially when the rating matrix is sparse.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-021-06129-w,en,Scalable multi-product inventory control with lead time constraints using reinforcement learning,"['OriginalPaper', 'S.I. : Adaptive and Learning Agents 2020']","Determining optimum inventory replenishment decisions are critical for retail businesses with uncertain demand. The problem becomes particularly challenging when multiple products with different lead times and cross-product constraints are considered. This paper addresses the aforementioned challenges in multi-product, multi-period inventory management using deep reinforcement learning (deep RL). The proposed approach improves upon existing methods for inventory control on three fronts: (1) concurrent inventory management of a large number (hundreds) of products under realistic constraints, (2) minimal retraining requirements on the RL agent under system changes through the definition of an individual product meta-model, (3) efficient handling of multi-period constraints that stem from different lead times of different products. We approach the inventory problem as a special class of dynamical system control, and explain why the generic problem cannot be satisfactorily solved using classical optimisation techniques. Subsequently, we formulate the problem in a general framework that can be used for parallelised decision-making using off-the-shelf RL algorithms. We also benchmark the formulation against the theoretical optimum achieved by linear programming under the assumptions that the demands are deterministic and known apriori. Experiments on scales between 100 and 220 products show that the proposed RL-based approaches perform better than the baseline heuristics, and quite close to the theoretical optimum. Furthermore, they are also able to transfer learning without retraining to inventory control problems involving different number of products.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1038/s41598-021-04124-8,en,SVBE: searchable and verifiable blockchain-based electronic medical records system,"['OriginalPaper', 'Article']","Central management of electronic medical systems faces a major challenge because it requires trust in a single entity that cannot effectively protect files from unauthorized access or attacks. This challenge makes it difficult to provide some services in central electronic medical systems, such as file search and verification, although they are needed. This gap motivated us to develop a system based on blockchain that has several characteristics: decentralization, security, anonymity, immutability, and tamper-proof. The proposed system provides several services: storage, verification, and search. The system consists of a smart contract that connects to a decentralized user application through which users can transact with the system. In addition, the system uses an interplanetary file system (IPFS) and cloud computing to store patients’ data and files. Experimental results and system security analysis show that the system performs search and verification tasks securely and quickly through the network.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/978-981-16-0739-4_47,en,Policy-Approximation Based Deep Reinforcement Learning Techniques: An Overview,OriginalPaper,"Sewak, Mohit Sahay, Sanjay K. Rathore, Hemant Until recently, Deep Reinforcement Learning was restricted to innovations in games like Atari, Dota2. Despite surpassing the benchmarks established by their human counterparts in multiple games, these methods could not scale to real-life and industrial automation tasks. The main reason for this was the essential requirement of complex and continuous action control and sophisticated physics of the domain involved in these tasks. Because of these reasons, most of the incumbent solutions for such applications involved the invent of custom planning algorithms. The design of such sophisticated custom solutions required complete knowledge to the dynamics of the domain and its derivatives and hence were not scalable. Policy-based DRL has democratized this space, as now deep reinforcement learning agents could be trained to learn similar sophisticated policies just by learning from the data generated by interacting with these systems or their respective simulations. This has led to significant innovations in real-life and high-value control automation applications like autonomous vehicles, drones, and industrial robots. Therefore, in this paper, we present an overview of different types of policy-approximation based technique in Deep Reinforcement Learning that are the basis of many advanced control automation systems.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Science and Technology Studies', 'Computer Applications']"
doi:10.1007/978-981-19-0638-1_4,en,Policy-Based Reinforcement Learning,OriginalPaper,"Some of the most successful applications of deep reinforcement learning have a continuous action space, such as applications in robotics, self-driving cars, and real-time strategy games.","['Computer Science', 'Machine Learning', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/978-981-16-7996-4_19,en,A Comparative Study of Algorithms for Intelligent Traffic Signal Control,OriginalPaper,"In this paper, methods have been explored to effectively optimize traffic signal control to minimize waiting times and queue lengths, thereby increasing traffic flow. The traffic intersection was first defined as a Markov Decision Process, and a state representation, actions and rewards were chosen. Simulation of Urban MObility (SUMO) was used to simulate an intersection and then compare a Round Robin Scheduler, a Feedback Control mechanism and two Reinforcement Learning techniques—Deep Q-Network (DQN) and Advantage Actor-Critic (A2C), as the policy for the traffic signal in the simulation under different scenarios. Finally, the methods were tested on a simulation of a real-world intersection in Bengaluru, India.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-1-4842-7915-1_14,en,Reinforcement Learning,OriginalPaper,"Reinforcement learning is another field of machine learning besides supervised learning and unsupervised learning. It mainly uses agents to interact with the environment in order to learn strategies that can achieve good results. Different from supervised learning, the action of reinforcement learning does not have clear label information. It only has the reward information from the feedback of the environment. It usually has a certain lag and is used to reflect the ""good and bad"" of the action.","['Computer Science', 'Machine Learning']"
doi:10.1007/978-3-030-82193-7_28,en,Intrinsic Rewards for Reinforcement Learning Within Complex 2D Environments,OriginalPaper,"In this paper, we propose an approach to train an intelligent agent using reinforcement learning in order to draw on a two-dimensional grid. Painting is a creative art, and it will take human beings years to learn how to draw. In the training process, we build grid environments with obstacles and challenges resembling abstract art and then place the agent in different environments to reach the goal. In phase I, We propose using intrinsic rewards based on the state of the model to stimulate the agent’s exploration desire and to increase adaptability in complex environments. In phase II, we prototype a rendering pipeline to translate the agent’s movement during the training process into a painting. Our results show the intrinsic reward method increased the agent’s ability to learn in environments of moderate complexity. The rendering pipeline prototype was evaluated in a single round of crowd sourced evaluation and steps to further improve outlined.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09030-1_4,en,Decision-Making and Learning in an Unknown Environment,OriginalPaper,"This chapter describes how the agent can explore an unknown environmental system in which it has been placed. In doing so, he discovers states with rewards and has to optimize the paths to these goals, on the one hand, but also explore new goals, on the other hand. In doing so, he must consider a trade-off between exploitation and exploration. On the one hand, he has to collect the possible reward of already discovered goals; on the other, hand he has to manage the exploration of better paths or the discovery of new goals. There are different approaches to this; some aim at processing experiences made in such a way that the agent behaves better under the same conditions in the future “model-free methods”; and others that aim at optimizing models that can predict what would happen if certain actions are chosen.","['Computer Science', 'Machine Learning', 'Java', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-981-16-6328-4_30,en,An Adaptive Speed Control Method Based on Deep Reinforcement Learning for Permanent Magnet Synchronous Motor,OriginalPaper,"In this paper, an adaptive PI controller based on deep Q network (DQN) is proposed, which improves the speed control performance of the permanent magnet synchronous motor (PMSM) drive system and solves the contradiction between the rapidity and overshoot of the traditional PI controller. The mathematical model of PMSM vector control system with series PI controller is established, and the parameters of PI controller are calculated by pole assignment method. The damping factor of the speed loop series PI controller is taken as the variable coefficient of the adaptive PI controller and adjusted dynamically. The effectiveness of the proposed method is verified by simulation.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-10986-7_16,en,Incorporating Explanations to Balance the Exploration and Exploitation of Deep Reinforcement Learning,OriginalPaper,"Discovering efficient exploration strategies is a central challenge in reinforcement learning (RL). Deep reinforcement learning (DRL) methods proposed in recent years have mainly focused on improving the generalization of models while ignoring models’ explanation. In this study, an embedding explanation for the advantage actor-critic algorithm (EEA2C) is proposed to balance the relationship between exploration and exploitation for DRL models. Specifically, the proposed algorithm explains agent’s actions before employing explanation to guide exploration. A fusion strategy is then designed to retain information that is helpful for exploration from experience. Based on the results of the fusion strategy, a variational autoencoder (VAE) is designed to encode the task-related explanation into a probabilistic latent representation. The latent representation of the VAE is finally incorporated into the agent’s policy as prior knowledge. Experimental results for six Atari environments show that the proposed method improves the agent’s exploratory capabilities with explainable knowledge.","['Computer Science', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computers and Education', 'Computer Applications']"
doi:10.1007/978-3-031-06458-6_7,en,Deep Reinforcement Learning for Bitcoin Trading,OriginalPaper,"Artificial intelligence (AI) is showing its success in various types of applications. Motivated by this trend, automatic trading has taken a keen interest in applying of artificial intelligence methods to predict the future price of a financial asset to overcome trading challenges including asset price fluctuations and dynamics, Investors must therefore understand when it is appropriate to use the optimal strategy that maximizes their investment return. But achieving a perfect strategy is difficult for an asset with a complex and dynamic price. To overcome these challenges, In this study, we apply a new rule-based strategy technique to train one of the successful machine learning algorithms, known as Deep Reinforcement Learning (DRL) for bitcoin trading. Our proposed method is based on dueling double deep q-learning networks, proximal policy optimization, and advantage actor-critic to achieve an optimal policy. The profit reward functions and Sharpe ratio are used to assess the proposed DRL. The results of the experiments demonstrate that combining three agents is the most efficient strategy for automatic bitcoin trading.","['Computer Science', 'Computer Applications', 'Data Mining and Knowledge Discovery', 'IT in Business', 'Machine Learning']"
doi:10.1007/978-3-031-15168-2_9,en,Transformer-Based Deep Reinforcement Learning in VizDoom,OriginalPaper,"Transformers is a novel neural network architecture that is successfully used in natural language processing tasks and is starting to be used in other areas such as video processing and image processing. However, transformers are yet to be studied in different aspects of reinforcement learning scenarios. In this work we combine transformer architectures with reinforcement learning and train them in the VizDoom game environment, producing agents that play better in comparison to traditional neural network architectures.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Information Systems Applications (incl. Internet)', 'Database Management', 'Computer Appl. in Social and Behavioral Sciences', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-030-92574-1_24,en,Building Intelligent Navigation System for Mobile Robots Based on the Actor – Critic Algorithm,OriginalPaper,"This article presents the construction of an intelligent automatic navigation system for mobile robots in a flat environment with defined and unknown obstacles. Programming tools used in the studies are the operating system for mobile robots (Robot Operating System – ROS). From the updated information on maps, operating environment, robot control position and obstacles (Simultaneous Localization and Mapping (SLAM)), we can calculate the motion trajectory of the mobile robot. The navigation system calculates the global and local trajectory for the robot based on the application of Actor-Critic (AC) algorithm. The results of simulation studies in the Gazebo environment and the experimental run on the real Turtlebot mobile robot showed the practical efficiency of automatic navigation for this mobile robot.","['Engineering', 'Mathematical and Computational Engineering', 'Mechanical Engineering', 'Electrical Engineering']"
doi:10.1007/978-3-031-20868-3_13,en,Sequential Decision Making with “Sequential Information” in Deep Reinforcement Learning,OriginalPaper,"By learning policy directly from high-dimensional visual inputs, e.g., video frames, Deep Reinforcement Learning (DRL) has achieved great successes for solving sequential decision-making problems, where 2D convolutional network is usually adopted for extracting the underlying spatial features. However, such spatial feature extraction methods do not consider the temporal information existed in the input frames. To address this issue, Transformer, 3D convolutional network and Long Short-Term Memory (LSTM) have been used in DRL, but often result in excessive increase of model parameters and computation cost. Furthermore, multiple down-sampling of images will lead to the loss of sequential information. In this paper, we propose a novel model for extracting sequential information, namely temporal aggregation network (TAN). Comparing with existing methods, TAN can extract the sequential information without the needs of multiple downsampling of consecutive images. Moreover, by decoupling the computation between spatial and channel dimensions, a lightweight model is built in TAN. Experiments in classic Atari 2600 games show that our method can improve the efficiency of decision-making of DRL algorithms compared with baselines.","['Computer Science', 'Artificial Intelligence', 'Computing Milieux', 'Information Systems and Communication Service', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-19214-2_25,en,Toward Multi-sided Fairness: A Fairness-Aware Order Dispatch System for Instant Delivery Service,OriginalPaper,"Instant delivery platforms, equipped with professional couriers to provide convenient delivery services, have emerged rapidly in many cities. For the benefit of platforms, many researchers focus more on maximizing overall efficiency but ignore individual fairness. Current fairness research in mobile systems mainly concentrates on one-sided or two-sided relationships, such as drivers and customers. However, instant delivery services have two new characteristics in fairness: (i) multi-stakeholder involvement , namely couriers, merchants and users should be considered comprehensively; (ii) more complicated matching relationship because of the concurrent dispatch mode , meaning one courier will handle multiple orders simultaneously. To handle this multi-sided fairness problem, our paper proposes a novel order dispatch system to balance the platform revenue and multi-stakeholder fairness. Motivated by the analysis of real-world datasets, we formulate the order dispatch problem as a sequential decision-making problem and incorporate multi-sided fairness into the decision criteria. Then, we design a multi-sided fairness-aware deep reinforcement learning algorithm to solve large-scale decision problem, with the fairness relying on Least Misery Fairness definition for users and Variance Fairness definition for couriers and merchants. Finally, extensive experiments show the effectiveness of our model in balancing multi-sided fairness among stakeholders and long-term profits of the whole platform.","['Engineering', 'Wireless and Mobile Communication', 'Artificial Intelligence', 'Computer Applications', 'Computing Milieux', 'Computer Communication Networks']"
doi:10.1007/978-981-19-0468-4_15,en,Skill Reward for Safe Deep Reinforcement Learning,OriginalPaper,"Reinforcement learning technology enables an agent to interact with the environment and learn from experience to maximize the cumulative reward of specific tasks, and get a powerful agent to solve decision optimization problems. This process is highly similar to our human learning process, that is, learning from the interaction with the environment. As we know, the behavior of an agent based on deep reinforcement learning is often unpredictable, and the agent will produce some weird and unsafe behavior sometimes. To make the behavior and the decision process of the agent explainable and controllable, this paper proposed the skill reward method that the agent can be constrained to learn some controllable and safe behaviors. When an agent finishes specific skills in the process of interaction with the environment, we can design the rewards obtained by the agent during the exploration process based on prior knowledge to make the learning process converge quickly. The skill reward can be embedded into the existing reinforcement learning algorithms. In this work, we embed the skill reward into the asynchronous advantage actor-critic (A3C) algorithm, and test the method in an Atari 2600 environment (Breakout-v4). The experiments demonstrate the effectiveness of the skill reward embedding method.","['Computer Science', 'Computer Applications', 'Cryptology', 'Mobile and Network Security', 'Computer Crime']"
doi:10.1007/978-3-030-95502-1_27,en,Evaluating the Efficacy of Different Neural Network Deep Reinforcement Algorithms in Complex Search-and-Retrieve Virtual Simulations,OriginalPaper,"In recent years, Deep Reinforcement Learning (DRL) has been extensively used to solve problems in various domains like traffic control, healthcare, and simulation-based training. Proximal Policy Optimization (PPO) and Soft-Actor Critic (SAC) methods are DRL’s latest state of art on-policy and off-policy algorithms. Though previous studies have shown that SAC generally performs better than PPO, hyperparameter tuning can significantly impact the performance of these algorithms. Also, a systematic evaluation of the efficacy of these algorithms after hyperparameter tuning in dynamic and complex environments is missing and much needed in literature. This research aims to evaluate the effect of the number of layers and nodes in SAC and PPO algorithms in a search-and-retrieve task developed in the Unity 3D game engine. In the task, a bot had to navigate through the physical mesh and collect ‘target’ objects while avoiding ‘distractor’ objects. We compared the SAC and PPO models on four different test conditions that differed in the ratios of targets and distractors. Results revealed that PPO performed better than SAC for all test conditions when the number of layers and units present in the architecture was the lowest. When the number of targets was more than the distractors (9:1), PPO outperformed SAC, especially when the number of units and layers were large. Furthermore, increasing the layers and units per layer was responsible for increasing PPO and SAC performance. Results also implied that similar hyperparameter settings might be used while comparing models developed using DRL algorithms. We discuss the implications of these results and explore the possible applications of using modern, state-of-the-art DRL algorithms to learn the semantics and idiosyncrasies associated with complex and dynamic environments.","['Computer Science', 'Artificial Intelligence', 'Information Systems and Communication Service', 'Computer Applications', 'Computers and Education', 'Computer Systems Organization and Communication Networks', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-16-9492-9_175,en,An Improved Neural-Network-Based Deterministic Policy Gradient Algorithm for Nonlinear Systems,OriginalPaper,"This paper considers the control problem for the general nonlinear systems with unknown dynamical model. An improved model-free neural-network-based deterministic policy gradient (NNDPG) algorithm is designed due to the potentially variable physical model of the nonlinear system in some practical applications. Markov decision processes of systems are established under unknown transition probabilities. Based on the deterministic policy gradient theory and neural network approximation, the policy network and evaluation network are designed to approximate the policy function and value function, respectively. The prioritized experience replay Sum Tree is incorporated into the algorithm to improve its performance and learning efficiency. Simulations illustrate that the performance of the proposed method is still competitive with model-based controllers.","['Engineering', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16564-1_18,en,Deep Reinforcement Learning for Automated Stock Trading: Inclusion of Short Selling,OriginalPaper,"Multiple facets of the financial industry, such as algorithmic trading, have greatly benefited from their unison with cutting-edge machine learning research in recent years. However, despite significant research efforts directed towards leveraging supervised learning methods alone for designing superior algorithmic trading strategies, existing studies continue to confront significant hurdles like striking the optimum balance of risk and return, incorporating real-world complexities, and minimizing max drawdown periods. This research work proposes a modified deep reinforcement learning (DRL) approach to automated stock trading with the inclusion of short selling, a new thresholding framework, and employs turbulence as a safety switch. The DRL agents’ performance is evaluated on the U.S. stock market’s DJIA index constituents. The modified DRL agents are shown to outperform previous DRL approaches and the DJIA index, in terms of absolute returns, risk-adjusted returns, and lower max drawdowns, while giving insights into the effects of short selling inclusion and proposed thresholding.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Data Mining and Knowledge Discovery', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-8069-5_46,en,Image Denoising Using Fully Connected Network with Reinforcement Learning,OriginalPaper,"Deep reinforcement learning (DRL), where an agent learns behaviors in an environment by actions and receiving rewards, has been applied successfully in the robotics area and game controllers at a human level. However, the application of DRL in image processing is still scarce. In this paper, we present a novel approach for image denoising by combining a fully connected network with the gated recurrent unit in an asynchronous advantage actor-critic scheme. The proposed method assigns an agent to every pixel of the input image, and the agent changes the value of each pixel by selecting an action from a predefined list. The goal is to learn an optimal policy to maximize the reward at all pixels of the image. We conduct the denoising experiments on the BSD68 dataset and the results show that the proposed approach produces equivalent or higher PSNR scores compared to several state-of-the-art models based on supervised learning. Our approach is interpretable to humans by showing the agent’s actions, which is a significant difference from original CNNs.","['Computer Science', 'Computer Applications', 'Artificial Intelligence', 'Systems and Data Security', 'Computers and Education']"
doi:10.1007/978-3-031-16822-2_5,en,Energy-Efficient Multi-Task Multi-Access Computation Offloading via NOMA,OriginalPaper,"Multi-access mobile edge computing (MA-MEC) has been envisioned as one of the key approaches for enabling computation-intensive yet delay-sensitive services in future wireless systems. This chapter leverages non-orthogonal multiple access (NOMA) for computation offloading in MA-MEC and studies a joint optimization of the multi-access multi-task computation offloading, NOMA transmission, and computation-resource allocation, with the objective of minimizing the total energy consumption of wireless device to complete its tasks. This chapter firstly focuses on a static channel scenario and proposes a layered algorithm to solve the joint optimization problem. Furthermore, this chapter considers a dynamic channel scenario in which the channel power gains from the wireless device to the edge-computing servers are time-varying. To tackle with the difficulty due to the huge number of different channel realizations in the dynamic scenario, this chapter proposes an online algorithm, which is based on deep reinforcement learning (DRL), to efficiently learn the near-optimal offloading solutions for the time-varying channel realizations. Numerical results are provided to validate the proposed layered algorithm for the static channel scenario and the DRL-based online algorithm for the dynamic channel scenario. The chapter is organized as follows. Section 5.1 illustrates this considered system model and problem formulation. Section 5.2 presents the layered energy-efficient multi-task multi-access offloading algorithm. Section 5.3 illustrates the performance evaluation. We review the related studies in Sect. 5.4. Finally, we conclude this chapter in Sect. 5.5 and discuss the future directions.","['Computer Science', 'Computer Communication Networks', 'Wireless and Mobile Communication', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-18192-4_2,en,Investigating Effects of Centralized Learning Decentralized Execution on Team Coordination in the Level Based Foraging Environment as a Sequential Social Dilemma,OriginalPaper,"In this work, we investigate the effects of centralized learning decentralized execution algorithms on agent coordination in a modified version of the Level Based Foraging environment that behaves as a sequential social dilemma. We show that with individual agent rewards, Level Based Foraging becomes a sequential social dilemma. When compared with previously reported results on the Level based Foraging environment using joint rewards [ 13 ], we observe significant convergence rate improvements for algorithms that perform state action value estimation: IQL, VDN and QMIX.","['Computer Science', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Software Engineering/Programming and Operating Systems', 'Information Systems and Communication Service', 'Computer Communication Networks']"
doi:10.1007/978-3-031-10769-6_35,en,Guiding an Automated Theorem Prover with Neural Rewriting,OriginalPaper,"Automated theorem provers (ATPs) are today used to attack open problems in several areas of mathematics. An ongoing project by Kinyon and Veroff uses Prover9 to search for the proof of the Abelian Inner Mapping (AIM) Conjecture, one of the top open conjectures in quasigroup theory. In this work, we improve Prover9 on a benchmark of AIM problems by neural synthesis of useful alternative formulations of the goal. In particular, we design the 3SIL (stratified shortest solution imitation learning) method. 3SIL trains a neural predictor through a reinforcement learning (RL) loop to propose correct rewrites of the conjecture that guide the search. 3SIL is first developed on a simpler, Robinson arithmetic rewriting task for which the reward structure is similar to theorem proving. There we show that 3SIL outperforms other RL methods. Next we train 3SIL on the AIM benchmark and show that the final trained network, deciding what actions to take within the equational rewriting environment, proves 70.2% of problems, outperforming Waldmeister (65.5%). When we combine the rewrites suggested by the network with Prover9, we prove 8.3% more theorems than Prover9 in the same time, bringing the performance of the combined system to 90%.","['Computer Science', 'Mathematical Logic and Formal Languages', 'Software Engineering/Programming and Operating Systems', 'Information Systems and Communication Service', 'Artificial Intelligence', 'Logics and Meanings of Programs']"
doi:10.1631/FITEE.2100331,en,Multi-agent deep reinforcement learning for end—edge orchestrated resource allocation in industrial wireless networks,"['OriginalPaper', 'Research Article']","Edge artificial intelligence will empower the ever simple industrial wireless networks (IWNs) supporting complex and dynamic tasks by collaboratively exploiting the computation and communication resources of both machine-type devices (MTDs) and edge servers. In this paper, we propose a multi-agent deep reinforcement learning based resource allocation (MADRL-RA) algorithm for end-edge orchestrated IWNs to support computation-intensive and delay-sensitive applications. First, we present the system model of IWNs, wherein each MTD is regarded as a self-learning agent. Then, we apply the Markov decision process to formulate a minimum system overhead problem with joint optimization of delay and energy consumption. Next, we employ MADRL to defeat the explosive state space and learn an effective resource allocation policy with respect to computing decision, computation capacity, and transmission power. To break the time correlation of training data while accelerating the learning process of MADRL-RA, we design a weighted experience replay to store and sample experiences categorically. Furthermore, we propose a step-by-step ε -greedy method to balance exploitation and exploration. Finally, we verify the effectiveness of MADRL-RA by comparing it with some benchmark algorithms in many experiments, showing that MADRL-RA converges quickly and learns an effective resource allocation policy achieving the minimum system overhead. 边缘人工智能通过协同利用设备侧和边缘侧有限的网络、计算资源,赋能工业无线网络以支持复杂和动态工业任务。面向资源受限的工业无线网络,我们提出一种基于多智能体深度强化学习的资源分配(MADRL-RA)算法,实现了端边协同资源分配,支持计算密集型、时延敏感型工业应用。首先,建立了端边协同的工业无线网络系统模型,将具有感知能力的工业设备作为自学习的智能代理。然后,采用马尔可夫决策过程对端边资源分配问题进行形式化描述,建立关于时延和能耗联合优化的最小系统开销问题。接着,利用多智能体深度强化学习克服状态空间维灾,同时学习关于计算决策、算力分配和传输功率的有效资源分配策略。为了打破训练数据的时间相关性,同时加速MADRL-RA学习过程,设计了一种带经验权重的经验回放方法,对经验进行分类存储和采样。在此基础上,提出步进的ε-贪婪方法来平衡智能代理对经验的利用与探索。最后,通过大量对比实验,验证了MADRL-RA算法相较于多种基线算法的有效性。实验结果表明,MADRL-RA收敛速度快,能够学习到有效资源分配策略以实现最小系统开销。","['Computer Science', 'Computer Science, general', 'Electrical Engineering', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Electronics and Microelectronics, Instrumentation', 'Communications Engineering, Networks']"
doi:10.1007/978-3-030-98064-1_9,en,Deep Reinforcement Learning for Mobile Edge Computing Systems,OriginalPaper,"In mobile edge computing (MEC) systems, network entities and mobile devices need to make decisions to enable efficient use of network and computational resources. Such decision making can be challenging because the environment in MEC systems can be complex and involve time-varying system dynamics. To address such challenges, deep reinforcement learning (DRL) emerges as a promising method. It enables agents (e.g., network entities, mobile devices) to learn the optimal decision-making policy through interacting with the environment. In this chapter, we describe how DRL can be incorporated into MEC systems for improving the system performance. We first give an overview of DRL techniques. Then, we present a case study on the task offloading problem in MEC systems. In particular, we focus on the unknown and time-varying load level dynamics at the edge nodes and formulate a task offloading problem for minimizing the task delay and the ratio of dropped tasks. We propose a deep Q-learning-based algorithm that enables the mobile devices to make their task offloading decisions in a decentralized fashion with local information. This algorithm incorporates double deep Q-network (DQN) and dueling DQN techniques for enhancing the algorithm performance. Simulation results demonstrate that the proposed algorithm can reduce the task delay and ratio of dropped tasks significantly when compared with the existing methods. Finally, we outline several challenges and future research directions.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Computer Communication Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-15931-2_8,en,Reinforcement Learning for the Pickup and Delivery Problem,OriginalPaper,"The pickup and delivery problem (PDP) and its related variants are an important part in the field of urban logistics and distribution, and there are many heuristic algorithms to solve them. However, with the continuous expansion of logistics scale, these methods generally have the problem of too long calculation time. In order to solve this problem, we propose a reinforcement learning (RL) model based on the Advantage Actor-Critic, which regards PDP as a sequential decision problem. The actor based on the attention mechanism is responsible for generating routing strategies. The critic is designed to improve the solution quality during training. The model is trained using policy gradient. The experimental results show that compared with the heuristic algorithms and previous RL approach, the proposed model has obvious advantages in computational time, and it is also competitive in terms of solution quality.","['Computer Science', 'Artificial Intelligence', 'Information Systems and Communication Service', 'Computing Milieux', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Applications']"
doi:10.1007/978-3-031-13844-7_1,en,Knowledge-Enhanced Scene Context Embedding for Object-Oriented Navigation of Autonomous Robots,OriginalPaper,"Object-oriented navigation in unknown environments with only vision as input has been a challenging task for autonomous robots. Introducing semantic knowledge into the model has been proved to be an effective means to improve the suboptimal performance and the generalization of existing end-to-end learning methods. In this paper, we improve object-oriented navigation by proposing a knowledge-enhanced scene context embedding method, which consists of a reasonable knowledge graph and a designed novel 6-D context vector. The developed knowledge graph (named MattKG) is derived from large-scale real-world scenes and contains object-level relationships that are expected to assist robots to understand the environment. The designed novel 6-D context vector replaces traditional pixel-level raw features by embedding observations as scene context. The experimental results on the public dataset AI2-THOR indicate that our method improves both the navigation success rate and efficiency compared with other state-of-the-art models. We also deploy the proposed method on a physical robot and apply it to the real-world environment.","['Computer Science', 'Artificial Intelligence', 'Software Engineering', 'Information Systems Applications (incl. Internet)', 'User Interfaces and Human Computer Interaction', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems']"
doi:10.1007/978-3-030-86261-9_6,en,Byzantine Resilient Aggregation in Distributed Reinforcement Learning,OriginalPaper,"Recent distributed reinforcement learning techniques utilize networked agents to accelerate exploration and speed up learning. However, such techniques are not resilient in the presence of Byzantine agents which can disturb convergence. In this paper, we present a Byzantine resilient aggregation rule for distributed reinforcement learning with networked agents that incorporates the idea of optimizing the objective function in designing the aggregation rules. We evaluate our approach using multiple reinforcement learning environments for both value-based and policy-based methods with homogeneous and heterogeneous agents. The results show that cooperation using the proposed approach exhibits better learning performance than the non-cooperative case and is resilient in the presence of an arbitrary number of Byzantine agents.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-22695-3_19,en,Evolution Strategies for Sparse Reward Gridworld Environments,OriginalPaper,"We investigate evolution strategies (ES) - an optimisation technique originally developed in the 1960s – for learning policies that work effectively in gridworld environments with sparse rewards. We combine the evolution strategies algorithm with an intrinsic reward, based on observation counts, to optimise the parameters of a neural network. We find that the resulting approach is able to obtain good scores on a number of MiniGrid environments, despite the challenges of sparse rewards, partial observability and the environments being procedurally generated. These scores are comparable with deep reinforcement learning approaches that optimise neural network parameters using gradient descent. However, evolution strategies has a number of advantages. It is simple to implement and uses relatively few hyperparameters, it has less reliance on specialised hardware and it is highly parallelisable. In combination, these properties make it a promising alternative to reinforcement learning for sparse reward gridworld problems.","['Computer Science', 'Artificial Intelligence', 'Computer Communication Networks', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-030-98064-1_3,en,Responsive Regulation of Dynamic UAV Communication Networks Based on Deep Reinforcement Learning,OriginalPaper,"In this chapter, the regulation of an Unmanned Aerial Vehicle (UAV) communication network is investigated in the presence of dynamic changes in the UAV lineup and user distribution. We target an optimal UAV control policy which is capable of identifying the upcoming change in the UAV lineup (quit or join-in) or user distribution, and proactively relocating the UAVs ahead of the change rather than passively dispatching the UAVs after the change. Specifically, a deep reinforcement learning (DRL)-based UAV control framework is developed to maximize the accumulated user satisfaction (US) score for a given time horizon which is able to handle the change in both the UAV lineup and user distribution. The framework accommodates the changed dimension of the state-action space before and after the UAV lineup change by deliberate state transition design. In addition, to handle the continuous state and action space, deep deterministic policy gradient (DDPG) algorithm, which is an actor-critic based DRL method, is exploited. Furthermore, to promote learning exploration around the timing of the change, the original DDPG scheme is adapted into an asynchronous parallel computing (APC) structure which leads to better training performance in both the critic and actor networks. Finally, extensive simulations are conducted to validate the convergence of the proposed learning approach, and demonstrate its capability in jointly handling the dynamics in UAV lineup and user distribution as well as its superiority over a passive reaction method.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Computer Communication Networks', 'Artificial Intelligence']"
doi:10.1007/s00500-021-06531-5,en,Cognitive linear discriminant regression computing technique for HTTP video services in SDN networks,"['OriginalPaper', 'Focus']","In recent times, the service providers and the operators are facing the problems like visual quality degradations and frequent interruptions during video streaming, due to higher dynamism of network conditions. The video parameters like occupancy, playback quality, underflow/overflow buffer, and rate switching frequency/amplitude affect the quality of user’s experience. In the present scenario, numerous adaptive streaming protocols are developed to provide continuous service to the user’s under complex network condition with heterogeneous devices. In this research article, linear discriminant regression technique is proposed based on cognitive computing in order to predict the user’s quality of experience over software-defined network. By varying the network and the objective parameters, initially the mean opinion score is collected from the users. Further, a new architecture is developed on the basis of linear discriminant regression technique that uses the mean opinion score under different network conditions to predict the expected mean opinion score. The simulation results showed that the linear discriminant regression technique achieved better video quality compared to linear regression technique in terms of peak signal-to-noise ratio, structural similarity index, and video quality metric by varying the video resolutions, bit-rates, and frame rates.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/978-3-031-19493-1_8,en,Reinforcement Learning with Success Induced Task Prioritization,OriginalPaper,"Many challenging reinforcement learning (RL) problems require designing a distribution of tasks that can be applied to train effective policies. This distribution of tasks can be specified by the curriculum. A curriculum is meant to improve the results of learning and accelerate it. We introduce Success Induced Task Prioritization (SITP), a framework for automatic curriculum learning, where a task sequence is created based on the success rate of each task. In this setting, each task is an algorithmically created environment instance with a unique configuration. The algorithm selects the order of tasks that provide the fastest learning for agents. The probability of selecting any of the tasks for the next stage of learning is determined by evaluating its performance score in previous stages. Experiments were carried out in the Partially Observable Grid Environment for Multiple Agents (POGEMA) and Procgen benchmark. We demonstrate that SITP matches or surpasses the results of other curriculum design methods. Our method can be implemented with handful of minor modifications to any standard RL framework and provides useful prioritization with minimal computational overhead.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-3-030-96772-7_50,en,RepBFL: Reputation Based Blockchain-Enabled Federated Learning Framework for Data Sharing in Internet of Vehicles,OriginalPaper,"Internet of Vehicles (IoV) enables the integration of smart vehicles with Internet and collaborative analysis from shared data among vehicles. Machine learning technologies show significant advantages and efficiency for data analysis in IoV. However, the user data could be sensitive in nature, and the reliability and efficiency of sharing these data is hard to guarantee. Moreover, due to the intermittent and unreliable communications of various distributed vehicles, the traditional machine learning algorithms are not suitable for heterogeneous IoV network. In this paper, we propose a novel reputation mechanism framework that integrates the IoV with blockchain and federated learning named RepBFL. In this framework, blockchain is used to protect the shared data between the vehicles. The Road Side Units (RSU) select the high reputation vehicular nodes to share their data for federated learning. To enhance the security and reliability of the data sharing process, we develop the reputation calculated mechanism to evaluate the reliability of all vehicles in IoV. The proposed framework is feasible for the large heterogeneous vehicular networks and perform the collaborative data analysis in distributed vehicles. The experimental results show that the proposed approach can improve the data sharing efficiency. Furthermore, the reputation mechanism is able to deal with malicious behaviors effectively.","['Computer Science', 'Theory of Computation', 'Computer Communication Networks', 'Computing Milieux', 'Special Purpose and Application-Based Systems', 'Software Engineering/Programming and Operating Systems', 'Artificial Intelligence']"
doi:10.1007/978-3-031-02462-7_32,en,Deep Catan,OriginalPaper,"Catan is a popular multiplayer board game that involves multiple gameplay notions: stochastic elements related to the dice rolls as well as to the initial placement of resources on the map and the drawing of development cards, strategic notions for the placement of the cities and the roads which call upon topological and shape recognition notions and notions of expectation of gains linked to the probabilities of the rolls of the dice. In this paper, we develop a policy for this game using a convolutional neural network. The used deep reinforcement learning algorithm is Expert Iteration [ 2 ] which has already given excellent results for Alpha Zero and its descendants.","['Computer Science', 'Theory of Computation', 'Mathematics of Computing', 'Information Systems and Communication Service', 'Machine Learning']"
doi:10.1007/978-3-031-10186-1_3,en,Motivations for Integrating Edge Intelligence with Blockchain,OriginalPaper,"As shown in Fig. 3.1, the limitations of EI and the complementary advantages of BC are painfully clear. Spontaneously, the appearance of BC-assisted EI would be expected to pave the way for the development of emerging intelligent services. In this chapter, we first discuss the limitations of EI. Then, we elaborate the benefits of BC in EI.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-4546-5_26,en,CATS: A Cache Time-to-Live Setting Auto Adjustment Strategy for an Air Ticket Query Service,OriginalPaper,"A cache is an efficient way to speed up the response of services, which can also be applied in air ticket query services. However, the time-to-live (TTL) setting is a challenging task for the caching mechanism especially when the validity periods of the records change with time, which is the case for the air tickets. In air ticket query services, the validity of air ticket information is verified when a user places the order. It provides a chance to adjust the initial TTLs. In this paper, we provide a CA che T ime-to-live setting auto adjustment S trategy (CATS) for air ticket query service based on the verification results during order placement. In addition, by considering the associations between cached records, CATS can also adjust the TTL settings for related cached records. Experiments are conducted to test and compare different implementations of CATS and the results show that CATS can maintain both the hit rates and verification success rates at a high level.","['Computer Science', 'Computers and Society', 'Information Systems and Communication Service', 'Artificial Intelligence', 'Computer Communication Networks', 'Computer System Implementation', 'Special Purpose and Application-Based Systems']"
doi:10.1007/978-3-031-10983-6_13,en,Improving Parking Occupancy Prediction in Poor Data Conditions Through Customization and Learning to Learn,OriginalPaper,"Parking occupancy prediction (POP) can be used for many real-time parking-related services to significantly reduce the unnecessary cruising for parking and additional congestion. However, accurate and fast forecasting in data-poor car parks remains a challenge. To tackle the bottleneck, this paper proposes a knowledge transfer framework that can customize a lightweight but effective pre-trained network to those data-deficient parking lots for POP. The proposed approach integrates two novel ideas, namely Customization: select source domain utilizing reinforcement learning based on parking-related feature matching; and Learning to Learn: extract insightful prior knowledge from the selected sources using Federated Meta-learning. Results of a real-world case study with 34 parking lots in Guangzhou City, China, from June 1 to 30, 2018, show that compared to the baseline, the proposed approach can 1) bring approximately 21 $$\%$$ % extra performance improvement; 2) improve the model adaptation and convergence speed dramatically; 3) stabilize predictions with error minor variance.","['Computer Science', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Information Systems Applications (incl. Internet)', 'Theory of Computation', 'Computer Appl. in Administrative Data Processing', 'Computer Appl. in Social and Behavioral Sciences']"
doi:10.1007/978-3-030-94763-7_10,en,Task Scheduling and Resource Management in MEC-Enabled Computing Networks,OriginalPaper,"The rapid development of the fifth generation (5G) promotes a variety of new applications, which will pose a huge challenge to the computing resources of networks. Computing networks is a promising technology, which can provide ubiquitous computing resources for applications in 5G. However, resource optimization in computing networks is still an open problem. In this paper, we propose a novel resource allocation framework for computing networks to investigate the energy consumption minimization problem in terms of delay constraint. To tackle the problem, we propose a dynamic task scheduling and resource allocation algorithm to utilizing the Lyapunov optimization method, which doesn’t need to know any prior knowledge of networks. In order to reduce the complexity of solving the problem, we decompose the original problem into several sub-problem to solve. Particulary, the solutions of transmit power and subcarrier assignment are obtained by using the Lagrangian dual decomposition method. The solutions of computation time, postponing time, and CPU-cycle frequency are achieved in the closed form. Simulation results show that the performance of the proposed algorithms and can achieve the tradeoff between the average delay and the average energy consumption.","['Computer Science', 'Special Purpose and Application-Based Systems', 'Computer Communication Networks', 'Computer Applications', 'Computer System Implementation']"
doi:10.1007/978-3-031-16224-4_24,en,Comparison of Reinforcement Learning Based Control Algorithms for One Autonomous Driving Problem,OriginalPaper,"Autonomous driving systems include modules of several levels. Thanks to deep learning architectures at the moment technologies in most of the levels have high accuracy. It is important to notice that currently in autonomous driving systems for many tasks classical methods of supervised learning are no longer applicable. In this paper we are interested in a specific problem, that is to control a car to move along a given reference trajectory using reinforcement learning algorithms. In control theory, this problem is called an optimal control problem for moving along the reference trajectory. Airsim environment is used to simulate a moving car for a fixed period of time without obstacles. The purpose of our research is to determine the best reinforcement learning algorithm for a formulated problem among state-of-the-art algorithms such as DDPG, PPO, SAC, DQN and others. As a result of the conducted training and testing, it was revealed that the best algorithm for this problem is A2C.","['Mathematics', 'Mathematical Applications in Computer Science', 'Data Structures and Information Theory', 'Discrete Mathematics in Computer Science', 'Numerical Analysis', 'Algorithms', 'Artificial Intelligence']"
doi:10.1007/978-3-031-10986-7_44,en,Towards Explainable Reinforcement Learning Using Scoring Mechanism Augmented Agents,OriginalPaper,"Deep reinforcement learning (DRL) is increasingly used in application areas such as medicine and finance. However, the direct mapping from state to action in DRL makes it challenging to explain why decisions are made. Existing algorithms for explaining DRL policy are posteriori, explaining to an agent after it has been trained. As a common limitation, these posteriori methods fail to improve training with the deduced knowledge. Face with that, an end-to-end trainable explanation method is proposed, in which an Adaptive Region Scoring Mechanism (ARS) is embedded into DRL system. The ARS explains the agent’s action by evaluating the features of the input state that are most relevant action before DRL re-learn from task-related regions. The proposed method is validated on Atari games. Experiments demonstrate that agent using the explainable proposed mechanism outperforms the original models.","['Computer Science', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computers and Education', 'Computer Applications']"
doi:10.1007/978-3-031-08011-1_24,en,Hybrid Offline/Online Optimization for Energy Management via Reinforcement Learning,OriginalPaper,"Constrained decision problems in the real world are subject to uncertainty. If predictive information about the stochastic elements is available offline, recent works have shown that it is possible to rely on an (expensive) parameter tuning phase to improve the behavior of a simple online solver so that it roughly matches the solution quality of an anticipative approach but maintains its original efficiency. Here, we start from a state-of-the-art offline/online optimization method that relies on optimality conditions to inject knowledge of a (convex) online approach into an offline solver used for parameter tuning. We then propose to replace the offline step with (Deep) Reinforcement Learning (RL) approaches, which results in a simpler integration scheme with a higher potential for generalization. We introduce two hybrid methods that combine both learning and optimization: the first optimizes all the parameters at once, whereas the second exploits the sequential nature of the online problem via the Markov Decision Process framework. In a case study in energy management, we show the effectiveness of our hybrid approaches, w.r.t. the state-of-the-art and pure RL methods. The combination proves capable of faster convergence and naturally handles constraint satisfaction.","['Computer Science', 'Mathematics of Computing', 'Artificial Intelligence', 'Theory of Computation', 'Information Systems and Communication Service', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-3-031-16302-9_5,en,Deep Learning in Audio Classification,OriginalPaper,"Audio processing technology is happening everywhere in our life. We ask our car to make a call for us while driving, or we let Alexa turn off the light for us when we don’t want to get out of bed before sleep. In all of these audio-based applications and research, it is AI and ML that makes the computer or the smart phone understand us via our voice [ 1 ]. As an important part of artificial intelligence (AI), especially machine learning (ML), which has had great influences in many areas of AI and ML-based research and applications. This paper focuses on deep learning structures and applications for audio classification. We conduct a detailed review of literature in audio-based DL and DRL approaches and applications. We also discuss the limitation and possible future works for audio-based DL approach.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Information Systems and Communication Service', 'Artificial Intelligence', 'Computer Appl. in Social and Behavioral Sciences', 'Computers and Education']"
doi:10.1007/978-3-030-92054-8_5,en,"Artificial Intelligence: Need, Evolution, and Applications for Transportation Systems",OriginalPaper,"Artificial intelligence (AI) is a concept in which entities and systems have the ability to learning and decision-making by imitating biological processes. In this chapter, we first introduce the evolution of AI to give the reader a good grasp of artificial intelligence (AI) and then introduce the existing machine learning (ML) with three typical classifications: unsupervised learning, supervised learning, and reinforcement learning. Supervised learning makes decision based on the output labels provided in training. Unsupervised learning works based on pattern discovery without having the pre-knowledge of output labels. The third machine learning paradigm is reinforcement learning (RL), which takes sequential actions rooted in Markov Decision Process (MDP) with a rewarding or penalizing criterion. Some other kinds of ML algorithms such as federated learning and transfer learning are also introduced in the first subsection. The future transportation network aims to develop a highly dynamic and intelligent system, which enables the networks to change the environment to satisfy various requirements and service types. Cellular-V2X, vehicular edge network, and unmanned aerial vehicle (UAV) are recently attacking network architecture to enable the future transportation. In the second section, we introduce how to integrate AI into cellular-V2X, vehicular edge network, and UAV. Leveraging AI into transportation helps the sector increase passenger safety, reduce traffic congestion and accidents, lessen carbon emissions, and also minimize the overall financial expenses. Finally, we review some existing research that uses AI to enable autonomous driving, traffic control and prediction, and path planning.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Automotive Engineering']"
doi:10.1007/978-3-031-02056-8_18,en,Multi-objective Genetic Programming for Explainable Reinforcement Learning,OriginalPaper,"Deep reinforcement learning has met noticeable successes recently for a wide range of control problems. However, this is typically based on thousands of weights and non-linearities, making solutions complex, not easily reproducible, uninterpretable and heavy. The present paper presents genetic programming approaches for building symbolic controllers. Results are competitive, in particular in the case of delayed rewards, and the solutions are lighter by orders of magnitude and much more understandable.","['Computer Science', 'Professional Computing', 'Algorithm Analysis and Problem Complexity', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4831-2_24,en,Implementing Reinforcement Learning to Design a Game Bot,OriginalPaper,"Artificial intelligence is the technology of the future, it has accomplished tasks that were assumed to be impossible earlier. It has very promising future aspects. It has proven to be useful in almost every field. Training an artificial intelligence model requires a lot of data. 90% of world’s total data has been generated over last two years only. Sometimes it is not feasible to have such huge amounts of data to train models. Here, comes in the situation “reinforcement learning” where the agent learns from the environment. Reinforcement learning has been applied to various fields like manufacturing, inventory management, delivery management, power systems, finance sectors and self-driving cars. Using the same procedure an artificial agent can be trained to perform desirable tasks. In this paper we discuss the implementation of reinforcement learning to develop a bot that plays a game just like humans.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Computer Applications', 'Machine Learning']"
doi:10.1007/978-3-030-95405-5_24,en,PS-QMix: A Parallel Learning Framework for Q-Mix Using Parameter Server,OriginalPaper,"With the development of deep reinforcement learning and multi-agent modeling, Multi-Agent Reinforcement Learning (MARL) has become a very active research topic recently. Q-Mix is a popular algorithm for solving MARL tasks where the individual agents are allowed to be trained in a centralized manner. As the scale and complexity of MARL tasks grow, there is an urging requirement for a more efficient training strategy. As a consequence, it is demanding to develop a Q-Mix training algorithm which can benefit from parallel computation. However, how classic distributed machine learning frameworks work with Q-Mix is a less studied problem. In this paper, we propose the PS-Qmix algorithm to apply the Parameter Server framework to training QMix agents in parallel. Our algorithm employs multiple distributed worker threads for data generation and model learning, where these two processes are decoupled and executed in alternation. To cater for different simulation speed of the environment, the proposed algorithm allows the user to tune the relative proportion of computation allocated to data generation and model learning. We evaluate the PS-Qmix algorithm on a StarCraft II micro-combat task. As we increase the number of worker threads, we observe significant speed-up in both data generation and model learning. The evaluation results indicate that our method is effective in utilizing distributed computation resources to train Q-Mix agents.","['Computer Science', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Information Systems and Communication Service', 'Computer Systems Organization and Communication Networks', 'Computer Appl. in Social and Behavioral Sciences']"
doi:10.1007/978-981-19-2259-6_101,en,A Scalable Multi-agent Reinforcement Learning Approach for the Dynamic Taxi Dispatch Problem,OriginalPaper,"In this work we approach the dynamic taxi dispatch problem as a Markov Game and solve it using a model free Deep Reinforcement Learning approach. We propose a novel Markov Game formulation, and we address the challenges associated with such formulation and strive to improve the efficiency of the solution. The biggest challenge is the size of the state space which grows exponentially with the size of the taxi fleet, the number of passengers, and the number of locations. We tackle this problem by representing the problem state as an image and using a Convolutional Neural Network as a function approximator, making the state space dependent only on the size of the map. The proposed algorithm was validated against a rule-based heuristic under different supply-demand ratios, and it was found to outperform the rule-based technique by a large margin when there is a lack of supply.","['Engineering', 'Control, Robotics, Mechatronics', 'Transportation Technology and Traffic Engineering']"
doi:10.1007/978-3-030-87687-6_4,en,Estimating Time Lost on Semaphores with Deep Learning,OriginalPaper,"Traffic flow congestion is a very present problem on the daily life of citizens of big cities. Furthermore, it is growing by the day because of the increase of population. Furthermore, it has undesirable consequences such as an increase of air pollution levels and a worse life quality. Traditional solutions, such as investing on public transport, are less effective nowadays because of the COVID-19 pandemic. A good alternative are traffic flow optimization methods, e.g., signal on-off times optimization methods. However, these methods use traffic simulators that are very time consuming and typically act as a bottleneck for the optimization algorithm. In this work, we study if and how Deep Learning models could replace traffic simulators for a more performant alternative for its use on optimization methods. We design several network architectures and use them to predict vehicle and pedestrian time lost in a specific intersection of the city of Salamanca (Spain). The best of our models has an average Mean Absolute Error (MAE) lower than a second using 10-fold cross-validation. Finally, we discuss mechanisms to generalize our models to other intersections using only a reduced amount of data.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-030-91585-8_2,en,Understanding Machine Learning,OriginalPaper,"Machine Learning is an emerging field of the Artificial Intelligence and data science. It is yet to be conceptualised and operationalised to be fully understood in its complexity and entirety. This chapter will consider it in a more detailed way through providing definitions to its constituting elements and analysing the learning process itself, and will review the accompanying factors around it.","['Computer Science', 'Machine Learning', 'Systems and Data Security', 'Mobile and Network Security', 'Security Science and Technology']"
doi:10.1007/978-3-030-89025-4_1,en,"Introduction Conceptualization of Security, Forensics, and Privacy of Internet of Things: An Artificial Intelligence Perspective",OriginalPaper,"The Internet of Things (IoT) is a rapidly evolving technology that empowers billions of globally distributed physical things to be interconnected over the internet to capture, collect, exchange, and share a wide variety of vast amounts of data. These physical things incorporate all of the connectable devices ranging from conventional household devices to complex industrial devices (Javed et al. in IEEE Commun. Surv. Tutorials, 2018). The emergence of INDUSTRY 4.0 has revolutionized the industrial domain globally with a large and ever-increasing number of embedded devices, and computerized chips because of the expanding availability, affordability, microprocessors, sensors’ capacity, and omnipresent communication technologies. Connecting such a very large number of heterogeneous objects and implanting sensors to them leads to some degree of digital intelligence at devices that, empowering them to communicate instantaneous data with no human intervention.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-04321-5_9,en,Cognitive Mobile Computing for Cyber-Physical Systems (CPS),OriginalPaper,"Cyber-Physical Systems (CPS) are combined with sensor networks that have embedded computing ability to sense, monitor, and control the physical environment. Due to the growth of sensor data traffic and mobile applications (e.g., object detection, cameras, and security), the infrastructure of cyber-physical systems has become more complex. These applications require computational intensive machine algorithms to make intelligent decisions. Therefore, we define the term as cognitive mobile computing, when a machine can intelligently sense the required data and compute efficiently in making the right decision based on the human thought process in any complex situation. The main characteristics of cognitive mobile computing are pervasive computing, extensive networking, and the degree of automation during wireless communications without any human supervision. However, mobile computing devices connected over a network face challenges for computational power constraints, communication delay and physical interactions. Therefore, the future of cognitive mobile computing shows the necessity of implementing an efficient framework in selecting the right computing platforms, machine learning algorithms, and data analytic models considering mobility.","['Engineering', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Mobile and Network Security']"
doi:10.1007/978-3-031-19842-7_18,en,Generative Meta-Adversarial Network for Unseen Object Navigation,OriginalPaper,"Object navigation is a task to let the agent navigate to a target object. Prevailing works attempt to expand navigation ability in new environments and achieve reasonable performance on the seen object categories that have been observed in training environments. However, this setting is somewhat limited in real world scenario, where navigating to unseen object categories is generally unavoidable. In this paper, we focus on the problem of navigating to unseen objects in new environments only based on limited training knowledge. Same as the common ObjectNav tasks, our agent still gets the egocentric observation and target object category as the input and does not require any extra inputs. Our solution is to let the agent “imagine"" the unseen object by synthesizing features of the target object. We propose a generative meta-adversarial network (GMAN), which is mainly composed of a feature generator and an environmental meta discriminator, aiming to generate features for unseen objects and new environments in two steps. The former generates the initial features of the unseen objects based on the semantic embedding of the object category. The latter enables the generator to further learn the background characteristics of the new environment, progressively adapting the generated features to approximate the real features of the target object. The adapted features serve as a more specific representation of the target to guide the agent. Moreover, to fast update the generator with a few observations, the entire adversarial framework is learned in the gradient-based meta-learning manner. The experimental results on AI2THOR and RoboTHOR simulators demonstrate the effectiveness of the proposed method in navigating to unseen object categories. The code is available at https://github.com/sx-zhang/GMAN.git .","['Computer Science', 'Image Processing and Computer Vision', 'User Interfaces and Human Computer Interaction', 'Computer Appl. in Social and Behavioral Sciences', 'Pattern Recognition', 'Machine Learning', 'Robotics']"
doi:10.1007/978-3-031-10186-1_4,en,Blockchain Driven Edge Intelligence,OriginalPaper,"BC-driven EI focuses on addressing the challenges of EI as described in Chap. 4. In this chapter, we present the EI benefits that can be realized with the assistance of BC, including computing-power management, data administration and model optimization.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-030-94774-3_48,en,Supply Chain Synchronization Through Deep Reinforcement Learning,OriginalPaper,"Synchronized supply chains can mitigate a cascading rise-and-fall inventory dynamic and prevent cycles of over and under-production. This paper demonstrated that a deep reinforcement learning agent could only perform adaptive coordination along the whole supply chain if end-to-end information transparency is ensured. Operational and strategic disruptions caused by the COVID-19 pandemic and the post-pandemic recovery can become a necessary kick-starter for required changes in information transparency and global coordination. This paper explores the capabilities of deep reinforcement learning agents to synchronize commodity flows and support operational continuity in the stochastic and nonstationary environment if end-to-end visibility is provided. The paper concludes that the proposed solution can perform adaptive control in complex systems and have potential in supply chain management and logistics. Among discovered benefits, it is essential to highlight that the proximal policy optimization is universal, task unspecific, and does not require prior knowledge about the system.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Mechanical Engineering', 'User Interfaces and Human Computer Interaction', 'Complexity']"
doi:10.1007/978-981-19-7532-5_8,en,A Secured Deep Reinforcement Learning Model Based on Vertical Federated Learning,OriginalPaper,"Deep reinforcement learning (DRL) has been widely used in diverse applications, which combines the key technologies of reinforcement learning and deep learning. In this work, a secured DRL based on vertical federated learning (VFL), named VF-DRL, is proposed to protect the model’s information. Specifically, in the proposed VF-DRL, each client trains a partial local model, and uploads the feature embedding cooperatively to a server for global model aggregation. In this way, if one of the client attempts to construct an adversarial attack on the VF-DRL, it cannot be easily succeeded with only its own features of data and local model alone. Meanwhile, the server aggregates the output embedding of each client to accomplish the global model training in a distributed fashion. Extensive experiments are carried out to testify the robustness of VF-DRL. The results show that the VF-DRL achieves the state-of-the-art defensibility compared with base-lines.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Applications']"
doi:10.1007/s12083-021-01203-5,en,Double agents-DQL based D2D computing-offloading for SHVC,OriginalPaper,"To make the network provide a better Quality-of-Service (QoS) guarantee for compute-intensive applications, many researchers study Device-to-Device (D2D) based Multi-access Edge Computing (MEC) for reducing computing task completion time and energy consumption. However, the majority of works applies only to the scenario, in which, a specific video coding algorithm with a fixed number of enhancement layer is employed, i.e., the computing task with certain computing amount. Then, the computing task can only be continuously executed by edge computing-offloading. In fact, the Scalable High-efficiency Video Coding (SHVC) algorithm has an uncertain computing amount. Also, the computing task has multiple execution steps by D2D computing-offloading. This is because the executor may interrupt the execution of the ongoing SHVC computing task and change its execution step. Therefore, it is a challenging problem that, how to minimize the completion time and energy consumption of this scenario. To solve this problem, firstly, we model the SHVC computing tasks by ten atomic operations. Secondly, we propose an action space dynamic generation method to solve the reward sparse problem, and we propose an action space coding method to reduce the storage space and searching cost of experience pool matrix, and we employ a double-agent deep Q-learning algorithm to further improve the efficiency of solving the problem. Finally, the simulation results show that the long-term QoS of users is improved.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1007/978-3-031-08038-8_14,en,Resource Management for Future Green Mobile Cloud Computing,OriginalPaper,"The number of mobile devices and the complexity of mobile applications have grown in recent years. While mobile devices have become more powerful, they still have limited computational power and battery capacity comparing to stationary computers. A growing number of applications demand extensive computation while also requiring real-time responses that cannot be satisfied by mobile devices alone. They require offloading of these application workloads into a cloud for processing, maybe through edge or fog node. With the energy and battery power limitation in mobile devices, along with the call for energy saving in data centers world-wide, green mobile cloud computing (GMCC) has become a timely, critical research focus. Different mobile cloud computing (MCC) architectures have been proposed to address these challenges. This chapter presents an overview of research in resource management in the traditional central cloud, the new emerging fog/edge, and peer-to-peer architectures.","['Computer Science', 'Professional Computing', 'Mobile Computing']"
doi:10.1007/978-981-19-1292-4_1,en,Intelligent Unmanned Air Vehicles for Public Safety Networks: Emerging Technologies and Research Directions,OriginalPaper,"Unmanned air vehicles (UAVs) has shown great potential to enable numerous applications ranging from industry verticals to public safety communications. However, various challenges also arises with its integration into the existing terrestrial networks such as efficient UAV positioning, power allocation, trajectory design, and resource allocation. The existing conventional optimization solutions are not intelligent enough to overcome those challenges. Thus, real-time optimization and machine learning assisted solutions, and emerging technologies are required to overcome those challenges. To address those challenges, we summarized key technologies and research directions for UAV deployment at the edge or in the cell center, the power allocation and localization schemes, and the federated learning solutions.","['Engineering', 'Communications Engineering, Networks', 'Mechanical Engineering', 'Artificial Intelligence']"
doi:10.1007/978-3-030-82193-7_19,en,Scope and Sense of Explainability for AI-Systems,OriginalPaper,"Certain aspects of the explainability of AI systems will be critically discussed. This especially with focus on the feasibility of the task of making every AI system explainable. Emphasis will be given to difficulties related to the explainability of highly complex and efficient AI systems which deliver decisions whose explanation defies classical logical schemes of cause and effect. AI systems have provably delivered unintelligible solutions which in retrospect were characterized as ingenious (for example move 37 of the game 2 of AlphaGo). It will be elaborated on arguments supporting the notion that if AI-solutions were to be discarded in advance because of their not being thoroughly comprehensible, a great deal of the potentiality of intelligent systems would be wasted.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-16-3637-0_1,en,Detection of Human Activities in Wildlands to Prevent the Occurrence of Wildfires Using Deep Learning and Remote Sensing,OriginalPaper,"Human activities in wildland are responsible for the largest part of wildfire cases. This paper presents a work that uses deep learning on remote sensing images to detect human activity in wildlands to prevent fire occurrence that can be caused by humans. Human activities can be presented as any human interaction with wildlands, and it can be roads, cars, vehicles, homes, human shapes, agricultural lands, golfs, airplanes, or any other human proof of existence or objects in wild lands. Conventional neural network is used to classify the images. For that, we used three approaches, in which one is the object detection and scene classification approach, the second is land class approach where two classes of lands can be considered which are wildlands with human interactions and wildland without human interaction. The third approach is more general and includes three classes that are urban lands, pure wildlands, and wildlands with human activities. The results show that it is possible to detect human activities in wildlands using the models presented in this paper. The second approach can be considered the most successful even if it is the simpler.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-16-7952-0_52,en,Survey on Edge Intelligence in IoT-Based Computing Platform,OriginalPaper,"The enormous development of Internet of things (IoT) and the establishment over Internet services with cloud have moved to another form computing environment, i.e. edge computing; it can process the information towards the end of the organization named as edge nodes on the cloud-based network environment. The issues related to the network response time requirement, transmission capacity, data security and privacy on cloud infrastructure are effectively addressed by the edge computing technique. The proposed research paper listed out various challenges in clouds computing, the definition and layered architecture of edge computing. The model enhances the edge layers for computing operations by adopting different algorithms in machine learning. The introduction of machine learning-based edge computing hybrid model lead into several advantages of edge-based intelligence on IoT platforms. Finally, the paper also listed out various challenges and future research implementation works involved in the edge computing environment. The objective of this survey paper is to open new avenues of research problems by adopting machine learning-based algorithm in the implementation of edge computing.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Multimedia Information Systems']"
doi:10.1007/s12083-021-01240-0,en,Trusted resource allocation based on proof-of-reputation consensus mechanism for edge computing,OriginalPaper,"Resource allocation aims at using the storage, computing and bandwidth between edge servers and end users while satisfying the requirement of different application scenarios. However, there are some security threats in the allocation process, such as unreal resources and denial of service, in which the trusted resource allocation becoming a critical research content. Blockchain technology is one important solution for this problem, but the high-delay and low-throughput shortcomings in blockchain cannot fulfill the requirements of time-sensitive applications. In this paper, we firstly utilize a reputation evaluation mechanism to calculate and rate the servers’ reputation values in the resource allocation process. The high-reputation servers are added as candidate leaders into a block generation group, while the remaining low-reputation edge servers are added as verifiers into a block verification group. Secondly, we utilize a round-robin mechanism to choose a leader from the block generation group. The leader is responsible for generating a new block in each timeslot and is updated in different timeslots. Thirdly, we utilize a reward and punishment mechanism to encourage low-reputation servers to participate in consensus. Finally, we propose a proof-of-reputation consensus mechanism to realize the trusted resource allocation based on smart contracts, which can match the end users with high-reputation edge servers automatically. Simulation results show the consensus delay and throughput of resource allocation achieve significant performance improvements in comparison with current schemes, and the system is more sensitive to dishonest edge servers and quickly reduces their reputation values.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1007/978-3-031-13822-5_16,en,SCRMA: Snake-Like Robot Curriculum Rapid Motor Adaptation,OriginalPaper,"Controllers for underwater snake-like robots are difficult to design because of their high DOF and complex motions. Additionally, because of the complex underwater environment and insufficient knowledge of hydrodynamics, the traditional control algorithms based on environment or robot modeling cannot work well. In this paper, we propose an SCRMA algorithm, which combines the characteristics of the RMA algorithm for rapid learning and adaptation to the environment, and uses curriculum learning and save &load exploration to accelerate the training speed. Experiments show that the SCRMA algorithm works better than other kinds of reinforcement learning algorithms nowadays.","['Computer Science', 'Artificial Intelligence', 'Information Systems Applications (incl. Internet)', 'User Interfaces and Human Computer Interaction', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Computer System Implementation']"
doi:10.1007/978-3-030-85383-9_9,en,Anomaly Detection in Industrial IoT Applications Using Deep Learning Approach,OriginalPaper,"Internet of Things (IoT) becomes popular in last two decade as it provides many advantages such as flexibility, autonomous, cost effective and productivity. Many industries adopted IoT to improve the efficiency, security and predictive maintenance. To improve the quality of service (QoS), it is essential to identify various types of anomalies in productive maintenance. An anomaly is a value, a status of resources or outcome that deviates from expected or normal values and it affects QoS of production. In this paper, a multi-agent based anomaly detection scheme is introduced to improve the QoS. The fog computing infrastructure is used to reduce the latency of communication. Multiple agents are deployed in fog node to perform the various operations of detecting anomalies. The proposed anomaly detection scheme uses a multi-step prediction technique and applies an anomaly detection algorithm to detect anomalies. The Gated Recurrent Unit (GRU) model is used for Multi-step time series prediction and a bio-inspired Artificial Bee Colony algorithm is used for tuning the GRU model hyperparameters to improve the accuracy. The proposed model detects various types of anomalies in the fog computing environment. The Google Colab using TensorFlow library Keras is used for experimental evaluation. The proposed model increases accuracy over existing approaches, according to the experiment evaluation.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2519-1_15,en,Fuzzy Reinforcement Learning for Canal Control,OriginalPaper,"The poor performance of surface irrigation water distribution systems in terms of reliability, sufficiency, and timely delivery makes researchers develop and employ new methods to reduce its consequent challenges, including environmental, energy, and groundwater withdrawal issues. In this regard, many model-based control systems have been considered to automate canal structures. Artificial intelligence, as model-free systems, has recently gained researchers’ attraction to be employed for canal control purposes. In this research, the Reinforcement Learning (RL) methods with critic-only architecture, Fuzzy SARSA Learning (FSL) and Fuzzy Q Learning (FQL) that use a scalar reward/penalty to adapt system parameters online were developed and introduced to control irrigation canals. The main difference between the mentioned methods lies in the mathematical guarantees regarding FSL convergence and FQL divergence observation. Applications of these two methods to a case study canal allows assessing their performance and convergence in this context using standard performance indicators.","['Engineering', 'Computational Intelligence', 'Water, general', 'Environmental Management']"
doi:10.1007/s11235-021-00838-2,en,Intelligent and resizable control plane for software defined vehicular network: a deep reinforcement learning approach,OriginalPaper,"Software-defined networking (SDN) has become one of the most promising paradigms to manage large scale networks. Distributing the SDN control proved its performance in terms of resiliency and scalability. However, the choice of the number of controllers to use remains problematic. A large number of controllers may be oversized inducing an overhead in the investment cost and the synchronization cost in terms of delay and traffic load. However, a small number of controllers may be insufficient to achieve the objective of the distributed approach. So, the number of used controllers should be tuned in function of the traffic charge and application requirements. In this paper, we present an intelligent and resizable control plane for software defined vehicular network architecture, where SDN capabilities coupled with deep reinforcement learning (DRL) allow achieving better QoS for vehicular applications. Interacting with SDVN, DRL agent decides the optimal number of distributed controllers to deploy according to the network environment (number of vehicles, load, speed etc.). To the best of our knowledge, this is the first work that adjusts the number of controllers by learning from the vehicular environment dynamicity. Experimental results proved that our proposed system outperforms static distributed SDVN architecture in terms of end-to-end delay and packet loss.","['Business and Management', 'IT in Business', 'Computer Communication Networks', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes']"
doi:10.1007/978-3-030-85428-7_6,en,Survey on Mobile Edge-Cloud Computing: A Taxonomy on Computation offloading Approaches,OriginalPaper,"With the technological evolution of Internet of Things (IoT) devices and wireless communications, a wide variety of new complex mobile applications and different services have rapidly increased. Nevertheless, these devices are considered constrained to processing such applications, due to the limitation of battery capacity and high-demand computation for these applications. Mobile cloud comping (MCC) is considered as an appropriate solution for addressing this problem and battery the battery lifetime of these devices, in which the intensive-computation tasks will be offloaded and processed at a conventional centralized cloud. However, cloud computing solution introduces a high communication delay which makes the computation offloading inappropriate for processing real-time applications. To tackle the problem of delay, a new emerging paradigm has been introduced, called mobile edge computing (MEC), in which the computation and storage capabilities of cloud computing have been provided at the edge of the network that enables such applications to be processed as well as satisfying the delay requirements. To this end, compared to other surveys, this paper provides a comprehensive survey of state-of-the-art MEC research with a focus on computation offloading on edge-cloud computing combination. In addition, we provide a novel taxonomy on computation offloading at edge-cloud computing combination and introduce the most and common recent computation offloading models regarding this taxonomy. Furthermore, we highlight the main strengths, weaknesses and other issues which require further consideration. Finally, open research challenges and new research trends in edge-cloud computing will be discussed.","['Engineering', 'Data Engineering', 'Cyber-physical systems, IoT', 'Computational Intelligence', 'Big Data']"
doi:10.1007/978-3-030-80119-9_6,en,Distributed Evolution of Deep Autoencoders,OriginalPaper,"Autoencoders have seen wide success in domains ranging from feature selection to information retrieval. Despite this success, designing an autoencoder for a given task remains a challenging undertaking due to the lack of firm intuition on how the backing neural network architectures of the encoder and decoder impact the overall performance of the autoencoder. In this work we present a distributed system that uses an efficient evolutionary algorithm to design a modular autoencoder. We demonstrate the effectiveness of this system on the tasks of manifold learning and image denoising. The system beats random search by nearly an order of magnitude on both tasks while achieving near linear horizontal scaling as additional worker nodes are added to the system.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks']"
doi:10.1007/978-3-030-82014-5_44,en,Markov-Chain-Based Agents for k-Armed Bandit Problem,OriginalPaper,"In this paper, we present our findings on applying Markov chain generative model to model actions of an agent in Markov decision process framework. We outlined a problem of current solutions to reinforcement learning problems that utilize the agent-environment framework. This problem arises from the necessity of performing analysis of each environment state (for example for q-value estimation in q-learning and deep q-learning methods), which can be computationally heavy. We propose a simple method of ‘skipping’ intermediate state analysis for which optimal actions are determined from analysis of some previous state and modeled by a Markov chain. We observed a problem of this approach that limits agent’s exploratory behavior by setting Markov chain’s probabilities close to either 0 or 1. It was shown that the proposed solution by $$L^1$$ L 1 -normalization of transition probabilities can successfully handle this problem. We tested our approach on a simple environment of k-armed bandit problem and showed that it outperforms commonly used gradient bandit algorithm.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-21689-3_8,en,When Less May Be More: Exploring Similarity to Improve Experience Replay,OriginalPaper,"We propose the COM Pact E xperience R eplay (COMPER) as a reinforcement learning method that seeks to reduce the required number of experiences to agent training regarding the total accumulated rewards in the long run. COMPER uses temporal difference learning with predicted target values for sets of similar transitions and a new experience replay approach based on two memories of transitions. We present an assessment of two possible neural network architectures for the target network with a complete analysis of the memories’ behavior, along with detailed results for 100,000 frames and about 25,000 iterations with a small experience memory on eight challenging 2600 Atari games on the Arcade Learning Environment (ALE). We also present results for a Deep Q-Network (DQN) agent with the same experimental protocol on the same set of games as a baseline. We demonstrate that COMPER can approximate a good policy from a small number of frame observations using a compact memory and learning the similar transitions’ sets dynamics using a recurrent neural network.","['Computer Science', 'Artificial Intelligence', 'Database Management', 'User Interfaces and Human Computer Interaction', 'Image Processing and Computer Vision', 'Computers and Education', 'Computer Appl. in Social and Behavioral Sciences']"
doi:10.1007/978-981-19-0361-8_9,en,Modelling Reminder System for Dementia by Reinforcement Learning,OriginalPaper,"Prospective memory refers to preparing, remembering and recalling plans that have been conceived in an intended manner. Various busyness and distractions can make people forget the activities that must be done the next time, especially for people with cognitive memory problems such as dementia. In this paper, we propose a reminder system with the idea of taking time and response into consideration to assist in remembering activities. Using the reinforcement learning method, this idea predicts the right time to remind users through notifications on smartphones. The notification delivery time will be adjusted to the user’s response history, which becomes feedback at any available time. Thus, users will get notifications based on the ideal time for each individual either, either with repetition or without repetition, so as not to miss the planned activity. By evaluating the dataset, the results show that our proposed modelling is able to optimize the time to send notifications. The eight alternative times to send notifications can be optimized to get the best time to notify the user with dementia. This implies that our algorithm propose can adjust to individual personality characteristics, which might be a stumbling block in dementia patient care, and solve multi-routine plan problems. Our propose can be useful for users with dementia because we can remind very well that the execution time of notifications is right on target, so it can prevent users with dementia from stressing out over a lot of notifications, but those who miss notifications can receive them back at a later time step, with the result that information on activities to be completed is still available.","['Engineering', 'Computational Intelligence', 'Statistics, general', 'Artificial Intelligence', 'Systems and Data Security', 'Health Informatics']"
doi:10.1007/978-3-031-20179-0_7,en,Multi-agent Traffic Signal Control via Distributed RL with Spatial and Temporal Feature Extraction,OriginalPaper,"The aim of traffic signal control (TSC) is to optimize vehicle traffic in urban road networks, via the control of traffic lights at intersections.","['Computer Science', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Software Engineering', 'Computer Appl. in Social and Behavioral Sciences', 'Numeric Computing']"
doi:10.1007/978-3-030-93842-0_11,en,Transfer Learning and Curriculum Learning in Sokoban,OriginalPaper,"Transfer learning can speed up training in machine learning, and is regularly used in classification tasks. It reuses prior knowledge from other tasks to pre-train networks for new tasks. In reinforcement learning, learning actions for a behavior policy that can be applied to new environments is still a challenge, especially for tasks that involve much planning. Sokoban is a challenging puzzle game. It has been used widely as a benchmark in planning-based reinforcement learning. In this paper, we show how prior knowledge improves learning in Sokoban tasks. We find that reusing feature representations learned previously can accelerate learning new, more complex, instances. In effect, we show how curriculum learning, from simple to complex tasks, works in Sokoban. Furthermore, feature representations learned in simpler instances are more general, and thus lead to positive transfers towards more complex tasks, but not vice versa. We have also studied which part of the knowledge is most important for transfer to succeed, and identify which layers should be used for pre-training (Codes we used for this work can be found at https://github.com/yangzhao-666/TLCLS ).","['Computer Science', 'Artificial Intelligence', 'Information Systems and Communication Service', 'Computer Appl. in Social and Behavioral Sciences', 'Computers and Education']"
doi:10.1007/978-3-031-16822-2_4,en,Deep Reinforcement Learning for Delay-Aware and Energy-Efficient Computation Offloading,OriginalPaper,"Mobile edge computing (MEC) deploys storage and computing resources in the proximity of end devices. With MEC, the data generated by end devices can be offloaded to the edge servers for processing, rather than sending them to the remote cloud servers. As a result, the service latency can be greatly improved and the network congestion can be mitigated. In this chapter, we investigate computation offloading in a dynamic MEC system with multiple cooperative edge servers, where computational tasks with various requirements are dynamically generated by end devices and offloaded to MEC servers in a time-varying operating environment (e.g., wireless channel condition and workloads of the edge servers change over time). The objective is to maximize the number of completed tasks before their respective required deadlines and minimize the energy consumption. To this end, this chapter presents an end-to-end Deep Reinforcement Learning (DRL) approach to select the best edge server for offloading and allocate the optimal computational resource such that the expected long-term utility is maximized. The simulation results are provided to evaluate the performance of this DRL approach.","['Computer Science', 'Computer Communication Networks', 'Wireless and Mobile Communication', 'Communications Engineering, Networks']"
doi:10.1007/978-3-658-36932-3_69,de,Reinforcement learning-basierte Patchpriorisierung zur beschleunigten Segmentierung von hochauflösenden Endoskopievideodaten,OriginalPaper,"Bei endoskopischen Computer-Vision-Anwendungen sind Echtzeitverarbeitung der Videodaten sowie geringe Latenzen für einen praktischen klinischen Einsatz maßgeblich. Gleichzeitig führt die kontinuierliche Hardwareweiterentwicklung zu einer stetigen Verbesserung der Bildauflösung. Eingangsbilddaten hoher Auflösung erfordern i. d. R. eine patchweise Verarbeitung, wobei durch Patch-Priorisierungsstrategien die Verarbeitung der Daten beschleunigt und Latenzen reduziert werden können. Mit der Bildsegmentierung als Beispielaufgabe wird im vorliegenden Beitrag untersucht, wie das Patch-Sampling zur Inferenzzeit als Reinforcement Learning (RL)-Problem formuliert warden kann. Anhand von synthetischen und realen Daten wird gezeigt, dass durch das entwickelte RL-basierte Patch-Priorisierungsmodell (PPM) eine beschleunigte Segmentierung relevanter Bildregionen realisiert werden kann.","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Health Informatics']"
doi:10.1007/978-3-658-37009-1_24,de,AI-based Parameter Optimization Method,OriginalPaper,"The drivability of a vehicle is strongly affected by its transmission. Especially dual clutch transmissions (DCT) offer the chance of a comfortable drivability (e.g. jerkless shifting) and high efficiency but come with the drawback of a high control effort for clutch engagement. Since particularly at low speeds the transmission behavior must meet the intention of the driver (drivers tend to be more perceptive at low speeds) the control of the launch behavior is crucial. The functions to control the behavior are typically developed using model-based programming languages and offer the possibility to influence the behavior with control parameters. Calibration engineers set these parameters at different ambient conditions to comply with customer requirements. Therefore, costs are increasing with increasing control opportunities. An approach for decreasing these costs is to automate the optimization of the calibration parameters. Several approaches have already been introduced but some suffer from lack of stability or time efficiency. Hence, to optimize these parameters a procedure is illustrated where a target state is approached with a hybrid solution of reinforcement learning and supervised learning to overcome existing drawbacks.","['Engineering', 'Automotive Engineering']"
doi:10.1007/978-3-658-33597-7_7,de,KI-Systeme für die nächste Medizintechnikgeneration,OriginalPaper,"In der Medizintechnik scheint zurzeit die künstliche Intelligenz (KI) als universelles Werkzeug für beinahe jedes bestehende Problem bemüht zu werden. Tatsächlich haben neue medizinische Erkenntnisse in Verbindung mit einer leistungsfähigen Informationstechnik und einer zunehmend digitalen Umgebung eine Renaissance der KI ausgelöst. Die Impulse treiben dabei nicht nur die medizinischen Disziplinen wie die Radiologie an, die bereits seit Jahren in der Forschung und Entwicklung mit dem KI-Methodenbaukasten arbeiten. KI wird im ganzen Spektrum der Medizintechnik als Schlüssel beispielsweise für (teil-)autonome Systeme gesehen und damit in vielen Bereichen als wichtiges Werkzeug zum Erreichen einer effizienten Präzisionsmedizin. Dieser Beitrag skizziert aber auch die Probleme bei der Realisierung von KI-Lösungen, die dazu führen könnten, dass KI-basierte Medizintechnik in größerem Umfang noch für längere Zeit ein Traum bleiben könnte.","['Business and Management', 'Health Care Management', 'Public Health', 'Artificial Intelligence']"
doi:10.1007/s11432-021-3348-6,en,Learning practically feasible policies for online 3D bin packing,"['OriginalPaper', 'Research Paper']","We tackle the online 3D bin packing problem (3D-BPP), a challenging yet practically useful variant of the classical bin packing problem. In this problem, the items are delivered to the agent without informing the full sequence information. The agent must directly pack these items into the target bin stably without changing their arrival order, and no further adjustment is permitted. Online 3D-BPP can be naturally formulated as a Markov decision process (MDP). We adopt deep reinforcement learning, in particular, the on-policy actor-critic framework, to solve this MDP with constrained action space. To learn a practically feasible packing policy, we propose three critical designs. First, we propose an online analysis of packing stability based on a novel stacking tree. It attains a high analysis accuracy while reducing the computational complexity from O ( N 2 ) to O ( N log N ), making it especially suited for reinforcement learning training. Second, we propose a decoupled packing policy learning for different dimensions of placement which enables high-resolution spatial discretization and hence high packing precision. Third, we introduce a reward function that dictates the robot to place items in a far-to-near order and therefore simplifies the collision avoidance in movement planning of the robotic arm. Furthermore, we provide a comprehensive discussion on several key implemental issues. The extensive evaluation demonstrates that our learned policy outperforms the state-of-the-art methods significantly and is practically usable for real-world applications.","['Computer Science', 'Information Systems and Communication Service']"
doi:10.1007/s10515-021-00313-x,en,Faults in deep reinforcement learning programs: a taxonomy and a detection approach,OriginalPaper,"A growing demand is witnessed in both industry and academia for employing Deep Learning (DL) in various domains to solve real-world problems. Deep reinforcement learning (DRL) is the application of DL in the domain of Reinforcement Learning. Like any software system, DRL applications can fail because of faults in their programs. In this paper, we present the first attempt to categorize faults occurring in DRL programs. We manually analyzed 761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues) developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl, Tensorforce) and identified faults reported by developers/users. We labeled and taxonomized the identified faults through several rounds of discussions. The resulting taxonomy is validated using an online survey with 19 developers/researchers. To allow for the automatic detection of faults in DRL programs, we have defined a meta-model of DRL programs and developed DRLinter , a model-based fault detection approach that leverages static analysis and graph transformations. The execution flow of DRLinter consists in parsing a DRL program to generate a model conforming to our meta-model and applying detection rules on the model to identify faults occurrences. The effectiveness of DRLinter is evaluated using 21 synthetic and real faulty DRL programs. For synthetic samples, we injected faults observed in the analyzed artifacts from Stack Overflow and GitHub. The results show that DRLinter can successfully detect faults in both synthesized and real-world examples with a recall of 75% and a precision of 100%.","['Computer Science', 'Artificial Intelligence', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11831-021-09552-3,en,Deep Reinforcement Learning Techniques in Diversified Domains: A Survey,"['OriginalPaper', 'Original Paper']","There have been tremendous improvements in deep learning and reinforcement learning techniques. Automating learning and intelligence to the full extent remains a challenge. The amalgamation of Reinforcement Learning and Deep Learning has brought breakthroughs in games and robotics in the past decade. Deep Reinforcement Learning (DRL) involves training the agent with raw input and learning via interaction with the environment. Motivated by recent successes of DRL, we have explored its adaptability to different domains and application areas. This paper also presents a comprehensive survey of the work done in recent years and simulation tools used for DRL. The current focus of researchers is on recording the experience in a better way, and refining the policy for futuristic moves. It is found that even after obtaining good results in Atari, Go, Robotics, multi-agent scenarios, there are challenges such as generalization, satisfying multiple objectives, divergence, learning robust policy. Furthermore, the complex environment and multiple agents are throwing new challenges, which is an open area of research.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s40295-021-00288-7,en,Reinforcement Learning for Uncooperative Space Objects Smart Imaging Path-Planning,"['OriginalPaper', 'Original Article']","Leading space agencies are increasingly investing in the gradual automation of space missions. In fact, autonomous flight operations may be a key enabler for on-orbit servicing, assembly and manufacturing (OSAM) missions, carrying inherent benefits such as cost and risk reduction. Within the spectrum of proximity operations, this work focuses on autonomous path-planning for the reconstruction of geometry properties of an uncooperative target. The autonomous navigation problem is called active Simultaneous Localization and Mapping (SLAM) problem, and it has been largely studied within the field of robotics. Active SLAM problem may be formulated as a Partially Observable Markov Decision Process (POMDP). Previous works in astrodynamics have demonstrated that is possible to use Reinforcement Learning (RL) techniques to teach an agent that is moving along a pre-determined orbit when to collect measurements to optimize a given mapping goal. In this work, different RL methods are explored to develop an artificial intelligence agent capable of planning sub-optimal paths for autonomous shape reconstruction of an unknown and uncooperative object via imaging. Proximity orbit dynamics are linearized and include orbit eccentricity. The geometry of the target object is rendered by a polyhedron shaped with a triangular mesh. Artificial intelligent agents are created using both the Deep Q-Network (DQN) and the Advantage Actor Critic (A2C) method. State-action value functions are approximated using Artificial Neural Networks (ANN) and trained according to RL principles. Training of the RL agent architecture occurs under fixed or random initial environment conditions. A large database of training tests has been collected. Trained agents show promising performance in achieving extended coverage of the target. Policy learning is demonstrated by displaying that RL agents, at minimum, have higher mapping performance than agents that behave randomly. Furthermore, RL agent may learn to maneuver the spacecraft to control target lighting conditions as a function of the Sun location. This work, therefore, preliminary demonstrates the applicability of RL to autonomous imaging of an uncooperative space object, thus setting a baseline for future works.","['Engineering', 'Aerospace Technology and Astronautics', 'Mathematical Applications in the Physical Sciences', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)']"
doi:10.1007/s00371-021-02269-1,en,Multi-agent reinforcement learning for character control,"['ReviewPaper', 'Survey']","Simultaneous control of multiple characters has been a research topic that has been extensively pursued for applications in computer games and computer animations, for applications such as crowd simulation, controlling two characters carrying objects or fighting with one another and controlling a team of characters playing collective sports. With the advance in deep learning and reinforcement learning, there is a growing interest in applying multi-agent reinforcement learning for intelligently controlling the characters to produce realistic movements. In this paper we will survey the state-of-the-art MARL techniques that are applicable for character control. We will then survey papers that make use of MARL for multi-character control and then discuss about the possible future directions of research.","['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s10845-021-01778-z,en,Modular production control using deep reinforcement learning: proximal policy optimization,OriginalPaper,"EU regulations on $$\textit{CO}_2$$ CO 2 limits and the trend of individualization are pushing the automotive industry towards greater flexibility and robustness in production. One approach to address these challenges is modular production, where workstations are decoupled by automated guided vehicles, requiring new control concepts. Modular production control aims at throughput-optimal coordination of products, workstations, and vehicles. For this np-hard problem, conventional control approaches lack in computing efficiency, do not find optimal solutions, or are not generalizable. In contrast, Deep Reinforcement Learning offers powerful and generalizable algorithms, able to deal with varying environments and high complexity. One of these algorithms is Proximal Policy Optimization, which is used in this article to address modular production control. Experiments in several modular production control settings demonstrate stable, reliable, optimal, and generalizable learning behavior. The agent successfully adapts its strategies with respect to the given problem configuration. We explain how to get to this learning behavior, especially focusing on the agent’s action, state, and reward design.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11390-021-0009-9,en,JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY,"['News', 'News']",,"['Computer Science', 'Computer Science, general', 'Software Engineering', 'Theory of Computation', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Information Systems Applications (incl.Internet)']"
doi:10.1007/s42514-021-00083-8,en,Energy-aware task scheduling optimization with deep reinforcement learning for large-scale heterogeneous systems,"['OriginalPaper', 'Regular Paper']","The energy consumption of large-scale heterogeneous computing systems has become a critical concern on both financial and environmental fronts. Current systems employ hand-crafted heuristics and ignore changes in the system and workload characteristics. Moreover, high-dimensional state and action problems cannot be solved efficiently using traditional reinforcement learning-based methods in large-scale heterogeneous settings. Therefore, in this paper, energy-aware task scheduling with deep reinforcement learning (DRL) is proposed. First, based on the real data set SPECpower, a high-precision energy consumption model, convenient for environmental simulation, is designed. Based on the actual production conditions, a partition-based task-scheduling algorithm using proximal policy optimization on heterogeneous resources is proposed. Simultaneously, an auto-encoder is used to process high-dimensional space to speed up DRL convergence. Finally, to fully verify our algorithm, three scheduling scenarios containing large, medium, and small-scale heterogeneous environments are simulated. Experiments show that when compared with heuristics and DRL-based methods, our algorithm more effectively reduces system energy consumption and ensures the quality of service, without significantly increasing the waiting time.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computer Hardware']"
doi:10.1007/s11227-021-03784-7,en,Availability-aware and energy-aware dynamic SFC placement using reinforcement learning,OriginalPaper,"Software-defined networking and network functions virtualisation are making networks programmable and consequently much more flexible and agile. To meet service-level agreements, achieve greater utilisation of legacy networks, faster service deployment, and reduce expenditure, telecommunications operators are deploying increasingly complex service function chains (SFCs). Notwithstanding the benefits of SFCs, increasing heterogeneity and dynamism from the cloud to the edge introduces significant SFC placement challenges, not least adding or removing network functions while maintaining availability, quality of service, and minimising cost. In this paper, an availability- and energy-aware solution based on reinforcement learning (RL) is proposed for dynamic SFC placement. Two policy-aware RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimisation (PPO), are compared using simulations of a ground truth network topology based on the Rede Nacional de Ensino e Pesquisa Network, Brazil’s National Teaching and Research Network backbone. The simulation results show that PPO generally outperformed A2C and a greedy approach in terms of both acceptance rate and energy consumption. The biggest difference in the PPO when compared to the other algorithms relates to the SFC availability requirement of 99.965%; the PPO algorithm median acceptance rate is 67.34% better than the A2C algorithm. A2C outperforms PPO only in the scenario where network servers had a greater number of computing resources. In this case, the A2C is 1% better than the PPO.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s11370-021-00398-z,en,A survey on deep learning and deep reinforcement learning in robotics with a tutorial on deep reinforcement learning,"['ReviewPaper', 'Review Article']","This article is about deep learning (DL) and deep reinforcement learning (DRL) works applied to robotics. Both tools have been shown to be successful in delivering data-driven solutions for robotics tasks, as well as providing a natural way to develop an end-to-end pipeline from the robot’s sensing to its actuation, passing through the generation of a policy to perform the given task. These frameworks have been proven to be able to deal with real-world complications such as noise in sensing, imprecise actuation, variability in the scenarios where the robot is being deployed, among others. Following that vein, and given the growing interest in DL and DRL, the present work starts by providing a brief tutorial on deep reinforcement learning, where the goal is to understand the main concepts and approaches followed in the field. Later, the article describes the main, recent, and most promising approaches of DL and DRL in robotics, with sufficient technical detail to understand the core of the works and to motivate interested readers to initiate their own research in the area. Then, to provide a comparative analysis, we present several taxonomies in which the references can be classified, according to high-level features, the task that the work addresses, the type of system, and the learning techniques used in the work. We conclude by presenting promising research directions in both DL and DRL.","['Engineering', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'User Interfaces and Human Computer Interaction', 'Vibration, Dynamical Systems, Control', 'Robotics and Automation']"
doi:10.1007/s12083-021-01146-x,en,Deep reinforcement learning-based incentive mechanism design for short video sharing through D2D communication,OriginalPaper,"With the development of 5th generation (5G) wireless communication networks and the popularity of short video applications, there has been a rapid increase in short video traffic in cellular networks. Device-to-device (D2D) communication-based short video sharing is considered to be an effective way to offload traffic from cellular networks. Due to the selfish nature of mobile user equipment (MUEs), how to dynamically motivate MUEs to engage in short video sharing while ensuring the Quality of Service, which makes it critical to design an appropriate incentive mechanism. In this paper, we firstly analyze the rationale for dynamically setting rewards and penalties and then define the rewards and penalties setting dynamically for maximizing the utility of the mobile edge computing server (RPSDMU) problem. The problem is proved NP-hard. Furthermore, we formulate the dynamic incentive process as the Markov Decision Process problem. Considering the complexity and dynamics of the problem, we design a Dynamic Incentive Mechanism algorithm of D2D-based Short Video Sharing based on Asynchronous Advantage Actor-Critic (DIM-A3C) to solve the problem. Simulation results show that the proposed dynamic incentive mechanism can increase the utility of mobile edge computing server by an average of 22% and 16% compared with the existing proportional incentive mechanism (PIM) and scoring-based incentive mechanism (SIM). Meanwhile, DIM-A3C achieves a higher degree of satisfaction than PIM and SIM.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1007/s12083-021-01239-7,en,Convergence of Edge Computing and Next Generation Networking,EditorialNotes,,"['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1007/s42979-021-00934-9,en,Extending the Capabilities of Reinforcement Learning Through Curriculum: A Review of Methods and Applications,"['ReviewPaper', 'Review Article']","Reinforcement learning has long been advertised as the one with the capability to intelligently mimic and understand human learning and behavior. While the upshot of the field’s advances is not underrated, its applicability and extension to large, complex and highly dynamic environments remain inefficient, inaccurate or unsolved. Curriculum learning presents an intuitive yet elegant solution to these problems and when incorporated into the solution, provides a structured approach to alleviate some of the core challenges. As reinforcement learning framework proceeds to tackle harder challenges, it necessitates the study of essential support frameworks including curriculum learning. Through this paper, we review the current state-of-the-art in the field of curriculum-based reinforcement learning. We analyze and classify numerous scientific articles and present a summary of their methodologies and applications. In addition to the detailed review and analysis of the targeted algorithms, we summarise the current progress in the field by tabulating distinct identifying features of reviewed works with respect to their curriculum design methodology and applications.","['Computer Science', 'Computer Science, general', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'Data Structures and Information Theory', 'Information Systems and Communication Service', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s44163-021-00011-3,en,Multi-objective optimization for autonomous driving strategy based on Deep Q Network,"['OriginalPaper', 'Research']","Autonomous driving is an important development direction of automobile technology, and driving strategy is the core of the autonomous driving system. Most works in this area focus on single-objective tasks, such as maximizing vehicle speed or lane-keeping, and rare attention has been paid to the quality of driving skills. Therefore, a multi-objective learning method is proposed for autonomous driving strategy based on deep Q-network, where two optimization objectives are involved, i.e., vehicle speed and passenger comfort. An end-to-end autonomous driving model is designed by using vehicle front camera images as inputs to the Q-network and makes decisions based on the output Q values. Considering the vehicle speed and passenger comfort, the reward function is designed for multi-objective optimization. To evaluate the effectiveness of the method, training and testing are performed in a simulator, and a single-objective strategy with the goal of maximizing speed is designed for comparison. The results show that the proposed multi-objective autonomous driving strategy can strike a balance between vehicle speed and passenger comfort. Compared with the single-objective strategy, the multi-objective strategy has a significant improvement in comfort, while the average speed is only slightly reduced.","['Engineering', 'Engineering, general', 'Computer Science, general', 'Artificial Intelligence']"
