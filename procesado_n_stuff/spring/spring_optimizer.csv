identifier,language,title,genre,abstract,subjects
doi:10.1007/s42064-022-0149-x,en,Adaptive connected hierarchical optimization algorithm for minimum energy spacecraft attitude maneuver path planning,"['OriginalPaper', 'Research Article']","Space object observation requirements and the avoidance of specific attitudes produce pointing constraints that increase the complexity of the attitude maneuver path-planning problem. To deal with this issue, a feasible attitude trajectory generation method is proposed that utilizes a multiresolution technique and local attitude node adjustment to obtain sufficient time and quaternion nodes to satisfy the pointing constraints. These nodes are further used to calculate the continuous attitude trajectory based on quaternion polynomial interpolation and the inverse dynamics method. Then, the characteristic parameters of these nodes are extracted to transform the path-planning problem into a parameter optimization problem aimed at minimizing energy consumption. This problem is solved by an improved hierarchical optimization algorithm, in which an adaptive parameter-tuning mechanism is introduced to improve the performance of the original algorithm. A numerical simulation is performed, and the results confirm the feasibility and effectiveness of the proposed method.","['Engineering', 'Aerospace Technology and Astronautics', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Vibration, Dynamical Systems, Control']"
doi:10.1007/s42064-022-0147-z,en,Stretching directions in cislunar space: Applications for departures and transfer design,"['OriginalPaper', 'Research Article']","Stable or nearly stable orbits do not generally possess well-distinguished manifold structures that assist in designing trajectories for departing from or arriving onto a periodic orbit. For some potential missions, the orbits of interest are selected as nearly stable to reduce the possibility of rapid departure. However, the linearly stable nature of these orbits is also a drawback for their timely insertion into or departure from the orbit. Stable or nearly stable near rectilinear halo orbits (NRHOs), distant retrograde orbits (DROs), and lunar orbits offer potential long-horizon trajectories for exploration missions and demand efficient operations. The current investigation focuses on leveraging stretching directions as a tool for departure and trajectory design applications. The magnitude of the state variations along the maximum stretching direction is expected to grow rapidly and, therefore, offers information for efficient departure from the orbit. Similarly, maximum stretching in reverse time enables arrival with a minimal maneuver magnitude.","['Engineering', 'Aerospace Technology and Astronautics', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Vibration, Dynamical Systems, Control']"
doi:10.1007/s41095-021-0267-z,en,Facial optical flow estimation via neural non-rigid registration,"['OriginalPaper', 'Research Article']","Optical flow estimation in human facial video, which provides 2D correspondences between adjacent frames, is a fundamental pre-processing step for many applications, like facial expression capture and recognition. However, it is quite challenging as human facial images contain large areas of similar textures, rich expressions, and large rotations. These characteristics also result in the scarcity of large, annotated real-world datasets. We propose a robust and accurate method to learn facial optical flow in a self-supervised manner. Specifically, we utilize various shape priors, including face depth, landmarks, and parsing, to guide the self-supervised learning task via a differentiable nonrigid registration framework. Extensive experiments demonstrate that our method achieves remarkable improvements for facial optical flow estimation in the presence of significant expressions and large rotations.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s41095-022-0270-z,en,A two-step surface-based 3D deep learning pipeline for segmentation of intracranial aneurysms,"['OriginalPaper', 'Research Article']","The exact shape of intracranial aneurysms is critical in medical diagnosis and surgical planning. While voxel-based deep learning frameworks have been proposed for this segmentation task, their performance remains limited. In this study, we offer a two-step surface-based deep learning pipeline that achieves significantly better results. Our proposed model takes a surface model of an entire set of principal brain arteries containing aneurysms as input and returns aneurysm surfaces as output. A user first generates a surface model by manually specifying multiple thresholds for time-of-flight magnetic resonance angiography images. The system then samples small surface fragments from the entire set of brain arteries and classifies the surface fragments according to whether aneurysms are present using a point-based deep learning network (PointNet++). Finally, the system applies surface segmentation (SO-Net) to surface fragments containing aneurysms. We conduct a direct comparison of the segmentation performance of our proposed surface-based framework and an existing voxel-based method by counting voxels: our framework achieves a much higher Dice similarity (72%) than the prior approach (46%).","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s41095-022-0273-9,en,Joint specular highlight detection and removal in single images via Unet-Transformer,"['OriginalPaper', 'Research Article']","Specular highlight detection and removal is a fundamental problem in computer vision and image processing. In this paper, we present an efficient end-to-end deep learning model for automatically detecting and removing specular highlights in a single image. In particular, an encoder—decoder network is utilized to detect specular highlights, and then a novel Unet-Transformer network performs highlight removal; we append transformer modules instead of feature maps in the Unet architecture. We also introduce a highlight detection module as a mask to guide the removal task. Thus, these two networks can be jointly trained in an effective manner. Thanks to the hierarchical and global properties of the transformer mechanism, our framework is able to establish relationships between continuous self-attention layers, making it possible to directly model the mapping between the diffuse area and the specular highlight area, and reduce indeterminacy within areas containing strong specular highlight reflection. Experiments on public benchmark and real-world images demonstrate that our approach outperforms state-of-the-art methods for both highlight detection and removal tasks.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s41095-022-0276-6,en,Point cloud completion via structured feature maps using a feedback network,"['OriginalPaper', 'Research Article']","In this paper, we tackle the challenging problem of point cloud completion from the perspective of feature learning. Our key observation is that to recover the underlying structures as well as surface details, given partial input, a fundamental component is a good feature representation that can capture both global structure and local geometric details. We accordingly first propose FSNet, a feature structuring module that can adaptively aggregate point-wise features into a 2D structured feature map by learning multiple latent patterns from local regions. We then integrate FSNet into a coarse-to-fine pipeline for point cloud completion. Specifically, a 2D convolutional neural network is adopted to decode feature maps from FSNet into a coarse and complete point cloud. Next, a point cloud upsampling network is used to generate a dense point cloud from the partial input and the coarse intermediate output. To efficiently exploit local structures and enhance point distribution uniformity, we propose IFNet, a point upsampling module with a self-correction mechanism that can progressively refine details of the generated dense point cloud. We have conducted qualitative and quantitative experiments on ShapeNet, MVP, and KITTI datasets, which demonstrate that our method outperforms state-of-the-art point cloud completion approaches.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s41095-022-0272-x,en,Imposing temporal consistency on deep monocular body shape and pose estimation,"['OriginalPaper', 'Research Article']","Accurate and temporally consistent modeling of human bodies is essential for a wide range of applications, including character animation, understanding human social behavior, and AR/VR interfaces. Capturing human motion accurately from a monocular image sequence remains challenging; modeling quality is strongly influenced by temporal consistency of the captured body motion. Our work presents an elegant solution to integrating temporal constraints during fitting. This increases both temporal consistency and robustness during optimization. In detail, we derive parameters of a sequence of body models, representing shape and motion of a person. We optimize these parameters over the complete image sequence, fitting a single consistent body shape while imposing temporal consistency on the body motion, assuming body joint trajectories to be linear over short time. Our approach enables the derivation of realistic 3D body models from image sequences, including jaw pose, facial expression, and articulated hands. Our experiments show that our approach accurately estimates body shape and motion, even for challenging movements and poses. Further, we apply it to the particular application of sign language analysis, where accurate and temporally consistent motion modelling is essential, and show that the approach is well-suited to this kind of application.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s12273-022-0935-7,en,Validation of virtual sensor-assisted Bayesian inference-based in-situ sensor calibration strategy for building HVAC systems,"['OriginalPaper', 'Research Article']","For building heating, ventilation and air-conditioning systems (HVACs), sensor faults significantly affect the operation and control. Sensors with accurate and reliable measurements are critical for ensuring the precise indoor thermal demand. Owing to its high calibration accuracy and in-situ effectiveness, a virtual sensor (VS)-assisted Bayesian inference (VS-BI) sensor calibration strategy has been applied for HVACs. However, the application feasibility of this strategy for wider ranges of different sensor types (within-control-loop and out-of-control-loop) with various sensor bias fault amplitudes, and influencing factors that affect the practical in-situ calibration performance are still remained to be explored. Hence, to further validate its in-situ calibration performance and analyze the influencing factors, this study applied the VS-BI strategy in a HVAC system including a chiller plant with air handle unit (AHU) terminal. Three target sensors including air supply (SAT), chilled water supply (CHS) and cooling water return (CWR) temperatures are investigated using introduced sensor bias faults with eight different amplitudes of [−2 °C, +2 °C] with a 0.5 °C interval. Calibration performance is evaluated by considering three influencing factors: (1) performance of different data-driven VSs, (2) the influence of prior standard deviations σ on in-situ sensor calibration and (3) the influence of data quality on in-situ sensor calibration from the perspective of energy conservation and data volumes. After comparison, a long short term memory (LSTM) is adopted for VS construction with determination coefficient R -squared of 0.984. Results indicate that σ has almost no impact on calibration accuracy of CHS but scanty impact on that of SAT and CWR. The potential of using a prior standard deviation σ to improve the calibration accuracy is limited, only 8.61% on average. For system within-control-loop sensors like SAT and CHS, VS-BI obtains relatively high in-situ sensor calibration accuracy if the data quality is relatively high.","['Engineering', 'Building Construction and Design', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Monitoring/Environmental Analysis']"
doi:10.1007/s12273-022-0927-7,en,Monitoring the green evolution of vernacular buildings based on deep learning and multi-temporal remote sensing images,"['OriginalPaper', 'Cover Article']","The increasingly mature computer vision (CV) technology represented by convolutional neural networks (CNN) and available high-resolution remote sensing images (HR-RSIs) provide opportunities to accurately measure the evolution of natural and artificial environments on Earth at a large scale. Based on the advanced CNN method high-resolution net (HRNet) and multi-temporal HR-RSIs, a framework is proposed for monitoring a green evolution of courtyard buildings characterized by their courtyards being roofed (CBR). The proposed framework consists of an expert module focusing on scenes analysis, a CV module for automatic detection, an evaluation module containing thresholds, and an output module for data analysis. Based on this, the changes in the adoption of different CBR technologies (CBRTs), including light-translucent CBRTs (LT-CBRTs) and non-light-translucent CBRTs (NLT-CBRTs), in 24 villages in southern Hebei were identified from 2007 to 2021. The evolution of CBRTs was featured as an inverse S-curve, and differences were found in their evolution stage, adoption ratio, and development speed for different villages. LT-CBRTs are the dominant type but are being replaced and surpassed by NLT-CBRTs in some villages, characterizing different preferences for the technology type of villages. The proposed research framework provides a reference for the evolution monitoring of vernacular buildings, and the identified evolution laws enable to trace and predict the adoption of different CBRTs in a particular village. This work lays a foundation for future exploration of the occurrence and development mechanism of the CBR phenomenon and provides an important reference for the optimization and promotion of CBRTs.","['Engineering', 'Building Construction and Design', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Monitoring/Environmental Analysis']"
doi:10.1007/978-3-031-17576-3_7,en,Markisa/Passion Fruit Image Classification Based Improved Deep Learning Approach Using Transfer Learning,OriginalPaper,"Fruit recognition becomes more and more important in the agricultural industry. Traditionally, we need to manually identify and label all the fruits in the production line, which is labor intensive, error-prone, and ineffective. Therefore, a lot of fruit recognition systems are created to automate the process, but fruit recognition system for Malaysia local fruit is limited. Thus, this project will focus on classifying one of the Malaysia local fruits which is markisa/passion fruit. We proposed two CNN models for markisa classification. The performances of the proposed models are evaluated on our own dataset collection and produces an accuracy of 97% and 65% respectively. The results indicated that the architecture of CNN model is very important because different architecture can produce different results. Therefore, first CNN model is selected because it can classify 4 types of markisa with a higher accuracy. In the proposed work, we also inspected two transfer learning methods in the classification of markisa which are VGG-16 and InceptionV3. The results showed that the performance of the first proposed CNN model outperforms VGG-16 (95% accuracy) and InceptionV3 (65% accuracy).","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-1-4842-8925-9_3,en,CNN and RNN Using PyTorch,OriginalPaper,"This chapter covers convolutional neural networks (CNN) and recurrent neural network and their implementation using PyTorch. Convolutional neural network is being used in image classification, object detection, object classification related tasks. The large scale image classification models requires PyTorch framework as it is considered to be faster than other frameworks, similarly the recurrent neural networks are used in natural language processing tasks such as text classification, sentiment classification, topic classification, audio classification etc. This chapter shows a few recipes on setting up CNN and RNN models, selecting the optimization function, saving a model, loading a model already trained and using the model for inference generation etc.","['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-3-031-17576-3_4,en,Salak Image Classification Method Based Deep Learning Technique Using Two Transfer Learning Models,OriginalPaper,"Salak is one of the fruits plants in Southeast Asia; there are at least 30 cultivars of salak. The size, shape, skin color, sweetness or even flesh color will be different depending on the cultivar. Thus, classification of salak based on their cultivar become a daily job for the fruit farmers. There are many techniques that can be used for fruit classification using computer vision technology. Deep learning is the most promising algorithm compared to another Machine Learning (ML) algorithm. This paper presents an image classification method on 4 types of salak (salak pondoh, salak gading, salak sideempuan and salak affinis) using a Convolutional Neural Network (CNN), VGG16 and ResNet50. The dataset consists of 1000 images which having 250 of images for each type of salak. Pre-processing on the dataset is required to standardize the dataset by resizing the image into 224 * 224 pixels, convert into jpg format and augmentation. Based on the accuracy result from the model, the best model for the salak classification is ResNet50 which gave an accuracy of 84% followed by VGG16 that gave an accuracy of 77% and CNN which gave 31%.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-981-19-2879-6_3,en,Overview of Deep Learning,OriginalPaper,"As a machine learning model based on neural networks, deep learning is particularly advantageous in fields like computer vision, speech recognition and natural language processing. This chapter mainly introduces the basic knowledge related to deep learning, including the history, the components of neural networks, the types of deep learning neural networks, and the common problems researchers may encounter in the deep learning projects.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16832-1_5,en,An Enhanced Gradient Based Optimized Controller for Load Frequency Control of a Two Area Automatic Generation Control System,OriginalPaper,"This work proposes the adoption of Enhanced Gradient-Based Optimizer (EGBO) as a new approach to the Load Frequency Control (LFC) problem in a two-area interconnected power system. The importance of determining the optimal parameters for the controllers for the LFC problem cannot be overstated, and the fact that estimating these parameters require complex and nonlinear computations makes the optimization procedure even more unique and challenging. Consequently, application of an efficient optimization algorithm to successfully attain optimal controller parameters is critical. To accomplish this task, the proposed EGBO algorithm is compared to the fundamental Gradient-Based Optimizer (GBO), Chimp Optimization Algorithm (ChOA), Sine Cosine Algorithm (SCA), Grey Wolf Optimization (GWO), and Particle Swarm Optimization (PSO) for optimizing an Integral-Time-multiplied-Absolute-Error (ITAE) based objective function. The relevant findings show that the EGBO algorithm is competitively superior in terms of resilience, precision, and latency when compared to other optimization methods. Lastly, the statistical comparison further strengthens the outcome of the study.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-3-031-20105-9_4,en,Comparison Study of Novel Evolutionary Algorithms for Elliptical Shapes in Images,OriginalPaper,"Shape recognition in digital image processing describes one of the difficult and hard-solving situations in artificial vision due to its nonlinear and stochastic structure. Traditional image processing methods have been commonly employed to solve this situation. Additionally, shape recognition considers evolutionary computation techniques. They have been exposed to better performance in terms of accurateness than traditional optimization methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18461-1_20,en,A General Framework of Particle Swarm Optimization,OriginalPaper,"Particle swarm optimization (PSO) is an effective algorithm to solve the optimization problem in case that derivative of target function is inexistent or difficult to be determined. Because PSO has many parameters and variants, we propose a general framework of PSO called GPSO which aggregates important parameters and generalizes important variants so that researchers can customize PSO easily. Moreover, two main properties of PSO are exploration and exploitation. The exploration property aims to avoid premature converging so as to reach global optimal solution whereas the exploitation property aims to motivate PSO to converge as fast as possible. These two aspects are equally important. Therefore, GPSO also aims to balance the exploration and the exploitation. It is expected that GPSO supports users to tune parameters for not only solving premature problem but also fast convergence.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_3,en,"Swarm Intelligence for Deep Learning: Concepts, Challenges and Recent Trends",OriginalPaper,"Machine learning and deep learning have undoubtedly contributed to tremendous achievements in Artificial Intelligence (AI) in recent years, and more are likely to follow. They have demonstrated extraordinary superiority in various real-world applications like computer vision, medical diagnostic systems, agriculture, robotics, and many more. It enables automating the computer-aided system and drastically reducing the human workload where correct prediction with accurate precision is needed. On the other side, as technology advances, a vast amount of data is generated, raising the problem complexity and computational challenges of real-world applications. Furthermore, machine learning, deep learning, and the majority of real-world applications have complex optimization problems within themselves that must be adequately addressed for better and more accurate analysis. Nonetheless, we believe that swarm intelligence-based approaches to deep learning have traditionally been understudied and may ultimately deliver similar advances in AI capabilities - either building on those provided by deep learning or offering whole new ones. Swarm intelligence approaches are frequently employed to solve a wide range of optimization issues. Nowadays, swarm intelligence-based methods are attracting a lot of attention from the research communities of different domains because previous research in complex optimization has shown that behavioral patterns and phenomena observed in nature have the ability to facilitate the foundation for many optimization algorithms and solve problems efficiently. Swarm intelligence, machine learning, and deep learning, on the other hand, each has its own set of advantages and disadvantages. Recently, research communities have discovered an interest in integrating these concepts in order to overcome the limitations of each domain and give rise to a new paradigm known as evolutionary machine learning or evolutionary deep learning. In the case of machine learning and deep learning, the “curse of dimensionality,” non-convex optimization, automatic parameter optimization, and optimal architecture are just a few of the issues that can be efficiently addressed with swarm intelligence, whereas in the case of swarm intelligence, slow convergence, local optima stagnation, and extensive computation cost can be addressed with the machine learning and deep learning community. Therefore, a robust and self-efficient model can be developed by integrating these concepts to solve the complex problem associated with real-world applications. This hybrid approach benefits the majority of research domains. Thus, this chapter will primarily present the ideas, challenges, and recent trends of an integrative approach of swarm intelligence with deep learning, which is currently in high demand for addressing industrial problems.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-1-4842-8925-9_10,en,PyTorch Model Interpretability and Interface to Sklearn,OriginalPaper,"Model interpretability is an area that needs special attention because it is connected with model adoption in particular and AI adoption in general. Users will adopt a model and framework if they can explain the decisions or predictions generated by the deep learning model. In this chapter, you will explore a new framework called Captum, which consists of a set of algorithms that can explain or help us interpret the predictions, model results, and layers of a neural network model. In this chapter, you are also going to use another framework called skorch, which is a library compatible for Scikit-learn users. Machine learning users prefer the sklearn library to train models, perform grid searches, and identify the best hyper parameters of the models—the same kind of seamless experience the users can experience when developing deep neural network models using PyTorch.","['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-981-19-4052-1_72,en,A Hybrid Gray Wolf Optimizer for Modeling and Control of Permanent Magnet Synchronous Motor Drives,OriginalPaper,"The model order diminution and controller design of the permanent magnet synchronous motor drive, which is commonly called PMSM, are performed in the complex delta framework using a hybrid metaheuristic algorithm. Two fundamental algorithms, viz., the firefly technique and the gray wolf optimizer (GWO) are combined to develop a new topology termed as the hybrid gray wolf optimizer (HGWO). A PMSM drive made up of both the speed and current controllers was originally generated, by using an identification method used for signal processing, to yield a lower-order model. The second-order model in cascade with an appropriate controller is thus compared with a chosen reference to estimate approximately the control parameters which are not known beforehand. The continuous parameters almost imitated the parameters set with the delta operator. Thus, for the drive, thereby, a unified control system is built. Therefore, both for order reduction and for calculating the control parameters of PMSM drives, the proposed algorithm is successfully used. Besides, a case study for the control of such special motor drives can also be investigated in the future for many household as well as commercial purposes.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19032-2_36,en,Global Memory Transformer for Processing Long Documents,OriginalPaper,"Transformer variants dominate the state of the art in different natural language processing tasks such as translation, reading comprehension and summarization. Our paper is more directed to use general memory slots added to the inputs and studying the results of adding these slots. This paper is a go on study of general memory slots rule that were added to the input of the proposed model in previous work [ 1 ]. We have two main tasks;1) pretraining task using masked language modeling and b) fine tuning task using HotpotQA . This study aims to verify the ability of the proposed model to handle chunks as if they were one chunk comparing with the base model. As baseline we used T5 transformer. We studied the rule of memory slots augmented to each input chunk and studied the model performance without selector. We found that adding memory to input chunks helped the proposed model to overcome the baseline on Masked language modeling task with specific training parameters. Ablation study reveals the ability of using the compressed input chunks with a degradation in performance.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-981-19-6004-8_55,en,Feature Extraction and Selection with Hyperparameter Optimization for Mitosis Detection in Breast Histopathology Images,OriginalPaper,"This paper attempted to detect mitotic nuclei by extracting different features from nuclei patches in the histopathology Images. We extracted features at multiple levels such as pixel, global, and local to find the best combination of features required for efficient mitosis analysis. We then passed an optimal subset of features to three different classifiers by applying the hyperparameter optimization technique. We observed the highest accuracy of 87.4 with a feature combination of the histogram of oriented gradients (HOG), oriented FAST and rotated BRIEF (ORB), center surround extremas (Censure), corner peaks, edge extract, and grayscale pixel value using tree-based pipeline optimization tool (TOPT) as the hyperparameter optimizer with support vector machine (SVM).","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-18516-8_9,en,Hybrid Approach Based on Grey Wolf Optimizer for Dropout Regularization in Deep Learning,OriginalPaper,"One of the advanced concepts in artificial intelligence is deep learning, which is a subfield of Machine Learning. Training deep neural networks requires setting optimal hyperparameters. Dropout is a regularization parameter that avoids overfitting when training deep neural networks. Despite the success of this technique, finding the optimal value of the dropout probability with a manual search is a time-consuming process. Therefore, metaheuristic algorithms are the best choice to find this optimal value. In this paper, we propose a hybrid search method based on Gray Wolf Optimizer (GWO) and Multi-Verse Optimizer (MVO) to select the dropout probability rate. The results obtained on the image classification task show clearly the good performance of the proposed method.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-3-031-17576-3_1,en,Artocarpus Classification Technique Using Deep Learning Based Convolutional Neural Network,OriginalPaper,"There are many species of Artocarpus fruits in Malaysia, which have different market potentials. This study classifies 4 species of Artocarpus fruits using deep learning approach, which is Convolutional Neural Network (CNN). A new proposed CNN model is compared with pre-trained models, i.e., VGG-16, ResNet50, and Xception. Effects of variables, i.e., hidden layers, perceptrons, filter number, optimizers, and learning rate, on the proposed model are also investigated in this study. The best performing model in this study is the new proposed model with 2 CNN layers (12, 96 filters) and 6 dense layers with 147 perceptrons, achieving an accuracy of 87%.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-981-19-0151-5_25,en,Hyper Parameters Tuning ResNet-50 for Alzheimer’s Disease Classification on Neuroimaging Data,OriginalPaper,"One of the most prominent types of dementia is Alzheimer’s disease (AD). It's among the main causes of death in elderly individuals in all developed nations. Numerous deep learning (DL) models for image classifications and object identification have been developed. However, DL techniques prepare the network model from scratch, which has some downfalls including demanding a massive amount of labeled training data source, which might be a trouble in the medical world, one in which practitioners annotate the data, being very expensive, and usually requires high computational resources. Transfer learning methods are currently being utilized to address these difficulties. In this research, a transfer learning-based ResNet 50 model that was pre-trained on the ImageNet dataset was adjusted on different hyper parameters using the ADNI dataset to provide the best possible results. There are four distinct optimizers utilized, SGD, Adagrad, rmsProp, and Adam, as well as two different batch sizes. The results show that the optimizers’ rmsProp and Adamax performed well with batch sizes of 16 and 32 when compared to the SGD and Adam optimizers. For batch size 32, the classification accuracy for AD vs. NC using rmsProp is 74.22%. However, using batch size 16 resulted in a 1.01% relative improvement with 75.65% classification accuracy.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/978-981-19-6068-0_22,en,Performance Analysis of Hyperparameters of Convolutional Neural Networks for COVID-19 X-ray Image Classification,OriginalPaper,"Analysis of chest X-ray images of COVID infected patients is one of the important diagnostic strategies. The manual identification of these images may be erroneous and faulty. So the computer-aided diagnosis of COVID infections using deep learning techniques is becoming useful. In this paper, the classification of chest X-ray images using CNN is conducted, and the performance of different optimizers is studied. The dataset containing chest X-ray images of normal and COVID infected patients is collected from Kaggle. The experimental study suggested that Adam optimizer achieved 95.83% classification accuracy, and it outperformed the other three optimizers.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-17576-3_3,en,Mango Varieties Classification-Based Optimization with Transfer Learning and Deep Learning Approaches,OriginalPaper,"Mango is one of the well known tropical fruits native to south asia and currently there are over 500 varieties of mangoes known. Depending on the variety, mango fruit can be varied in size, skin color, shape, sweetness, and flesh color which may be pale yellow, gold, or orange. However, sometimes it is difficult for us to differentiate what type of mango it is. Thus, in this paper, four types of mango classification approach is presented. Thus, we are going to use convolutional neural network (CNN) algorithm and transfer learning methods (VGG16 and Xception) to train on the 1000 mango images collected and obtain a deep learning model which is able to classify four types of mango (Alampur Baneshan, Alphonso, Harum Manis and Keitt) automatically. In summary, the objective in this paper is to develop a deep learning algorithm to automatically classify four types of mango cultivar.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-981-19-2821-5_32,en,Ship Detection from Satellite Imagery Using Deep Learning Techniques to Control Deep Sea Oil Spills,OriginalPaper,"Our planet Earth is presently being disturbed by a variety of environmental concerns. One of the top critical environmental issues affecting our planet’s ecosystem is oil spills. Oil spills mostly occur due to ship leakage which highly influences our food supply chain and leads to a high-level drop in the economic division. Therefore, monitoring and tracking those vessels are extremely vital to determine the responsible ships for the occurrence of an incident. This study revolves around an implementation of an automated ship detection software application by building a high-level algorithm that embeds deep learning networks. The algorithm is built in a way that can predict and classify vessels from high-resolution satellite images with 98.5% accuracy.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-08246-7_7,en,Implementation of Reinforcement-Learning Algorithms in Autonomous Robot Navigation,OriginalPaper,"The problem of autonomous robot navigation in indoor environments must overcome various difficulties such as the dimensionality of the data, the computational cost, and the possible presence of mobile objects. This chapter addresses the implementation of an algorithm for autonomous navigation of robots in indoor environments based on machine learning. It characterizes some strategies that the literature reports and specifies a Deep Q-Network reinforcement-learning algorithm to implement on the Turtlebot robotic platform of the Gazebo simulator. Besides, a series of experiments changing the parameters of algorithm to validate the strategy shows how the robotic platform, through the exploration of the environment and the subsequent exploitation of the information, creates effective route planning.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-1-4842-8925-9_5,en,Supervised Learning Using PyTorch,OriginalPaper,"Supervised machine learning is the most sophisticated branch of machine learning. It is in use in almost all fields, including artificial intelligence, cognitive computing, and language processing. Machine learning literature broadly talks about three types of learning: supervised, unsupervised, and reinforcement learning. In supervised learning, the machine learns to recognize the output; hence, it is task driven and the task can be classification or regression. In unsupervised learning, the machine learns patterns from data; thus, it generalizes a new dataset and the learning happens by taking a set of input features. In reinforcement learning, the learning happens in response to a system that reacts to situations.","['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-981-19-5845-8_56,en,Modeling and Control of Induction Machine and Drive in the Combined Domain with New Chaotic Gorilla Troop Optimizer,OriginalPaper,"In this work, a new chaotic version of gorilla troop optimizer is developed. The position equation of the algorithm is modified with the help chaotic maps. Around ten widely cited chaos maps, one-dimensional in nature, are considered to develop new chaotic algorithms. Two unimodal and three multi-modal test functions are employed to validate the efficacy of the proposed technique. A 50 hp induction motor model is also reduced and further its controller is designed utilizing the benefit of delta operator with the help of this proposed algorithm. Finally, a practical test system of induction motor drive is taken up for modeling and control action as well. The controller realization in both cases is carried out using approximate model matching framework. The convergence speed and accuracy of the proposed techniques are better as compared to the standard and latest methods. Thus, the results in all experimentations performed show great promise.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17576-3_2,en,Rambutan Image Classification Using Various Deep Learning Approaches,OriginalPaper,"Rambutan ( Nephelium lappaceum L.) is a widely grown and favored fruit in tropical countries such as Malaysia, Indonesia, Thailand, and the Philippines. This fruit is classified into tens of different cultivars based on fruit, flesh, and tree features. In this project, five different rambutan cultivars classification models using deep learning techniques were developed based on a 1000 rambutan images dataset. Common deep learning methods for the image classification task, Convolutional Neural Network (CNN), and transfer learning method were applied to recognize each rambutan variant. Results have shown that the VGG16 pre-trained model performed best as it achieved 96% accuracy on the test dataset. This indicates the model is reliable for the rambutan classification task.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-3-031-13702-0_9,en,An Approach to Optimizing Food Quality Prediction Throughout Machine Learning,OriginalPaper,"Everything in our life requires real-time monitoring and control, especially when preventing food from spoiling. Keeping the food from spoiling bits helps keep food from spoiling and reduces incidental losses in the business. An Internet of Things (IoT)-based framework for food monitoring is proposed in this paper to protect food from spoilage caused by changes in environmental conditions during storage. In the current scenario, the prediction was made using recorded sensed data, and a detailed analysis was performed to identify the factors causing the food to spoil. This chapter proposes an automated controlling mechanism for environmental parameters using Adaptive Random Forest prediction and IoT. Environmental parameters such as temperature, humidity, moisture, light, and so on, which affect the nutritional value of food, are considered in the proposed work. All sensed data would be stored on the cloud, and analysis will be performed to predict the environmental condition at the storage location to avoid food spoilage by changing to the appropriate environmental condition. In the proposed work, the Random Forest Optimizer method is used to predict the state of food if it is fresh or not at the location where food is stored, harmful changes are monitored, and action is taken to provide appropriate conditions at the stored location. Random Forest optimizer has scored the highest accuracy of 94.01%.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Artificial Intelligence', 'Food Science']"
doi:10.1007/978-3-031-16072-1_19,en,Robust Control Design Solution for a Permanent Magnet Synchronous Generator of a Wind Turbine Model,OriginalPaper,"The paper addresses the development of a perturb and observe algorithm implemented for maximum power point tracking control of a permanent magnet synchronous generator. It is shown that this algorithm tracks the optimum operation point and provides fast response even in the presence of faults. The strategy implements the tracking algorithm by using real—time measurements, while providing maximum power to the grid without using online data training. The solution is simulated in the Matlab and Simulink to verify the effectiveness of the proposed approach when fault–free and faulty conditions are considered. The simulation results highlight efficient, intrinsic and passive fault tolerant performances of the algorithm for electric generators and converters with low inertia.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_22,en,Multi-Objective Artificial Hummingbird Algorithm,OriginalPaper,"This chapter introduces Multi-Objective Artificial Hummingbird Algorithm (MOAHA), a multi-objective variation of the newly established Artificial Hummingbird Algorithm (AHA). The AHA algorithm simulates the specific flight skills and intelligent search strategies of hummingbirds in the wild. Three types of flight skills are used in food search strategies, including axial, oblique, and all-round flights. Multi-objective AHA is tested through 5 real-world engineering case studies. Various performance indicators, such as Spacing (S), Inverted Generational Distance (IGD), and Maximum Spread (MS), are used to compare the MOAHA to the MOPSO, MOWOA, and MOHHO. The suggested algorithm may produce quality Pareto fronts with appropriate precision, uniformity, and very competitive outcomes, according to the qualitative and quantitative.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2635-8_74,en,Comparison of Deterministic and Stochastic Global Optimization Methods for Real-Time Generation of Guidance Trajectories,OriginalPaper,"Guidance algorithms for the return flight of suborbital spaceplanes must generate a variety of guidance trajectories that satisfy terminal conditions even in unexpected abort operations. To tackle this issue, a trajectory optimization method that combines convex quadratic programming and a global derivative-free optimization technique in a nested structure has been recently studied by the authors. This hybrid method efficiently explores the three-dimensional Bezier trajectories and associated guidance commands that exactly fulfill the equality terminal conditions and command continuity. In this paper, Monte-Carlo simulations are performed to investigate the applicability of this guidance method to the realistic scenario of unpowered return flight. Six stochastic evolutionary algorithms, a Bayesian optimization method, and three deterministic search algorithms are implemented and tested as global optimizers. They are compared in terms of computational and implementational complexities, robustness, and diversity of solutions obtained. The results show that reliable and real-time trajectory generation is possible, when an optimizer and its settings are properly chosen. It also reveals that diverse trajectories between initial and terminal conditions are successfully generated.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-981-19-2879-6_5,en,Huawei MindSpore AI Development Framework,OriginalPaper,"This chapter focuses on the AI development framework developed by Huawei—MindSpore. Firstly, it introduces the architecture of MindSpore and how it is designed. Next, it analyses the problems and difficulties of AI development frameworks. Lastly, it further presents the framework base on the development and application of MindSpore.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2004-2_42,en,Segmentation and Classification of Skin Cancer Using K-means Clustering and EfficientNetB0 Model,OriginalPaper,"Nowadays, skin cancer is the widely recognized cancer all over the world. As the spreading rate of skin cancer is increasing day by day, so, there is a need to develop a technique that can detect skin cancer at an early stage. These days, deep learning has attained outstanding success for the detection and diagnosis of cancers. In this paper, a transfer learning-based EfficientNetB0 model is improved by adding one average pooling layer, one dropout layer, one batch normalization and one dense layer with softmax activation function. The proposed model has been simulated using the Kaggle database. The training and calculation are done with different hyper parameters such as batch size, optimizer and epochs. The data augmentation technique is applied to solve the problem of less amount of images. The proposed model has attained 87% accuracy on Adam optimizer with 32 batch size and 30 epochs.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-3-031-18516-8_21,en,Efficient Coronavirus Herd Immunity Optimizer for the UAV Base Stations Placement Problem,OriginalPaper,"This paper proposes an improved version of the Coronavirus Herd Immunity Optimizer (CHIO) algorithm, called RFDB-CHIO, for solving the Unmanned Aerial vehicle carried Base Stations (UAV-BSs) placement problem in 5G networks. The proposed RFDB-CHIO is based on the integration of the Roulette Fitness Distance Balance (RFDB) selection mechanism into the original CHIO algorithm. RFDB-CHIO is validated in terms of user coverage and mean coverage radius under 16 scenarios with different numbers of drones and users. The simulation results demonstrated that RFDB-CHIO obtained better results than CHIO, Whale optimization algorithm (WOA), and Grey Wolf Optimization (GWO) algorithms.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-3-031-09835-2_2,en,Introductory Review of Swarm Intelligence Techniques,OriginalPaper,"With the rapid upliftment of technology, there has emerged a dire need to ‘fine-tune’ or ‘optimize’ certain processes, software, models or structures, with utmost accuracy and efficiency. Optimization algorithms are preferred over other methods of optimization through experimentation or simulation, for their generic problem-solving abilities and promising efficacy with the least human intervention. In recent times, the inducement of natural phenomena into algorithm design has immensely triggered the efficiency of optimization process for even complex multi-dimensional, non-continuous, non-differentiable and noisy problem search spaces. This chapter deals with the Swarm intelligence (SI) based algorithms or Swarm Optimization Algorithms, which are a subset of the greater Nature Inspired Optimization Algorithms (NIOAs). Swarm intelligence involves the collective study of individuals and their mutual interactions leading to intelligent behavior of the swarm. The chapter presents various population-based SI algorithms, their fundamental structures along with their mathematical models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-21595-7_5,en,Computational Microarray Gene Selection Model Using Metaheuristic Optimization Algorithm for Imbalanced Microarrays Based on Bagging and Boosting Techniques,OriginalPaper,"Genomic microarray databases encompass complex high dimensional gene expression samples. Imbalanced microarray datasets refer to uneven distribution of genomic samples among different contributed classes which can negatively affect the classification performance. Therefore, gene selection from imbalanced microarray dataset can give rise to misleading, and inconsistent nominated genes that would alter the classification performance. Such unsatisfactory classification performance is due to the skewed distribution of the samples across the microarrays toward the majority class. In this paper, we propose a modified version of Emperor Penguin Optimization (EPO) algorithm combined with Random Forest (RF) of Bagging and Boosting Classification named by EPO-RF to select the most informative genes based on classification accuracy using imbalanced microarray datasets. The modified version of EPO was built to be based on decision trees that takes in consideration the criterion of tree splitting weights to handle the imbalanced microarray datasets. Average gene expression binary values are used as a preliminary step for exploring disease trajectories with the aid of metaheuristic optimization feature selection algorithms. Results show that the proposed model revealed its superiority compared to well-known established metaheuristic optimization algorithms, e.g., Harris Hawks Optimization (HHO), Grey Wolf Optimization (GWO), Salp Swarm Optimization (SSO), Particle Swarm Optimization (PSO), and Genetic Algorithms (GA’s) using several pediatric sepsis microarray datasets for patients who admitted to the Intensive Care Unit (ICU) for the first 24 h.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Computer Communication Networks', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Machine Learning']"
doi:10.1007/978-1-4842-8005-8_5,en,Deep Learning and Neural Networks,OriginalPaper,"Neural networks, specifically known as artificial neural networks (ANNs), were developed by the inventor of one of the first neurocomputers, Dr. Robert Hecht-Nielsen. He defines a neural network as follows: “…a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.”","['Computer Science', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Machine Learning', 'Python']"
doi:10.1007/978-3-031-18516-8_19,en,A Comparative Study of Metaheuristics Based Task Scheduling in Cloud Computing,OriginalPaper,"Cloud computing is a standard way of hosting software applications and services that is booming day by day thanks to the different utilities offered to users according to their needs and contracts. Most of these services are in the form of tasks and their execution in such environments requires efficient scheduling strategies that take into account both algorithmic and architectural features. The objective is to orchestrate the suitable assignment of the submitted tasks to the available resources on the basis of various functional requirements of end users. To overcome the scheduling issue, which is an NP-hard problem, various metaheuristic algorithms are used in literature to achieve near optimal solution. For this purpose, this paper aims to perform a comparative investigation of three common metaheuristic algorithms in the optimization process such as Shuffled Frog Leaping Algorithm (SFLA), Flower Pollination Algorithm (FPA) and Gray Wolf Optimization (GWO). Both standard and synthetic workloads are employed to analyze the performance of these algorithms by evaluating its objective function in term of two metrics which are makespan and resource utilization rate. The simulation results obtained using the CloudSim framework are very satisfactory and clearly show the value of our study.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-981-19-2273-2_7,en,Combined Passenger and Cargo Transport: A Hybrid Simulation and Optimization Approach Focusing on the Transshipment of Cargo Between Tram Vehicles,OriginalPaper,"Offering a promising opportunity for cargo transport in urban areas, cargo is transported along with passengers sharing the same infrastructure and vehicle. Cargo must be routed efficiently through the network and assigned to vehicles. The resulting problem can be modeled as a network flow problem. Additionally, constraints such as varying capacities due to fluctuations in passenger numbers during the day must be considered. These numbers, as well as the demand for cargo transports, are assumed to be stochastic and therefore considered in a simulation study. To solve this problem, a hybrid simulation and optimization approach are presented. The deterministic network flow problem is solved to optimality with the help of a commercial solver. A simulator uses this solution to assign transport units to vehicles and determine a route through the network. As soon as the solution becomes infeasible, the deterministic problem is resolved with the current parameters. The new solution serves once again as an input for the simulation. This approach is expected to be more efficient compared to considering scenarios for every possible form of uncertainty.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Building Construction and Design', 'Mechanical Engineering']"
doi:10.1007/978-1-4842-8864-1_22,en,Query Store,OriginalPaper,"The Query Store captures the history of queries, their plans, and statistics. It allows DBAs to easily view the plans that were used by queries and troubleshoot performance issues. In this chapter, we will discuss how Query Store can be enabled and configured. We will also examine how the Query Store can be used to diagnose and resolve performance issues.","['Computer Science', 'Microsoft and .NET', 'Database Management']"
doi:10.1007/978-3-031-17576-3_10,en,Comparative Study on Arabic Text Classification: Challenges and Opportunities,OriginalPaper,"There have been great improvements in web technology over the past years which heavily loaded the Internet with various digital contents of different fields. This made finding certain text classification algorithms that fit a specific language or a set of languages a difficult task for researchers. Text Classification or categorization is the practice of allocating a given text document to one or more predefined labels or categories, it aims to obtain valuable information from unstructured text documents. This paper presents a comparative study based on a list of chosen published papers that focus on improving Arabic text classifications, to highlight the given models and the used classifiers besides discussing the faced challenges in these types of researches, then this paper proposes the expected research opportunities in the field of text classification research. Based on the reviewed researches, SVM and Naive Bayes were the most widely used classifiers for Arabic text classification, while more effort is needed to develop and to implement flexible Arabic text classification methods and classifiers.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-981-19-4863-3_5,en,People Count from Surveillance Video Using Convolution Neural Net,OriginalPaper,"People counting is used to count the quantity of people in the picture. People counting is not an easy task if it is done manually by our hand because we can lost count in the middle of doing this laborious task, especially when dealing with object that intersects with each other or dense crowd. This project automates the counting process by building a machine learning system that can convert a video into frames, then the model will output number of objects in a particular frame. We built the model using convolutional neural network (CNN) technique. The system that we built is capable of counting pedestrians in a mall. The frames/images are generated from CCTV that is placed somewhere in the mall. From those frames/images, the system will output how many pedestrians at that particular place in the mall. VGG16 is used to excerpt the topographies of the image and structural similarity index (SSIM) for measuring the similarity among the given images. Then, use the similarity measure as a loss function, named Euclidean and local pattern consistency loss. The experimental results show the predicted number of people and exact number of people in the image with 90% of accuracy using convolution neural net.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-6004-8_9,en,The Facial Expression Recognition Using Deep Neural Network,OriginalPaper,Humans communicate verbally as well as non-verbally through facial expression. Facial expression recognition is an active area and several studies are performed with the help of a convolution neural network. This research is carried out using well-known FER-2013 dataset available on Kaggle. This paper proposed a deep convolution neural network architecture to provide higher accuracy. Different models have been tested on the dataset by varying parameters and layers. This proposed model achieves competitive results with different facial recognition methods by obtaining an accuracy of 68.71%.,"['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-3575-6_64,en,Identification of Skin Diseases Using Deep Learning Architecture,OriginalPaper,"Dermatology is the branch of medicine dealing with the skin. Skin diseases vary from place to place and from season to season and are the more common disease that everyone faces. The cause for the disease may be of fungal, bacterial, viral, or many other reasons. But proper identification of the disease is the most difficult part related skin diseases. Sometimes the skin disease may be identification or symptom to chronic disease. In a vast country like India, educating the people remote areas about skin disease is a difficult task. So a system with ability to identify skin disease will be of a great help in giving timely treatment to all people irrespective of their location. The aim of the project is to develop a system by using concept of deep learning that is able to identify the seven skin disease by the image of the affected area. The process involves steps like choosing the dataset and developing a model that can predict the skin disease with more accuracy and least loss.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-17576-3_6,en,Comparison of Pre-trained and Convolutional Neural Networks for Classification of Jackfruit Artocarpus integer and Artocarpus heterophyllus,OriginalPaper,"Cempedak ( Artocarpus heterophyllus) and nangka ( Artocarpus integer ) are highly similar in their external appearance and are difficult to recognize visually by a human. It is also common to name both jackfruits. Computer vision and deep convolutional neural networks (DCNN) can provide an excellent solution to recognize the fruits. Although several studies have demonstrated the application of DCNN and transfer learning on fruits recognition system, previous studies did not solve two crucial problems; classification of fruit until species level, and comparison of pre-trained CNN in transfer learning. In this study, we aim to construct a recognition system for cempedak and nangka, and compare the performance of proposed DCNN architecture and transfer learning by five pre-trained CNNs. We also compared the performance of optimizers and three levels of epoch on the performance of the model. In general, transfer learning with a pre-trained VGG16 neural network provides higher performance for the dataset; the dataset performed better with an optimizer of SGD, compared with ADAM.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-3-031-16075-2_11,en,Gauging Biases in Various Deep Learning AI Models,OriginalPaper,"With the broader usage of Artificial Intelligence (AI) in all areas of our life, accountability of such systems is one of the most important topics for research. Trustworthiness of AI results require very detailed and careful validation of the applied algorithms, as some errors and biases could reside deeply inside AI components, which might affect inclusiveness, equity, justice and irreversibly influence human lives. It is critical to detect them and to reduce their negative effect on AI users. In this paper, we introduce a new approach to bias detection. Using the Deep Learning (DL) models as examples of a broader scope of AI systems, we make the models self-detective of the underlying defects and biases. Our system looks ‘under the hood’ of AI-model components layer by layer, treating the neurons as similarity estimators – as we claim the main indicator of hidden defects and bias. In this paper, we report on the result of applying our self-detection approach to a Transformer DL model, and its Detection Transformer object detection (DETR) framework, introduced by Facebook AI Research (FAIR) team in 2020. Our approach automatically measures the weights and biases of transformer encoding layers to identify and eventually mitigate the sources of bias. This paper focuses on the measurement and visualization of the weights and biases of the DETR-model layers. The outcome of this research will be our implementation of a modernistic Bias Testing and Mitigation platform. It will be open to the public to validate AI applications and mitigate their biases before their usage.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3148-2_15,en,Performance Evaluation of Optimizers in the Classification of Marble Surface Quality Using CNN,OriginalPaper,"The recital of a convolutional neural network (CNN) is dependent on several things (i.e., optimization, weight initialization, network topology, batches and epochs and activation/loss function and learning rate), as well as on the quality of the input data and specific blend of these model characteristics. When dealing with a classification challenge, relying on a single optimizer is deemed insufficient testing or validity, unless the choice of an optimizer is supported by a compelling reason. As a result, optimizer selection techniques are critical for validating the use of a single optimizer to solve various choice issues. The research begins by evaluating the effectiveness of traditional machine learning algorithms on a dataset of defective/good marble surfaces. The classification was then benchmarked against prominent CNN optimizers to determine where it may be improved. We compare four distinct state-of-the-art gradient descent-based optimizers for CNNs, namely stochastic gradient descent (SGD), adaptive max pooling (Adamax), root mean square propagation (RMS Prop), and Nesterov adaptive momentum (Nadam). The RMS Prop optimizer achieved the highest accuracy of 81% in terms of increasing CNN's classification capabilities.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-6068-0_4,en,Brain Tumour Detection by Multilevel Thresholding Using Opposition Equilibrium Optimizer,OriginalPaper,"The detection of the exact location of a tumour in a complex brain structure is one of the emerging fields of a medical image segmentation study. The ability to segment tumours from magnetic resonance imaging (MRI) brain pictures is crucial for providing effective treatment and surgical planning. Radiologists also accept the importance of the optimised result of multilevel thresholding for segmenting the desired region from the medical images. The use of entropy-based multilevel thresholding with the opposition equilibrium optimizer (OEO) to segment MRI images of the brain into the distinct regions including the tumour is presented in this paper. Finally, the region growing method is used to isolate the complete tumour part. Furthermore, the suggested method is tested on the BRATS 2018 segmentation Challenge dataset, demonstrating its efficacy with better and acceptable Precision, Jaccard index, and dice coefficient values. As a result, the proposed segmentation method is therapeutically relevant.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-14537-7_10,en,The Effect of Harmony Memory Integration into the Bees Algorithm,OriginalPaper,"The advent of optimization algorithms facilitated finding good solutions for engineering problems. This paper presents a comparative case study between two algorithms relevant to bee search methods. One of the algorithms was modified by adding harmonic memory, which is a stage of the Harmonic Search Algorithm. Both algorithms were applied to a spherical four-link Four -link mechanism for gripper design as a case study. The results in terms of the coupler trajectory of the mechanism showed the superiority of integrating harmony memory Harmony Memory into the Bees Algorithm. A prototype is manufactured to show the success of rapid design and production.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-981-19-4250-1_6,en,Integrated Optimization of Differential Steering Chassis by Wire,OriginalPaper,"The differential steer-by-wire system is a new type of steer-by-wire system developed based on the full chassis-by-wire system. On the basis of the traditional mechanical steering mechanism, the driving force difference between the two front wheels of the chassis-by-wire is used for steering assistance, and the driving force difference between the two rear wheels is used for steering stability control. According to its operation principle, the steering system and the drive/brake system of the chassis by wire act simultaneously in the steering process.","['Engineering', 'Mechanical Engineering', 'Control and Systems Theory']"
doi:10.1007/978-3-031-20650-4_6,en,A Review of Capsule Networks in Medical Image Analysis,OriginalPaper,"Computer-aided diagnosis technologies are gaining increased focus within the medical field due to their role in assisting physicians in their diagnostic decision-making through the ability to recognise patterns in medical images. Such technologies started showing promising results in their ability to match or outperform physicians in certain specialities and improve the quality of medical diagnosis. Convolutional neural networks are one state-of-the-art technique to use for disease detection and diagnosis in medical images. However, capsule networks aim to improve over these by preserving part-whole relationships between an object and its sub-components leading to better interpretability, an important characteristic for applications in the medical domain. In this paper, we review the latest applications of capsule networks in computer-aided diagnosis from medical images and compare their results with those of convolutional neural networks employed for the same tasks. Our findings support the use of Capsule Networks over Convolutional Neural Networks for Computer-Aided Diagnosis due to their superiority in performance but more importantly for their better interpretability and their ability to achieve such performance on small datasets.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-15928-2_56,en,Numerical Optimization of a Composite Sandwich Panel with a Novel Bi-directional Corrugated Core Using an Animal-Inspired Optimization Algorithm,OriginalPaper,"Composite sandwich panels with honeycomb, corrugated, tetrahedral, trapezoidal, 3D periodic and hybrid lattice cores have long been studied for their use in various industrial fields. In this study, several numerical analyses were conducted in ANSYS APDL environment in order to analyze the effect of a novel bi-directional corrugated core configuration on the flexural performance of a CFRP sandwich panel. In particular, the sandwich core is obtained by repeating a regular unit cell in two different directions to form a three-dimensional lattice structure. In order to determine the optimal values of the geometrical parameters of the core unit cell and to evaluate how the layout of the composite laminate could affect the mechanical performances of the structure, a numerical study was conducted by using the Group Search Optimizer (GSO) algorithm, a metaheuristic animal-inspired optimization algorithm used to solve various real-world problems. The obtained results show that the GSO algorithm is very effective to optimize the main geometrical parameters of the composite sandwich panel with the novel bi-directional corrugated core. More generally, the implemented procedure provides an open framework to solve complex optimization problems that are very difficult to solve using exact methods, making the GSO algorithm particularly attractive for many industrial applications.","['Engineering', 'Engineering Design', 'Industrial and Production Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-981-19-6379-7_3,en,"Nature-Inspired Computing in Breast Cancer Research: Overview, Perspective, and Challenges of the State-of-the-Art Techniques",OriginalPaper,"Nature-inspired computing (NIC) is a relatively new concept to design new algorithms for solving complex problems based on natural phenomenon. It is a stochastic search technique that is successfully applied in diverse applications in the medical domain and predominantly provides small to large-scale problem-solving solutions. NIC is an emerging approach having different computing techniques to address complex problems in an improved manner. Breast cancer is the second most common cancer in women after skin cancer. Breast cancer research is in prime focus due to its high rate of mortality. One can analyze the large-scale data produced in breast cancer using NIC methods more efficiently in the early prediction of the disease. The objective of this chapter is to present the application, challenges, and advancements in nature-inspired computing for timely diagnosis of breast cancer and also motivate research in this new trend-setting direction.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-981-19-2188-9_4,en,Equilibrium-Optimized IMC-PD Double-Loop Control Strategy for Industrial Processes with Dead Time,OriginalPaper,This paper deals with the application of equilibrium optimizer (EO) in tuning the proportional-derivative (PD)-based internal model control (IMC) double-loop structure for integrating and unstable industrial processes involving time delays. Integrating and unstable processes with significant dead time encountered in chemical process industry present very challenging control requirements. The unity feedback PID controllers with conventional tuning methods are not effective enough to control the dynamics of such industrial processes. The proposed method involves a stabilizing PD controller in the inner loop which is based upon Routh-Hurwitz (R-H) stability criteria. The ranges of controller gain setting obtained from R-H criteria along with sensitivity considerations are utilized by the equilibrium optimizer algorithm to optimize the outer-loop IMC controller performance. The dynamic response of the proposed structure with EO is compared with that of other algorithms show the effectiveness of the proposed work.,"['Engineering', 'Industrial and Production Engineering', 'Mechatronics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Energy Storage', 'Materials Engineering']"
doi:10.1007/978-981-19-3951-8_60,en,An Overview of Recent Nature Inspired Computational Techniques for Dynamic Economic Dispatch,OriginalPaper,"A practical power system network is highly dynamic, non-convex and nonlinear in nature subjected to various discrete and continuous variable constraints. Over the last few decades, several computational techniques, both traditional and nature inspired have been developed to solve the practical dynamic dispatch problem. A large variety of nature-inspired computational (NIC) techniques have been proposed to solve the power dispatch problem owing to their excellent performance, simple constraint handling mechanism and veracity to handle all kinds of functions. Unlike NIC techniques, the traditional methods suffer from convexity, continuity assumptions and may not always be attractive options to solve practical optimization problems of different complexities. In this paper, a review of a large variety of NIC techniques applied to solve the dynamic dispatch problem over the last decade has been summarized.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4863-3_6,en,Detection of Pneumonia and COVID-19 from Chest X-Ray Images Using Neural Networks and Deep Learning,OriginalPaper,"Early detection of pneumonia and COVID-19 is extremely vital in order to guarantee timely access to medical treatment. Hence, it is necessary to detect pneumonia/COVID-19 from the X-ray images. In this paper, convolutional neural networks along with transfer learning are used to aid in the detection of the disease. A CNN model is proposed with four convolutional layers with four max pooling layers, one flatten layer followed by one fully connected hidden layer and output layer. Pre-trained models, namely AlexNet, InceptionV3, ResNet50, and VGG19 are implemented. Chest X-ray images (pneumonia), chest X-ray (COVID-19 and pneumonia), and COVID-19 radiography database are used for implementation for all the models. Precision, recall, and accuracy are used as performance evaluation metrices. The performance of all the models are compared. Experimental results show that the proposed CNN model outperforms all pre-trained models with improved accuracy with reduced trainable parameters. The highest accuracy achieved across all three datasets is 94.25% for the chest X-ray (COVID-19 and pneumonia) dataset.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-3571-8_62,en,A Text Classification Optimization Framework for Prodigious Datasets,OriginalPaper,"This paper introduces Tunicate Swarm Algorithm-based Hierarchical Attention Network (TSA-HAN). TSA-HAN is the combination of Tunicate Swarm Optimization Algorithm (TSA) that uses jet propulsion and Swarm Intelligence and Hierarchical Attention Network (HAN) which makes use of leveled document structure. The proposed optimized algorithm is used for text classification. The performance of TSA-HAN is evaluated based on five parameters, namely accuracy, TPR, TNR, precision, and FNR. For this, purpose self-created dataset named real-time dataset consisting of 5000 documents and popular datasets, i.e., Reuters datasets and 20-Newsgroup datasets, have been utilized and the potency of the said optimized algorithm has been further compared with an existing improved sine cosine algorithm (ISCA). The comparative analysis results show that TSA-HAN performs slightly better than ISCA.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-17576-3_5,en,Image Processing Identification for Sapodilla Using Convolution Neural Network (CNN) and Transfer Learning Techniques,OriginalPaper,"Image identification is a useful tool for classifying and organizing fruits in agribusiness. This study aims to use deep learning to construct a design for Sapodilla identification and classification. Sapodilla comes in a various of varieties from throughout the world. Sapodilla can come in different sizes, form, and taste depending on species and kind. The goal is to create a system which uses convolutional neural networks and transfer learning to extract the feature and determine the type of Sapodilla. The system can sort the type of Sapodilla. This research uses a dataset including over 1000 pictures to demonstrate four different types of Sapodilla classification approaches. This assignment was completed using Convolutional Neural Network (CNN) algorithms, a deep learning technology widely utilised in image classification. Deep learning-based classifiers have recently allowed to distinguish Sapodilla from various images. Furthermore, we utilized different versions of hidden layer and epochs for various outcomes to improve predictive performance. We investigated transfer learning approaches in the classification of Sapodilla in the suggested study. The suggested CNN model improves transfer learning techniques and state-of-the-art approaches in terms of results.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-3-031-09835-2_9,en,"Grey Wolf Optimizer, Whale Optimization Algorithm, and Moth Flame Optimization for Optimizing Photonics Crystals",OriginalPaper,"In this chapter, three recent swarm intelligence algorithms are used to solve a challenging optimization problem in the field of photonics, including Grey Wolf Optimizer, Whale Optimization Algorithm, and Moth Flame Optimization Algorithm. The problem is to optimize the radii of several rods in a photonics crystal to minimize light wave loss when there is a bend corner. This problem is first presented and formulated in details. It is discussed that due to the use of complex simulations, analytics equations are ill-defined for this problem thereby justifying the use of black-box optimization algorithms. The above-mentioned algorithms are then employed to estimate the global optimal for this problem by finding the optimal values for its structural parameters. The results show that the GWO algorithm provides the best results. The chapter also considers a convergence analysis of all algorithms that led to interesting insights about the process of solution improved during the course of optimization. It is observed that GWO shows constant improvement while others tend to show steady and slow improvement.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3679-1_25,en,A Hybrid Approach to Optimize Handover Margin in UWSN by Integration of ACO with PSO and MVO: A Comparative Analysis,OriginalPaper,"Underwater Wireless Sensor Network (UWSN) in the ocean is becoming more and more popular as a tool for marine monitoring and data collection. Sensor nodes’ mobility models for UWSN vary from WSN devices on the ground. This variation complicates handover prediction in these networks, which is a key difficulty. As a result, the current study focuses on handover optimization. UWSN handover and optimization in UWSN handover have received only sporadic attention. Thus, this paper offers a simulation of sensor nodes’ movement calculated data. The speed and direction of the water flow between the data points are included in this dataset. Sensor nodes and base stations in a UWSN are used to simulate the suggested simulation. For the handover optimization job, all of the handover events that occur throughout the simulation are collected. Handover events are optimized using PSO, MVO, and ACO techniques based on historical data obtained from previous handovers. This paper provides the ideal option to increase reliability in the case of UWSN. Performance analysis of the proposed model indicates the excellent quality in the case of the measured evolution scores.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16035-6_5,en,A New Approach for Selecting Features in Cancer Classification Using Grey Wolf Optimizer,OriginalPaper,"The need to detect cancer in early stages is essential for cancer treatment. One of the best ways to classify cancer is with feature (gene) selection to choose the genes that hold the most promise. This step contributes significantly to the classification performance of microarrays and solves the issue of high dimensionality in microarray data. This paper proposes two novel feature selection approaches, both based on the Grey Wolf Optimizer (GWO), to compare and determine the best classifier, whether k-nearest neighbors (KNN) or support vector machine (SVM), with a leave-one-out cross-validation (LOOCV) classifier to classify high dimensional cancer microarray data and solve the feature selection problem. The experiments were implemented on six public cancer microarray data sets to show the remarkable results of the proposed methods. In addition, we compared the proposed algorithms with other recently published algorithms to demonstrate the proposed algorithms’ effectiveness.","['Engineering', 'Computational Intelligence', 'Data Engineering']"
doi:10.1007/978-981-19-3951-8_14,en,Comparison Between Genetic Algorithm and Grey Wolf Optimiser to Solve Capacitated Vehicle Routing Problem,OriginalPaper,"The vehicle routing problem (VRP) is one of the commonly faced problems by the vendors in their daily routine. There are different variants of VRP. One such variant is capacitated vehicle routing problem. To meet client needs for a particular commodity, CVRP requires a group of identical carriers in terms of capacity. The aim is to minimise the cost incurred by vehicles taking into account a variety of restrictions such as the vehicle's capacity and other constraints. The most prevalent approach for solving this problem is the cluster-first route-second strategy. In this process, clients are grouped into several clusters, and one vehicle serves one cluster. This problem can be solved by nature-inspired grey wolf optimiser, genetic algorithm, etc. In this paper, K-GWO (GWO combined with K-means cluster algorithm) and genetic algorithm are used to solve CVRP. Both the algorithms are tested on a number of test cases.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4052-1_9,en,Estimated Time of Arrival for Sustainable Transport Using Deep Neural Network,OriginalPaper,"Time of arrival estimation is a challenge in a world where swiftness and accuracy are a new normal. During the year 1997, a summit in the United Nations showed concern for sustainable transport. This sustainability is for rural–urban linkage, pollution-free environment, health, etc. We understood the importance of estimating the time of arrival for the sustainable transportation and formulated a deep neural network model which increases our estimation of the time of heavy vehicles from source to destination. In our proposed research paper, we created a grid-based network dataset from a raw GPS truck data stored in a form of Data Lake. We introduced a vanilla neural network with three-layered architecture which works well with our grid-based dataset. Gradually, the velocity and volume of data increased and the dataset obtained was with labeled attributes of continuous type. The traditional machine learning algorithms failed or took more time to train on rapidly increasing GPS-dataset. The vanilla deep neural network in a three-layered architecture outperforms various deep learning method with an accuracy of 85%.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-11154-9_6,en,Classification of Arrhythmia Signals Using Hybrid Convolutional Neural Network (CNN) Model,OriginalPaper,"Arrhythmia is a fatal cardiovascular disease that presents an excessively fast heartbeat, excessively slow heartbeat or an irregular heartbeat rhythm. Atrial fibrillation (AF) is a common type of arrhythmia that can be diagnosed using an electrocardiogram (ECG) pattern. Identification of arrhythmia through ECG can be very challenging because the process is highly dependent on experts and very time consuming. The use of deep learning in automatically assisting the detection of arrhythmia using one-dimensional (1-D) input is proposed in this study. Deep learning is preferred over standard neural networks because it facilitates training in an end-to-end manner and directly trains the classification system with raw signals. This study aims to investigate the performance of 1-D convolutional neural network (CNN) for arrhythmia classification and improve its performance by introducing a hybrid approach based on long short-term memory (LSTM) approach. Experimental data are obtained from PhysioNet CinC Challenge 2017 database. ECG signals are preprocessed via filtering, QRS detection, segmentation and median wave selection. One-dimensional CNN, hybrid CNN–long short-term memory (CNN–LSTM) and hybrid CNN–bidirectional LSTM (CNN–biLSTM) models are developed and evaluated in this study to classify ECG signals into (1) normal rhythm, (2) AF rhythm, (3) other rhythms and (4) noisy signal. Accuracies of the 1-D CNN, hybrid CNN–LSTM and 1-D hybrid CNN–biLSTM models were 91.67%, 82.33% and 94.67%, respectively. The experimental results showed that the proposed CNN models can aid in atrial fibrillation (AF) diagnosis for healthcare advancement.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Health Informatics']"
doi:10.1007/978-981-19-0095-2_43,en,Detecting Depression in Tweets Using Natural Language Processing and Deep Learning,OriginalPaper,"COVID-19 has caused physical, emotional, and psychological distress for people. Due to COVID-19 norms, people were restricted to their homes and could not interact with other people, due to which they turned to social media to express their state of mind. In this paper, we implemented a system using TensorFlow, which consists of multilayer perceptron (MLP), convolutional neural networks (CNN), and long short-term memory (LSTM), which works on preprocessing, semantic information on our manually extracted dataset using Twint scraper. The models were used for classifying tweets, based upon whether they indicate depressive behavior or not. We experimented for different optimizer algorithms and their related hyperparameters for all the models. The highest accuracy was achieved by MLP using sentence embeddings, which gave an accuracy of 94% over 50 epochs, closely followed by the other two.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Systems and Data Security', 'Artificial Intelligence', 'Computational Intelligence']"
doi:10.1007/978-981-19-6379-7_7,en,Overview and Classification of Swarm Intelligence-Based Nature-Inspired Computing Algorithms and Their Applications in Cancer Detection and Diagnosis,OriginalPaper,"With the emergence of nature-inspired computing (NIC) techniques, researchers have understood and modeled solutions for realistic and complex problems. NIC, a branch of artificial intelligence worked on the transferring of knowledge from natural phenomenon to engineered systems, applicable in various fields. Although there are many techniques to be used in disease diagnosis, NIC algorithms are very efficient and have gained more attention to problems of modern research. In recent years, these algorithms gained popularity in the detection and diagnosis of cancer, a life-threatening disease that led to a high rate of mortality in individuals. Swarm Intelligence (SI), one of the most used NIC-based algorithms motivated by the collection of social insects’ behavior such as termites, bees, wasps, etc. helps in solving various bioinformatics-related problems. Herein, a chapter has presented various nature-inspired computing intelligence algorithms, with more focus on different types of SI-based nature-inspired algorithms that focus on principles, developments, and application scopes. Further, the chapter has also described applications of SI-based algorithms in detecting and diagnosing different stages and types of cancers. Finally, it has focused on strengths and limitations followed by future directions of these techniques in cancer diagnosis.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-981-19-2397-5_68,en,Hyperparameter Optimization for Object Detection Network,OriginalPaper,"Object detection is a growing research area in the field of neural networks and deep learning with many different applications. One of the most popular applications is in the development of self-driving cars. Hyperparameter optimization is a common mechanism to improve the performance of a deep learning network. However, a lot of the research today tends to use heuristics to set the hyperparameter values. In this study, several methods are discussed and implemented to provide specific techniques that researchers can use to find optimal settings for their network. The experiments are conducted on a camera radar fusion network for detecting and classifying objects in the Nuscenes dataset scene collection. Training the model with the optimization techniques led to a 56.2% improvement in accuracy from the baseline model.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17576-3_12,en,Arabic Text Classification Using Modified Artificial Bee Colony Algorithm for Sentiment Analysis: The Case of Jordanian Dialect,OriginalPaper,"Arab customers give their comments and opinions daily, and it increases dramatically through online reviews of products or services from companies, in both Arabic, and its dialects. This text describes the user’s condition or needs for satisfaction or dissatisfaction, and this evaluation is either negative or positive polarity. Based on the need to work on Arabic text sentiment analysis problem, the case of the Jordanian dialect. The main purpose of this paper is to classify text into two classes: negative or positive which may help the business to maintain a report about service or product. The first phase has tools used in natural language processing; the stemming, stop word removal, and tokenization to filtering the text. The second phase, modified the Artificial Bee Colony (ABC) Algorithm, with Upper Confidence Bound (UCB) Algorithm, to promote the exploitation ability for the minimum dimension, to get the minimum number of the optimal feature, then using forward feature selection strategy by four classifiers of machine learning algorithms: (K-Nearest Neighbors (KNN), Support vector machines (SVM), Naïve-Bayes (NB), and Polynomial Neural Networks (PNN). This proposed model has been applied to the Jordanian dialect database, which contains comments from Jordanian telecom company’s customers. Based on the results of sentiment analysis few suggestions can be provided to the products or services to discontinue or drop, or upgrades it. Moreover, the proposed model is applied to the database of the Algerian dialect, which contains long Arabic texts, in order to see the efficiency of the proposed model for short and long texts. Four performance evaluation criteria were used: precision, recall, f1-score, and accuracy. For a future step, in order to build on or use for the classification of Arabic dialects, the experimental results show that the proposed model gives height accuracy up to 99% by applying to the Jordanian dialect, and a 82% by applying to the Algerian dialect.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-981-19-3716-3_23,en,Feature Selection Based on Gaussian Ant Lion Optimizer for Fault Identification in Centrifugal Pump,OriginalPaper,"Fault diagnosis of the rotating machinery using vibration signal is largely carried out by experience with some prior knowledge of the signal. The diagnosis process is simplified by using machine learning algorithms. The learning capabilities and classification performance of such machine learning models are mostly influenced by the quantity and quality of the input features. Thus, the appropriate selection of a subset of the most prominent features for machine learning becomes essential to eliminate redundancy of high dimension or irrelevant measurements of the features. In this paper, a filter-based feature selection technique is introduced to select the optimal feature space. A Gaussian ant lion optimization (GALO) is put in with a filter-based selection technique to select the feature subset from a high dimension feature dataset obtained from the vibration signals of centrifugal pump under different health conditions (normal, clogging, wheel cut and blade cut). The K-nearest neighbour (KNN) classifier is applied to the selected feature subset to find the classification accuracy. In addition, the proposed method has been compared with other art of work. The results reveal that the proposed GALO-KNN with filter-based feature selection technique outperforms both in feature reduction and classification accuracy and secures the best feature subset with less computational effort. Thus, the proposed method is capable enough to perform the selection task and shows excellent potential in fault diagnosis.","['Engineering', 'Machinery and Machine Elements', 'Robotics and Automation', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/978-1-4842-8925-9_7,en,Natural Language Processing Using PyTorch,OriginalPaper,"Natural language processing is an important branch of computer science. It is the study of human language by computers performing various tasks. Natural language study is also known as computational linguistics . There are two different components of natural language processing: natural language understanding and natural language generation. Natural language understanding involves analysis and knowledge of the input language and responding to it. Natural language generation is the process of creating language from input text. Language can be used in various ways. One word may have different meanings, so removing ambiguity is an important part of natural language understanding.","['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-981-19-1844-5_33,en,Spice Yield Prediction for Sustainable Food Production Using Neural Networks,OriginalPaper,"The world population is increasing rapidly, and the consumption pattern of mankind has made a drastic drift over the recent years. Sustainable food production is important for our existence. The main focus of the study is to build a model that can predict the crop yield for spices such as black pepper, dry ginger, and turmeric based on given factors such as the district of cultivation, year of cultivation, area of production, production per year, temperature, and rainfall. The dataset was obtained from the Spice Board of India and Meteorological Database of India. The region primarily focused on is the districts of Kerala. Neural networks were used for the prediction, and a comparative study was done on different models such as deep neural network (DNN), recurrent neural network (RNN), gradient recurrent unit (GRU), long short-term memory (LSTM), bi directional long short-term memory (BiLSTM), backpropagation neural network (BPNN). The validation techniques taken into consideration include normalized mean absolute error (MAE), normalized root mean square error (RMSE), and mean absolute percentage error (MAPE). For dry ginger, GRU performed better compared to other algorithms followed by SRN. For black pepper, DNN performed better compared to other algorithms followed by simple recurrent network (SRN). For turmeric, GRU performed better compared to other algorithms followed by BPNN.","['Engineering', 'Communications Engineering, Networks', 'Mobile and Network Security', 'Artificial Intelligence', 'Big Data']"
doi:10.1007/978-981-16-9967-2_31,en,3G Cellular Network Fault Prediction Using LSTM-Conv1D Model,OriginalPaper,"Cellular network plays an important role in daily life by exploring digital world of communication. Cellular network technology continuously evolves in past decades from 1 to 5G and beyond. The evolution results in more network accessibility and data utilization. As the availability of network, mobility, and portability of cellular devices are increasing, the network traffic will also be increasing. Higher the transmission rates, higher will be the fault occurrence possibility. Monitoring network parameters and finding fault in cellular network are key factor in determining consistency of network. Cellular network which is highly dynamic than usual networks needs intelligent way of fault handling as the human over head will be unpredictable and very high. Modeling intelligent network fault identification system can simplify human efforts and improve efficiency with better accuracy. The research is on real-time data of 3G cellular network including various network parameters like uplink threshold and identifies the behavior of data usual or unusual to predict the fault occurrence. The study is on various LSTM techniques such as bidirectional LSTM, vanilla LSTM, and stacked LSTM combined with time distributed Conv1D.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Wireless and Mobile Communication', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-18516-8_24,en,Cardiovascular Diseases Prediction Based on Dense-DNN and Feature Selection Techniques,OriginalPaper,"Cardiovascular Diseases (CVDs) are a group of disorders affecting the heart and blood vessels. They have been considered in recent years as one of the main causes of death in the world. Patients with heart disease do not feel sick until the very last stage of the disease and most heart patients die before receiving any treatment. Machine Learning and Deep Learning techniques play an important role in early prediction of heart disease, to improve the quality of healthcare and help individuals to avoid earlier health complications as coronary artery infection and decreased function of blood vessels . Nowadays, the field of health care produces a large amount of data. The need for efficient techniques for processing this data has become necessary. In this paper, a model for cardiovascular disease prediction based on Dense Deep Neural Networks (Dense-DNN) is developed and attributes selection is performed via a Genetic Algorithm (GA). The GA is used to identify the best subset of attributes from the entire features in the dataset, to improve the performances and reduce the training time of the classification model. Our prediction model is compared to several traditional Machine Learning techniques. The performances of our system have been evaluated based on six parameters: (1) accuracy, (2) sensitivity, (3) specificity, (4) F-measure, (5) RMSE, and (6) MAE. Experimental results show that our proposed model outperforms state-of-the-art methods in terms of performance evaluation metrics. The achieved accuracy of the proposed model is 91.7% without using feature selection and 95% with the use of feature selection.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-3-031-15699-1_4,en,Quantum True Random Number Generator,OriginalPaper,"A quantum computer (QC) can generate true random numbers using the quantum superposition property of quantum bits (qubits). However, many types of noise impart bias into the created number, impairing its randomness. To compensate for the noise-induced bias and generate reliable random numbers, we propose gate parameter optimization of the TRNG circuit. We employ a hybrid quantum-classical loop to optimize the gate parameter. The parameter optimization routine can compensate for the bias and enhance the random number’s quality by utilizing even the lowest quality qubits. However, finding the ideal parameter involves lengthy repetitions between classical and quantum machines in the hybrid configuration. We conduct a series of error characterization and quantum tomography experiments to examine the effects of various noises such as gate error, decoherence, and readout error on QC-based true random number generators (TRNG)s. Leveraging insights from the study, we develop a regression-based machine learning approach that predicts the optimal gate parameter from qubit error specification without invoking the costly quantum-classical loop. Moreover, we propose another method by merging the hybrid loop and regression-based model to fine-tune the parameter. We validate our approaches by using experiments on real quantum computers from IBM and testing the generated bitstrings with the NIST statistical test suite. Experimental results suggest that the techniques can correct bias by up to 88.57%, even in worst-case qubits.","['Engineering', 'Circuits and Systems', 'Electronic Circuits and Devices']"
doi:10.1007/978-981-19-5845-8_43,en,A Deep Learning Framework for Social Distance Monitoring and Face Mask Detection,OriginalPaper,The Covid outbreak has caused a worldwide calamity with its poisonous spreading. It has become very important to protect ourselves and the people around us from this infection. The dangers of contagiousness can be limited only by following Covid rules such as wearing facemask and keeping up social distance. This paper proposes a system to distinguish whether the person is wearing a facemask or not and also if the people are maintaining a social distance. The framework used is MobileNetV2 for object recognition. The model is prepared on an image dataset and tested with live real time video with a decent precision. The precision is represented by red and green bounding boxes which indicates facemask accuracy as well as the depth for social distance. Red bounding box appears when the particular object is not wearing a mask or not following social distance and green bounding box displays if the object is following the criteria.,"['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3788-0_11,en,Using Data-Driven Approach in 4D Trajectory Prediction: A Comparison of Common AI-Based Models,OriginalPaper,"Artificial intelligence (AI) is developing strongly and widely applied in many fields, including in the aviation. At the 13th Air Navigation Conference (AN-Conf/13-WP/232) organized by ICAO in Montreal from 9 to October 19 2018, participants discussed AI benefits and preparation for AI-enabled air traffic management (ATM). In this paper, a comparison was carried out to evaluate common AI-based models: Linear regression (LR), Random forest (RF) regression, Extremely gradient boosting (XGBoost) regression and deep neural network (DNN) models for predicting four-dimensional (4D) flight trajectory under weather uncertainties. The datasets used in this paper contain actual ADS-B historical trajectory data of flights from Ho Chi Minh to Ha Noi for 14 days (November 12–26 2021) and time-synchronized weather data along the waypoints of each flight. After comparing the training performance of LR, RF, XGBoost, DNN models, the best-fit DNN models were chosen for further improvement. By tuning their main hyperparameters, the training results are significantly improved in terms of training time and mean absolute errors.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Engineering Thermodynamics, Heat and Mass Transfer']"
doi:10.1007/978-981-19-7808-1_6,en,A Feasibility Review of Novel Avian-Based Optimization Algorithms for Damage Detection in a Truss Bridge,OriginalPaper,"During their lifecycle, bridge structures have to withstand various uncertainties loads such as wind, typhoon, and accident loads which may pose serious threats to the integrity as well as the safety of the structure, especially when they induced significant damages to the structure. For many years, researchers have been trying to develop heath monitoring tools, which can identify accurately not only the location, but also the level of structural damage. In this paper, two novel avian-based optimization algorithms-Artificial Hummingbird Algorithm (AHA) and African Vulture Optimization Algorithm (AVOA) are reviewed for their feasibility in detecting structural damages in truss bridge. The accuracy of the proposed algorithms is compared against two other famous algorithms: particle swarm optimization (PSO) and cuckoo search (CS). The results of the feasibility review for damage detection capability are discussed.","['Engineering', 'Solid Mechanics', 'Structural Materials', 'Computational Science and Engineering']"
doi:10.1007/978-1-4842-8925-9_8,en,"Distributed PyTorch Modelling, Model Optimization, and Deployment",OriginalPaper,"In this chapter, you will use PyTorch to implement the steps that are most commonly used in installation, training, and setting up distributed PyTorch for model training. The architecture followed for distributed data parallel training and distributed model parallel training can be explained using the following figures. The model optimization process reduces the model parameter’s size so that the model object becomes lighter. The bigger the model object, the slower the inference generation. If you reduce the number of layers in the deep learning model, the parameters that are getting trained also lessen, but this may impact the model accuracy. Hence, one technique used to reduce the model size is called quantization . There are different types of model quantization that need to be applied in order to put the model into production. Otherwise, bigger model objects are not compatible for deployment.","['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-981-19-4971-5_3,en,Coordinated Strategy of Ultra-Capacitors and UPFC for LFC of Dual Area Conventional System Having Classical PID Controller with Set Point Filter,OriginalPaper,"This paper introduces the design of classical PID with set point filter ( N ) PIDN controller making use of seagull optimization algorithm (SOA) subjected to squared error over integral (ISE) function for power system load frequency control (LFC). For this purpose, a realistic system of dual area conventional units is conceived and analyzed by laying load disturbance in area-1 of 1% step load (1% SLP). However, the efficacy of different time domain objective functions is tested and ISE is proven as the best among them. Moreover, the devices of ultra-capacitors (Ucs) are placed in dual areas, and unified power flow controller (UPFC) is incorporated with tie-line to further diminish the deviations in system responses.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-3-031-16832-1_2,en,Metaheuristic Algorithms in IoT: Optimized Edge Node Localization,OriginalPaper,"In this study, a new hybrid method is proposed by using the advantages of Grey Wolf Optimizer (GWO) and Moth-Flame Optimization (MFO) algorithms. The proposed hybrid metaheuristic algorithm tries to find the near-optimal solution with high efficiency by using the advantage of both algorithms. At the same time, the shortcomings of each will be eliminated. The proposed algorithm is used to solve the edge computing node localization problem, which is one of the important problems on the Internet of Things (IoT) systems, with the least error rate. This algorithm has shown a successful performance in solving this problem with a smooth and efficient position update mechanism. It was also applied to 30 famous benchmark functions (CEC2015 and CEC2019) to prove the accuracy and general use of the proposed method. It has been proven from the results that it is the best algorithm with a success rate of 54% and 57%, respectively.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-1-4842-8864-1_8,en,Indexes and Statistics,OriginalPaper,"Recent versions of SQL Server support many different types of index that are used to enhance query performance. These include traditional clustered and nonclustered indexes, which are built on B-tree (balanced-tree) structures and enhance read performance on disk-based tables. There are also indexes that support complex data types, such as XML, JSON, and geospatial data types. These advanced data type indexes are beyond the scope of this book, but a full discussion can be found in the Apress title SQL Server Advanced Data Types , which can be found at www.apress.com/gp/book/9781484239001 . DBAs can also create Columnstore indexes to support data warehouse–style queries, where analysis is performed on very large tables. SQL Server also supports in-memory indexes, which enhance the performance of tables that are stored using In-Memory OLTP. This chapter discusses many of the available index types inside the Database Engine.","['Computer Science', 'Microsoft and .NET', 'Database Management']"
doi:10.1007/978-981-19-4182-5_20,en,Design and Development of Micro-grid Networks for Demand Management System Using Fuzzy Logic,OriginalPaper,"Micro-grid is designed to operate with an Energy Management System (EMS), which dispatches the units, in order to optimize generation costs. One of the inputs to this system corresponds to the prediction of demand and also incorporates a demand management system. The objective of this paper is to design the demand prediction block, taking into account the non-linear behavior presented by the demand. The model is designed to deliver the predictions that EMS needs, that is, for a 2-day horizon. When deriving the model, a stability analysis based on the fuzzy theorems is included in the identification stages. The final model consists of four rules and 96 regressors, that is, the future demand depends on the demand of the previous day. As a result, a model is obtained that manages to deliver predictions for horizons of 2 days, with errors of around 14%. The prediction was also analyzed using the EMS optimizer; the fuzzy model prediction had an error 11% lower than the prediction originally used, which translated into a 15% decrease in costs for the 2-day optimization. The second objective corresponds to developing a methodology to model the variation in consumption in the face of demand management signals, using this fuzzy model.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-981-19-4052-1_73,en,Ensemble Method of Feature Selection Using Filter and Wrapper Techniques with Evolutionary Learning,OriginalPaper,"The improvement in data collection and mining methods has expanded the range of dimensionality or features in the data, which brings about an obstacle to many existing feature selection methodologies. This paper brings forth a fresh feature selection methodology rooted in particle swarm optimization (PSO) as wrapper method and an ensemble method to merge the results of the different filter techniques (chi-square, F-regression, and mutual information) to find an optimal feature set that covers most of the key variables of the dataset. The local search is executed on the global best and makes use of a filter-based method, which then intends to take the advantage of the filter and wrapper methods. Our results exhibit that the proposed methodology can be successfully used to select fewer features and, at the same time, increase the classification efficiency over using all features. The proposed methodology also shows how well an evolutionary learning algorithm like the particle swarm optimizer can be used for search optimization of optimum features in the dataset.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4960-9_25,en,"Reinforcement Learning-Based Traffic Engineering in SDN: Problem Formulation, Parameters, and Topology",OriginalPaper,"Traffic engineering solutions in software-defined networks employing reinforcement learning approaches have evolved in recent years due to the rise in interrelated devices and the resulting complexity in the network administration. Network operators can observe the network traffic with efficiency, scalability, and a centralized management in the software-defined networks architecture. Because of separation of control and forwarding planes in software-defined networks, reinforcement learning agents may now be included in networking structure to enforce the traffic pattern modifications during the network congestion. This paper surveys the reinforcement learning methods and the various parameters adopted to improve TE in SDN. We reviewed the usage of reinforcement learning methods and the different parameters used in other policies. We further investigated the recent reinforcement learning-based traffic engineering schemes. The article finally summarizes the various parameters and algorithms used to manage the network performance in SDN efficiently.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-2879-6_4,en,Deep Learning Frameworks,OriginalPaper,"This chapter firstly introduces the development frameworks that are widely used in deep learning and their characteristics, and illustrates one of the representative frameworks, TensorFlow, in detail. This chapter aims at helping readers to deepen their understanding of through practices after learning the concept at the theoretical level, and to tackle the practical problems. problem. In the latter part of this chapter, the MindSpore framework developed by Huawei is introduced. The framework features some advantages that many of today’s frameworks cannot outperform. After reading this chapter, our readers can decide whether to read this section based on their own needs.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_11,en,Automatic Data Clustering Using Farmland Fertility Metaheuristic Algorithm,OriginalPaper,"Data clustering is a data mining task, and it means finding clusters from among data whose labels are not predetermined. It is a popular analytics tool for statistical data in various domains. The k-means algorithm is a basic algorithm for data clustering, which has initial problems such as dependence on the cluster centers’ initial value, sensitivity to outliers, and non-guaranteed optimal solutions to unbalanced cluster formation. This book chapter uses Farmland Fertility Algorithm (FFA) for the data clustering algorithm. Ten standard data sets used to evaluate the effectiveness of FFA are compared Harmony Search (HS), Monarch Butterfly Optimization (MBO), Artificial Bee Colony (ABC), Symbiotic Organism Search (SOS), Differential Evolution (DE), and Crow Search Algorithm (CSA) in terms of statistical criteria such as analysis of variance (ANOVA) and the convergence rate. Experimental results demonstrate that FFA has better performance than other optimization algorithms and is more stable than these algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19958-5_17,en,Detecting Spam SMS Using Self Attention Mechanism,OriginalPaper,"Short Message Service (SMS) is swiftly emerging as the most secure method of communication due to its extensive coverage, dependability, and power efficiency. When compared to application to person (A2P) communications, person to person (P2P) texting is less secure, allowing anyone to send messages, which could result in an assault. Spammers take use of this chance to transmit dangerous content, engage in destructive actions, and harass others. Furthermore, such messages might waste a lot of time, and vital messages can be missed. Therefore, reliable spam identification in SMS has emerged as a critical issue. More recent extension of self–attention mechanism in transformer increases the ability of context in natural language processing. Transformer such as Bidirectional Encoder Representations from Transformers (BERT) works better than Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) for its scanning ability in both order (left to right, right to left) and generating context. This paper interpreted a spam detection model based on self mechanism using BERT on kaggle dataset. Our proposed model outperforms than the machine learning algorithms and deep learning with accuracy 98.80%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2397-5_60,en,Analysis of an Independent Double-Boost Interleaved Converter Operating as Power Optimizer in a PV Application,OriginalPaper,"In this paper, an analysis of three DC-DC interleaved boost converters, working as power optimizers as part of a renewable energy application, was carried out. The topology of the proposed and analyzed power stages is called “independent double-boost interleaved converter” (IDBIC) with three-level output. The working principle of the specified power stage topology was studied in the context of a renewable energy application which involved three copies of the same DC-DC converter with their outputs connected in series. The specified configuration scheme has been used as an interface between three independent PV panels and the deserved load. Every converter has been used to harvest energy from a single PV panel. The overall resultant power of the system represents the sum of the three converter’s output power. Each converter has been driven by its own control law based on the Maximum Power Point Tracking—Perturb and Observe technique. The main purpose of the described control law is to extract the maximum power from each PV panel at different solar irradiation levels. Several tests were performed using a MATLAB–Simulink and PLECS Blockset co-simulation model. The resultant current and voltage waveforms of each PV panel at different values of solar irradiation were represented and analyzed. Also, the efficiency of each power stage was determined using switching and conduction power losses provided by thermal models of the power switching elements being used within the study model.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_5,en,Performance Analysis of Hybrid Memory Based Dragonfly Algorithm in Engineering Problems,OriginalPaper,"Hybrid Memory Based Dragonfly Algorithm with Differential Evolution (DADE) is one of the most prominent swarm-based optimization techniques due to its better computational complexity and high convergence rate for achieving optimum results. In DADE, the best solution is memorized and processed with Differential Evolution (DE) for enhanced diversity and balanced exploration and exploitation rate. DADE brings two distinct advantages: First, the superior convergence rate due to continuous update of personal best individual in the search process. Second, better exploration due to the inclusion of global best and global worst individuals in the hybridization process. Comparative simulations have been performed on 24 standard benchmark functions along with the benchmark function of CEC2005 and CEC2017. Comparative analysis of the result demonstrates the competitiveness of the DADE algorithm in terms of optimal cost, computational complexity and convergence characteristics compared to other considered optimization algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3035-5_56,en,Training Logistic Regression Model by Enhanced Moth Flame Optimizer for Spam Email Classification,OriginalPaper,"Spam email is a massive issue that bothers and consumes receivers’ time and effort. Because of its effectiveness in identifying mail as wanted or unwanted, machine learning approaches have become a popular technique in spam detection. Current spam detection methods, on the other side, typically have low detection performance and are incapable of handling high-dimensional information easily. As a result, a unique spam detection approach that combines an improved moth flame optimization algorithm and a logistic regression classification model was proposed in this paper. The research evidence on two accessible datasets (CSDMC2010, Enron) indicates that the suggested methodology can tackle high-dimensional data due to its very powerful local and global search skills. The suggested technique was evaluated for spam detection accuracy to that of logistic regression, naive Bayes classifiers, and support vector machine, as well as the performance of earlier research’ that includes state-of-the-art approaches. In terms of classification performance, the suggested methodology outperforms the other spam detection algorithms examined in this work.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Statistics, general']"
doi:10.1007/978-1-0716-2655-9_9,en,Computational Approaches to Assess Abnormal Metabolism in Alzheimer’s Disease Using Transcriptomics,OriginalPaper,"Transcriptome-integrated human genome-scale metabolic models (GEMs) have been used widely to assess alterations in metabolism in response to disease. Transcriptome integration leads to identification of metabolic reactions that are differentially inactivated in the tissue of interest. Among the methods available for mapping transcriptome data on GEMs, we focus here on an Integrative Metabolic Analysis Tool (iMAT), which we have recently applied to the analysis of Alzheimer’s disease (AD). We provide a detailed protocol for applying iMAT to create models of personalized metabolic networks, which can be further processed to identify reactions associated with abnormal metabolism.","['Biomedicine', 'Neurosciences']"
doi:10.1007/978-981-19-6004-8_49,en,Advanced Approach for Heart Disease Diagnosis with Grey Wolf Optimization and Deep Learning Techniques,OriginalPaper,"Electrocardiography (ECG) has gained popularity in recognizing abnormally quick and slow heart rates. Previously, clinical data was used for heart disease diagnosis. Advance technology led to improvement in deep learning (DL) and optimization algorithms that use image data to classify and predict heart diseases. DL eliminates data pre-processing as in machine learning (ML), to scan and search features that correlate and combine them to enable faster learning. DL requires huge data to perform accurately. DL is expensive concerning hardware requirements and training which increases the overall cost. Hence, grey wolf optimization (GWO) can provide competitive results of improved local optima. This paper presents an advanced approach of index matching to perform analysis in two categories by merging clinical data with ECG for classifying and diagnosing heart diseases. Best features are selected by GWO on merging 12-lead Physikalisch-Technische Bundesanstalt (PTB-XL) ECG data and clinical data. Evaluations using deep neural networks on the merged data set are performed. In comparison, the results show that convolutional neural networks (CNNs) using Visual Geometry Group (VGG-16) outperform all other algorithms. Performance is evaluated with various parameters like accuracy, recall, precision, and area under the receiver operator characteristic curve (AUC). Deep learning-based algorithms have a promising future in ECG analysis regarding quantitative exactness and quality.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-3590-9_15,en,Towards Design of a Novel Android Malware Detection Framework Using Hybrid Deep Learning Techniques,OriginalPaper,"Malware designers have switched their focus to the android platform as a result of the widespread use of android smartphones in our daily lives. There is  some exiting state-of-the-art techniques whose performance needs to be improved in malware detection for android based system. In this paper, we have proposed a novel android malware detection framework using hybrid deep learning techniques. In the proposed framework, at first pre-processing steps are employed to get optimized feature set. For the feature selection, this paper has used gain information and Pearson correlation coefficient techniques in which k-best features are selected using the gain information technique. Furthermore, the Pearson correlation technique is applied to remove the features which have similar coefficients and do not contribute much to overall results. For the detection of malware, optimized feature-based dataset is used for training of the proposed hybrid of bidirectional long short-term memory (BiLSTM) and merged sparse auto-encoder (MSAE) with softmax deep learning model. Performance analysis of the proposed malware detection framework is compared with three state-of-the-art techniques such as CNN-BiLSTM, CNN-LSTM and CNN-GRU in terms of accuracy, f 1-score, precision and recall. It is observed that the proposed framework performs better than the existing models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security', 'Mobile and Network Security']"
doi:10.1007/978-3-031-16832-1_6,en,A Meta-Heuristic Algorithm Based on the Happiness Model,OriginalPaper,"Recent work has attempted to determine the appropriate global minimum for complex problems. The paper presents a population and direct-based swarm optimization algorithm called the happiness optimizer (HPO) algorithm. An HPO algorithm is designed based on personal behavior and demonstrated in 30 and 100 dimensions on benchmark functions. The model includes four questions: “what do you want?”, “what do you have?”, “what do others have?”, and “what happened?”, which guide the development of a happiness behavior model. By considering the balancing between exploration and exploitation operators in the search space problem, efficiency, robustness, and stability were demonstrated for synthetic and real cases. For comparison, our proposed algorithm and some well-known algorithms will be 30 times applied on the benchmark functions and then compared with statistical value and Wilcoxon signed-rank test. As a consequence, the performance, reliability, and stability of our work have been demonstrated better than the others.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-981-19-4676-9_6,en,Comparison and Analysis of Various Autoencoders,OriginalPaper,"The autoencoder is family of deep neutral network which learns to reconstruct its input. It has three main parts encoder, code, and decoder. Autoencoders are effective unsupervised learning method which encode an input into a lower dimensional representation. This representation input consist of input as features are useful for image processing applications. The size of hidden representation is lesser then the original image that’s under complete autoencoder. If the size is greater than the hidden representation that is over complete autoencoder. This paper compares and evaluates many architectures of autoencoders model.","['Engineering', 'Computational Intelligence', 'Systems and Data Security', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-12127-2_4,en,The Novel Characterizing Method of Collective Behavior Pattern in PSO,OriginalPaper,"Although swarm intelligence algorithms have attracted extensive attention, there is little research on the behavior of collective dynamics. Inspired by the moving patterns of fish school, we propose a visualization method of collective behavior patterns based on the velocity field, discover various collective behavior patterns, and propose a discriminate index named swarm trend factor. In addition, this paper proposes a novel swarm states division method, on the basis of swarm trend factor. In the experiments, we demonstrate that swarm trend factor can reflect the performance of PSO. And, we also compare the difference between swarm trend factor and another swarm state division method.","['Engineering', 'Computational Intelligence', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/978-3-031-07322-9_43,en,Intelligent Health Indicators Based on Semi-supervised Learning Utilizing Acoustic Emission Data,OriginalPaper,"Health indicators are indices that act as intermediary links between raw SHM data and prognostic models. An efficient HI should satisfy prognostic requirements such as monotonicity, trendability, and prognosability in such a way that it can be effectively used as an input in a prognostic model for remaining useful life estimation. However, discovering or designing a suitable HI for composite structures is a challenging task due to the inherent complexity of the evolution of damage events in such materials. Previous research has shown that data-driven models are efficient for accomplishing this goal. Large labeled datasets, however, are normally required, and the SHM data can only be labeled, respecting prognostic requirements, after a series of nominally identical structures are tested to failure. In this paper, a semi-supervised learning approach based on implicitly imposing prognostic criteria is adopted to design a novel HI suitable. To this end, single-stiffener composite panels were subjected to compression-compression fatigue loading and monitored using acoustic emission (AE). The AE data after signal processing and feature extraction were fused using a multi-layer LSTM neural network with criteria-based hypothetical targets to generate an intelligent HI. The results confirm the performance of the proposed scenario according to the prognostic criteria.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-3938-9_8,en,Optimal Impulsive Maneuver Planning for Station Acquisition of Geosynchronous Satellites,OriginalPaper,Station acquisition is an important phase of a geosynchronous satellite mission. The method developed in this work deals with the planning of tangential impulsive maneuvers for station acquisition while optimizing the total propellant consumption. The problem is formulated as an orbit reconfiguration problem using relative orbital elements with respect to a virtual spacecraft at the target orbit. Differential Evolution is used in conjunction with the linearized relative orbital element dynamics model to identify an initial guess over a defined search space of maneuver times and velocity impulses satisfying the common mission constraints. This initial guess is further used to initiate constrained numerical optimizer to optimize total magnitude of velocity impulse required for the reconfiguration while employing high fidelity dynamics model incorporating various perturbations significant for modeling the dynamics of a geosynchronous spacecraft. Results highlight the practical applicability of the algorithm and certain advantages over the existing literature.,"['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-3-031-17544-2_9,en,Diabetes Twitter Classification Using Hybrid GSA,OriginalPaper,"In today’s world, it is important to understand individuals’ health related opinions through sentiment analysis. Recently, many deep learning methods such as deep convolution neural network (CNN), Gated Recurrent Unit (GRU) and LSTM (Long short-term memory) are used to classify sentiments. This chapter focuses on diabetes tweet classification using the proposed Hybrid GSA, which is a combination of a Capsule Network (Deep Learning technique) and a Gravitational Search Algorithm. This approach is applied to perform sentiment classification such as positive, strong positive, negative, strong negative, and neutral using tweets on Twitter. It is observed from the results that the proposed approach produced better classification results compared to the existing approach. This work proved to be very effective in handling health tweets and accurate in classification.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-981-19-3951-8_38,en,Renewable Energy Integrated Economic Dispatch Using Intelligent Techniques: An Overview,OriginalPaper,"Nature-inspired and metaphor-less population-based computational techniques have gained tremendous importance for solving real-world constrained optimization complications during the last twenty years. This is due to their unconventional direct search mechanism, ease of application and non-dependence on the mathematical nature of objective function. Amongst the various engineering optimization problems, the economic dispatch (ED) is one of the most important problems which has been addressed using a large number of these techniques. In power system operation, ED has a special place and therefore this topic has received immense attention from researchers. With changing operational philosophies and technological advancements, the ED problem formulation has undergone many changes over the years. Beginning with simple and approximate models, various different practical constraints were integrated into the classical ED problem over time, the latest being integration and modelling of the renewable energy (RE) sources. The paper presents a review of different metaheuristic techniques proposed for various types of ED problems with renewable energy integration based on the last decade.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3391-2_5,en,Text Summarization Approaches Under Transfer Learning and Domain Adaptation Settings—A Survey,OriginalPaper,"Data explosion is becoming a well-known concern as a result of rapid improvements in Internet-related technologies. The ease with which people can share and access information over the Internet has resulted in an abundance of data on any topic. Text summarization powered by deep neural networks can analyse vast volumes of text input and create a short summary on any topic to address the problem of information overload. However, deep neural networks require a lot of labelled data, and therefore, training them using tiny labelled text datasets is inefficient. Transfer learning and domain adaptation are possible solutions to this problem, and researchers have recently begun to investigate them in text domains. In this study, we analyse a number of recent articles that use transfer learning and domain adaptation to tackle the text summarization problem. The methodology for text summarization in transfer learning and domain adaption settings is explained in this survey. The application of transfer learning in sequence-to-sequence- (Seq2Seq) and non-Seq2Seq-based techniques for text summarization is discussed, with emphasis on crucial unique methodology and contributions from different works. Latest approaches for text summarization under different configurations of domain adaptation are discussed and compared. Issues, challenges and opportunities pertaining to this domain have also been identified and discussed for the benefit of the readers.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-5224-1_65,en,Operational Availability Optimization of Cooling Tower of Thermal Power Plants Using Swarm Intelligence-Based Metaheuristic Algorithms,OriginalPaper,"Cooling towers are mainly utilized to disperse the heat of thermal power plants (TPP). The availability of cooling tower is directly proportional to the maximum availability of TPP. To ensure the maximum availability of cooling towers, a mathematical model is developed followed by optimization using four swarm intelligence-based metaheuristic algorithms, viz. Grey Wolf Optimizer, Grasshopper Optimization Algorithm, Dragonfly Algorithm, and Whale Optimization Algorithm. The Markovian birth–death process and Chapman-Kolmogorov differential–difference equations are utilized to derive the objective function of availability associated with the proposed model. It is observed from the numerical investigation that the Whale Optimization Algorithm performs better than all other metaheuristic algorithms in providing the optimized values of various failure and repair rates and predicting the overall availability of the cooling tower.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-981-19-3148-2_36,en,Hybrid Gorilla Troops Optimizer-Based NMF Algorithm for Integrative Data Analysis,OriginalPaper,"Cancer subtype identification using integrative analysis of high-dimensional and heterogeneous multi-omics data has gained a lot of attention. Clustering analysis using data integration has become a desirable approach to obtain hidden substructure of the datasets reflecting the correlation between and within the data. In this paper, for integrative clustering of multi-omics data joint non-negative matrix factorization (jNMF) and sparse-jNMF has been adopted. The nature of NMF is iterative and is inherently non-convex, non-differentiable and multimodal, therefore, the initial point estimation of the NMF factor matrices to a great extent affects the quality of the solution. Metaheuristics optimized initialization of NMF is considered as a favorable choice. In this paper, high-dimensional GTO encoded structure (HD-GTO)-based initialization of jNMF and sparse-jNMF has been proposed. The experimental results are conducted on two multi-omics cancer datasets. It is observed that HD-GTO-guided initialization of sparse-jNMF shows improvement in accuracy and purity when compared with other state-of-the-art metaheuristics. Experimental results also confirm that HD-GTO sparse-jNMF produces 3.5% average improvement in accuracy and 4.1% average improvement in purity on two datasets when compared with jNMF.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-3938-9_17,en,Winglet Aerodynamic Optimization of a 19-Seater Turboprop Aircraft,OriginalPaper,"Aerodynamic optimization of winglets for a 19-seater turboprop aircraft was carried out using response surfaces and non-gradient-based optimization algorithms. Four winglet parameters, viz., span, sweep, twist, and cant angle were considered as design variables. Sampling was done at fifty locations in the parameter space using Latin hypercube method. Navier–Stokes CFD solutions were generated at these sampling locations by integrating OpenVSP, an open-source parametric aircraft geometry modeler with a commercial mesh generator and Navier–Stokes CFD solver (HexpressHybrid/Hexpress and FineOpen, respectively). Response surfaces were fit on these samples using Gaussian process regression (also known as Kriging method) for various objective functions. The entire process of sample geometry generation, meshing, and CFD solution evolution was automated by utilizing the scripting capabilities and Python interface of the aforementioned software. GPy, an open-source Gaussian process framework in Python, was used to fit the response surfaces. Genetic algorithm and Monte-Carlo methods were both separately used to find the global optimum on the generated response surfaces, and the optimum solution was verified using CFD. Objectives considered for arriving at a suitable design were maximization of L/D (Lift-to-drag ratio of the aircraft) and maximization of a drag-weighted L/D function, which was formulated to ensure that drag reduces during maximization of L/D.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-981-19-5292-0_43,en,An Effective and Secured Routing Protocol Based on Cluster Head in WSN,OriginalPaper,"In wireless sensor network (WSN), the field data is detected by tons little sensors. The sensors having restricted amount of resources. The sensed information must be conveyed to base station to increase the lifespan of a network in an appropriate manner. The clustering of sensor nodes becomes the way to do this. For WSN, the current project would implement a new energy efficient threshold-based routing protocol (ET-LEACH). The head of the cluster receives the sensed information and aggregates it. The aggregated data is forwarded to the base station. The network abnormalities may degrade the performance of a WSN. Black-hole and misdirection attacks are most commonly known abnormality. To overcome this issue in this work, Secret Key Comparison (SKC) method is proposed in WSN. SKC method is proposed that facilitates deployment to prevent attacks. The SKC is a dynamic method and attacks are detected with high accuracy. In WSN, intrusion detection system plays major role. Security applications require it. The selection of optimum cluster head in the proposed work is performed by an Enhanced Grey Wolf Optimizer (EGWO) algorithm. Network’s data transmission as well as the throughput can be improved by this proposed method.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-20105-9_8,en,Comparison of Metaheuristics Techniques and Agent-Based Approaches,OriginalPaper,"Agent-based models represent new approaches to characterize systems through simple rules. Under such techniques, complex global behavioral patterns emerge from the agent interactions produced by the rules.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19958-5_5,en,Metaheuristic Solver for Problems with Permutative Representation,OriginalPaper,"Today, a large proportion of combinatorial optimization problems can be efficiently formulated as a mixed-integer program and solved with an exact solver. However, exact solvers do not scale well and thus custom metaheuristic algorithms are being designed to provide better scalability at the cost of no optimality guarantees and time-consuming development. This paper proposes a novel formalism for a large class of problems with permutative representation, together with a metaheuristic solver addressing these problems. This approach combines the advantages of both exact and metaheuristic solvers: straightforward problem formulation, scalability, low design time, and ability to find high quality solutions. Three different problems are formulated in the proposed formalism and solved with the proposed solver. The solver is benchmarked against the Gurobi Optimizer and significantly outperforms it in experiments with a fixed computational budget.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_16,en,Novel Chaotic Best Firefly Algorithm: COVID-19 Fake News Detection Application,OriginalPaper,"The COVID-19 pandemic period is approaching the two-year mark of its lifetime. During the pandemic, the people experienced various restrictions and different problems were raised unlike before. While the people stayed in their homes, electronic device usage had a decisive spike. This increased time spent on the internet and the amount of misinformation followed the same trend. The problem with the information on the COVID-19 is that there are too many different sources presenting different data. While some are just trying to help and do not have the latest or the most accurate information, some sources are malicious. Either way, everything but the latest and the most accurate data needs to be filtered when regarding a serious matter as such. The research proposed in this manuscript is aimed to identify COVID-19 fake news by performing wrapper-based feature by using an improved version of the well-known firefly algorithm. Practical simulations were done against a well-known dataset used in the domain of this problem, Koirala. The proposed method managed to achieve high accuracy of classification by using a smaller number of features in comparison with the state-of-the-art methods tested in the same experimental environment.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-7083-2_1,en,Introduction,OriginalPaper,"This chapter introduces the background knowledge of the book, including the most widely used artificial neural network models and decision trees, gradient based learning methods, evolutionary algorithms and their applications to single- and multi-objective machine learning, traditional privacy-preserving computing methods such as multi-party secure computation, differential privacy, and homomorphic encryption, and the federated learning paradigm for privacy-preserving machine learning. An overview of horizontal and vertical federated learning, together with a description of the basic federated learning algorithm, known as federated averaging, is presented, before knowledge transfer in federated learning is briefly explained. Finally, the main challenges of federated learning over non independent and identically distributed data are discussed in detail.","['Computer Science', 'Machine Learning', 'Privacy', 'Cryptology']"
doi:10.1007/978-981-19-4863-3_44,en,Facial Micro-expression Recognition Using Deep Learning,OriginalPaper,"Micro-expressions are those which reveal a person’s true intentions, because it lasts for less than 0.5 s. So in that short time, there will be no chance to hide or fake their emotions. This helps to know the real intention of a person in any unexpected situations. Subtle expression popularity is a getting popularity owing to its capability in revealing subtle intention of humans, particularly while under excessive stake conditions. The main applications of micro-expression recognition are to detect lies and investigate a thief. Human facial micro-expression is divided into several universal emotions such as happy, sad, angry, fear, neutral, and surprised. For detecting these expressions, the proposed research has taken videos from MEVIEW dataset and converted videos into frames according to their expressions and considered images from SAMM dataset which is used to detect the micro-expression. To identify the face first to detect the micro-expression, the pre-trained model Haar cascade is used for this purpose, and with the help of that, a bordered rectangular box appears around the face. A deep learning technique called convolutional neural network (CNN) is used for image processing, where there are various strategies. A précised model is defined, to know the level of emotions, with the help of the SAMM and MEVIEW datasets and obtained better results compared to previous research in the literature.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-3387-5_172,en,Cell Expansion Priority Recommendation Based on Prophet Algorithm,OriginalPaper,"In recent years, both LTE and 5G experiences fast development and construction world-widely. Under the explosive business demand and massive mobile terminal equipment connection, the guarantee of network load is of vital importance. This paper proposes a recommendation model for cell expansion priority. Initially, it uses the Prophet algorithm to predict cell traffic trends based on user behavior characteristics in different scenarios. Then it defines expansion warning thresholds according to different expansion types. Finally, combining cell traffic trends and expansion thresholds, a recommendation model is established. The proposed model can assist network optimizer more accurately grasp the current network capacity situation and future capacity trend, monitor the indexes of network expansion dynamically. Besides, this model can also follow the expansion principle of timeliness and predictability, which can make rational use of expansion investment and guarantee user perception.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-981-19-3575-6_14,en,Application of Deep Learning for COVID Twitter Sentimental Analysis Towards Mental Depression,OriginalPaper,"The coronavirus pandemic hit the worldwide population to a large extent. But, one of the subtle effects of the COVID-19 pandemic was the depletion of the mental health of the people. Social media has become an efficient platform to express oneself, and Twitter is one of the most used platforms. There has been work where machine and deep learning were employed for tweet sentimental analysis for different applications including mental depression. Most of tweets sentimental analysis were focussed on positive and negative. There has been some research where neutral tweets were taken into consideration. In this research work, we have focussed on predicting depression of people, i.e. depressed, non-depressed and neutral from tweets during lock down period by employing deep learning models like bidirection long short-term memory (Bi-LSTM), bi-directional encoder representations from transformers (BERT), and XLNET. Also, the BERT model has been modified by adding classification layer for tweet classification. In addition, the exploratory data analysis was performed for postlockdown tweets.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-14537-7_16,en,Bees Traplining Metaphors for the Vehicle Routing Problem Using a Decomposition Approach,OriginalPaper,"In this study, the bees traplining Traplining metaphor was adopted for the Bees Algorithm Bees Algorithm, THE (BA) and the Combinatorial Bees Algorithm Bees Algorithm (BA C ) and applied to solve the vehicle routing problem Vehicle routing problem . The two-parameter Continuous and Combinatorial Bees Algorithms (BA 2 and BA C2 ), equipped with a traplining Traplining metaphor intensifier Intensifier , Bees Routing Optimiser Bees routing optimiser (BRO), were used to solve the capacitated vehicle routing problem Vehicle routing problem with a decomposition Decomposition approach. In the first phase of the proposed method, the two-parameter Bees Algorithm (BA 2 ) was employed to solve the capacitated facility location problem, resulting in clusters of customers that did not violate the vehicles’ capacity. Then, BA C2 combined with BRO Bees routing optimiser was used to produce the routing plan for each cluster. BA 2 and BA C2 implement the traplining Traplining foraging Foraging point technique of bees Bees , which integrates their exploratory and exploitative search mechanisms, to simplify parameter Parameters setting and use their threat avoidance tactics to intensify the solution. The results of comparisons with other BA versions indicate that the proposed algorithm improves the accuracy of the basic version by at least 4% while speeding it up fourfold.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-981-19-0151-5_6,en,Comparative Analysis of Machine Learning Algorithms with Ensemble Techniques and Forecasting COVID-19 Cases in India,OriginalPaper,"Unanticipated information in December 2019 changed the world around us. A relatively contagious disease unfolded through the SARS-CoV-2 virus that travelled throughout the globe and was declared an epidemic by WHO in March 2020. The need of examining the scenario became the inducement behind this research. The assessment of COVID-19 in India is performed from 1 April 2020 to 20 May 2021 which amassed a total of 415 instances. Further, preprocessing of the dataset is executed with the use of normalization. The experimentation is executed through the use of four ensemble strategies which are bagging, boosting, stacking and voting with four distinct machine learning algorithms linear regression, sequential minimal optimizer for regression, multilayer perceptron and Gaussian process. The splitting of the dataset is completed at 75%, and machine learning algorithms with ensemble techniques are applied. Linear regression with the bagging ensemble method gives satisfactory outcomes with the correlation coefficient of 0.935 and 0.919 for confirmed cases and recovered cases, respectively, and Gaussian process presented the best results for deceased cases. In the case of ensemble strategies, bagging indicates the best correlation coefficient in each case. Therefore, with the help of the three best algorithms, confirmed cases, recovered cases and deceased cases predictions are performed. The paper has potential implementations that can foresee the COVID-19 confirmed cases, recovered cases and deceased cases based on historic data and subsequently structure the plan for the future.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/978-981-16-7487-7_12,en,Error-Tolerant Mapping for Quantum Computing,OriginalPaper,"Quantum computers are built with fragile and noise/error-prone qubits. Some prominent errors include, decoherence/dephasing, gate error, readout error, leakage, and crosstalk. Furthermore, the qubits vary in terms of their quality. Some qubits are healthy whereas others prone to errors. This presents an opportunity to exploit good quality qubits to improve the computation outcome. This chapter reviews the state-of-the-art mapping techniques for error tolerance. We take quantum benchmarks as well as approximate algorithms for applications covering MaxCut, object detection and factorization to illustrate various optimization challenges and opportunities.","['Engineering', 'Circuits and Systems', 'Electronic Circuits and Devices', 'Processor Architectures', 'Nanotechnology', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/978-3-031-16832-1_4,en,Minimum Transmission Power Control for the Internet of Things with Swarm Intelligence Algorithms,OriginalPaper,"More than 212 billion devices will use the internet at the end of 2020. According to this information, more accurate artificial intelligence (AI) approaches are required for more efficient Internet of Things (IoT) usage. Wireless sensor networks (WSNs) contain energy-limited devices and calculating the minimum transmission power control (TPC) is a tackling process. Swarm intelligence is a subsection of AI and in the last four decades, many swarm intelligence algorithms are proposed for solving optimization problems. The minimization of energy usage and maximizing the network lifetime are many useful and essential for IoT. In this work, four different swarm intelligence algorithms—particle swarm optimization (PSO), artificial bee colony (ABC), salp swarm algorithm (SSA), and tree-seed algorithm (TSA)—are used for solving the minimum TPC optimization problem. The obtained results, convergence graphs, and standard deviations are showed that ABC is the best swarm intelligence algorithm, and the TSA is the most robust algorithm in this experimental environment.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-3-031-16832-1_11,en,Multi-circle Detection Using Multimodal Optimization,OriginalPaper,"Object and shape detection in digital image were one of the hot topic over the last two decades. Especially automatic multi circle detection has received more attention over last years. Hough transform (HT) is a well-known and most popular method for lines and circles detection. However, HT has huge computational complexity expense. This paper proposed a new successful heuristic method to reduce computation time and improve the speed of HT for circle detection. In this proposed method the edges information of the image is obtained by means of Robert edge detection. Then, multimodal particle swarm optimization (PSO) and local search is employed to locate all exciting circle in the image. The experiments on benchmark images show that our scheme can perform multi circle detection successfully.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-3-031-17544-2_12,en,GACO: A Genetic Algorithm with Ant Colony Optimization—Based Feature Selection for Breast Cancer Diagnosis,OriginalPaper,"Breast cancer is the most prevalent cancer diagnosed and the basis of mortality among women worldwide. However, the early prognosis and treatment can avoid the death rate of the patients. Since the traditional method of detecting cancer is error-prone, machine learning has shown significant promise in aiding the accurate diagnosis. Moreover, using a minimal number of features is highly pertinent in decision-making. Therefore, this chapter proposes a novel evolutionary algorithm-based feature selection method to identify the most appropriate attributes. The suggested model fuses the Genetic Algorithm with Ant Colony Optimization to increase the search operation in the global search space. Finally, the Random Forest classifier is employed on the reduced attribute subset to examine and determine the nature of breast tumors. The developed system is evaluated on the Wisconsin Diagnostic Breast Cancer dataset. The experimental outcomes demonstrate the efficiency of the proposed method over other popular single algorithms and ensemble learners.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-981-19-4147-4_47,en,Multi Parametric Optimization of Dry Turning of Titanium Alloy (Ti6Al4V Graded 5) using Coated Carbide Insert: A Novel Hybrid RSM-Artificial Gorilla Troop Optimization and Dingo Optimization Algorithm,OriginalPaper,"The present research piloted dry turning of titanium alloys (Ti-6Al-4V Grade 5) utilizing modern titanium carbonitrides chemical vapor deposition (MT-CVD) coated carbide tool inserts. The impacts of cutting parameters such as feed rate, cutting speed, and depth of cut on output responses such as cutting forces, flank wear, and surface roughness were investigated using a Box-Behnken design based on response surface methodology (RSM). To find the optimum cutting condition, a newly developed hybrid optimization method, namely RSM-linked Artificial Gorilla Troop Optimization Algorithm and Dingo Optimization Algorithm, was utilized. The flank wear along with cutting force was recorded when the depth of cut was less while better surface finish was achieved with lower cutting speed. Additionally, analysis of variance was used to determine the most important component in each of the three responses, followed by a confirmatory test that revealed a high degree of agreement between anticipated and experimental results. As per the analysis of variance (ANOVA) results, the depth of cut was determined to be the most crucial component in attaining the lowest cutting force, flank wear, and surface roughness responses. The results obtained using the artificial gorilla optimization algorithm and the dingo optimization algorithm are found to be more precise than those obtained using the RSM-designed experimentation of dry turning operations, as cutting force, surface roughness, and flank wear have been minimized by utilizing the factor settings achieved using both the artificial gorilla optimization algorithm and the dingo optimization algorithm.","['Engineering', 'Materials Engineering', 'Robotics and Automation', 'Structural Materials', 'Biomaterials']"
doi:10.1007/978-3-031-13786-0_10,en,Wide Band THz Antenna Design Using Salp Swarm Algorithm for 6G Communications Systems,OriginalPaper,"The future 6G wireless communications will need the definition of new spectral bands and the employment of novel advanced physical layer solutions. The millimeter-wave (mmWave) frequency bands have been allocated for the fifth generation (5G) of cellular systems, while additional mmWave sub-bands have been assigned as well. The need to support higher data rates than 5G in the order of terabits per second requires more bandwidth. However, the total consecutive available bandwidth in mmWave bands is still less than 10 GHz, so such data rates cannot be supported. In this context, future 6G communication systems require the use of the terahertz communication band (0.1–10 THz). The THz band is envisioned as a critical wireless technology for meeting future demands in 5G and beyond. For several years, there has been a lack of THz transceivers and antennas, so that the THz band has become one of the electromagnetic (EM) spectrum’s least studied frequency ranges in terms of wireless communication. However, the need for 6G communication systems has redefined the requirements for THz antennas. In this book chapter, we provide a complete framework for circular polarized antenna design in the low THz band. This optimization framework is based on a swarm intelligence algorithm, namely, the salp swarm algorithm (SSA). The numerical results show that the SSA has been successfully applied in designing antenna with wide band operation and circular polarization.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Computer Communication Networks']"
doi:10.1007/978-3-031-16078-3_47,en,Predicting Interruptions of Medium Voltage Customers Using Fully Connected Networks,OriginalPaper,"The losses caused by the lack of electricity typically exceed the cost of the electricity itself. Improving power quality is a way to reduce or avoid loss of production in the industry, prevent fires or explosions, and minimize damages to industrial equipment. Therefore, finding customers that probably will have interruptions in advance will generate value for both the company and customers. The purpose of this study is to analyze data from units that consume electricity using different types of decision trees, such as binaries nodes and multiple branches of a single parent node, and using fully connected neural networks to predict the interruption index for the next year. The results reveal an important space for improvements such as the connection between non-compliance of established indicators over time and specific points of electrical network with problems. That way supports the concessionaries to manage their infrastructure to get a better quality of the electric power network.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17576-3_9,en,"A Novel Big Data Classification Technique for Healthcare Application Using Support Vector Machine, Random Forest and J48",OriginalPaper,"In this study, the possibility of using and applying the capabilities of artificial intelligence (AI) and machine learning (ML) to increase the effectiveness of Internet of Things (IoT) and big data in developing a system that supports decision makers in the medical fields was studied. This was done by studying the performance of three well-known classification algorithms Random Forest Classifier (RFC), Support Vector Machine (SVM), and Decision Tree-J48 (J48), to predict the probability of heart attack. The performance of the algorithms for accuracy was evaluated using the Healthcare (heart attack possibility) dataset, freely available on kagle. The data was divided into three categories consisting of (303, 909, 1808) instances which were analyzed on the WEKA platform. The results showed that the RFC was the best performer.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-1-0716-2647-4_12,en,In Vitro Analysis of CTLA-4-Mediated Transendocytosis by Regulatory T Cells,OriginalPaper,"Regulatory T Cells (Tregs) Regulatory T cells (Tregs) constitutively express the inhibitory receptor CTLA-4, which is fundamental to their role in immune suppression Suppression . Mechanistically, CTLA-4 on Tregs Regulatory T cells (Tregs) can attenuate T cell activation by physically removing and internalizing costimulatory ligands CD80 CD80 and CD86 CD86 from the surface of antigen-presenting cells by transendocytosis Transendocytosis . Therefore, the process of transendocytosis Transendocytosis can be harnessed as a tool to study the molecular basis of CTLA-4 biology and a key aspect of Treg Regulatory T cell (Treg) suppressive function. In this chapter, we describe a method of human Treg Regulatory T cell (Treg) isolation and expansion Expansion resulting in high CTLA-4 expression. We then detail a transendocytosis Transendocytosis assay using artificial antigen-presenting cells (DG-75 B Cell lines) expressing fluorescently tagged ligands mixed with the expanded Tregs Regulatory T cells (Tregs) . This methodology can be applied to testing of patients carrying CTLA-4 mutations, providing a robust model to assess the degree of functional disruption.","['Biomedicine', 'Immunology']"
doi:10.1007/978-3-030-96025-4_1,en,Introduction,OriginalPaper,"This chapter presents a brief introduction of the transmission and distribution systems problems such as power system monitoring based on phasor measurement units (PMUs) and the enhancement of distribution network performance based on distributed generations (DGs) and/or capacitor banks. In addition, a brief introduction of different optimization techniques such as analytical, artificial intelligence (AI) and modern optimization techniques for solving those problems is considered. Finally, it summarizes the objectives and contributions of the book.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Systems', 'Renewable and Green Energy']"
doi:10.1007/978-981-19-4863-3_9,en,"Deep Generative Models Under GAN: Variants, Applications, and Privacy Issues",OriginalPaper,"Deep learning has lately acquired a lot of attention in machine learning because of its capacity to train features and classifiers at the same time, resulting in a significant boost in accuracy. To attain a high level of accuracy, the models require huge amounts of data and processing capacity, both of which are now available due to the advancements in big data, Internet of Things, and cloud computing. Even though, some applications like medical diagnosis, image recognition, and biometric authentication faces the problem of data scarcity which affects the predictive analytics of deep learning. In order to tackle the issue, deep generative models like Generative Adversarial Networks (GAN) come into existence that are capable of artificially generating synthetic data for specific problems. In this article, various models of GAN and their applications were explored and a comparison of the models were also given. As the data increases, another issue faced by the applications is of data privacy. With rising privacy concerns, more priority has to be given for privacy issues while developing intelligent applications. GAN and its variants are nowadays used as an attacker as well as a defender against various privacy risks which were also presented in this review. As a future work, GANs potential to solve the issues of data privacy and security has to be deeply explored.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-1-0716-2732-7_9,en,"One-Shot Generation of Epitope-Directed Monoclonal Antibodies to Multiple Nonoverlapping Targets: Peptide Selection, Antigen Preparation, and Epitope Mapping",OriginalPaper,This chapter describes an epitope-directed approach to generate antipeptide monoclonal antibodies to multiple nonoverlapping protein sites using a cocktail of fusion peptides as immunogen. It provides a step-by-step protocol on how antigenic peptides on a target protein can be identified by in silico prediction and discusses considerations for final peptide selection. Each antigenic peptide (10–20 amino acids long) is displayed as three-copy inserts on the surface exposed loop of a thioredoxin scaffold protein. The corresponding DNA coding sequence specifying the tripeptide insert flanked by Gly-Ser-Gly-Ser-Gly linkers is cloned in-frame into the Rsr II site of the thioredoxin gene in the pET-32a vector. The presence of a C-terminal polyhistidine tag (His 6 -tag) allows the soluble fusion proteins to be purified by one-step native immobilized metal affinity chromatography (IMAC) to greater than 95% purity. Multiple thioredoxin fusion proteins are mixed in equimolar concentrations and used as an immunogen cocktail for animal immunization. The use of short antigenic peptides of known sequence facilitates direct epitope mapping requiring only small mutagenesis scan peptide libraries in the multipin peptide format.,"['Life Sciences', 'Biological Techniques', 'Immunology', 'Pharmacology/Toxicology']"
doi:10.1007/978-981-19-3998-3_35,en,Dynamic Target Search of UAV Swarm Based on Improved Pigeon-Inspired Optimization,OriginalPaper,"In this paper, an improved pigeon-inspired optimization (IPIO) algorithm based on natural selection and Gauss-Cauchy mutation is proposed for unmanned aerial vehicle (UAV) swarm to rapidly realize cooperative dynamic target search and full coverage of target area under uncertain environment. Firstly, the environment awareness map is established, which includes coverage distribution map, target probability map (TPM), digital pheromone map and their updating mechanism. Meanwhile, in order to improve the possibility of discover targets, the target probability map is integrated into the attraction pheromone updating mechanism. Next, by the helps of the above environment awareness map, a reasonable collaborative search task optimization model is designed. Furthermore, based on the classical PIO algorithm, the integer encoding method, discrete compass operator and discrete landmark operator are designed in detail. Gaussian mutation and Cauchy mutation operators are introduced to guarantee the evolution escaping from local optimum, and natural selection is applied to accelerate the convergence. Finally, the simulation results show the effectiveness and superior of the proposed target search strategy.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-2764-5_21,en,Power Enhancement of Total-Cross-Tied Configured PV Array During Dynamic Irradiance Change Using Metaheuristic Algorithm-Based MPPT Controllers,OriginalPaper,"Partial shading condition (PSC) is the major threat to the building-integrated PV systems as they are sorely affected in terms of drastic reduction in PV output power and efficacy. To enhance the maximum power production capability and efficacy, the PV system needs a robust maximum power point tracking (MPPT) controller capable of tracking global maximum power peak (GMP) under PSCs. Many conventional algorithms, i.e., incremental conductance (Inc) , perturb and observe (P&O), etc., are reported in literature, but they are failed to track GMP and also create significant power oscillations in steady state during PSCs. Hence, this paper proposes metaheuristic algorithms-based TCT-configured PV MPPT system. In this, metaheuristic algorithms such as artificial bee colony (ABC), grey wolf optimization (GWO), and particle swarm optimization (PSO) techniques are applied to TCT-configured PV array to operate at GMP under four dynamic PSCs. All the metaheuristic algorithm-based MPPT methods are simulated in MATLAB/Simulink platform and their performances are compared with each other and also with conventional P&O and Inc techniques with respect to achieved GMP, tracking speed/convergence time, efficiency, and oscillations at GMP. The presented simulation results confirm that PSO algorithm outperforms other methods by achieving the highest GMP, efficiency, less convergence time, and reduced oscillations around GMP.","['Energy', 'Energy Systems', 'Artificial Intelligence', 'Machine Learning', 'Cyber-physical systems, IoT', 'Professional Computing', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/978-3-031-05405-1_9,en,The Application of a Force Identification Method Based on Particle Swarm Optimization to Compression Steel Bars,OriginalPaper,"The non-destructive determination of internal forces in bars for existing building structures is a forward-looking challenge for the construction industry. The application of validated techniques would not only allow the detection of overloads or structural changes according to the load-bearing behavior, it would also allow the verification of the original structural analysis. Based on an experimental modal analysis, tried and tested methods to determine the tensile forces of cables in the operating condition have existed for many years. Due to the slenderness of the cables, even normal forces have a large influence on their natural frequencies: The higher the tensile forces, the higher the geometrical system stiffness, which increases natural frequencies. An equivalent effect occurs when considering load-bearing elements subjected to compression, whereby larger compressive forces lead to a reduction in the geometrical system stiffness and thereby to a reduction in natural frequencies. While various methods have been experimentally investigated for tension cables and tension rods, only a few elaborations are known with respect to structural compression rods. This research presents a system identification method based on the experimental determination of the modal parameters of a compression steel bar. Using an iterative optimization procedure, the design parameters of a basic model including the compressive force are iteratively adjusted until the best possible agreement between the experimental measurements and a theoretical analysis is found. For this purpose, a deviation function is formulated and an evolutional algorithm — in this case facilitating a Particle Swarm Optimization — is used to solve the optimization problem. The Particle Swarm Optimization is an innovative metaheuristic algorithm that allows searching for the global minimum even for complicated nonlinear functions. The method is experimentally validated on slender steel beams with varying compressive forces. Laboratory results from three different steel profiles combined with two different bearing conditions each are presented. Therefore, six specimens were constructed and tested with four different force levels each. The average deviation over all 24 tests between the optimized force and the actual force directly measured by a load cell was determined as 7.4%.","['Engineering', 'Mechanical Statics and Structures', 'Building Construction and Design', 'Complexity', 'Vibration, Dynamical Systems, Control', 'Mechanical Engineering', 'Statistical Theory and Methods']"
doi:10.1007/978-3-031-17544-2_6,en,NIANN: Integration of ANN with Nature-Inspired Optimization Algorithms,OriginalPaper,"Artificial neural networks (ANNs) are stimulated according to the biological brain's connection of axons and dendrons. These neural networks perform a major part in the advancement of artificial intelligence and learning algorithms. Though initially used for image classification, in modern times applications of ANNs have been useful over numerous fields such as medical data mining, bioinformatics, natural language processing, time series forecasting, and in various optimization problems as well. Nature-inspired algorithms are a set of novel problem-solving approaches that are derived from various incidents occurring in nature around us. Each of the methods such as the BAT, genetic algorithm, or colony optimization methods were created by keeping a specific hard problem in mind. In recent times general purpose use of these nature-inspired algorithms has become widely popular in solving mainly optimization problems derived from the fields of NLP, machine learning, deep learning, classification, and feature selection as well. Nature-inspired algorithms mainly work by mimicking phenomena occurring in nature among various species on a macro scale. A set of nature-inspired algorithms such as the genetic algorithm family mimics the processes that occur in a microorganism such as a cell division, mutation, etc. Since these algorithms are inspired by nature and were developed keeping in mind achieving an optimal solution of a given hard problem, their application in general-purpose problems also yields satisfactory results. If an algorithm fails to achieve a satisfactory solution to a problem, it is easy to modify them according to the need of the given problem to overcome any obstacle. In this chapter, an approach is introduced that aims to combine the nature-inspired optimization algorithm with the learning model of artificial neural networks to provide a more accurate and streamlined output generation of the neural network. Nature-inspired algorithms can be used as a learning method in the ANN model. In contrast to that, an ANN can also be used as an objective function to a nature-inspired algorithm to improve its capability to generate an optimal solution. This chapter aims to explore both approaches in detail.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-3-031-15699-1_6,en,Margin Optimization of Single Flux Quantum Logic Cells,OriginalPaper,"Single Flux Quantum (SFQ) logic family is an attractive alternative to CMOS technology with the promise of more than two orders of magnitude improvement in the energy-delay product. However, component-level parameter variations during the fabrication process of SFQ logic cells are quite high. Therefore, optimizing SFQ logic cells to maximize their operating parameter margin (and parametric yield) under variability sources is a necessity. In this chapter, a hybrid design optimization technique based on Automatic Niching Particle Swarm Optimization and Fireworks Algorithm is presented where the objective is to maximize the upper and lower bound margins of the design parameters of a SFQ logic cell. The proposed algorithm can efficiently optimize both simple and complex multi-stage logic cells with various fan-in and fan-out counts. The proposed method improves the critical margin range and parametric yield values for 6 different logic cells by 22.83% and 15.22% on average, when compared to a previously optimized open-source cell library.","['Engineering', 'Circuits and Systems', 'Electronic Circuits and Devices']"
doi:10.1007/978-3-031-17576-3_8,en,Enhanced MapReduce Performance for the Distributed Parallel Computing: Application of the Big Data,OriginalPaper,"Now a days and previous years, the increase in the volume of data has accelerated and this requires more storage places with the increase of data, as big data has a huge number of users and cloud computing, and these users need to access data securely and privately from any device at any time. Therefore, it is important to provide a safe flow of data in the Internet of Things (IOT records file) and to reduce its size in a way that does not affect its purpose or its purpose. The most important field of data mining is the search for items and repetitive data inside storage locations. Apriori algorithm was the most common algorithm for finding a set of repeated elements from data. This needs to delete a group of data that is repeated more than once and create a number of new groups after deleting the repeated ones, which leads to an increase in the storage space and an increase in the speed of its performance. In this paper, we implemented the MapReduce Apriori (MRA) algorithm on the Apache Hadoop cluster that includes two functions (Map and Reduce) to find the repeated sets of k-elements.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Big Data']"
doi:10.1007/978-3-031-19958-5_93,en,Hybrid Particle Swarm Optimization for a Feature Selection Problem with Stability Analysis,OriginalPaper,"In this paper a hybrid Particle Swarm Optimization (HPSO) method is developed and applied to a Hybrid Feature Selection problem. The objective of applying the HPSO method to the Hybrid Feature Selection problem is to increase the classification accuracy and decrease the number of features. In the proposed HPSO method we have introduced the concepts of mutation and crossover taken from Genetic algorithm and then introduced into Particle Swarm Optimization (PSO) method. It helps in order to increase the exploration opportunity. Using Von Neumann stability criterion and the concepts of Fourier series, the mathematical explanation of stability of the proposed HPSO method has been explained with proof. Convergence of the proposed HPSO algorithm is explained using the Markov chain concept. Then results are compared graphically and statistically with other meta-heuristic algorithms. Friedman test and Mann-Whitney U test are used to check the statistical significance of the proposed algorithm against other meta-heuristic algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-12011-4_77,en,Size Optimization of Truss Structures Using Real-Coded Genetic Algorithm with a Novel Constraint Handling Method,OriginalPaper,"Minimum weight design has been increasingly important in the twenty-first century due to rising concerns about reducing the carbon footprint of structures. Further, it also reduces the cost by better utilization of the materials. The advances of metaheuristic algorithms have resulted in the ability to attain global optimum even for highly non-linear, non-convex problems such as structural size optimization of complex structures. However, the computational time for the optimization process could increase drastically with the complexity of the problem. Therefore, the requirement for reducing the computational time of the process is essential. This research aims to do weight minimization for truss structures by size optimization using a real-coded genetic algorithm. A new constraint handling technique called the ‘corner bounding fly-back mechanism’ is proposed in this work. The proposed method enables a feasible set of solutions during each iteration and produces a higher convergence rate than other traditional methods. A series of well-known benchmark examples in the literature are optimized using the proposed method, coded in MATLAB. The numerical results obtained indicate that the convergence rate for the proposed method is about four to ten times when compared to other conventional constraint handling methods in the literature.","['Engineering', 'Construction Management', 'Building Construction and Design', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/978-981-19-4193-1_47,en,IoT-Enabled Automated Analysis and Classification of COVID-19 Disease in Lung CT Images Based on Edge Computing Environment,OriginalPaper,"A new coronavirus outbreak (COVID-19) has created a dire scenario around the world, making it one of the most acute and severe diseases to strike in the last century. Daily, the number of people infected with COVID-19 increases across the globe. Despite the fact that there are no vaccines for this pandemic, deep learning techniques have shown to be a helpful addition to the arsenal of diagnostic tools available to clinicians. IoT-enabled edge computing environments necessitate the use of the federated deep learning (FDL-COVID) COVID-19 detection model. First, the FDL-COVID method allows IoT devices to collect patient data, and secondly, using SqueezeNet architecture, the DL model is developed. The encrypted variables are uploaded to the cloud server by the IoT devices, and the SqueezeNet model is used to perform FL on the major variables in order to generate a global cloud model. As a result, a glowworm swarm optimization (GSO) algorithm-based hyperparameter optimizer is applied to the SqueezeNet model’s hyperparameter selection. The CXR dataset was used to run a large number of simulations on the SqueezeNet model, and the results were analyzed using a variety of metrics to create a global cloud model. Additionally, the SqueezeNet architecture’s hyperparameters are optimized using the glowworm swarm optimization (GSO) technique. The benchmark CXR dataset was used to conduct a wide range of experiments, the results of which were analyzed using several metrics. The experimental results showed that the FDL-COVID technique outperformed the other methods in terms of performance.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_6,en,Optimum Design and Tuning Applications in Structural Engineering via Swarm Intelligence,OriginalPaper,"As all engineering disciplines, structural engineering problems are needed to be optimized and due to the nonlinear behavior of these problems, it is not possible to solve them mathematically, but metaheuristic methods are very successful in iterative optimization by assuming values for the design variables within a desired range of the user. In structural engineering problems, metaheuristic methods including swarm-intelligence-based algorithms are used in two groups of problems. Design optimization is the first group and the design like dimension, amount of material and orientations are optimally found for minimizing objectives related to cost, weight, CO 2 emission and others. In these problems, constraints are found via design codes like steel and reinforced concrete structure design regulations. This group belongs to a design of a structure. The second group includes optimum tuning and it generally covers structural control applications. This group involves the optimum tuning of the additional control system of the structure that can be added to the newly constructed structure for better performance or existing ones to correct the failure or increase the existing performance. The role of engineers is to make the best possible structural design and optimization is important. More especially, tuning optimization is a must to provide acceptable performance. In this chapter, a review of existing studies about the design optimization of structural systems is presented for swarm intelligence-based algorithms. Then, optimum tuning applications are mentioned including the most important studies about tuned mass dampers. Finally, optimization problems are presented for design and tuning optimization. The RC retaining wall optimization was presented for two cases with and without toe projection and the optimization of a toe is 5% effective on reduction of cost. In span length optimization of frame structures, frame models with different stories have similar optimum span lengths. Active tuned mass dampers are up to 22.08% more effective than passive tuned mass dampers.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-1-4842-8864-1_5,en,Configuring the Instance,OriginalPaper,"The installation and configuration of your SQL Server instance does not end when setup successfully completes. There are many other considerations that you should take into account, both inside the database engine and outside of it, using tools such as SQL Server Configuration Manager. In this chapter, we will discuss many of the most important instance-level configuration options, including processor and memory configuration, SQL Server’s buffer pool extension, and hybrid buffer pools. We will also explore important configuration choices for system databases, how your instance can configure and how to configure SQL Server, to work with your firewall. We will also look at some useful trace flags and how to set them.","['Computer Science', 'Microsoft and .NET', 'Database Management']"
doi:10.1007/978-981-19-6290-5_10,en,CNN-Based Models for Image Forgery Detection,OriginalPaper,"Image forgery (IF) is a technique in which images are manipulated through several tampering software such as Photoshop, Adobe, Corel, etc., and it becomes difficult to discriminate between authentic and forged images. Conventional techniques suffered from the weakness that they can only extract specific kinds of features and can identify only one type of tampering. This chapter introduces deep learning methods especially convolutional neural network (CNN) models, ResNet-50, and MobileNetv2 for tampering detection. Two datasets are used—CASIA v1.0 and CASIA v2.0 for experiments. These datasets have been divided into 80% training set and 20% testing set and achieved an overall highest accuracy of 95%.","['Computer Science', 'Systems and Data Security', 'Cyber-physical systems, IoT', 'Professional Computing', 'Artificial Intelligence']"
doi:10.1007/978-981-16-9131-7_5,en,Data-Driven RUL Prediction,OriginalPaper,"In this chapter, the data-driven RUL prediction methods for mechanical systems are presented. Since the deep learning algorithm has shown remarkbale advantages on prognosis problems in the current literature, the neural network-based methods are focused on in this chapter. First, the deep separable convolutional neural network-based RUL prediction method is introduced, which establishes a direct mapping relationship between raw monitoring data and RUL by implementing separable convolution and constructing information refinement units. Next, the recurrent convolutional neural network-based RUL prediction method is illustrated. A network with temporal memory capability is constructed using recurrent connections and gating mechanisms. At last, we present a multi-scale convolutional attention network-based RUL prediction method. By the integration of multi-scale representation learning strategy, the degradation information of the mechanical system can be extracted in different time scales. Throughout this chapter, experiments on multiple run-to-failure datasets are carried out, which validate the effectiveness of the presented methods.","['Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-3-031-16078-3_42,en,Intrusion Detection Systems Using Support Vector Machines on the KDDCUP’99 and NSL-KDD Datasets: A Comprehensive Survey,OriginalPaper,"With the growing rates of cyber-attacks and cyber espionage, the need for better and more powerful intrusion detection systems (IDS) is even more warranted nowadays. The basic task of an IDS is to act as the first line of defense, in detecting attacks on the internet. As intrusion tactics from intruders become more sophisticated and difficult to detect, researchers have started to apply novel Machine Learning (ML) techniques to effectively detect intruders and hence preserve internet users’ information and overall trust in the entire internet network security. Over the last decade, there has been an explosion of research on intrusion detection techniques based on ML and Deep Learning (DL) architectures on various cyber security-based datasets such as the DARPA, KDDCUP’99, NSL-KDD, CAIDA, CTU-13, UNSW-NB15. In this research, we review contemporary literature and provide a comprehensive survey of different types of intrusion detection technique that applies Support Vector Machines (SVMs) algorithms as a classifier. We focus only on studies that have been evaluated on the two most widely used datasets in cybersecurity namely: the KDDCUP’99 and the NSL-KDD datasets. We provide a summary of each method, identifying the role of the SVMs classifier, and all other algorithms involved in the studies. Furthermore, we present a critical review of each method, in tabular form, highlighting the performances measures, strengths, and limitations, of each of the methods surveyed.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16078-3_50,en,Hybrid Evolutionary Algorithm for Optimal Control Problem,OriginalPaper,"The optimal control problem is well known long time ago, but there is no a general numerical method for it. More precisely researchers try always to solve this problem various numerical methods. Recently it was established, that if the optimal control problem has phase constraints included in quality criterion, then a functional is multimodal and this optimization problem belongs to global class optimization. Therefore, to solve the optimal control problem it is better to use evolutionary algorithms. But this proposition doesn’t give useful information for researcher, because now there are huge quantity evolutionary algorithms. In this paper the best evolutionary algorithm for the optimal control problem is proposed. It is constructed on the base three evolutionary algorithms, genetic algorithm, particle swarm optimization, and grey wolf optimizer. It is shown computational experiments, where this hybrid algorithm is compared with everyone from listed above on the complex problem with phase constraints.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3387-5_165,en,Research on Interference Source Identification and Location Based on Big Data and Deep Learning,OriginalPaper,"The thesis is mayor in based on the evaluation of base station interference sources in multiple dimensions by China Unicom Shanghai Branch. Based on the value mining of multiple data sources such as KPI/XDR/MDT, it closely integrates the traditional interference source investigation process and big data/machine learning Technology. At the same time, combined with BPNN neural network, the interference source and interference type can be identified intelligently. By using the BFL boundary point fitting algorithm, the interference source can be detected, classified and simulated. At the same time, a set of interference source overall optimization scheme is given.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-15211-5_59,en,Comparative Study of Different Metaheuristics on CEC 2020 Benchmarks,OriginalPaper,"Metaheuristic algorithms have increased in usage in all the scientific fields during the last decades. Since no optimisation algorithm is valid for all optimisation problems, many metaheuristics have been developed for various applications. Accordingly, this paper presents a comparative study on CEC 2020 optimisation problems among different algorithms. The goal is to give an overall sight of selecting a specific metaheuristic algorithm for a particular application. The algorithms in this study are; dynamic differential annealed optimisation, particle swarm optimisation, fertilisation optimisation algorithm, grey wolf optimisation, whale optimisation algorithm, firefly algorithm, artificial bee colony, ant lion optimisation, harris hawks optimisation, and sine cosine optimisation algorithm. The results are discussed in the respective sections with a focus on the convergence behaviour of the algorithms.","['Engineering', 'Automotive Engineering']"
doi:10.1007/978-3-031-20601-6_4,en,Overview of Gradient Descent Algorithms: Application to Railway Regularity,OriginalPaper,"Optimization is an important branch which aims to conceptualize, analyze, and solve problems of minimization or maximization of a function on a specific dataset. Several optimization algorithms are discussed in machine learning and particularly in deep learning (DL) based systems such as the Gradient Descent (GD) algorithm. Given the importance and the efficiency of the gradient descent algorithm, several research works have made it possible to optimize it and demonstrate its performance, Otherwise, regularity of a train is essential to ensure the continuity of the entire rail system. Non-regularity can spread quickly and influence the rest of the means of transport: rail, road, air, navy etc. In this paper, we perform a comparative study of different optimizations algorithms which are largely used in context of machine learning on the prediction of the regularity of trains, the data used is publicly available. The optimization algorithms studied are Momentum, Adagrad, RMSprop Adam and Adamax. In our context, the overall experimental results obtained show that RMSprop performed better compared to other optimization techniques, while Momentum represents the lowest performances to improve regularity.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19958-5_53,en,Trash Image Classification Using Transfer Learning Based Deep Neural Network,OriginalPaper,"The growing amount of trash in the environment endangers nature and is hazardous to wildlife and the ecosystem. With the increase in population, the number of trash in the environment is also increasing exponentially. Trash management has become a worrisome issue in most developing countries, affecting the drainage and sewerage systems. Manual trash segregation is inefficient and perilous to workers, necessitating automation of the trash classification process. Since the introduction of deep neural networks, many new methods of trash classification have been devised. We propose a new dataset of 17,628 images in this work by integrating three existing datasets. Our combined dataset comprises eleven types of trash frequently found in the outdoor environment. On our dataset, we utilize multiple transfer learning algorithms such as ResNet152, DenseNet169, and MobileNetV2 to investigate the best model empirically. Experimental evaluation with DenseNet169 has recorded a remarkable result, outperforming other models by attaining a 93.10% accuracy.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4182-5_13,en,Automatic Enhancement of Deep Neural Networks for Diagnosis of COVID-19 Cases with X-ray Images Using MLOps,OriginalPaper,"The novel coronavirus (COVID-2019) pandemic has caused the devastating effect on public health and global economy. It has infected about 231 million individuals globally, and approximately 4.7 million have died as a result. Recent findings suggest that X-ray imaging techniques can provide salient information about the COVID-19 virus. Since then, many deep learning models have been developed and open-sourced. During the development of the deep learning models, several hyper-parameters need to be tuned. In this paper, the authors have proposed a method through which the process of hyper-parameter tuning of the deep learning models can be automated using the concept of MLOps. The collection of procedures aimed at maintaining deep learning models in a reliable and efficient manner is termed as MLOps. The proposed approach helped in achieving an improved accuracy of 97.03%, in less time without human interface. This will eliminate the requirement of trained personnel during the model re-training stage as the system has the facility to retrain itself continuously, till the permissible accuracy is achieved.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-981-19-3575-6_20,en,Face Model Generation Using Deep Learning,OriginalPaper,"Computer vision systems have embraced learning using networks in recent years. On the other hand, unsupervised learning with convolutional neural networks has received less attention. The proposed method will help to close the gap between convolutional networks’ performance and that of other machine learning algorithms. The goal of this paper is to use deep convolutional generative adversarial networks, a type of convolutional neural network, to create fake face images. The research demonstrating deep convolutional generative adversarial networks outperform generative adversarial networks is used in this paper. By training on picture datasets, the deep convolutional adversarial pair learns a series of representations in both the discriminator and generator leading to realistic face images. At the end of this paper, the results obtained by using deep convolutional generative adversarial networks are shown.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-11847-0_9,en,Bases for Computer Programming,OriginalPaper,"It is essential to run various programs on computers to realize “Computational Mechanics with Deep Learning”. This chapter provides an overview of “Computational Mechanics with Deep Learning” from the perspective of programming. Section 9.1 describes some programs in the field of computational mechanics used in the Data Preparation Phase, including three topics discussed in the case study: the element stiffness matrix by using numerical quadrature in the finite element analysis (Sect. 9.1.1 ), parameters representing the shape of an element (Sect. 9.1.2 ), and NURBS basis functions (Sect. 9.1.3 ). Section 9.2 discusses some programs in C and Python for deep learning (neural networks) used in the Training Phase, where the mathematical formulas are described in detail so that they can be easily compared with practical programs.","['Engineering', 'Mechanical Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20650-4_15,en,A Study on the Autonomous Detection of Impact Craters,OriginalPaper,"Planet surface studies is one of the most popular research areas in planetary science, as it is useful to attain information about a planet’s history and geology without directly landing on its surface. Autonomous detection of craters has been of particular interest lately, especially for Mars and Lunar surfaces. This review study deals with the technical implementation, training, and testing of YOLOv5 and YOLOv6 to gauge their efficiency in detecting craters. YOLOv6 is the most recent member of the YOLO family, and it is believed that it outperform all of its predecessors. In addition to comparing the aforementioned two models, the performance of the most widely used optimization functions, including SGD, Adam, and AdamW is studied as well. The methods are evaluated using mAP and mAR to verify whether YOLOv6 potentially outperforms YOLOv5, and whether AdamW is capable to generalize better than its peer optimizers.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-3391-2_19,en,Skin Cancer Classification for Dermoscopy Images Using Model Based on Deep Learning and Transfer Learning,OriginalPaper,"Often cases of cancer are misdiagnosed at an early stage, resulting in serious repercussions, including patient death. There are also instances when individuals have other issues that are misdiagnosed as skin cancer by physicians. As a result, time and money are wasted on needless diagnostic procedures. In this paper, convolution neural network and transfer learning architecture to solve both of the issues are discussed. Both training and testing of the proposed model were done using publicly accessible ISIC 2019 Skin Lesion dataset. For automated skin lesion analysis, the deep learning models produce excellent results. The proposed model that has been used to classify melanoma and non-melanoma without any augmentation, yielded an AUC of 0.812, precision of 71%, recall of 97% and F1-score of 85%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-030-99075-6_15,en,Bearing Fault Diagnosis Based on Improved Residual Network,OriginalPaper,"In the wind power generation system, the bearing plays a very important role. Whether it can run stably directly determines the quality of the electricity produced and has a great influence on the efficiency of power generation. Due to the harsh working environment, the bearing has become one of the most vulnerable components in the entire wind turbine system. Therefore, bearings of wind turbines need to be maintained regularly. However, it needs to be shut down every time for maintenance, which will incur high maintenance cost. So, the fault diagnosis of the bearing is particularly important. A fault diagnosis method is proposed based on deep learning in this paper. This method is based on the residual module to construct a new ResNet model and embeds the attention mechanism in it to select information that is more critical to the current task goal from a lot of information. In addition, a long short-term memory is added to the network to extract the long-term dependence of the vibration signal and ensure that the information on the time series will not be lost as the training progresses. The experimental results show that the method proposed in this paper is very effective for the fault classification of fan bearings.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-3-031-17544-2_10,en,Advance Machine Learning and Nature-Inspired Optimization in Heart Failure Clinical Records Dataset,OriginalPaper,"ML is a subset of computing procedures that aims to imitate human astuteness by swotting from its surroundings. It has become a challenging task to diagnose the ailment and provide the appropriate treatment at the right time because of the increasing population and disease. The recent technological advancements have propelled the adoption of innovative functional biomedical solutions in the public health sector. Procedures based on traditional ML have been applied effectively in computational biology to biomedical and medical applications. Biomedical solutions entail a complex series of procedures ranging from consultation to treatment and beyond to ensure that patients react optimally. These are considered the working horse in the new era of the so-called big data. The process's complexity can vary and encompass multiple phases of nuanced human–machine interplay with decision-making, which certainly derive the application of ML algorithms to enhance and systematize the automate processes. A population-based Natured inspired swarm algorithms is proposed to extract the relevant parameters of Tree-based ML algorithms by using hyperparameter tuning. The proposed framework attains the desired performance by using “Heart failure clinical records dataset” prediction from the UCI ML data repository.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-981-19-5217-3_30,en,Surrogate Based Multi-objective Optimization for Energy-Saving Building Design,OriginalPaper,"In order to coordinate building energy consumption and indoor comfort for architectural design, a surrogate based multi-objective optimization approach is proposed in this paper. Taking a rectangular planar layout of civil housing as an example, several design parameters closely related to building energy consumption and indoor comfort are selected. With the help of EnergyPlus energy consumption simulation analysis software, the sample data can be generated by simulating the software. Thus, a three-layer radial basis function (RBF) neural network is used to train and learn the sample data, and the models of building energy consumption and indoor comfort can be established. The established energy consumption and comfort models are verified by the experiments. On this basis, NSGA-II multi-objective optimization algorithm is used to balance the building energy consumption and indoor comfort to obtain the value range of design parameters and the Pareto front. So, the optimization results can be used to guide energy saving architectural design.","['Engineering', 'Civil Engineering', 'Public Policy', 'Arts']"
doi:10.1007/978-981-19-2004-2_41,en,Implementation of Different Classification and Prediction Models on Skin Cancer Using Deep Learning Techniques,OriginalPaper,Skin cancer is widely menacing forms of cancer in North America and South East Asia and some part of Australia also. The main reason of skin cancer is caused by damaged deoxyribonucleic acid (DNA) in skin cells of human body which is inherited from genetic disorder or mutations on the skins. Skin cancer is to gradually spreading over other body parts with acute pain and it is only curable in initial stages. That is always recommended to detect at early stages of cancer as we know that there are four stages of cancer. The skin cancer has high mortality rate all over the globe as compare to other types of cancer and its treatments are very expensive. This paper presents a detailed review of deep learning techniques like convolutional neural network for the early detection of skin cancer and their types. We have develop a 2-D CNN model and evaluated the model with different parameters and finally evaluated model with data augmentation and predict the incorrect probability for different types of Skin cancer for HAM 1000 datasets.,"['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-981-19-2635-8_60,en,Model Predictive Sliding Mode Control with Neural Network for UAVs,OriginalPaper,"To operate an unmanned aerial vehicle (UAV) within a set of maneuverability constraints, herein, we use the Model Predictive Control (MPC) method to find the optimal control input under various control input constraints. Although the MPC method can predict future states and reflect them it the present state optimally, in real-world scenarios, its computational load increases exponentially with the number of state variables and the length of the time window. To reduce the computational burden and obtain the predicted optimal control input by using the MPC technique, herein, we devise an approach involving neural networks. To evaluate the weighting parameters of the neural networks, a considerable volume of learning data is required, which can be generated by conducting numerical simulations. Herein, we generate the input and output data pairs for a given time window by using the MPC method and by means of simulations. This learning process is expected to mitigate the computational burden dramatically. Lastly, one of the drawbacks of MPC is that the model for evaluating the optimal control input for a given time interval must be extremely accurate to guarantee system stability. Therefore, to increase the robustness of the MPC method against external disturbances and internal uncertainties, we augment it by using the Sliding Mode Control (SMC) method. The effectiveness of the suggested neural-network-based Model Predictive Sliding Mode Control method is demonstrated by means of numerical simulations.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-981-19-5845-8_13,en,Comparative Study of Conditional Generative Models for ISL Generation,OriginalPaper,"Deep Generative Models are widely used to generate data that look similar to training data. Synthesis and sampling of data using Deep Generative Models can be useful for certain situations where generating data by hand would be expensive or time consuming. Data-sets for Indian Sign Language often are of small size, which hinders training of Deep Learning models to a good accuracy. In this project we attempt to compare various state-of-the-art Conditional Generative Models for Indian Sign Language Recognition task and evaluate them using various performance metrics.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20601-6_36,en,Effect of Inertia Weight of PSO Used to Solve the Continuous p-Center Location Problems,OriginalPaper,"Particle swarm optimization (PSO) has several benefits including few parameters, quick convergence, and ease of implementation; however, for NP-hard problems such as p -center location problems, it often falls within local minima. Inertia weight parameter can be used to overcome this problem, since it increases the diversity of particles. To study the effect of inertia weight, and based on different values of inertia weight used in the lectures (0.5, 0.7, 0.9, 1.1, and 1.3). We examined the effect of inertia weight used to solve benchmark continuous p -center location problems/instances. The validity of the results is demonstrated by experiments. 1,050 experiments have been conducted. The results show that inertia weight = 0.9 yield the minimum objective function values. Also, the results show that low value of inertia weight leads to faster convergence, however, it does not guarantee the best minima values.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-14771-5_24,en,Efficient Extraction of Pathologies from C-Spine Radiology Reports Using Multi-task Learning,OriginalPaper,"Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. Generally, if one has multiple tasks on a given dataset, one may finetune different models or use task specific adapters. In this work, we show that a multi-task model can beat or achieve the performance of multiple BERT-based models finetuned on various tasks and various task specific adapter augmented BERT-based models. We validate our method on our internal radiologist’s report dataset on cervical spine. We hypothesize that the tasks are semantically close and related and thus multitask learners are powerful classifiers. Our work opens the scope of using our method to radiologist’s reports on various body parts.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4017-0_4,en,Modeling Attacks on PUF,OriginalPaper,"Modeling attacks are considered to be the greatest threat against strong PUF implementations, and wide-ranging intensive research is currently underway to develop newer attacks.","['Engineering', 'Circuits and Systems', 'Artificial Intelligence', 'Mathematics, general', 'Special Purpose and Application-Based Systems', 'Computer Science, general']"
doi:10.1007/978-3-030-96025-4_2,en,Optimization Techniques,OriginalPaper,"This chapter presents a comparison between conventional, artificial intelligence (AI) and modern optimization techniques. The mathematics of conventional optimization techniques such as linear programming (LP), quadratic programming (QP), integer programming (IP) and dynamic programming (DP) are considered. The AI techniques such as artificial neural network (ANN), fuzzy linear programming (FLP) and expert systems (ES) are also introduced. In addition, modern optimization techniques such as genetic algorithm (GA), differential evolution (DE) algorithm, particle swarm optimization (PSO), seeker optimization algorithm (SOA) and ant colony optimization (ACO) algorithm are considered. The description of each technique with its mathematical model and the steps for solving the problem are presented in this chapter.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Systems', 'Renewable and Green Energy']"
doi:10.1007/978-3-031-18256-3_8,en,Artificial Intelligence Applied to Breast Cancer Classification,OriginalPaper,"One in eight women is likely to develop breast cancer at some stage in her life, with a 12.5% average risk rate of developing breast cancer. Early detection and treatment are of vital importance to ensure the patient's survival. Currently, mammography is the main diagnostic study to identify breast cancer. However, since mammography requires a human, medical radiologist, to make a diagnosis, it is prone to errors. Recently, deep learning techniques have proven to be a suitable tool for breast cancer classification and detection. Therefore, this research proposes an algorithm based on convolutional neural networks (CNN) for screening classification of cancer in mammography images. The evaluation results of the proposed algorithm respect state-of-the-art algorithms demonstrate competitive accuracy results of up to 99% and the fastest training time. Therefore, our algorithm is well suitable for automatic breast cancer detection using the public All-MIAS database.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-981-19-2412-5_13,en,Presentation of Real-Time Lab Analysis for Multiple-Area Renewable Sources-Thermal-Hydro System by Implementation of Cat Swarm Optimization,OriginalPaper,"This work explores automatic generation control learning under traditional situation for a three-area system: Sources in area-1 are thermal–solar thermal (ST); thermal–geothermal power plant (GPP) in area-2; and thermal-hydro in area-3. The work involves various assessments in the presence of constraints such as governor rate constraint, governor dead band, and time delay. An original endeavor has been set out to execute cascade controller with amalgamation of proportional-derivative and fractional order integral-derivative (FOID), hence named as PD-FOID. The performance of PD-FOID has been compared with varied controllers like integral (I), proportional-integral (PI), and proportional-integral-derivative (PID). Various investigation express excellency of PD-FOID controller over other controller from outlook regarding lessened level of peak_overshoot (P_O), peak_undershoot (P_U), settling_time (S_T). A swarm-based meta-heuristic cat swarm optimization (CSO) algorithm is applied to acquire the controller’s gains and parameters. Action in existence of redox flow battery is also examined which provides with noteworthy outcome. PD-FOID parameter values at nominal condition are appropriate for higher value of disturbance without the need for optimization.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Renewable and Green Energy', 'Energy Storage']"
doi:10.1007/978-981-19-6379-7_6,en,Optimized Nature-Inspired Computing Algorithms for Lung Disorder Detection,OriginalPaper,"Using computational methods inspired by nature, new models for analyzing natural phenomena and behavior can be developed in order to address complex issues. Swarm intelligence and evolutionary computation are just two of the many cutting-edge research areas that fall under this authority. Medical image segmentation has been a major research topic and a major goal in computer vision for a long time. Emerging academic research in biomedical engineering is aided by nature-inspired intelligent methods in solving biomedical engineering problems. To keep up with the most recent developments in biomedical technology, this research includes extensive coverage of relevant topics like machine learning, clinical decision support systems, and swarm intelligence. Medical imaging relies heavily on lung tumor research nowadays because of the impact of COVID-19 and the wealth of morphological and physiological information it provides, which makes diagnosis and treatment planning much simpler. There are numerous detection and segmentation methods in use today, but they all fall short in terms of accuracy. The lung segmentation categorization is based on the usage of a learning algorithm in the selected technique, which is called a filter, a wrapper, or an embedded feature selection method. This research presents the analysis of different natural-inspired computational models in performing segmentation of lung images for the detection of COVID-19 and other health issues. The best performance exhibiting classifier is suggested in the detection process for accurate diagnosis.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-981-19-0095-2_55,en,Email Spam Detection Using Multilayer Perceptron Algorithm in Deep Learning Model,OriginalPaper,"Email spam detection is a filtering process which identifies either it is spam or not. It also removes the unsolicited data present in the user’s email inbox. Certain type of spam mails contains malware which misuse the users’ data. Hence, we need to identify spam mails and take necessary actions. Many machine learning algorithms have proposed for differentiate spam mails from normal mails. Tokenization of emails between length and frequency is one of the techniques. It helps to split the raw emails into tokens known as small words. After tokenization, tokenized count has taken into consideration for process the emails. Based on that spam emails to be identified which are present in the dataset of spam and ham emails. To extracting these features, term frequency—inverse document frequency (TF-IDF) has used to train the model. In this method, multilayer perceptron deep learning algorithm is applied to compute the model. It has two layers. When input is given to the perceptron, the input is multiplied by the hidden layers, and it holds the activation function such as sigmoid activation with regularization function. For the better optimization, the model uses the Adam optimizer with gradient descent for fastest optimization. The network learns the model. The learning rate is set to true. While computing the model, it goes in the forward direction to train the model and comeback again (backpropagation). This process will be repeated. Going to the forward direction and comes back, then again, maintaining forward approach is called one epoch. The epoch rate has computed in the model. In the comparison between multilayer perceptron algorithm and machine learning algorithms such as support vector machine (SVM), random forest, and XGBoost, the deep learning algorithm produces 99% of accuracy on precision, recall, and F-measure and holds less computation time. Hence, the results prove that deep learning algorithm performs better than machine learning algorithms.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Systems and Data Security', 'Artificial Intelligence', 'Computational Intelligence']"
doi:10.1007/978-981-19-4786-5_38,en,Time Series Prediction Model of Spacecraft Health Management System Based on Wavenet Convolutional Neural Network,OriginalPaper,"In this paper, we introduce the Time Series Prediction Model Based on Wavenet Convolutional Neural Network. This means can give early warning for the future working state and possible faults of spacecraft in advance. As the signal of spacecraft thermal control system belongs to one-dimensional time series, this topic proposes to use the time series model based on WaveNet convolution neural network to predict the time series signal of spacecraft thermal control system and interpret the prediction information. The experimental results show that the time series prediction model of WaveNet convolutional neural network has good effect in fault prediction in a certain range","['Engineering', 'Manufacturing, Machines, Tools, Processes', 'Engineering Economics, Organization, Logistics, Marketing', 'Industrial and Organizational Psychology', 'Artificial Intelligence', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-981-19-6049-9_2,en,CPACS LTA—Using Common Data Structures for Visualization and Optimization of Airship Designs,OriginalPaper,"Complex engineering projects require compartments from different engineering fields to work in close contact and exchange data from their workflows and specialized software tools regularly. Common data models are an immanent feature of systems engineering applications that can be used to reduce the number of interconnections between software modules. Using common open-source data structures in airship design helps engineers in evaluating and visualizing early designs using several methods. In the presented paper, we used the Extensible Markup Language (XML) data structure Common Parametric Aircraft Configuration Schema (CPACS) and associated high-level software libraries by German Aerospace Center (DLR) for a design optimization. The design optimization maximizes payload of a 15 t airship by its hull shape. Handbook methods for estimating weights and drag as well as a brute force algorithm are used in the design optimization. The result shows an explicit optimum of the design. With the experiences made, we developed an updated version of CPACS, allowing the definition of an additional vehicle category named ‘dirigible’.","['Engineering', 'Aerospace Technology and Astronautics', 'Engineering Design', 'Building Materials']"
doi:10.1007/978-981-19-0503-2_32,en,Developing a Comprehensive Relational Database for Optimizing Land Utilization in Sustainable Farming,OriginalPaper,"Over the past few decades, researchers have been studying different approaches to promote self-sustaining communities, especially in developing countries, to combat the formidable challenges of rapid urbanization, climate change, as well as food and energy depletion. While the analysis of previous literature has shown a significant shift towards the design and application of smart data and automated tools in farming and landscaping, there is still a need for comprehensive databases that help users identify efficient and economic approaches pertaining to utilizing a land plot for either open field farming or the use of greenhouses. This study offers an overview of how a comprehensive relational database can be developed and implemented into an automated interface that optimizes the decisions for such operations, through the acquisition and organization of big data in farming and greenhouse construction. The relational database established in this study links different crops with soil types, water and climatic requirements. Furthermore, information regarding the type of greenhouses and their construction requirements is also integrated into the database. This relational database will facilitate the optimal utilization of land through identifying solutions that maximize potential returns while minimizing the life cycle cost and water consumption. Not only will this assist planners and farmers to approach their agricultural operations in the most sustainable manner, but also help them overcome the typical intuitive process of selecting how a land is utilized, which does not necessarily provide optimal return.","['Engineering', 'Building Construction and Design', 'Geoengineering, Foundations, Hydraulics', 'Transportation Technology and Traffic Engineering', 'Environment, general']"
doi:10.1007/978-3-031-09835-2_4,en,Advances on Particle Swarm Optimization in Solving Discrete Optimization Problems,OriginalPaper,"Particle Swarm Optimization (PSO) is a well-known optimization method which optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. PSO initially proposed for continuous optimization and, to till, different discrete optimizations methods are developed adapting PSO in different time periods. The major concerns to solve discrete optimization task with PSO are the adaptation of particle encoding, velocity measurement and position update. The aim of this study is to demonstrate the evolution of PSO in solving discrete optimizations conceiving different adaptations in its operations. This study explains adaptation of PSO for four different discrete optimization problems: knapsack problem (KP), traveling salesman problem (TSP), vehicle routing problem (VRP), and university course scheduling problem (UCSP). The selected problems are well diverse having different constraints and objectives; KP seems a simplest one and UCSP is the most complex optimization task. The rhythmic presentation of PSO adaptation in solving KP, TSP, VRP and UCSP in this chapter may be a proper demonstration of PSO transformation from its original continuous domain to different discrete domains. The study will be made easy to understand other PSO-based discrete optimization methods as well as will be helpful to solve any new discrete optimization task adopting PSO.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6347-6_2,en,Classification of Electrooculography Signals Using Convolutional Neural Networks for Interaction with a Manipulator Robot,OriginalPaper,"Electrooculography (EOG) has been widely applied in human–machine interfaces (HMI) because it provides a reliable communication channel to assist people with disabilities. However, signal behavior under different conditions hinders eye movement classification when algorithms based on voltage threshold detection are used. Therefore, recalibration of the system is required for the classification algorithm to work correctly. Based on the above, a classification algorithm was developed to analyze the data vector corresponding to the entire EOG waveform, instead of just one characteristic value of the signal, thus avoiding the system recalibration process. A convolutional neural network (CNN) was implemented to classify six targets corresponding to different eye movements. The proposed model was compared with a feedforward neural network (FNN) to evaluate its performance. The results were implemented in an HMI for interaction with a manipulator robot.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Statistics, general']"
doi:10.1007/978-981-19-6737-5_20,en,CNN-Based Leaf Wilting Classification Using Modified ResNet152,OriginalPaper,"Plants are prone to climatic changes which is considered to be one of the crucial challenges in agriculture. As a major part of the agricultural industry depends on rainwater for irrigation, climatic changes such as lack of rainfall may lead to shortage in water supply which may cause leaf wilting. If the extent of leaf wilting is not identified in the early stages, it may adversely affect the total yield. Leaf wilting is widely used as a parameter to compute drought tolerance in plants. The breeders manually collect the data for the extent of leaf wilting which consumes a lot time and efforts. This paper proposes a model with ResNet152 as the base architecture using transfer learning approach. The developed model uses leaf images as the input data and predicts the extent of leaf wilting into five different classes, namely 0, 1, 2, 3, and 4; 0 being the least amount of wilting and 4 being the highest amount of wilting. The proposed model achieves an accuracy of 87.00% which is better than other existing models. Further, the model was tested against some unlabelled images taken from the Internet and from the field for classification.","['Computer Science', 'Computer Communication Networks', 'Computer Applications', 'Computer System Implementation', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-5403-0_1,en,Breast MRI Registration Using Gorilla Troops Optimization,OriginalPaper,"Metaheuristics assume a significant part in enhancing issues, and most of them are enlivened by the aggregate knowledge of living peculiarities in nature. In excess of 10% of ladies are experiencing bosom disease in their lives in the entire world. Registration of breast magnetic resonance imaging is a method to align pre- and post-contrast images to analysis and classifying of cancer category. In this research, breast magnetic resonance imaging registration using gorilla troops algorithm is proposed. GTO is a metaheuristics-based optimizations algorithms that tested to registering the image of breast magnetic resonance. After that, images are successfully registered using GTO algorithm. The result of GTO-based registration method is compared with registration based on PSO method. The results implicates, in case of registration of breast magnetic resonance images, the GTO-based registration methods beat the PSO-based registration method.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-3-031-16072-1_16,en,"A New Approach for Optimal Selection of Features for Classification Based on Rough Sets, Evolution and Neural Networks",OriginalPaper,"In number recognition, one of the challenges is to deal with the high dimensionality of data that affects the performance of algorithms. On the other hand, pattern recognition allows establishing fundamental properties among sets of objects. In this context, Rough Set Theory applies the concept of super-reducts in order to find subsets of attributes that preserve the capability of the entire set to distinguish objects that belong to different classes. Nevertheless, finding these reducts for large data sets has exponential complexity due to the number of objects per class and attributes per object. This paper proposes a new approach for dealing with this complex problem in real data sets to obtain a close enough to a minimal discriminator. It takes advantage of the theoretical background of Rough Set Theory, especially considering those super-reducts of minimal length. In literature, there is an algorithm for finding these minimal length reducts. It performs well for a small sampling of objects per class of the entire data set. An evolutionary algorithm is performed to extend it over a huge data set, taking a subset of the entire list of super-reducts as the initial population. The proposed discriminator is evaluated and compared against state-of-the-art algorithms and data set declared performance for different models.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-22200-9_18,en,An Enhanced Hybrid Jaya Algorithm for Size Optimization of Truss Structure Under Frequency Constraints,OriginalPaper,"This paper proposed an enhanced hybrid Jaya algorithm, called AEHJ. The proposed AEHJ is a new improvisation of the Jaya algorithm (Jaya) and the differential evolution algorithm (DE) with two modifications. Firstly, the local search is improved by using DE/best/1, DE/best/2, and Jaya operators. Secondly, an elitist selection approach is used for choosing the best solution for the next population. For validating the feasibility of AEHJ, the well-known benchmark example of size optimization for a 10-bar truss is performed.","['Engineering', 'Mathematical and Computational Engineering', 'Mechanical Engineering', 'Electrical Engineering']"
doi:10.1007/978-981-19-2188-9_49,en,Flow Rate Optimization of a Parabolic Trough Solar Collector Using Multi-objective Genetic Algorithm,OriginalPaper,"Due to the rising greenhouse gas emissions and high energy prices, it is now more important than ever to use renewable energy sources in various applications. Parabolic trough solar collector (PTSC) converts solar energy to thermal energy and can serve applications like power generation, industrial process heat, desalination, etc. Optimization of governing process parameters for such a system can improve its efficiency and increase its share in different applications. Heat transfer fluid (HTF) flow rate is one of the critical parameters in the working of PTSC. The current study investigates the impact of HTF flow rate on the efficiency of PTSC and estimates the optimal flow rate for a specific system and at a certain temperature. It is revealed that despite increased pumping efforts, increasing the flow rate above a critical value did not guarantee an increase in system performance. This highlighted that an optimum mass flow rate value should be determined that maintains acceptable thermal efficiency while requiring least pumping effort. The value of optimal flow rate has been computed using a multi-objective genetic algorithm (MOGA). The best compromise solution identified after a diligent Pareto front analysis was observed at HTF flow rate = 11.34 m 3 /h with pump work = 0.66 W and thermal efficiency = 73.08%.","['Engineering', 'Industrial and Production Engineering', 'Mechatronics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Energy Storage', 'Materials Engineering']"
doi:10.1007/978-981-16-9967-2_7,en,A Novel Approach to Detect Plant Disease Using DenseNet-121 Neural Network,OriginalPaper,"The disease of crops is a major risk to food security and can incur a makeable loss to the people. But, the latest development in deep learning for solving this problem surpasses all the traditional methods in terms of efficiency, time period for detection and accuracy. In this paper, we came up with a rapid identification of leaf image and classify the image to correct class by using classical deep neural network architecture, DenseNet-121. This deep learning model has the ability to recognize 15 types of different plant disease, three of which are healthy ones, for better accurate results. The algorithm is highly optimized to produce results in less than 5 s after being fed into the system. The model’s total testing accuracy for plant disease detection is 99%.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Wireless and Mobile Communication', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2225-1_15,en,A Novel Application of HPSOGWO Trained ANN in Nonlinear Channel Equalization,OriginalPaper,"In a communication channel, there is a possibility of distortions such as ISI, CCI, and another source of noise that interfere with useful signals, and the signal becomes corrupted. Therefore, equalizers are needed to counter such types of distortions. In this paper, we presented a nature-inspired hybrid algorithm which is an amalgamation of PSO and GWO. The proposed algorithm is called HPSOGWO. During this work, we pertain to ANN trained with the proposed HPSOGWO in the channel equalization. The foremost initiative is to boost the flexibility of the variants of the proposed algorithm and the utilization of proper weight, topology, and transfer function of ANN in the channel equalization. The performance of the proposed equalizer can be evaluated by estimating MSE and BER by considering popular nonlinear channels and added with nonlinearities. Extensive simulations show the performance of our proposed equalizer, better than existing NN-based equalizers also as neuro-fuzzy equalizers.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Statistics, general', 'Mobile and Network Security']"
doi:10.1007/978-3-031-09835-2_13,en,A Hybrid African Vulture Optimization Algorithm and Harmony Search: Algorithm and Application in Clustering,OriginalPaper,"Data clustering is one of the necessary research fields in data analysis. Clustering is an unsupervised classification method for assigning data objects to separate groups, which are called clusters. So that the similarity of the data within each cluster and the difference between the cluster data is high, a variety of meta-heuristic algorithms can be used to solve this problem. In this paper, a new algorithm created using a combination of African Vulture Optimization Algorithm (AVOA) and Harmony Search (HA) is used. The proposed algorithm is implemented on the clustering dataset of the UCI machine learning repository. Furthermore, the results obtained from the proposed algorithm are compared with other meta-heuristic algorithms. The experiments show that the proposed method has good and better performance than other optimization algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5331-6_49,en,Two-Phase Mitotic Detection Using Deep Learning Techniques,OriginalPaper,"Mitosis identification in breast cancer pictures is critical for determining the tumour’s aggressiveness. In cancer grading, the mitotic count is an important metric. Currently, a pathologist visualizes several high power fields (HPFs) on a glass slide under a super microscope in a clinic setting, which is a difficult and time-consuming operation. The development of automated mitosis detection techniques is critical, but it also faces challenges such as size invariance, data scarcity, poor image staining, and an uneven sample class. These constraints nevertheless restrict the automated histopathology picture interpretation to be employed in clinical practice. The proposed work is a deep learning model, which is combination of object detection network and classification network. The validation of proposed work was carried out on the MITOS-ATYPIA-14 data set. The object detection network which is yolov5 architecture detects the nuclei. Then with the detected nuclei, bbox images are extracted and fed to classification layer which is EfficientNetV2-S, which decides whether the detected nuclei are mitotic or not. Finally, the predicted label class along with its confidence score is plotted on test image. The nuclei detection model which is yolo network achieved mAP@0.5 of 0.93 and classification network of proposed architecture achieved accuracy of 97.1%","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-07254-3_17,en,Structural Health Monitoring of Offshore Jacket Platforms via Transformers,OriginalPaper,"The goal of this project is to monitor the structural health of jacket-type platforms for offshore wind turbines. The methodology is based on vibration-response-only accelerometer measurement and a transformer-based framework for multivariate time series. The original transformers paper proposed an architecture applied to a natural language processing task, meanwhile later works approached the use of transformers for forecasting, missing value imputation, and classification of time series. In general, the transformers based on attention mechanisms demonstrate being superior in terms of quality and performance on many sequential tasks in comparison to other architectures. Similar results are expected with time series data. Thus, this work proposes to use transformers for the classification of different structural types of damage in jacket-type wind turbines. The methodology follows the next steps: (i) accelerometer data is acquired, (ii) data is cleaned and wrangled into time series, (iii) a transformer-based framework classifies different damage scenarios. In a down-scaled experimental laboratory structure, the method is validated. The results demonstrate the feasibility of the proposed methodology.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering', 'Monitoring/Environmental Analysis', 'Analytical Chemistry']"
doi:10.1007/978-981-19-3391-2_25,en,Lesion Segmentation in Skin Cancer Detection Using UNet Architecture,OriginalPaper,"Malignant melanoma really is growing increasingly frequent, especially among those with fair skin who are exposed to the sun. Variations in size and colour, the fuzzy boundary and the low contrast between lesion and normal skin are the adverse factors for deficient or excessive delineation of lesions, or even inaccurate lesion location detection. Despite the widespread usage of artificial intelligence like convolutional neural networks (CNN) for accurate segmentation, existing encoder–decoder models based on tightly connected networks and residual networks (ResNet) for skin lesion applications utilised non-biomedical data. The difficulty of parameter settings, inadequate information in pre-trained features, and a shortage of multi-scale information all limit the effectiveness of skin lesion segmentation. To overcome this issue, the design is built on the notion of an encoder–decoder-based convolution neural network. UNet is used in the system to ensure optimum lesion segmentation performance. On ISIC 2018 lesion pictures, the recommended models are tested. The accuracy, dice coefficient, Jaccard index, sensitivity, and specificity of the models are used to assess their effectiveness.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-17254-0_3,en,Smart Factory Framework,OriginalPaper,"The Smart Factory Framework consists of a detailed representation and analysis of the 44 generic use cases. These use cases are relevant to a company's operational strategy. A detailed understanding of the use cases is critical for selection and prioritization. In this chapter, each use case is described and summarized in a standardized manner to provide information about the nature and application of each use case.","['Engineering', 'Engineering Economics, Organization, Logistics, Marketing', 'Management', 'Manufacturing, Machines, Tools, Processes', 'Robotics and Automation']"
doi:10.1007/978-3-030-99112-8_30,en,Controlling of the Upper Limb Prosthesis Using Camera and Artificial Neural Networks,OriginalPaper,"The loss of the upper limb, especially the hand, can affect the level of autonomy. Developing an effective control system for the upper limb prostheses could improve the quality of users’ life. The aim of this project was to design artificial neural networks for automatic grasp classification. A subset of the grips allowing to perform everyday activities was proposed. The proposed artificial neural networks were evaluated and the maximal accuracy reached 97%.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Computational Intelligence']"
doi:10.1007/978-981-19-4025-5_7,en,Deep Learning-Aided High-Throughput Multiple Access,OriginalPaper,"In this chapter, we discuss the deep learning-aided high-throughput multiple access. Section 7.1 introduces the motivation of applying deep learning to grant-free NOMA in tactile Internet of Things. Section 7.2 introduces the system model of grant-free NOMA. Section 7.3 presents the neural network model for grant-free NOMA. Section 7.4 analyzes the loss function and the training algorithm of the proposed network model. Section 7.5 presents the evaluation results to validate the feasibility and efficiency of the proposed scheme. Section 7.6 draws the conclusions.","['Engineering', 'Communications Engineering, Networks', 'Wireless and Mobile Communication']"
doi:10.1007/978-3-031-16072-1_2,en,Robust Rule Based Neural Network Using Arithmetic Fuzzy Inference System,OriginalPaper,"Deep Neural Networks (DNNs) are currently one of the most important research areas of Artificial Intelligence (AI). Various type of DNNs have been proposed to solve practical problems in various fields. However the performance of all these types of DNNs degrades in the presence of feature noise. Expert systems are also a key area of AI that are based on rules. In this work we wish to combine the advantages of these two areas. Here, we present Rule-Based Neural Networks (RBNNs) where each neuron is a Fuzzy Inference System (FIS). RBNN can be trained to learn various regression and classification tasks. It has relatively a few trainable parameters. It is robust to (input) feature noise and it provides a good prediction accuracy even in the presence of large feature noise. The learning capacity of the RBNN can be enhanced by increasing the number of neurons, number of rules and number of hidden layers. The effectiveness of RBNNs is demonstrated by learning real world regression and classification tasks.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-1610-6_65,en,Detection of Respiratory Disease Patterns Using Mask R-CNN,OriginalPaper,"The analysis and identification of pathological signs associated with different respiratory diseases are is not an easy task. One of the imaging modalities for these signs identification is examining chest CT scans. However, it requires expert knowledge to avoid human error. The purpose of this work is to implement, test, and analyze the performance of a neural network based on a mask R-CNN model able to identify some pathological signs of respiratory disease. The CT images used were manually labeled and pre-classified as positive and negative cases by specialists to prepare them for the training process. Preliminary results reached detection of ground-glass opacity with a sensitivity of 81.89% using the validation set and 92.66% using the test set. Nevertheless, low percentages were obtained for pulmonary nodules detection with a sensitivity of 51.08 and 40.34% using validation and test sets, respectively.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-030-99075-6_77,en,Differentiable Architecture Searched Network with Tree-Structured Parzen Estimators for Rotating Machinery Fault Diagnosis,OriginalPaper,"Deep learning is widely used in the field of rotating machinery fault diagnosis. However, manually designing the neural network structure and adjusting the hyperparameters for specific fault diagnosis task are complex and requires a lot of expert knowledge. Aiming at these problems, Differentiable Architecture Searched Network with Tree-Structured Parzen Estimators (DASNT) is proposed for fault diagnosis. Differentiable Architecture Search (DARTS) is utilized to automatically search network structure for specific fault diagnosis task. Tree-Structured Parzen Estimators (TPE) is utilized to optimize the hyperparameters of the network searched by DARTS, which can further improve the fault diagnosis accuracy. The results of comparison experiments indicate that the network architecture and hyperparameters optimized by DASNT can achieve superior fault diagnosis performance.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-3-031-12851-6_14,en,Automatic Parameter Calibration of Two Advanced Constitutive Models,OriginalPaper,"This paper presents the automatic parameter calibration of two advanced constitutive models for sand: Hypoplasticity with Intergranular Strain and Sanisand. The application of the software is demonstrated by automatically calibrating the parameters for Karlsruhe Sand using data of oedometric compression and drained monotonic triaxial tests. The quality of the obtained parameter sets is demonstrated by the comparison of the experimental data with simulations, using 1) the automatically calibrated material parameters and 2) two reference parameter sets calibrated “by hand”. It is shown that the developed calibration software outperforms the “by hand” calibration in terms of accuracy, simplifies the parameter calibration and lowers the entry hurdle for the use of the advanced constitutive models.","['Engineering', 'Geoengineering, Foundations, Hydraulics', 'Geotechnical Engineering & Applied Earth Sciences', 'Offshore Engineering']"
doi:10.1007/978-981-19-4162-7_14,en,Detection of COVID-19 Using CNN and ML Algorithms,OriginalPaper,"As we see coronavirus is the very dangerous diseases and to identify this diseases in one’s body is also not as easy. So during identification of diseases there are many false positive cases we see that person does not have corona and still the prediction comes true and also in some cases, it happens that person has corona but it does not get detected (false negative case). So due to this problem, we here come up with the two approaches and make comparison between these two approaches and decide which one is better to analyze the diseases in the body. We are using CNN to scan chest X-ray dataset and ML algorithms for tabular dataset as it contains many text information too. So in this project, we explain in detail, what is CNN, what is ML, how to implement CNN and ML algorithms on particular dataset, what output we will get as a comparison.","['Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery', 'Systems and Data Security', 'Mobile and Network Security', 'Information Systems Applications (incl. Internet)']"
doi:10.1007/978-3-031-13150-9_20,en,Analysis and Survey of Soil Moisture Prediction Techniques for Agricultural Applications,OriginalPaper,"Soil Moisture (SM) is an important factor disturbing the growth of crop. Thus, sophisticated understanding or precise prediction of future SM states are important in scheduling of irrigation, improving utilization of agricultural water, and forecasting of yield. Hence, the detection and prediction of SM are of major concern in the present era. This review article provides the detailed review of latest research papers presenting the SM prediction approaches for the prediction of SM. The papers are classified as conventional methods of SM prediction, Remote sensing based SM prediction approaches, Machine learning based SM prediction methods, and the deep learning based SM prediction approaches. In addition to this, various research gaps and the challenges associated with the existing works of SM prediction are discussed. The reviewed works are analyzed in the basis of performance metrics, performance attained using various methods, and the datasets employed for analysis. In addition, this review presents the future scope for the researchers with the analysis of issues associated with the existing literary works.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Management']"
doi:10.1007/978-981-19-5845-8_31,en,Tomato Leaf Disease Detection Based on Convolutional Neural Network,OriginalPaper,"Tomatoes are a popular and important crop in India, with a large economic price and great production capacity. Diseases harm the health of the plant, which has an impact on its growth. It is critical to monitor the progress of the farmed crop to guarantee minimal losses. There are a slew of tomato diseases that are wreaking havoc on the crop’s leaves. One of the major linkages in the avoidance and control of crop diseases is the identification of infections in the leaf portions during the planting phase. Tomato leaves, including six popular species (Bacterial Spot, Black Mold, Early Blight, Late Blight, Mosaic Virus, and Septoria Spot), are used as experimental objects in this work to extract disease features from the leaf surface. Deep learning-based disease identification might help prevent such a catastrophe. A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is currently commonly used for image categorization. In our studies, we used the CNN architecture to identify diseases in tomato leaves. This data set contains 2800 pictures of plant diseases. The Convolutional Neural Network was used in our proposed system to detect plant leaf diseases in seven categories, comprising six classes for diseases found in various plants and one class for healthy leaves. As a result, we were able to attain remarkable training and testing accuracy, with a training accuracy of 97.190% and a testing accuracy of 96.607% for all data sets used.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3148-2_30,en,Detection and Classification of Fruit Tree Leaf Disease Using Deep Learning,OriginalPaper,"Plant disease identification is extremely important in agriculture since it is critical for boosting crop output. Visual plant disease analysis is a modern technique to handle this problem, following recent developments in imaging. In this study, we look at the challenge of plant disease detection which is visually done for identification of plant disease. Plant disease images, in comparison with other types of photographic images, are likely to have randomly dispersed lesions, varied symptoms, and complex backgrounds, making discriminative information difficult to capture. To facilitate plant disease recognition research, we had taken the PlantVillage dataset with 13,347 images with 14 classes. Models were trained using the PlantVillage dataset. The performance of EfficientNet architecture for classifying the plant leaf disease was compared against ResNet-50, Inception V3, AlexNet, and Xception deep learning algorithms in this analysis. The outcomes of the test dataset revealed that B3 models of the EfficientNet architecture had the greatest accuracy of 99.90 percent when related to other deep learning algorithm in the dataset.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-13150-9_8,en,Dress Pattern Classification Using ResNet Based Convolutional Neural Networks,OriginalPaper,"The fashion industry tries to adapt to changing fashion trends and is extremely versatile. Globally, the fashion industry is valued at three trillion dollars, and with the world GDP increasing by 400% by 2050, it would create a greater demand for clothing. E-commerce has become an integral tool in the fashion industry and users are looking for a better shopping experience and easier ways on online sites to find clothes according to their preferences. Many Machine Learning and Deep Learning approaches have helped a lot in many areas of the fashion industry. Convolutional neural networks (CNN) perform extremely well in tasks such as image classification. Inspired by this, a deep learning model is proposed based on the pre-trained ResNet which can detect the pattern of the cloth worn by a person with an accuracy of 82.13%. This dress classification approach will be beneficial in increasing the efficiency of the cloth-producing factories by automating their storage and the organizing process and when integrated with search engines of various websites, it can help in creating a recommendation system for the user.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Management']"
doi:10.1007/978-3-031-12409-9_7,en,Deep Learning,OriginalPaper,"The core of this book are deep learning methods and neural networks. This chapter considers deep feed-forward neural (FN) networks. We introduce the generic architecture of deep FN networks, and we discuss universality theorems of FN networks. We present network fitting, back-propagation, embedding layers for categorical variables and insurance-specific issues such as the balance property in network fitting, as well as network ensembling to reduce model uncertainty. This chapter is complemented by many examples on non-life insurance pricing, but also on mortality modeling, as well as tools that help to explain deep FN network regression results.","['Mathematics', 'Applications of Mathematics', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Machine Learning', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3571-8_67,en,Inverse Imaging: Reconstructing High-Resolution Images from Degraded Images,OriginalPaper,"Inverse problems such as image denoising, super-resolution and image reconstruction are based on complex unsupervised machine learning techniques. We propose a supervised deep learning approach for image denoising and high resolution. Our proposed system simply follows the U-Net architecture which takes images with noise as input. In this work, we show that U-Net architecture consisting of convolutional and de-convolutional or transpose convolutional neural networks does a pretty good job of removing noise from the image and producing high-resolution images. The task belongs to a general class of inverse problems that fall under the mathematical branch of problems based on the posterior probability distribution which is the probability of the parameter theta given the evidence X: Pr(theta|X). We show that supervised learning algorithms based on deep learning are enough capable to produce equivalent or better results than the unsupervised machine learning approach and maintain the complexity even if the generality is extended.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2535-1_14,en,Ensemble Deep Learning Models for Vehicle Classification in Motorized Traffic Analysis,OriginalPaper,"Automation of vehicle classification is essential in the establishment of effective Intelligent Transportation Systems (ITS). Based on the MIOvision Traffic Camera Dataset (MIO-TCD), this paper categorizes the types of vehicles as car, bus, van, light truck, motorcycle and multi-axle truck. The classification of surveillance images is achieved using an ensemble of Deep Networks. Three networks are trained separately to make up the deep learning ensemble model with ConvNet, LeNet and EfficientNet achieving 89%, 68% and 87% classification accuracy, respectively. Results of experiments unveil that the ensemble of networks outperforms the individual networks. The ensemble of networks achieves 92.77%, which is high when compared to the performance based on genetic method in the recent literature.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17544-2_11,en,Early Detection of Chronic Obstructive Pulmonary Disease Using LSTM-Firefly Based Deep Learning Model,OriginalPaper,"Identifying Chronic Obstructive Pulmonary Disease (COPD) is essential for reducing mortality and cost burden. However, the population suffers from an underdiagnosis of chronic obstructive pulmonary disease. This chapter aims to create COPD detection models and assess the relative effectiveness of several modeling paradigms to discover the optimal model for the task on the dataset of 563 hospital or emergency ward visits in China-Japan Friendship Hospital performed between February 2011 and March 2017. We investigated the use of a Long Short Term Memory Network (LSTM), a kind of deep learning, for the automated identification of COPD, with the model hyperparameters modified using the firefly algorithm. Three optimization variations have been used to optimize the hyperparameters of the proposed LSTM Model: random search, hyperband, and firefly algorithm. Firefly algorithm with LSTM obtained superior results than the LSTM-Random Search and LSTM-Hyperband. Therefore, the adoption of LSTM-Firefly is beneficial in terms of COPD detection and diagnosis with clinically acceptable performance compared to LSTM—Random Search, LSTM—Hyperband, LSTM, and other machine learning algorithms such as LR, KNN, NB, DT, and RF.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-981-19-7622-3_14,en,Management and Impact of COVID-19 on Intelligent Transportation System,OriginalPaper,"A safe and sustainable intelligent transportation and human behavior system can reliably extract knowledge from traffic data in cities. The data is safeguarded utilizing blockchain and learning techniques, with a threaded Graphics Processing Unit (GPU) for scalability. Different improvements are also available to efficiently process information on the GPU. To combine local information found on every site into global knowledge, a reinforced profound trained model is also built. Internet of Things (IoTs) technologies play a critical role in global transportation. Communication between automobiles via IoT will usher in a new era of communication that will lead to intelligent transportation systems (ITS). ITS technology is emerging as a worldwide revolution in recent years, having a significant influence on the transport and automation industries. As a result, academics’ focus has shifted to the utility of ITS technology, motivating us to give a complete study. As a result, this study brings together current research as well as some potential uses of intelligent transportation system technology in transportation. It traces the growth of ITS studies, also known as transportation models, since their inception and tries to provide a comprehensive analysis of various ITS technologies, including their strengths and drawbacks. Energy management is viewed as a cost-effective, creative solution to high-efficiency power plants. It optimizes conventional sources of the IoT-based intelligent transport system while also assisting in the automation of railroads, highways, airways, and shipways, hence improving customer experience. ITS is being developed to provide informed judgment and cooperative sensing in order to address the growing need for better autonomous transportation. However, in the dynamic era of current apps and the inflexible architecture of legacy Internet, software-defined transportation infrastructure must be adaptable, inventive, adaptive, and programmable (SD-ITS). The logically centralized intelligence of SD-ITS might be a key target of current cyber-threats and assaults, causing the entire network to go down in flames.","['Engineering', 'Communications Engineering, Networks', 'Automotive Engineering', 'Transportation Technology and Traffic Engineering', 'Computer Applications']"
doi:10.1007/978-1-0716-2617-7_16,en,Machine Learning Methods for Survival Analysis with Clinical and Transcriptomics Data of Breast Cancer,OriginalPaper,"Breast cancer is one of the most common cancers in women worldwide, which causes an enormous number of deaths annually. However, early diagnosis of breast cancer can improve survival outcomes enabling simpler and more cost-effective treatments. The recent increase in data availability provides unprecedented opportunities to apply data-driven and machine learning methods to identify early-detection prognostic factors capable of predicting the expected survival and potential sensitivity to treatment of patients, with the final aim of enhancing clinical outcomes. This tutorial presents a protocol for applying machine learning models in survival analysis for both clinical and transcriptomic data. We show that integrating clinical and mRNA expression data is essential to explain the multiple biological processes driving cancer progression. Our results reveal that machine-learning-based models such as random survival forests, gradient boosted survival model, and survival support vector machine can outperform the traditional statistical methods, i.e., Cox proportional hazard model. The highest C-index among the machine learning models was recorded when using survival support vector machine, with a value 0.688, whereas the C-index recorded using the Cox model was 0.677. Shapley Additive Explanation (SHAP) values were also applied to identify the feature importance of the models and their impact on the prediction outcomes.","['Life Sciences', 'Bioinformatics']"
doi:10.1007/978-981-19-6004-8_59,en,Sine Cosine Algorithm with Tangent Search for Neural Networks Dropout Regularization,OriginalPaper,"Convolutional neural networks belong to the group of deep learning methods, largely influenced by the structure and functioning of the human brain. Their primary usage is to perform image classifying tasks. All neural networks, including convolutional neural networks, are susceptible to the overfitting issue, which can occur when the network exhibits good performance on the train data while failing to accurately predict the new data when it is fed to the inputs. Few regularization approaches exist that may help in avoiding the overfit. This paper proposes a novel swarm intelligence optimization method to address the overfitting problem by choosing the adequate dropout parameter value. Swarm algorithms have previously been used to optimize the structure of neural networks; however, the full potential of these algorithms has yet not been thoroughly investigated. Scientists must invest a lot of time to choose the appropriate dropout value if they execute this task manually. In this research, an automated framework, that uses improved sine cosine metaheuristics to perform this task, is proposed. The proposed framework was tested on four standard benchmark datasets, namely MNIST, CIFAR10, Semeion, and UPS. The simulation results have been validated against the results generated by several other state-of-the-art swarm intelligence algorithms. The comparison shows that the proposed method outperforms other cutting-edge algorithms in terms of classification error, therefore achieving higher accuracy percentage.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-11154-9_7,en,Polyp Segmentation with Deep Ensembles and Data Augmentation,OriginalPaper,"Globally, colorectal cancer is one of the leading causes of mortality. Colonoscopies and the early removal of polys significantly increase the survival rate of this cancer, but this intervention depends on the accurate detection of polys in the surrounding tissues. Missing a poly has serious consequences. One way to guard against human error is to develop automatic polyp detection systems. Deep learning semantic segmentation offers one approach to solving the problem of poly detection. In this work, we propose an ensemble of ensembles composed of two deep convolutional neural networks (DCNNs): DeepLabV3+ and HarDNet. Diversity among the single classifiers is enforced on the data level using different data augmentation approaches and on the classifier level with two DCNNs: DeepLabV3+ and HardNet, each using an encoder-decoder unit. In addition, ensembles of DeepLabV3+ are built using fifteen loss functions. Our best ensembles are tested on a large dataset composed of samples taken from five polyp benchmarks. Ensembles are assessed and compared with the best method reported in the literature and shown to produce state-of-the-art results. The source code, the dataset, and the testing protocol used in this study are freely available at https://github.com/LorisNanni .","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Health Informatics']"
doi:10.1007/978-981-19-0151-5_11,en,Performance Evaluation of EfficientNet Model Towards Malaria Parasite Detection in Segmented Blood Cells from Thin-Blood Smear Images,OriginalPaper,"Malaria is a potentially fatal disease caused by infected Anopheles mosquitoes. Traditional diagnosis of malaria involves examination of thin blood smear slides under a microscope by trained microscopists to detect infected blood cells. This process is expensive, and results depend both on the quality of smear and on the expertise of the microscopist. Thus, work done in this field is focussed on automating this detection of infected cells. Early work for this task included using hand-engineered features and machine learning algorithms. This approach was taken over by the advent of CNNs which provided an end-to-end solution, right from feature extraction to classification. Work done in this field thus shifted towards using state-of-the-art CNNs. The authors of this paper found that most of the existing models that had been used for this problem had good classification accuracy but had big sizes that could not be run on a normal computing device. They identified that a new set of state-of-the-art CNNs, called EfficientNets which promised similar performance at much smaller sizes, had not been used for this task. So, they worked to evaluate the performance of EfficientNet models for this medical image classification task. The authors trained the EfficientNet-B0 model on the malaria images dataset taken from the National Library of Medicine (NLM) (Dataset used: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html ). The authors then computed the key performance parameters of the model and compared them against the existing models for this problem. The work done showed that using Efficient-Net models for this task achieved similar accuracy to existing models and significantly reduced the number of parameters.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/s40544-021-0584-3,en,Long short-term memory based semi-supervised encoder—decoder for early prediction of failures in self-lubricating bearings,"['OriginalPaper', 'Research Article']","The existing knowledge regarding the interfacial forces, lubrication, and wear of bearings in real-world operation has significantly improved their designs over time, allowing for prolonged service life. As a result, self-lubricating bearings have become a viable alternative to traditional bearing designs in industrial machines. However, wear mechanisms are still inevitable and occur progressively in self-lubricating bearings, as characterized by the loss of the lubrication film and seizure. Therefore, monitoring the stages of the wear states in these components will help to impart the necessary countermeasures to reduce the machine maintenance downtime. This article proposes a methodology for using a long short-term memory (LSTM)-based encoder—decoder architecture on interfacial force signatures to detect abnormal regimes, aiming to provide early predictions of failure in self-lubricating sliding contacts even before they occur. Reciprocating sliding experiments were performed using a self-lubricating bronze bushing and steel shaft journal in a custom-built transversally oscillating tribometer setup. The force signatures corresponding to each cycle of the reciprocating sliding motion in the normal regime were used as inputs to train the encoder—decoder architecture, so as to reconstruct any new signal of the normal regime with the minimum error. With this semi-supervised training exercise, the force signatures corresponding to the abnormal regime could be differentiated from the normal regime, as their reconstruction errors would be very high. During the validation procedure for the proposed LSTM-based encoder—decoder model, the model predicted the force signals corresponding to the normal and abnormal regimes with an accuracy of 97%. In addition, a visualization of the reconstruction error across the entire force signature showed noticeable patterns in the reconstruction error when temporally decoded before the actual critical failure point, making it possible to be used for early predictions of failure.","['Engineering', 'Mechanical Engineering', 'Nanotechnology', 'Tribology, Corrosion and Coatings', 'Physical Chemistry', 'Surfaces and Interfaces, Thin Films']"
doi:10.1007/978-3-030-93236-7_46,en,Nonlinear Model Classification of HDR-S Bearing Under Low Temperature Using Artificial Neural Network,OriginalPaper,"The seismic isolation design for bridges mainly focused on increasing the damping properties of the seismic isolator under controlled period. To adopt to the demand of high damping properties, there were newly developed isolators and dampers nowadays. However, in Japan, the seismic isolator’s design standard for bearing’s nonlinear parameter standard was limited to some existing isolator types and was fitted to bilinear model under controlled experiment environment settings. Furthermore, in actual environment, the nonlinear behavior of some key members like bearings and dampers were somewhat complicated, that’s why there was a need to select the proper nonlinear model carefully to represents the bearing’s nonlinear behavior more realistic. The nonlinear model selection can be difficult considering the new types of isolators and different external factors which makes it a trial-and-error process and highly depends on the engineer’s expertise. Therefore, inversion process was proposed for structural key member’s nonlinear model selection using neural network. The AI model was trained using four existing nonlinear models which was capable to identify the nonlinear model of an HDR-S bearing under low temperature. The training data used the displacement of an actual bearing`s experimental data while the force was numerically simulated using different existing nonlinear models. The proposed method will greatly help to guide the engineers on nonlinear model classification which was important prior to nonlinear parameter identification and seismic isolation design.","['Engineering', 'Civil Engineering', 'Vibration, Dynamical Systems, Control', 'Mechanical Engineering', 'Structural Materials', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5221-0_43,en,Classification of Attractiveness of Clothing Using Convolution Neural Network,OriginalPaper,"With the advancement in technology and trends, it is important to be upgraded with the latest trends. A consumer wants good quality products which are of variable design. Going through a plethora of products to pick up a handful of products and again filter them to find out the best is a long and tedious process. Therefore, online shopping has taken such a hike, enabling users to cancel out the long process. Consumers can view the products sold by different online e-commerce websites and compare the prices, brands, and sizes of the products. But due to changes in trends, consumers fail to buy the product according to the market flow. This problem has motivated us to make a recommendation system, where the user can click an image or use an image in their gallery to find out the attractiveness of the jeans and top wear.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2535-1_48,en,Analysis of Phonocardiogram Signal Using Deep Learning,OriginalPaper,Phonocardiogram (PCG) plays an important role in the initial diagnostic screenings of patients to assess the presence of cardio-vascular abnormalities. It is also used to complement the ECG-based cardiac diagnosis for detecting cardio-vascular abnormalities. This task has been proposed to classify the heart sounds by performing deep learning technique known as CNN (Convolutional neural network). Two types of datasets were collected from the clinical environment and used as an input to this project. The experimental results show CNN provides the better results in the detection of abnormal heart sounds with good accuracy.,"['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19958-5_22,en,Tomato Leaf Disease Classification Using Transfer Learning Method,OriginalPaper,"Human civilization is completely reliant on plants to provide our dietary needs. Despite having a population of over seven billion people that is rapidly expanding, our cultivable land has declined rather than increased. Plants, on the other hand, are susceptible to a variety of illnesses. Leaf disease, which comprises leaf spots, bacterial spots, black spots, and other conditions, is one of them. Bacteria and fungi are the most common causes of these disorders. This has an evident negative effect on the plant in the long run. As a result, it should be recognized early in order to save the crop’s productivity. We choose to focus on tomato leaf disease especially in our study report. Where we have used transfer learning technology to detect early blight, late blight, bacterial spots, and a few other diseases in tomato leaves images. Inception-V3 model has been deployed to have the best predictive outcome from the dataset which includes ten sets of tomato leaf images. The training and testing accuracy of transfer learning based inception-V3 model is 99.58% and 97.19% respectively. We also compare our model with other three transfer learning model which are VGG19, MobileNet and ResNet50.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-10956-0_11,en,"User Interface Design Based on Traditional Japanese Air Finger Drawings, “Soragaki”",OriginalPaper,"Due to the spread of the Internet of Things (IoT), our living spaces contain various devices that are connected to the network. Multiple controllers are necessary to operate these devices, and a new user interface (UI) for intuitive operation is required. A UI using hand gestures enables intuitive operation and is less restrictive than operating devices with controllers. Real-time recognition is essential for the efficient use of gestures in UI. In this article, a gesture to control home appliances is discussed. To define the gesture, a gesture based on “Expectorogy” and the three elements beautiful, easy, and new, is proposed. The gesture utilized is based on traditional Japanese behavior and “Soragaki,” Japanese air finger drawings. The gesture is beautiful, comfortable, and self-satisfying. The gesture is constructed using natural numbers from zero to nine. In the experiment, the gesture is performed by the right index finger using a wearable six axis motion sensor. Then two models are applied, the 1D-convolutional neural network (1D-CNN) and recurrent neural network (RNN), to classify the gesture. The results show that the recognition accuracy of 1D-CNN was 96% on average. The recognition accuracy of RNN was 99% on average. If multiple parameters are tested, 1D-CNN is better because the execution time of training is six times longer for RNN than for 1D-CNN. However, RNN provides a more accurate learning model than that provided by 1D-CNN. The size of the RNN learning model is half that of the 1D-CNN learning model, thus it is easier to install. The results of this study show that it is possible to use RNN-based gesture recognition to control internet connected devices. While this study demonstrates that the gesture is easy to use, it does need to be tested in real-world environments.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-08927-5_11,en,"Artificial Intelligence, Deep Learning, and Machine Learning Applications in Total Hip Arthroplasty",OriginalPaper,"Artificial intelligence (AI) recently gained popularity in total hip arthroplasty (THA) applications due to several reasons including technological improvements such as availability of data storage, processor capabilities, AI technique developments, and surgery-related improvements including presurgical Pre-surgical analysis techniques developed and data collected for input to algorithms (Mont, et al. J Arthroplast. 34(10):2199–200, 2019). In this work the focus will be on the research literature covering AI, deep learning (DL), and machine learning Machine learning (ML) (ML) techniques that relate to only THA. This coverage excludes the combined results for total knee Total knee arthroplasty (TKA) arthroplasty Arthroplasty (TKA) and THA unless THA is analyzed independently from TKA. Applications determined include THA-related economic analysis Economic analysis and payment models Payment model , patients’ well-being, risk of blood Blood transfusion, hip fracture Hip fracture detection Detection (Kim and MacKinnon. Clin Radiol. 73:439–45, 2018). Biomechanical considerations, optimal implant design Implant design , post-THA implant brand detection Implant brand detection , hip disability upon THA, inpatient and outpatient THA surgery detection, automating and improving angle of acetabular component, text-based database Database search for THA-related factors, mechanical Mechanical loosening detection loosening Loosening detection of the transplant, patient comfort after THA, and implant failure detection Implant failure detection . Many more applications are possible using AI, DL, and ML with few of them suggested in the conclusion section.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomechanics', 'Surgical Orthopedics', 'Robotics']"
doi:10.1007/978-981-19-4182-5_4,en,Sign2Sign: A Novel Approach Towards Real-Time ASL to ISL Translation,OriginalPaper,"Sign language is the main form of communication for people with speech and hearing imparity. There are over 300 types of sign languages across the world, with American Sign Language (ASL) and Indian Sign Language (ISL) most popularly used in the United States of America and India, respectively. When a person familiar with ASL has to communicate with a person familiar with ISL, there is bound to be a communication gap and calls for the need of skilled translators. We aim to automate this process by proposing a novel solution that can translate real-time ASL input into ISL. The proposed methodology uses a CNN trained to recognize 36 different classes of ASL signs. The recognized signs are mapped to the respective ISL signs and joined together to form a video. The CNN model for recognizing ASL achieved a testing accuracy of 96.43%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-981-19-0105-8_12,en,A Deep Convolution Neural Networks Framework for Analyzing Electroencephalography Signals in Neuromarketing,OriginalPaper,"Neuromarketing is the field of neuroscience that potentially infers to gain knowledge of customers’ choice by analyzing the physiological signals. Here, the motive is to maintain the sustainability among traditional market research, which depends upon the preference of the consumer explicitly, on the other side neuromarketing, which shows the preference implicitly. Research work in this fastly emerging field is highly demanded due to its inherent potential. But, due to the paucity of sophisticated data-mining approaches for predicting and classifying consumers’ choices, it is not yet touched a gruntled level. Therefore, in this work, a deep convolutional neural network (DCNN) is proposed to understand consumer choice using electroencephalogram (EEG) signals which were recorded while consumers were browsing through various E-commerce products. The arbitrary time-domain signals are transformed into two-dimensional images, which are then used for the classification of the EEG signals using DCNN. The score-level fusion is used to examine the model performance and got 81% accuracy which surpasses the existing approaches.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-981-19-3590-9_7,en,A Comparative Analysis of Deep Learning-Based Frameworks for Face Vitality Detection,OriginalPaper,"Despite abundance of applications of facial biometric systems in human authentication, they are defenseless to attack presentations. With the advancements in attack generation techniques that provides realistic look for fake face presentations, it becomes a progressively challenging task. Hence, determining the root of bonafide or artificial face presentations has received a substantial attention. Recently, deep learning (DL) has made breakthroughs in the field of presentation attack detection (PAD) or face vitality detection (FVD). The DL-based face PAD approaches are better compared to traditional methods, as they do not require the user to choose the image features beforehand. Although, handcrafted feature-based techniques were the prominent ones over the past decades but since 2014, the research in this field is shifting toward the features extracted through deep neural networks. Hence, in this work, we present a review of various deep networks that are been successfully designed for face PAD mechanisms. It is well known that DL-based techniques, being able to extract significant patterns automatically, and deliver good detection accuracies in unknown attacking scenarios too. Additionally, the live face samples closed to a specific attack group such as makeup, obfuscation, and transparent mask has made the task of vitality detection as more challenging. The increased training overhead and the requirement of large training data is a trade-off for the substantial growth of these techniques. Consequently, the notion of transfer learning (TL) aims to transfer the existing learnt knowledge with larger dataset to new smaller database. Therefore, TL in face PAD mechanism is feasible solution that deserves more attention with an aim of increasing face vitality detection accuracies.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security', 'Mobile and Network Security']"
doi:10.1007/978-3-031-11058-0_71,en,Automation of Monitoring of Unauthorized Buildings,OriginalPaper,"The purpose of this study is to create an automated system for monitoring unauthorized buildings. In the course of the study, the reasons for unauthorized construction were considered, as well as problems arising from the withdrawal of land for state and municipal needs (in particular, for the construction of transport infrastructure). The data provided by the Federal Service for State Registration, Cadastre and Cartography were analyzed. The analysis of the cases of the Arbitration Courts of the Krasnodar Territory under Article 222 of the Civil Code of the Russian Federation was carried out, as a result of which the number of violations in dynamics over the past five years was revealed. The disadvantages of obtaining information through satellite images are listed, and an alternative method for obtaining relevant data using unmanned aerial vehicles is proposed, as well as their use for monitoring purposes. The developed system for monitoring unauthorized buildings, which is based on a neural network, is presented. The positive and negative aspects of the developed system for monitoring unauthorized buildings are revealed. An algorithm has been developed, on the basis of which a system for monitoring unauthorized buildings has been proposed. The activities that can be carried out by local governments using the proposed system are listed. The problems that must be solved by synchronizing several unmanned aerial vehicles to perform joint surveys are noted. The paper uses such research methods as: theoretical analysis, statistical analysis, their comparison and systematization.","['Engineering', 'Control and Systems Theory', 'Control, Robotics, Mechatronics', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-2126-1_13,en,Automatic Pathological Myopia Detection Using Ensemble Model,OriginalPaper,"People having myopia are unaware about the progression of the disease until it is severe or untreatable. This can be avoided by regular checkup for the disease, but this increases the load on the professionals, and the time required for the results is also more, so an automated method using deep learning would be helpful in early detection of diseases. In order to reduce the time for detecting the disease, we propose a deep learning solution for automatic myopia detection. Our proposed model involves three models built by applying transfer learning on pretrained Xception, DenseNet201 and InceptionV3 models, respectively. Stack ensembling is used to ensemble the above three models. The ensemble model classifies the fundus images into two classes, i.e. pathological myopia and no pathological myopia, with an accuracy of 95.23%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-3-031-14537-7_13,en,Method for the Production Planning and Scheduling of a Flexible Manufacturing Plant Based on the Bees Algorithm,OriginalPaper,"Production planning Production planning and scheduling of flexible manufacturing plants Scheduling of flexible manufacturing plant are still highly manual labor-intensive tasks. The production efficiency is constrained due to the large number of combinations of feasible machine selection Machine selection and operation sequence Operation sequence arrangement. In this study, a mathematic model approximating the real working environment and two different Bees Algorithms Bees Algorithm were compared. In the improved Bees Algorithm with site abandonment technology Bees Algorithm with site abandonment technology , different strategies were used for the abandonment of initial sites and elite sites. The simulation results based on actual factory data from Trumpf (China) show that the mathematical model and the Bees Algorithm Bees Algorithm, THE could help to improve production effectiveness. Moreover, the improved Bees Algorithm with site abandonment technology Bees Algorithm with site abandonment technology shows its excellent ability to solve problems such as production planning issues in flexible manufacturing plants.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-981-19-3998-3_171,en,Adaptive Controller Based on Interfered Fluid Dynamical System for Spacecraft Safe Approach,OriginalPaper,"In order to improve controller performance in the process of spacecraft rendezvous with collision-free, Adaptive Interfered Fluid Dynamical System Sliding Mode Control (AIFDS-SMC) is proposed, which is based on the theory of Interfered Fluid Dynamical System (IFDS), State Dependent Riccati Equation (SDRE) and Sliding Mode Control (SMC). This method uses an improved IFDS feedback control method. Combining the attraction potential function to ensure that different initial states can converge to any target state. Based on the IFDS-SMC, the parameters of approaching controller are adjusted by using the optimal control theory SDRE, and AIFDS-SMC with optimized fuel consumption is obtained. The above methods are applied to the problem of spacecraft rendezvous and obstacle avoidance, also comparisons of the simulation results are made. The results show that the AIFDS-SMC has better fuel economy and better control accuracy than the IFDS-SMC.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-2065-3_34,en,Optimization and Pursuance Analysis of RDBMS for Relational Algebraic Operations—MySQL,OriginalPaper,"A researcher can find problems and their solutions on any Web site and its database so that database can be utilized appropriately, and anyone can apply tools on the data for query optimization and query processing. For optimization, relational database management systems (RDBMSs) trust an optimizer that converts SQL statements into executable code. If indexes are removed, the RDBMS can still access the data (less efficiently). If a column is added to the table being accessed, the data can still be manipulated without changing the program code. This is all possible because the physical access paths to the data are generated by the relational optimizer. Problems in optimizing MYSQL [ 1 ] DB are as follows: huge database, least usage of index, the problem with MVD, wrong schema, unnecessary columns, apply integrity constraints, response time, and time of query processing. These problems are related to relational databases. In this paper, we focus on performance tuning, time utilization or response time, a technique used for query execution and memory usage. To solve these problems, we need to create three-tier architecture which includes front end, back end, and business logic. This paper includes relational DB as MYSQL in which we check pursuance analysis of employee database; we check the usage of the index, the definition of schema, multivalued dependency, provide consistency and response time or reaction time. This work has been accomplished by validating and providing a user-friendly environment through a front-end application in HTML where  verification is done through PHP. We need to perform selection, projection, and join operation in a query. To complete this paper, we use MY_SQL, HTML, and PHP. To do assessment, we use profiling and checking the status of each query, time required to respond. MYSQL uses SHOW STATUS, and SHOW PROFILE solves basic problems in RDBMS, i.e., MYSQL.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Machine Learning']"
doi:10.1007/978-981-19-7842-5_14,en,Truss Structure Optimization Design Based on FE-PSO-SQP Algorithm,OriginalPaper,"Compared with other structural optimization design algorithms, particle swarm optimization (PSO) gains many superiorities, like being easy to understand the principle and fewer parameters in the calculation model. When we use the PSO to deal with truss structure optimization problems, this algorithm usually has low computational accuracy, slow rates of convergence, and poor population varieties in the further model calculation. To overcome these shortcomings and better solve the truss structure optimization problem, FE-PSO-SQP algorithm, a new structure optimization method, is proposed herein by combining the PSO algorithm with the sequential quadratic programming (SQP) algorithm and finite element method (FE). In addition, a set of calculation program is developed by ANSYS software. When the self-made program is used to conduct simulation calculation on the truss structure optimization problem, the calculation results show that FE-PSO-SQP algorithm has faster convergence speed and higher calculation accuracy than FE-PSO algorithm, and can be used for structure optimization design.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5073-5_2,en,Methods Used to Improve Generalization Performance,OriginalPaper,"Historically, the intuition behind developing classification algorithms was often to identify a hypothesis that minimizes training error. A major problem encountered with this approach is overfitting, which occurs when the hypothesis becomes too complex in comparison to the size of training data. In such cases, it is likely that an algorithm minimizing training error will find a hypothesis that fits the training data very well, but generalizes poorly to previously unseen data. Good generalization here refers to low generalization error which is defined as the difference between the training error and true error.","['Computer Science', 'Machine Learning', 'Computational Intelligence', 'Pattern Recognition']"
doi:10.1007/978-981-19-5221-0_49,en,Lightweight Deep Learning Facial Expression Recognition Model,OriginalPaper,"Due to the broad application potential and business demand, facial expression recognition is one of the prevalent topics in the deep learning domain. Since most of the application domain of facial expression is deployed on devices with low computational power, this work majorly focuses on a lightweight model. Therefore, in the proposed model, a lightweight vision transformer model, MobileVit that surpasses other light weight CNN like MobileNetV3, NasNet on the ImageNet classification dataset is chosen for the classification of facial expression. The model was initially pretrained on CelebA dataset, then was trained on RAF-DB dataset, and fine-tuning of the model was carried out. For improving the generalization capability of the model, advanced augmentation methods like cut mix with basic augmentation methods are deployed. For the visualization of the model performance, interpretation of feature maps was carried out utilizing the Score-Cam technique. Squeeze-and-excitation layer was added to improve the feature extraction performance of the model. After fine-tuning of the model, the performance of the model was evaluated on KDEF, FER-2013, and AffectNEt dataset.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-11047-4_9,en,Exploratory methodology for power delivery,OriginalPaper,"The conventional power network design process requires iterative modifications to the existing power network to eliminate hot spots and to converge to target impedance parameters. At later stages in the IC design process, this procedure may require significant time and labor due to the limited flexibility to accommodate necessary changes. Power delivery exploration during early stages of the design process may bring considerable savings to the system development effort. The number of iterations may be greatly reduced by choosing the initial parameters sufficiently close to the optimum. A power delivery exploration framework based on constrained global optimization is presented in this chapter. The parameters characterizing the power network are estimated at early stages of the development process, while considering both electrical and nonelectrical factors, such as area and cost. A Laplace transform-based circuit simulator is well suited for optimization purposes due to the high computational efficiency when a large number of iterations is required. In the first case study, a 15% reduction in decoupling capacitance along with a 38.6% reduction in power consumption is achieved while satisfying performance and power quality constraints. The proposed framework has also been applied to the distribution of voltage domains in a large scale integrated system, while minimizing the decoupling capacitance. The optimal number of voltage rails are determined, requiring approximately 40% less on-chip area than alternative solutions.","['Engineering', 'Circuits and Systems']"
doi:10.1007/978-3-030-96025-4_5,en,Optimal Capacitor Placement for Power Loss Reduction and Voltage Profile Improvement,OriginalPaper,"This chapter presents a two-stage procedure to determine the optimal locations and sizes of capacitors with an objective of power loss reduction in radial distribution systems. In first stage, the loss sensitivity analysis using two loss sensitivity indices (LSIs) is employed to select the candidate locations for the capacitors to reduce the search space in the optimization procedure. The suggested LSIs are based on the following physical quantities; the variation of the active power losses with respect to the load bus voltage at variant nodes, the variation of the active power losses with respect to the level of reactive power at variant nodes. In second stage, the ant colony optimization (ACO) algorithm is used to find the optimal locations and sizes of capacitors considering the minimization of total energy loss and total costs of capacitors as an objective function, while the security and operational constraints are fully achieved. The fixed and practical switched capacitors are considered to find the optimal solution. The backward/forward sweep (BFS) algorithm is introduced for the load flow calculations. The proposed procedure is applied to different standard test systems as 10-bus, 34-bus and 85-bus radial distribution systems. In addition, the application of the proposed procedure on a real distribution system of the East Delta Network (EDN) as a part of the Unified Egyptian Network (UEN) is used as a test system. The numerical results are compared with other methods to show the capability of the proposed procedure to find the optimal locations and sizes of capacitors for significant saving in the total cost with more accuracy and efficiency, especially with increasing in the distribution system sizing.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Systems', 'Renewable and Green Energy']"
doi:10.1007/978-3-031-07258-1_25,en,Vibration-Based Quality Assessment of Metallic Turbine Blades Considering Measurement Uncertainty,OriginalPaper,"Assessing the structural quality of metallic turbine blades is a challenging task due to their complex geometry and wide range of possible defect features. Process Compensated Resonance Testing (PCRT) is an effective quality assessment tool that uses a broadband sinusoidal swept input to excite the resonant modes of the component and employs a supervised learning algorithm to interpret the resonant modes and as such to determine the structural quality of the component. Our previous work has mostly centered on the exploitation of classifiers that are based on Mahalanobis distance. However, in practice, the measurement uncertainty may lead to bias in the trained classifier, potentially resulting in misclassification of the turbine blade. In this study, the concept of interval Mahalanobis space is introduced in the classifier in order to cope with measurement uncertainty. The resulting Integrated Interval Mahalanobis Classification System (IIMCS) classifier employs BPSO to screen those resonant frequencies that contribute favorably to the system and analyzes the sensitivity of resonant frequencies to measurement uncertainty under a Monte Carlo simulation scheme. The developed classifier was applied to an experimental case study of equiaxed nickel alloy first-stage turbine blades with a range of defect features, showing a good and robust classification performance.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering', 'Monitoring/Environmental Analysis', 'Analytical Chemistry']"
doi:10.1007/978-981-19-2840-6_12,en,Automated Intelligent Hematology Classification System Using Image Processing and Neural Networks,OriginalPaper,"In this paper, a method has been proposed which uses an Image Processing and Deep learning-based approach to classify microscopic blood smear images based on 7 classes of blood diseases namely, Leukemia, Anemia, Lymphoma (CLL, FL, MCL), Myeloma and Malaria, from the healthy blood images. Image preprocessing techniques based on Feature Extraction and Ni-black Thresholding were used on image dataset to obtain features for identification and classification of Leukemia and Anemia. Thereafter, a neural network based on VGG16 was implemented to train the model for classification of all the diseases which included pretrained weights from ImageNet. For validation of the model, the scores of Precision, Recall, and F-score were taken into account to calculate the accuracy of the model. Through this methodology, the model was able to achieve an accuracy of 98.6% with minimum loss of 0.47. The proposed system will help hematologists to identify blood diseases more accurately and faster with this automatic analysis system.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-11154-9_11,en,BioGNN: How Graph Neural Networks Can Solve Biological Problems,OriginalPaper,"Graph Neural Networks (GNNs) have known an important and fast development in the last decade, with many theoretical and practical innovations. Their main feature is the capability of processing graph structured data with minimal loss of structural information. This makes GNNs the ideal family of models for processing a wide variety of biological data: metabolic networks, structural formulas of molecules, and proteins are all examples of biological data that are naturally represented as graphs. As an example, GNNs were employed, with very good results, for the prediction of protein-protein interactions. This was achieved by applying a clique detection model on graphs representing the interaction of the secondary structures of pairs of proteins. The introduction of composite GNN models, designed for processing heterogeneous graphs, has allowed researchers to study even more complex networks. For instance, drug side-effects were predicted based on a graph describing the interactions between drugs and human genes. Another very important innovation was brought by generative models, that were introduced for graph data after the success of generative models for images. In particular, GNNs were used to build a sequential model for the generation of potential drug candidates, in the form of molecular graphs, with the purpose of enhancing existing drug discovery techniques. The increasing accuracy and efficacy of these models, as well as the development of more complex biological databases, ensure even more interesting future developments in the application of GNNs to biological data.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Health Informatics']"
doi:10.1007/978-3-031-04435-9_39,en,Glioma Brain Tumor Segmentation in Four MRI Modalities Using a Convolutional Neural Network and Based on a Transfer Learning Method,OriginalPaper,"Accurate segmentation of brain tumors from magnetic resonance imaging (MRI) images is still a challenging task for many applications in the medical domain. To gain a better segmentation outcome, it is vital to employ more information from brain tissue, which is available in different modalities. So, in this study, to segment brain tumors in MRI images, a new four different feature extraction Convolutional Neural Network (CNN) architecture is utilized. Also, we used four image modalities, including T1, T2, T1-c, and FLAIR. Weights and biases from a ResNet-50 network were imported to our structure to increase the segmentation accuracy. We designed two main building blocks to extract additional features from each modality. Our model uses both local and global features by defining two different sizes of patches. The proposed framework was evaluated on the BRATS 2018 dataset and showed that our architecture has a competitive dice score (0.9223, 0.8993, and 0.9211 for Enhanced, Whole, and Core tumor areas, respectively).","['Social Sciences', 'Science and Technology Studies']"
doi:10.1007/978-3-031-15928-2_54,en,Energy Efficient Trajectory Planning in Robotic Cells via Virtual Prototyping Tools,OriginalPaper,"The constant growth in global energy demand, and corresponding prices rise, is soaring new engineering methods for reducing energy consumption in manufacturing processes. For decades, industrial robotics have been enabling quality enhancement of end-products by using flexible manufacturing processes, without much concern to energy cost, but now a makeover is happening. Calls for sustainable and green manufacturing processes are being promoted across the globe with the aim to produce more goods and with less consumption. In this paper, a new method is presented focusing on the optimization of energy intake by industrial robots, without the need to change their hardware set and just modifying the trajectory planning of the end-effector. A test case scenario consisting of a robotic cell with 4 pick-and-place manipulators has been set to validate the method. Starting from a pre-scheduled trajectory, robots are moved at the highest speed and acceleration and, by performing the sequenced operations, the optimal trajectories are defined. The goal is to find a trajectory that minimizes the time cycle and the total energy consumption, while avoiding collisions between the robots’ links: comparing the results thereof to those of the pre-scheduled trajectory, noticeable energy saving has been obtained along with possible decrease of the cycle time.","['Engineering', 'Engineering Design', 'Industrial and Production Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-981-19-4960-9_26,en,Wireless Underground Soil Networks-Based Multiparameter Monitoring System for Mining Areas,OriginalPaper,"The regular mining mishaps have caused genuine setbacks and enormous monetary misfortunes. It is critical for the worldwide mining industry to increase operational productivity and make strides in general mining security. This paper proposes a wireless underground soil network-based mining security check. The checking system collects temperature, dampness, noxious gas, and earth shake readings around the mine and, after that, sends the information through blocked off underground soil organization. It will be checked and overhauled by the Web of Things. Here, acceptance-based transmission has been recommended in this technique; the soil could be a source for communication, so based on the soil conductivity, the weighed data is traded. This structure, too, gives a warning prior to on, which can offer assistance to all the individuals who work within the mine to save their lives in case of a mishap.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-3-031-07654-1_13,en,Finger Knuckle Print Recognition Using Complex Conjugate Feature Vector,OriginalPaper,"Biometrics are human-specific traits that are employed in person identification and access control. Various biometrics such as fingerprints, finger knuckle, iris, palmprint, vein patterns, and DNA are used in recognition. Among these numerous biometrics, the researcher is drawn to the hand-based finger knuckle print (FKP). On the dorsal side of the hand, the FKP biometric is found. The creases and folds in the finger knuckle print are rich in textural pattern and can be utilized to identify individuals. The recognition of finger knuckles based on a complex feature fusion is proposed in this chapter. The subspace techniques such as principle component analysis (PCA) and linear discriminant analysis (LDA) are used to extract complex number features. These extracted complex vectors are fused using a parallel fusion strategy. Finally, finger knuckle PolyU and IIT Delhi datasets are used to test the developed parallel fusion complex features. The experimental results show that the proposed parallel fusion of complex vector for feature extraction techniques improve the classification accuracy.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Computer Communication Networks', 'Computational Intelligence']"
doi:10.1007/978-981-19-2879-6_1,en,A General Introduction to Artificial Intelligence,OriginalPaper,"The emergence and rise of artificial intelligence undoubtedly played an important role during the development of the Internet. Over the past decade, with extensive applications in the society, artificial intelligence has become more relevant to people’s daily life. This chapter introduces the concept of artificial intelligence, the related technologies, and the existing controversies over the topic.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3938-9_9,en,Assessment of Nonlinear Optimization Algorithms on Weighted Least-Square-Based Method for Re-Entry Prediction,OriginalPaper,"Accurate prediction of space object re-entry time and location is important to mitigate impact risks. There are several uncertainties associated with the orbital propagation models, atmospheric density models and observational data which make the problem of re-entry prediction challenging. An integrated model based on weighted least-square error function minimization to estimate an essential ballistic parameter (EBP) is implemented and has been found to work well for re-entry predictions. The process requires manual intervention due to reasons namely, (i) wide search range, (ii) outlier removal, (iii) lack of good initial guess, (iv) presence of invalid ballistic parameters in the search range and (v) variation in EBP during the re-entry exercise. This paper presents the implementation of optimization logic for EBP estimation to reduce user-interference and automatize the complete re-entry prediction procedure. This is based on assessment of optimization algorithms, for re-entry prediction of different space objects, subjected to their sensitivity to initial guess, convergence and number of function evaluations. Some failure cases are also presented, and methodology implemented to overcome these failure cases is discussed here.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-3-031-18461-1_8,en,Hybrid Context-Content Based Music Recommendation System,OriginalPaper,"Due to the increase in technology and research over the past few decades, music had become increasingly available to the public, but with a vast selection available, it becomes challenging to choose the songs to listen too. From research done on music recommendation systems (MRS), there are three main methods to recommend songs; context based, content based and collaborative filtering. A hybrid combination of the three methods has the potential to improve music recommendation; however, it has not been fully explored. In this paper, a hybrid music recommendation system, using emotion as the context and musical data as content is proposed. To achieve this, the outputs of a convolution neural network (CNN) and a weight extraction method are combined. The CNN extracts user emotion from a favorite playlist and extracts audio features from the songs and metadata. The output of the user emotion and audio features is combined, and a collaborative filtering method is used to select the best song for recommendation. For performance, proposed recommendation system is compared with content similarity music recommendation system (CSMRS) as well as other personalized music recommendation systems.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3679-1_3,en,Comparative Analysis of Object Detection Models for the Detection of Multiple Face Masks,OriginalPaper,"Deep learning has immense prospective in many real-life practices, one of them being object detection. Object detection based on deep learning has shown encouraging results. Since December 2019, deadly virus named CORONA or COVID-19 started to engulf the whole planet with its impact. One of the easiest and simplest ways to protect oneself from this virus is by wearing a mask. In order to detect whether a person is wearing mask or not, we propose a model to detect various face masks that include cloth masks, N-95 masks, medical masks, and no mask. The proposed model consists of two major components—annotating, labeling images and detection of face masks. A new dataset has been created by combining images from Medical Masks Dataset and Google Images, and then these images were annotated according to the mentioned categories. A comparative study has been presented among different object detection algorithms along with a proposed detection algorithm. Results show that YOLOv5 performs best in the detection of face masks when compared to other detection models. It achieved a mAP of 0.51 in just 0.24 h on our dataset. On comparing YOLOv5 to the proposed model, we found that our model achieved a precision of 0.9 as compared to 0.88 of YOLOv5. Among existing approaches YOLOv5 performed the best with precision of 0.88. The model proposed in the work results in precision of 0.90 outperforming all existing models.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3951-8_15,en,Deep Learning Techniques for Leaf Health Prediction,OriginalPaper,"Food security is one of the most important issues discussed worldwide. Furthermore, it becomes more challenging in countries like India, where major population is vegetarian and farmers still follow old conventional farming methods. Plants’ growth is often affected by viral, bacterial diseases. However, experts’ advice on these plant diseases may be costly and time-consuming matter. Recently, computer vision and machine learning are successfully applied to the smart farming. Plant health can be easily monitored, and syndromes can be easily identified by applying machine learning and image processing techniques, over the conventional methods. Leaves are important part of the plant. It generates food for plants using photosynthesis. Hence, damage to leaf may result in reduced food supply to the plant. This results to lesser growth of the plant and lesser flower and fruit bearing capacity. This paper addresses various bacterial and fungal diseases among plants. Impact of each disease on the leaves such as color and shape is also discussed in this paper. This paper studies various deep learning techniques such as convolutional neural network (CNN) model and learning vector quantization (LVQ) algorithm which can be used to distinguish among healthy and disease plants. Difference between healthy and diseased leave was used to train these deep learning classifier. This paper also addresses remedial actions such as recommendation of specific pesticide and its quantity. It was observed that there exists and tradeoff between practical usage of automated system by farmers and accuracy of the system.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20353-4_2,en,Towards Topology Optimization of Pressure-Driven Soft Robots,OriginalPaper,"Soft robots are made of compliant materials that perform their tasks by deriving motion from elastic deformations. They are used in various applications, e.g., for handling fragile objects, navigating sensitive/complex environments, etc., and are typically actuated by Pneumatic/hydraulic loads. Though demands for soft robots are continuously increasing in various engineering sectors, due to the lack of systematic approaches, they are primarily designed manually. This paper presents a systematic density-based topology optimization approach to designing soft robots while considering the design-dependent behavior of the actuating loads. We use the Darcy law with the conceptualized drainage term to model the design-dependent nature of the applied pressure loads. The standard finite element is employed to evaluate the consistent nodal loads from the obtained pressure field. The robust topology optimization formulation is used with the multi-criteria objective. The success of the presented approach is demonstrated by designing a member/soft robot of the pneumatic networks (PneuNets). The optimized member is combined in several series to get different PneuNets. Their CAD models are generated, and they are studied with high-pressure loads in a commercial software. Depending upon the number of members in the PneuNets, different output motions are noted.","['Engineering', 'Nanotechnology and Microengineering', 'Mechatronics', 'Machinery and Machine Elements']"
doi:10.1007/978-981-19-2065-3_53,en,Animal Accident Prevention on Railway Tracks Using Convolution Neural Network,OriginalPaper,"Animal accidents on railway tracks have been menacing to the wildlife population. The main victims of this being the bigger and slower animals like elephants, rhino and buffalo. These accidents also causes damage to the railway infrastructure. Many solutions have been proposed, some solely depending upon sensors, some only consisting software solutions. But, since the inclusion of machine learning, both technologies are used to solve this existing problem. Deep learning being the latest innovation in field of machine learning has helped in the pre-existing solution. For classifying the images as animal detected on track or not, deep learning strategies have been highly efficient. The model discussed in the paper is based on convolution neural network (CNN). With the layers of CNN, there was normalization and pooling layers used to train the model with less loss. The dataset involved contains images of elephants, rhino and buffaloes in the wildlife. The dataset have less images due to non-availability of public datasets of railway tracks, as well as animals. The model was also compared with the other models with the help of transfer learning. The model gives accuracy of 96%.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Machine Learning']"
doi:10.1007/978-981-19-4025-5_6,en,Artificial Intelligence-Enhanced Multiple Access,OriginalPaper,"In this chapter, we discuss the artificial intelligence-enhanced multiple access. Section 6.1 introduces the motivation of applying deep multi-task learning to end-to-end optimization of NOMA. Section 6.2 describes the system model and formulate the end-to-end optimization problem. Section 6.3 proposes the general DeepNOMA framework. Sections 6.4 and 6.5 propose DeepMAS and DeepMUD. Section 6.6 presents the experiment and simulation results. Section 6.7 illustrates the conclusions.","['Engineering', 'Communications Engineering, Networks', 'Wireless and Mobile Communication']"
doi:10.1007/978-3-031-14771-5_26,en,Mining Adverse Drug Reactions from Unstructured Mediums at Scale,OriginalPaper,"Adverse drug reactions/events (ADR/ADE) have a major impact on patient health and health care costs. While most ADR’s are not reported via formal channels, they are often documented in a variety of unstructured conversations such as social media posts or customer support call transcripts. In this paper, we propose a natural language processing (NLP) solution that detects ADR’s in such unstructured free-text conversations, which improves on previous work in three ways. First, a new Named Entity Recognition (NER) model obtains state-of-the-art accuracy for ADR and Drug entity extraction on the ADE, CADEC, and SMM4H benchmark datasets ( 91.75 , 78.76 , and 83.41 % F1 scores respectively). Second, two new Relation Extraction (RE) models are introduced—one based on BioBERT while the other utilizing crafted features over a Fully Connected Neural Network (FCNN)—perform on par with existing state-of-the-art models, and outperform them when trained with a supplementary clinician-annotated RE dataset. Third, a new text classification model, obtains new state-of-the-art accuracy on the CADEC dataset ( 86.69% F1 score). The complete solution is implemented as a unified NLP pipeline in a production-grade library built on top of Apache Spark, making it natively scalable for processing millions of records on commodity clusters.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2004-2_36,en,Human Emotion Prediction Analysis on Post-COVID-19 Crisis in Digital Media Using Deep Learning,OriginalPaper,"The COVID-19 pandemic has produced a significant impact on society. Apart from its deadliest attack on human health and economy, it has also been affecting the mental stability of human being at a larger scale. Though vaccination has been partially successful to prevent further virus outreach, it is leaving behind typical health-related complications even after surviving from the disease. This research work mainly focuses on human emotion prediction analysis in post-COVID-19 period. In this work, a considerable amount of data collection has been performed from various digital sources, viz. Facebook, e-newspapers, and digital news houses. Three distinct classes of emotion, i.e., analytical, depressed, and angry, have been considered. Finally, the predictive analysis is performed using four deep learning models, viz. CNN, RNN, LSTM, and Bi-LSTM, based on digital media responses. Maximum accuracy of 97% is obtained from LSTM model. It has been observed that the post-COVID-19 crisis has mostly depressed the human being.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-981-19-3387-5_154,en,Orientation-Aware Vehicle Re-identification via Synthesis Data Orientation Regression,OriginalPaper,"Owing to the need for smart city construction, vehicle re-identification (re-ID) has been widely used in the field of computer vision. Given a probe vehicle image, all of the same vehicles need to be found in the gallery data. However, because of the different camera shooting angles and vehicle driving directions, extreme changes in the viewing angle in vehicle images leads to a dissimilarity in shape, resulting in a difference in vision and having a significant impact on the accuracy. To address this issue, we propose a method for eliminating bias between different viewpoints, specifically, orientation regression training is conducted on a free synthetic dataset through the VehicleX engine, orientation-aware features are extracted using the trained network above, and the similarity calculated by the original re-ID feature and the orientation-aware feature are then fused to obtain the final similarity, which can effectively remove the orientation bias that exists with conventional re-ID features. Extensive experiments on two public datasets confirmed the effectiveness of the proposed method.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-981-19-1412-6_24,en,Development of Sign Language Recognition Application Using Deep Learning,OriginalPaper,"Deaf and dumb people use a sign language that can only be communicated through hand gestures to express their ideas and views. This coded language is mainly used by people who have speech and/or hearing impairment. The sign language is constructed by various movement of hands, arms, legs, or facial expressions to express their opinions. Meanings are communicated for every movement or position of gesture. Hand gesture plays a significant role to make mother tongue of impairment people for daily communication. The captured image feature can be extracted to translate the hand gesture communication to text\voice format to minimize the gap between the deaf and normal persons. This work considers the images of sign numerals to classify the numbers 0–9 and the alphabets for A–Z (including space).","['Engineering', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security']"
doi:10.1007/978-3-031-16075-2_25,en,Intelligent Bixby Recommender Systems,OriginalPaper,"In today’s society, there is an explosion of data, which brings with it the inherent challenge of dealing with this data. One such issue is analyzing data and making personalized suggestions of utterances to the users before any query is issued to the voice assistant. It should be possible to recommend the most relevant set of queries for quick access in advance to the user. These set of queries should be based on what the user might want to ask the voice assistant at a particular time based on the context such as the location, occasion and other features. Currently, ‘Bixby’ does not have a feature to recommend utterances to the users based on their demographics and usage patterns. Handling implicit data is problematic since it is difficult to analyze the user’s preferences. In this study, we analyze our strategy of recommending personalized utterances to users based on similar user profiles registered with ‘Bixby’ by taking into account several characteristics of the user such as the current context (time, place, occasion, etc.), demographics, utterances, and the frequency with which the utterances are recorded.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3796-5_7,en,Concern Levels During COVID-19: An AI-Based Approach for Social Media Analysis,OriginalPaper,"Unknown nature of CO rona VI rus D isease (COVID-19) has had a huge impact on community, raising worries. Being confined to homes as a safeguard against it, the community voiced its concerns in large numbers via microblogging. The increased tweeting can reflect the severity of the concerns about pandemic. As a result, it is critical to keep an eye on the amount of worry in order to establish a resilient community. Understanding the importance, we propose to analyse the concern and awareness levels during pandemic. We begin with fine-tuning RoBERTa to identify COVID-19-related tweets. Then, the model categorizes tweets published from January 2021 to January 2022. This time period corresponds to the fading stage of the first wave to the third wave of the pandemic. From the classification results, we generate the time series of a social media metric, P ublic C oncern I ndex (PCI). Then, we subject the obtained time series to change point detection. Our classification model achieves an F 1 score of 97.5%. Further, time series analysis suggests an increase in concern levels during the pandemic. During the fading stage of the wave, the levels decline, reflecting that people’s levels of concern are decreasing. Furthermore, we observe a similar acceleration in PCI levels during the second and third waves.","['Engineering', 'Communications Engineering, Networks', 'Systems and Data Security', 'Artificial Intelligence', 'Software Engineering/Programming and Operating Systems', 'Computational Intelligence']"
doi:10.1007/978-3-031-21333-5_78,en,Reliability Analysis of Smart Home Sensor Systems Based on Probabilistic Model Checking,OriginalPaper,"With the rapid development of IoT in recent years, Smart Home, one of the IoT application markets, has also been gaining popularity. The emergence of Smart Homes has brought convenience to people’s lives, especially for people who live alone with physical illness. Smart Home users normally have higher expectations for reliability and safety of sensor systems, particularly in light of how complicated and uncertain the living environment is. The present work attempts to propose a data-knowledge integrated solution to analyze, model and evaluate the reliability of sensor systems in a smart home by combining quantitative reliability analysis and probabilistic model checking. Probabilistic model checking techniques use logical reasoning to check quantitative properties (as system requirements) and provide mathematical guarantee for them. More specifically, Smart Home Sensor Systems (SHSS) is described as a Markov Chain, commonly used probabilistic model, which models the system behaviour (e.g., probabilistic choice of state transition), and SHSS reliability properties are defined by Probabilistic Computation Tree Logic (PCTL). These choices of model and specification formula allow us to use one of the most recently developed open source probabilistic model checkers, PRISM, to perform the model checking of reliability verification task in SHSS. A real world smart home dataset (Van Kasteren dataset) is employed along with PRISM to illustrate the modeling approach and demonstrate the feasibility and applicability of the proposed approach.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3895-5_12,en,Parameter Optimization of Friction Stir Welding of Aluminum Alloy-AA 5083 Using Novel Ant Lion Algorithm,OriginalPaper,"To encounter the issues in fusion welding, friction stir welding (FSW) was developed during the early 90s. On the abutting surfaces, the frictional heat is generated by the non-consumable tool which is used to weld the work piece. The quality of the weld is influenced by several parameters such as welding speed, rotational speed, and tool tilt angle. In this paper, the quality of friction stir welded aluminum alloy AA 5083 is optimized. Full factorial experimental design is used to conduct experiments. The output performances considered in this paper are tensile strength and impact strength. These output performances define the quality of the weld. In this work, an attempt is made to optimize the weld quality using ant lion optimization (ALO) to find the ideal combination of welding parameters. Finally, a confirmation test is also done to validate the findings of the ALO. Through ALO, the better mechanical strengths are predicted at 1002 rpm, 2° tool tilt angle, and 2.5 cm/s welding speed.","['Materials Science', 'Structural Materials', 'Nanotechnology', 'Materials Science, general']"
doi:10.1007/978-3-031-12127-2_13,en,Task-Level Consistency Semi-supervised Based Domain Adaptation for Lung Nodules Segmentation,OriginalPaper,"The pixel-level segmentation labels in volumetric images are expensive and time consuming. Using a model in a new environment without labeled data is a normal case. Domain adaptation can tackle this issue by learning unlabeled data from the target domain and improving target testing performance. In this paper, we propose an out-of-the-box semi-supervised based domain adaptation framework DTCnnU-Net, which used dual-task consistency. Our framework is based on nnU-Net, which is used to perform primarily data-driven automatic machine learning in different datasets. We improved the way of generating level set ground truth to adapt small objects and redesigned the loss function to better weight each loss term. Furthermore, we propose dual-task deep supervision to tackle the problem that small object was invisible in downsampling ground truth when performing the previous deep supervision method. Experiments show that DTCnnU-Net is superior to the state-of-the-art nnU-Net supervised learning framework in domain adaptation of lung nodule segmentation. Our framework improved target testing dice by 3.87% compared to the nnU-Net baseline. The source code is available at: https://github.com/XHMY/nnUNet.","['Engineering', 'Computational Intelligence', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/978-3-031-16868-0_4,en,Architecture Design for Convolutional Auto-Encoders,OriginalPaper,"Although the CAE and its variations have proven benefits in a variety of applications, one key restriction is that their stacked architectures are incompatible with those of state-of-the-art CNNs. The amount of convolutional and pooling layers in the stacked CNN is the same because each CAE contains a pooling layer and a convolutional layer in the encoder. State-of-the-art CNNs, on the other hand, have different amounts of convolutional and pooling layers. The constraint on the number of convolutional and pooling layers contained in CAEs ought to be lifted because the architecture of CNN is one of the important ingredients contributing to the final performance. However, because of the non-differentiable and non-convex properties in practice, it is intractable to determine optimal numbers for convolutional layers and pooling layers, which is related to NAS.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4960-9_28,en,Model Validation to Enhance Precision Agriculture Using DeepDream and Gradient Mapping Techniques,OriginalPaper,"In this paper, an effective deep learning validation model to improve the quality of dataset under study and to enhance model performance in order to predict deficiencies in tomato plants was developed (Ganesan et al. (2021) Fault detection in satellite power system using convolutional neural network [ 1 ]). This solution framework eliminates the background influence in the image plane and differences in lighting conditions and stays true to picturize whatever characteristics are synonymous to each class the model has learned from its training stage. It is possible to obtain positive financial and environmental benefits from these activities by carefully utilizing the above modern computer vision techniques (Sakthivel et al. (2010) Application of support vector machine (SVM) and proximal support vector machine (PSVM) for fault classification of monoblock centrifugal pump. Int J Data Anal Tech Strat [ 2 ]). Mathematical numbers can be misleading at times like the model can be prone to overfitting failing to generalize well to unknown test images. So, the best way to validate our model is to form a visualization mechanism for the user (Hari et al. (2022) Fault detection in SPS using image encoding and deep learning [ 3 ]). Agriculture is a field that is largely deprived of visualizing deep networks ( https://www.kdnuggets.com/2020/06/crop-disease-detection-computer-vision.htm , [ 4 ]) as the complexity that comes with it is huge. The techniques of DeepDream and Gradient Class Activation Mapping proposed here help us to solve the issue, which directly meets the flaw. The advantages that we can extract from these validation techniques are that they can assist us in comprehending whether our model has learned the fundamental orientations in our dataset properly, whether the training procedure should be re-evaluated (Soni et al. (2021) Deep learning-based approach to generate realistic data for ADAS applications. In: 2021 5th international conference on computer, communication and signal processing (ICCCSP) [ 5 ]), and also help us understand whether we may be in need to collect furthermore data for improved performance.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-5292-0_11,en,A Comprehensive Review of IoT-Based IDS Using Intelligence Technique,OriginalPaper,"The Internet of Things (IoT) is a collection of connected computing devices that includes several of our everyday gadgets which allow data to be transferred over the network. The IoT system has its application in various fields including, transportation, smart home, hospitals, smart grid, etc. The ability of devices connected to the web makes them exposed to multiple security intrusions and affects the security traits of the system. Hence, it is vital to investigate intrusion techniques in the IoT context to prevent or identify these intrusions. The primary focus of this review is on intrusion detection systems (IDS) for the IoT system. Therefore, this paper presents a comprehensive review of the latest IDS schemes for the IoT system designed using intelligence techniques, including machine learning, deep learning, and bio-inspired learning. The issues and challenges faced by the IoT-based IDS are presented. Finally, the comparative study and discussion on reviewed IDS scheme are described.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-6379-7_14,en,"Nature-Inspired Computing Techniques in Drug Design, Development, and Therapeutics",OriginalPaper,"Drug design and development is a long process that consumes lots of time and money. The process divides into different stages, where the most important step is to evaluate the safety and efficacy of the drugs after finding the best lead compounds. Several in-vitro methods have been developed to evaluate the toxicity of the drugs during the preclinical screening stage; however, these assays are super expensive and costly. However, the safety assessment of the drugs is very important to develop a very accurate and precise therapeutic application. Therefore, it is needed to design new alternative methods such as computational methods for high throughput drug designing and development for very precise and effective therapeutic applications. Development of highly advanced nature-inspired intelligent computing (NIC) technologies such as particle swarm optimization (PSO), ant colony optimization (ACO), DNA computing connected with the artificial immune systems, and machine learning helps in accurate drug designing, big data processing, integration of big data for the development of prediction models, disease-based image processing to evaluate pre- and post-drug effects on biological systems, etc. In this chapter, we provide a deep insight into the usage of nature-inspired intelligent computing technologies in drug design, development, and therapeutics.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-981-19-1607-6_74,en,Medical X-Ray Image Classification Employing DCGAN and CNN Transfer Learning Techniques,OriginalPaper,"Over the decades, a typical imaging test that has been used is an X-ray. It allows doctors to see into the body without an incision. As a result, an X-ray can aid in diagnosing, monitoring, and treating a variety of medical disorders by detecting diseases beforehand. Among the diseases, pneumonia got major heed because of its intensity. As the lungs are the most vulnerable part of the body when it comes to pneumonia, doctors rely on the chest X-ray to diagnose the disease. In this research, we have worked on the X-ray images to discern pneumonia using our custom CNN model and different types of transfer learning models and manifested a comparison of those methods in terms of their ability to detect the disease. Furthermore, we performed generative adversarial networks (GAN) with deep convolutional layers to generate and merge a new training dataset using existing image data. Then, we executed the models anew after acquiring a new artificial dataset. Before using GAN, we got accuracy of 94%, 94%, 73%, 73%, 96%, 97%, and 94% in Custom CNN, InceptionV3, ResNet50, EfficientNetB0, VGG16, DenseNet201, and Xception, respectively. However, we observed improved accuracy from all models applying GAN except for DenseNet201. Moreover, VGG16, DenseNet201, and custom CNN acquired the higher accuracy overall.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09909-0_12,en,A Statistical Approach to Analyzing Turkish Morphology,OriginalPaper,"Morphological analyzers are essential components of Turkish language processing pipelines due to complex and rich morphology of Turkish language. Previous work usually focuses on two-level description of Turkish morphology, thus leads to lexicon and two-level rules oriented FST implementations. However, two-level based implementations are not robust to spelling errors and new loan words entering the Turkish lexicon. In this paper, we introduce a statistical approach to analyzing Turkish word forms by training and comparing two seq2seq models on our annotated dataset. We approach analyzing Turkish word forms as a machine translation problem, where the source sequence is a Turkish word form and the target sequence is a sequence of morphemes. Evaluating on three testsets of informal written language word forms, we show that our approach offers a robust approach to analyzing the Turkish words and proposes a strong baseline for sequential modeling of Turkish morphology.","['Engineering', 'Robotics and Automation', 'Robotics', 'Engineering Design', 'Biomedical Engineering and Bioengineering']"
doi:10.1007/978-981-19-6513-5_4,en,Prediction of Suitable Percentage of Waste Materials for Improving Geotechnical Properties of Black Cotton Soil Using AI Approaches,OriginalPaper,"Black cotton (BC) soil consists of montmorillonite minerals. Montmorillonite is one of the reasons to show the swell and shrink behaviour of black cotton soil. The unequal settlement develops under the structures due to this behaviour of black cotton soil. Thousands of researchers and investigators conducted an experimental study to improve soil black cotton soil properties using different materials. These researchers and investigators reported that the geotechnical properties of soil can be enhanced using waste materials. Still, the determination of the geotechnical properties of soil by the experimental procedure is a cumbersome and time-consuming task. A suitable quantity of waste materials is predicted to improve the geotechnical properties of BC soil. The regression analysis, random forest (RF), support vector regression (SVR), decision tree (DT), Gaussian process regression (GPR), and artificial neural networks (ANNs) AI approaches are used to predict the suitability of waste materials in the present research work. The artificial neural network models are developed by one to five hidden layers with ten neurons. The hidden layers are selected in the range of one to five. The performance of MLR, SVR, GPR, RF, DT, LMNN_1H10, LMNN_2H10, LMNN_3H10, LMNN_4H10, LMNN_5H10 is 0.4447, 0.3593, 0.8788, 0.7296, 0.6341, 0.5025, 0.7770, 0.6320, 0.5389, and 0.3856, respectively. The GPR model is identified as an optimum performance AI model and used to predict the suitability of waste materials to improve the geotechnical properties of soil.","['Engineering', 'Geoengineering, Foundations, Hydraulics', 'Building Materials', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/978-981-19-0151-5_28,en,Channel-Based Similarity Learning Using 2D Channel-Based Convolutional Neural Network,OriginalPaper,"Object identification is one of the major aspects of computer vision. In recent years, the development the computing as well as the storage capacity has increased drastically. These breakthroughs in the technology have blessed us with various data storage technologies and computational engines. Because of the breakthrough in recent years, we are generating humongous amounts of data of which 80% of data is unstructured data and only 20% of data is structured. Unstructured data are mainly composed of images, video and as well as the natural language, i.e. text. These 80% unstructured data consist of the enormous information, but it is difficult to unravel the information contained in these data. Convolution neural network (CNN) is backbone of computer vision and deals with extracting information from the image and video, before the invention of recurrent neural network (RNN), CNN was also employed for natural language processing (NLP) task such as classification and text generation, but the specialty of CNN lies where the dataset consists of sound signals, images or sequence of frames. On Internet, we can find 60% of the unstructured dataset consists of images or sequence of image or text. Basically, image consists of the features which is the orientation of the pixels in a well-defined pattern which can be extracted by using kernel’s known as the feature maps and Maxpooling layers to extract the underlying feature present in the image to train the neural network. CNN is one of the parts of supervised learning techniques which uses labelled data, but it is difficult to label huge number of images. The similarity-based learning enables us to control the similarity percentage as well as it has minimum labelling procedure, i.e. labelling of the dataset is to be labelled 0 or 1. Similarity learning is used to compute the percentage of the features which are similar in the target image with respect to the input image. Image consists of three channels, i.e. red, green and blue channels, which is basically a 2D vector with pixel values in range of 0–255. These individual channels contribute to the features present in the images, and if we can calculate the similarity between input image and the query image, then we can be able to present the unstructured images in relation to the similarity with respect to the input image by using the channels in channel-based CNN tower.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/978-3-031-15191-0_13,en,Classification of Credit Applicants Using SVM Variants Coupled with Filter-Based Feature Selection,OriginalPaper,"For financial institutions and in particular banks, Credit risk management is one of the most crucial issues in financial decision making. Accurate credit scoring models are extremely important for financial agencies to classify a new applicant, in order to decide whether to approve or reject its credit application. This paper presents a credit scoring model based on SVM variants ( C -SVM, $$\nu $$ ν -SVM) combined with two filter feature selection methods, to enhance the preprocessing task and models performances. In this study, a public credit dataset namely Australian has been used to test our experiments. Experimental results indicate that our methods are efficient in credit risk analysis. They make the assessment faster and increase the accuracy of the classification.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security']"
doi:10.1007/978-3-030-99075-6_60,en,A Sensor Fault Identification Method Based on Adaptive Particle Swarm Optimization Support Vector Machine,OriginalPaper,"Accurate identification of fault types is an important part of sensor fault diagnosis. Therefore, a sensor faults identification method based on Adaptive Particle Swarm Optimization Support Vector Machine (APSO-SVM) is proposed in this paper. Firstly, the appropriate Time-domain parameters are extracted from the fault data to realize feature extraction and dimension reduction. Then the Particle Swarm Optimization (PSO) algorithm is improved by adjusting the particle velocity with weight and introducing mutated particles, so as to improve the optimization ability of the algorithm and to optimize the parameters of Support Vector Machine (SVM). Finally, the optimized model is used to identify the sensor faults, and compared with other advanced algorithms, the results show that the proposed method can identify the sensor faults more accurately.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-981-19-1607-6_11,en,Advanced Processing and Classification of Plant Disease,OriginalPaper,"Weather, pests and various other factors cause a lot of crop yield to decrease. Crop losses are more in countries which are tropical and, knowledge and investments in crop health management is very less (Sufola Das Chagas Silva Araujo, Meenakshi Sundaram Karuppaswamy. Comparative Analysis of K-Means, K-Nearest Neighbor Segmentation Techniques, IEEE (2016) [15]). Manual detection are taxing as our eyes have to perceive the indications of the disease based on shape and color. A model of Guntur-4 variety of chili plants was developed that can classify particular diseases. There has been made use of multiple models to train and detect such diseases to figure out which model is more accurate. Each model uses object detection techniques to recognize certain features on leaves and categorize them into different diseases. Different infected leaf images of whitefly, Yellowing, Curled, and Healthy were collected and tested on different models built to try and find which model is best suited for this particular data set. Time complexity, accuracy, and resource usage were computed to build the best automatic leaf image disease detection model.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-1-0716-2617-7_17,en,Machine Learning Using Neural Networks for Metabolomic Pathway Analyses,OriginalPaper,"Elucidating the mechanisms of metabolic pathways helps us understand the cascade of enzyme-catalyzed reactions that lead to the conversion of substances into final products. This has implications for predicting how newly synthesized compounds will affect a person’s metabolism and, hence, the development of novel treatments to improve one’s health. The study of metabolomic pathways, together with protein engineering, may also aid in the extraction, at a scale, of natural products to be used as drugs and drug precursors. Several approaches have been used to correlate protein annotations to metabolic pathways in order to derive pathways directly related to specific organisms. These could range from association rule-mining techniques to machine learning methods such as decision trees, naïve Bayes, logistic regression, and ensemble methods. In this chapter, we will be reviewing the use of machine learning for metabolic pathway analyses, with a step-by-step focus on the use of deep learning to predict the association of compounds (metabolites) to their respective metabolomic pathway classes. This prediction could help explain interactions of small molecules in organisms. Inspired by the work of Baranwal et al. (2019), we demonstrate how to build and train a deep learning neural network model to perform a multi-label prediction. We considered two different types of fingerprints as features (inputs to the model). The output of the model is the set of metabolic pathway classes (from the KEGG dataset) in which the input molecule participates. We will walk through the various steps of this process, including data collection, feature engineering, model selection, training, and evaluation. This model-building and evaluation process may be easily transferred to other domains of interest. All the source code used in this chapter is made publicly available at https://github.com/jp-um/machine_learning_for_metabolomic_pathway_analyses .","['Life Sciences', 'Bioinformatics']"
doi:10.1007/978-3-030-94285-4_2,en,On-Phone CNN Model-Based Implicit Authentication to Secure IoT Wearables,OriginalPaper,"The connectivity of smart technologies, such as smartphones and smart wearables, is ever-increasing with the emergence of the internet of things (IoT). This technological advancement makes it possible to serve emerging applications, such as financial transactions, healthcare check-ups, and property access, easily through smart wearables, such as Apple Watch. This also presents a new vulnerability as hackers have more opportunities to attack users via the wearables. As the current knowledge-based wearable authentication schemes, such as passwords, PINs, or pattern locks, are overwhelming for users, we need an authentication system that can validate a user implicitly, i.e., without the need for active user interaction. In this work, we present an authentication system for the wearables leveraging the sensing and computation power of smartphones and IoT connectivity. We develop a smartphone application ( TFL Auth app) using the TensorFlow Lite framework and an on-phone convolutional neural network (CNN) model that listens to a user’s breathing patterns through the microphone and verifies the user’s identity in real-time before sending the acceptance/rejection notification to a paired wearable that we want to secure. From a detailed analysis, we are able to achieve an average accuracy of 0.92 ± 0.01 using the Mel-frequency cepstral coefficients.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Professional Computing']"
doi:10.1007/978-981-19-2412-5_15,en,A QSSA Optimized Fractional-Order Controller for Improving Transient Response in AC Autonomous Microgrid VSC System,OriginalPaper,"As part of the primary control of the autonomous microgrid (MG) voltage source converter (VSC) system, the inner loop voltage and current controller help to provide a fast transient response for frequency and voltage restoration. This paper proposed a fractional-order proportional plus integral (FOPI) controller for effective voltage and frequency management in autonomous MG VSC systems. Because of their fractional features, FO controllers make the VSC system more resilient than traditional PI controllers. Along with the conventional PI controller K p and K i gains, the FOPI controller has an extra edge of flexibility “ λ ”. The FO controllers parameters are tuned using the quasi-oppositional salp swarm algorithm (QSSA), a novel metaheuristic process. A droop controller that utilizes the dynamic change in the droop coefficients is also used to condense power transient and enhance the systems dynamic response while operating in the islanded mode. Furthermore, simulating the MG system in MATLAB Simulink, the dynamic performance of the proposed controllers is validated for the various abrupt change in system condition such as different initial load switching conditions for unequal ratings of distributed generation inverter and the effect of momentary fault (i.e., double line to ground fault). This paper compares the performance of conventional droop with PI controllers in inner voltage and current controllers and the suggested QSSA optimized FOPI controller with modified droop controller for autonomous MG systems. The simulation findings showed that the MG performance has improved using the proposed controller.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Renewable and Green Energy', 'Energy Storage']"
doi:10.1007/978-3-031-08815-5_12,en,Neural Inspired Ant Lion Algorithm for Resource Optimization in Cloud,OriginalPaper,"There are various task scheduling models that are in use currently, but the one which we will be focusing on is the ANN (Artificial Neural Network) based model. This model was set up to estimate the task execution status for resource allotment among the candidates. An ANN-based model makes use of various scheduling algorithms to find the best results possible in terms of quality of service (QoS), total cost, service satisfaction, etc. Through our paper, we are simulating various task scheduling algorithms in a virtual environment and comparing their efficiency based on the results we obtain from these simulations. While our focus will be on an emerging meta-heuristic optimization algorithm called the Ant lion Algorithm, we are also running simulations for the Whale Optimization algorithm, and the Genetic Algorithm. For the prediction and allocation of cloud resources we use the Ant Lion Optimization Algorithm. Artificial Neural Network (ANN) is used for resource allocation. We discuss the results that depicts we get better results compared to the existing methods with proper allocation of resources and minimal cost.","['Engineering', 'Computational Intelligence', 'Transportation Technology and Traffic Engineering']"
doi:10.1007/978-3-031-19620-1_48,en,Development and Research of a Hybrid Algorithm for Placement of Elements of Digital Computing Equipment,OriginalPaper,"In the article, the problem of placing different-sized elements is discussed. The analysis of the current state of research on the topic has been completed, the main problems and prospects for the development of existing methods and technologies have been identified. The necessity and relevance of developing new effective approaches to solving problems of physical synthesis of elements of digital computing devices is noted. The problem statement, as well as the main constraints and optimization criterion, are formulated. A model of a hybrid algorithm for solving the placement problem has been developed. A model of a parallel multipopulation genetic algorithm is proposed. Mechanisms for parallelization of the computational process have been developed. A modified procedure for performing a migration operation to perform decision exchange between populations is proposed. A procedure that allows implementing the principle of multithreading at the local level, when calculating the values of objective functions, has been developed. The principles of operation and the structure of the fuzzy control block are described. The scheme of operation of the fuzzy logic controller is given. A model of a multilayer neural network that implements the function of a neuro-fuzzy control block is proposed. The characteristics of the current population used to evaluate the dynamics of the search for the optimal solution are determined. The control parameters of the genetic algorithm are selected. The proposed hybrid algorithm is implemented as an application program. A series of computational experiments to determine the effectiveness of the developed algorithm and the choice of optimal values of control parameters was carried out.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18050-7_26,en,Lightweight Models in Face Attribute Recognition: Performance Under Oclussions,OriginalPaper,"In this paper we will study the performance of several light-weight convolutional neural networks with respect to state-of-the-art models for facial attribute classification. Specifically, we will try to determine the attributes of gender, age and ethnicity. There are many models based on lightweight architectures, from which we have chosen MobileNet and EfficientNet. The results obtained match or improve the state of the art in gender and race, achieving good results in age classification as well. On the other hand, we have performed a comparative study of these classifications with respect to two datasets. The first dataset is UTK-Face which contains the facial images aligned and a higher number of individuals, having a lower total number of samples, while the second dataset is VGG-Face2 which has a much higher total number of samples, having fewer individuals than UTK-Face and with a lower quality facial alignment.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-10784-9_22,en,Facility Layout Design Optimization of Wing Assembly of Unmanned Aerial Vehicle Based on Particle Swarm Optimization,OriginalPaper,"The complex structure, a large number of parts and diverse assembly relations of UAV (Unmanned Aerial Vehicle) wing affect its assembly efficiency, which is an urgent scientific problem to be solved. In this paper, based on the original data of facility Facility layout in wing assembly workshop, Systematic Layout Planning method was used to determine the comprehensive relationship between the work units. Then, a multi-objective optimization Optimization mathematical model was established from the perspective of minimizing the total cost of logistics operations and maximizing the degree of close relationship with non-logistics between working units. Finally, the particle swarm optimization Optimization algorithm Algorithms is used to solve the optimization Optimization model of facility Facility layout, and the layout scheme that can improve the efficiency of unmanned aerial vehicle wing assembly workshop is found. The research results not only provide technical support for the layout characteristics of UAV wing assembly workshop, but also provide specific ideas and methods for other similar production enterprises.","['Engineering', 'Computational Intelligence', 'Robotics and Automation', 'Transportation Technology and Traffic Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5845-8_26,en,Image Classification Using Quantum Machine Learning,OriginalPaper,"When quantum algorithms are used in machine learning systems, it is referred to as “quantum machine learning.” An approach known as “quantum-enhanced machine learning” utilizes a quantum computer to evaluate classical data to boost machine learning. Data may be processed and stored more quickly and efficiently with the help of quantum machine learning. Using neural networks as analogies for physical systems is an important part of quantum machine learning. This paper summarizes the CIFAR-10 dataset. For the dataset, “five training batches and one testing batch” are used to divide the ten thousand photographs. One thousand images from each class are randomly selected for inclusion in the test batch. Even though each batch comprises all of the remaining photographs, some batches have a greater number of images from a particular category. It is estimated that each training batch contains around 5000 photographs. This section includes an evaluation of the classifier’s overall performance. Quantum neural networks describe “a parameterized quantum computational model best” implemented “on a quantum computer” (QNN). Third-party libraries such as PyTorch, Qiskit, and matplotlib are frequently loaded into the program. PyTorch is a popular option for GPU and CPU-based Deep Learning applications because it is built on Torch rather than merely Python. All of your quantum computing needs may be met by Qiskit, a Python library. Importing it will be necessary after the system is installed. Creating static, animated, and interactive graphics is easy with Matplotlib, a Python toolkit. To begin, we need to identify the quantum layers that will make up the circuit’s structure.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20601-6_68,en,A New Framework for Multi-objective Route Planning in Smart Cities,OriginalPaper,"Route planning is a crucial map navigation feature. Nevertheless, standard commercial map programs only offer optimum routes based on a particular objective, such as time, distance, or other metrics, and disregard the safety objective. Therefore, there is a need for a multi-objective criterion to avoid accidents and to be able to locate not only a short route but also a safe route. This paper proposes a new framework for multi-objective route planning in smart cities. The framework is evaluated by multiple symmetric travelling salesman problem (TSP) instances with varying scale sizes. Several assessment measures are employed to evaluate the proposed framework. The experimental findings demonstrate that the proposed multi-objective framework outperforms and achieves the safety goals compared with other alternatives that considered only the shortest path objective. These findings reveal the robustness of the proposed framework that could be used as a reliable tool for improving the safety and management of road traffic.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4960-9_70,en,An LSTM-Based Word Prediction in Bengali,OriginalPaper,"In this paper, Bengali text information has been utilized for predicting the next word contingent based on the previous one. To do that, one should consider two key aspects such as the natural language processing (NLP) stage and the word predicting stage. When both work together, the system gets a new predicted word that is relevant to the previous word. For achieving such correct predicted words, long short-term memory (LSTM) has been used which is best known for its memory management. LSTM embeds the input words and fits them into the model, then after successful training of the model, it can predict the next word from a given sentence. The user can also initialize the number of predicted words. This paper gives an overview of word prediction for the Bengali language based on LSTM and describes the database integration and proposed approach obtained 97.60% accuracy.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-3-031-11154-9_4,en,Unsupervised Domain Adaptation Approach for Liver Tumor Detection in Multi-phase CT Images,OriginalPaper,"For computer-aided diagnosis, automatic and accurate liver tumor detection in multi-phase CT images is essential. Nowadays, deep learning has been widely used in various medical applications. Deep learning-based AI systems require a large amount of training data for model learning. However, acquiring sufficient training data with high-quality annotation is a major challenge in Healthcare. As a result, deep learning-based models face a lack of annotated training data problem. While the generalization of a label-rich training domain (source) to a new test domain (target) causes a domain shift problem in deep learning-based models. To solve the lack of training data and domain shift problem, domain adaptation-based methods have recently been developed as a technique to bridge the domain gap across datasets with different feature characteristics and data distributions. In this chapter, we have proposed domain adaptation-based technique for liver tumor detection in multi-phase CT images. We discuss the domain-shift problem in different phases of multiphase liver CT images and introduce our domain adaptation technique for multi-phase CT images. We have used PV phase images to learn a model and applied the learnt model to ART and NC phase images (i.e. different domains) by adapting the domain knowledge. To address the domain gap between the different phases of CT images, we employ adversarial learning scheme using an anchor-free object detector. Further, we propose to use the maximum square loss for mid-level output feature maps to improve the performance. Our method does not require separate object-level annotations for each phase of Multiphase CT image while training. The results of the experiments show that models trained using our proposed domain adaptation technique perform much better than those trained in normal setting.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Health Informatics']"
doi:10.1007/978-981-19-3266-3_45,en,Particle Swarm Optimization Based Search for Optimal Operating Condition in WEDM Operation of A286 Superalloy,OriginalPaper,"Attaining improved level of process performance in wire-electro discharge machining (WEDM) at any particular parametric condition is always a complicated task due to the inherent nature of the process. Endeavour to improve one performance measure is always done at the cost of the remaining performance measures. Amongst the performance measures, material removal rate (MRR) and surface roughness (SR) are of interest in this paper and they are contradicting in nature. Thus, the present paper targets to optimize the WEDM performances concurrently using a popular solvere. Particle swarm optimization (PSO) solver for an Iron-based superalloy namely A286 superalloy. To pursue the optimization procedure, the two objectives are clubbed together to form a single objective by the addition of weights. The optimum MRR and SR obtained are 19.90 mm 2 /min and 3.49 µm respectively. Additionally, field emission scanning electron microscopy (FESEM) is employed to throw light on the morphological characteristics of the machined surfaces obtained by machining at low pulse energy (LPE) setting and at high pulse energy (HPE) setting.","['Engineering', 'Industrial and Production Engineering', 'Engineering Design', 'Industrial Chemistry/Chemical Engineering']"
doi:10.1007/978-3-031-13150-9_42,en,Cyber Security Intruder Detection Using Deep Learning Approach,OriginalPaper,"Intrusion detection systems (IDS) are among the most promising approaches for securing data and networks; through the years, numerous categorization algorithms have been utilized in IDS. In recent years, as the alarming increase in computer connectivity and the substantial number of applications associated with computer technology have increased, the challenge of cyber security is constantly rising. A proper system of protection for numerous cyber-attacks is also required. This is how incoherence and attacks in a computer network are detected and IDS developed, which could play a possible role in cyber security. The authors used the CICIDS2017 dataset to meet this objective. It is the 2017 set of the Canadian Cyber Security Institute. The authors propose an IDS based on the deep learning technique to increase safety. The purpose was to use a neural network classifier to predict the network and web attacks.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Management']"
doi:10.1007/978-3-030-96025-4_4,en,Optimal Placement of PMUs in Smart Power Systems,OriginalPaper,"This chapter presents a multi-stage method to make the power system complete observability by the optimal placement of phasor measurement units (PMUs) taking into account the minimum availability of PMUs measuring channels. In order to solve the optimization problem, a two-stage optimal method is introduced with and without considering zero injection buses (ZIBs). In stage-1, the ant colony optimization (ACO) algorithm is applied to find the optimal number and locations of PMUs considering measuring channels and maximize the measurement redundancy (MR) at normal operating condition as well as emergency conditions such as any single line or PMU outage. In Stage-2, the reduction strategy (RS) is proposed to reduce the number of PMUs measuring channels with keeping the complete observability. The proposed method is tested on different standard test systems, namely IEEE 14-bus, 24-bus, 30-bus, New England (NE) 39-bus, 57-bus and 118-bus. In addition, the application of the proposed method is employed on a real power system of the West Delta Network (WDN) as a part of the Unified Egyptian Network (UEN) which is considered as a test system. To prove the robustness and the superiority of the proposed method, the results are compared with other optimization techniques. This comparison show the great capability of the proposed method to find the optimal PMU placement for significant saving in the total cost with more accuracy and efficiency, especially with increasing in the power system sizing.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Systems', 'Renewable and Green Energy']"
doi:10.1007/978-3-031-16203-9_8,en,The Comprehensive Model of Using In-Depth Consolidated Multimodal Learning to Study Trading Strategies in the Securities Market,OriginalPaper,"The paper describes the relevance of machine learning methods, namely training with reinforcement, to the problems of predicting financial time series. An overview of existing applications based on machine learning in the issues of financial market forecasting is presented. The reasons for the popularity of my research topic are highlighted both from the scientific (increasing the number of publications on issues relevant to the research topic over the past five years) and from the practical side. The analysis of scientific works, the subject and purpose of which are related to the issues and objectives of my research and their features are presented. The main problems associated with the problem of predicting stochastic time series are identified. According to the analysis, the purpose of work is defined, and also the list of tasks for the achievement of the set goal is made. The article is devoted to studying the use of the ensemble of neuro-learning networks with the strengthening of the securities trading market. The practical significance of the work is to use the model of efficient distribution of investments in the market. This paper will explore a set of models that use in-depth consolidated learning to explore trading strategies to maximize return on investment. The potential of using acting-critical models as an ensemble has been studied. Models such as Proximal Policy Optimizer (PPO), Advantage Actor-Critic (A2C) and Deep Determinist Police Gradient (DDPG) were used to teach trading strategy. To adapt the model to different situations, analyzes are analyzed according to three algorithms: the Dow Jones average and a portfolio that minimizes fluctuations in the Charpy ratio by balancing risk and return. The article compares ensembles by the method of fixing deep neural networks. To optimize the balance of risk and profit, the indicators of the ensemble model are analyzed. The ensemble and three-component models that apply well to market collapse conditions are considered. The models have learned to use the turbulence index for early stock sales to minimize losses during a stock collapse. The turbulence index threshold is demonstrated using ensemble models to regulate risk avoidance.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-030-93236-7_45,en,Autonomous Multiple Damage Detection and Segmentation in Structures Using Mask R-CNN,OriginalPaper,"There are approximately 730,000 road bridges in Japan. As of 2020, more than 23% of them have been aged over 50 years or more and the percentage will go over 50% within the year 2030. Over the last decade there are numerous bridge accidents all over the world that have cost both monetary values and human lives significantly. Thus, regular inspection of bridges is very important to check the overall condition. However, the inspection is predominantly maintained manually which depends on person’s expertise and often the task is cumbersome, expensive and error prone. Over the years, different deep learning-based techniques, such as Convolutional Neural Network (CNN), have been utilized to detect the damages automatically. However, most of them concentrate on damage detection and often been used for single class of detection only. Instance segmentation is a method where each object is detected as separate instance and by adopting a Region-based CNN model, such as Mask R-CNN, the instances can be shown separately. Though instance segmentation has been applied extensively for the detection of common objects in the real world, the application for multiple structural damage detection is very limited so far. Specially, the training and testing of the R-CNN model for multiple structural damage detection is different and challenging than common objects. This study is a step towards the feasibility and application of instance segmentation for multiple damage detection in structures and to evaluate the feasibility for real time detection with complex background.","['Engineering', 'Civil Engineering', 'Vibration, Dynamical Systems, Control', 'Mechanical Engineering', 'Structural Materials', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-4703-2_1,en,A Multitask Learning Approach for Chinese National Instruments Recognition and Timbre Space Regression,OriginalPaper,"Musical instrument recognition is an essential task in the domain of music information retrieval. So far, most existing research are focused on western instruments. In this research, we turn to Chinese national instruments recognition. First, a dataset containing 30 Chinese national instruments is created. Then, a well-designed end-to-end Convolutional Recurrent Neural Network is proposed. Moreover, we combine instrument recognition with instrument timbre space regression using a multitask learning approach to improve performances of both tasks. We conduct experiments in instrument recognition and timbre space regression to evaluate our model and multitask learning approach. Experimental results show that our proposed model outperforms previous algorithms, and the multitask approach can further improve the results.","['Engineering', 'Signal, Image and Speech Processing', 'Engineering Acoustics', 'Mathematics in Music', 'Music']"
doi:10.1007/978-3-031-16035-6_6,en,A Smart Video Surveillance System for Helping Law Enforcement Agencies in Detecting Knife Related Crimes,OriginalPaper,"With recent technological developments, criminal investigation has witnessed a revolutionary change in identifying crimes. This has empowered Law Enforcement Agencies (LEAs) to take benefit of such revolution and build a smart criminal investigation ecosystem. Generally, LEAs collect data through surveillance systems (e.g., cameras); which are implemented on public places in order to recognize people behaviors and visually identify those who may form any danger or risk. In this paper, we focus on knives-related crimes or attacks that have been increased in recent years. In order to ensure public safety, it is crucial to detect such type of attacks in an accurate and efficient way in order to help LEAs in reducing potential consequences. We propose a smart video surveillance system (SVSS), which is based on a modified Single Shot Detector (SSD) and is combined with InceptionV2 and MobileNetV2 models. The proposed system is believed to enable LEAs to analyze big data collected from sensor cameras in a real-time and to accurately detect knives-based attacks. Experimental result show that SVSS can achieve better results in real-life scenario in terms of obtaining rapid and accurate attack warnings.","['Engineering', 'Computational Intelligence', 'Data Engineering']"
doi:10.1007/978-981-19-6774-0_16,en,Relationship Between Index Properties and CBR of Soil and Prediction of CBR,OriginalPaper,"The California bearing ratio (CBR) of soil is the ratio of test load to standard load. The California bearing ratio depends on several factors, i.e., liquid limit (LL), plasticity index (PI), plastic limit (PL), maximum dry density (MDD), and optimum moisture content (OMC). A relationship is developed between LL, PI, PL, MDD, and OMC with soaked CBR. The relationships are mapped using simple linear regression analysis. The correlation coefficients are also calculated for CBR. The experimental procedure to determine the CBR of soil is cumbersome and time-consuming. The California bearing ratio of soil is predicted using an artificial neural network (ANN), decision tree (DT), Gaussian process regression (GPR), support vector regression (SVR), and random forest (RF) AI approaches in the present research work. The performance of ANN, DT, GPR, SVR, and RF is 0.9736, 0.9052, 0.9468, 0.7502, and 0.9292, respectively. The artificial neural network model is identified as the best architecture model based on the model's performance, and soaked CBR of soil is predicted. The actual vs predicted CBR curve is plotted to determine the correlation coefficient, and the correlation coefficient of predicted CBR is 0.9731 determined. It is concluded that the LMNN_CBRs model has the potential to predict the soaked CBR of soil.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Environment, general', 'Geoengineering, Foundations, Hydraulics']"
doi:10.1007/978-981-19-4193-1_41,en,Path Segmentation for Visually Impaired People Using U-Net Architecture,OriginalPaper,"According to WHO, at least 2.2 billion people are suffering from some type of visual impairment, and the number is rising continuously. So, the research to assist the visually impaired person is gaining much importance nowadays. So far, there are many assisting methods like white cane, guide dog and several electronic travel assist (ETA), but they all come with various limitations. To overcome these limitations, we have proposed an assistance system to help the visually impaired person in low structured environment. The system will capture the images from the low structured environment with a camera. The image will be processed using a GPU at backend which in turn segment the path from the image with the help of artificial neural network and will provide the appropriate feedback for the visually impaired person. This paper will present the segmentation of the traversing path in the captured low structured environment images using the artificial neural network. A dataset is formed, and UNET architecture is evaluated. The optimized architecture is managed to segment the image with the IOU score of 0.9012 and can also perform real-time segmentation with a frame rate.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3998-3_4,en,Summary of Path Planning Approaches of Multiple UAVs,OriginalPaper,"At present, the multiple UAVs formation is more and more widely used in military and civil fields in modern complicated environment, and the path planning system will play a key role. Both the modeling and solving are important for the path planning problem. In this paper, the path planning approaches of multi-UAVs formation are summarized. Firstly, the development status of typical mission planning projects at home and abroad is introduced, and the development context of the system is combed. The mission planning is divided into task allocation and path planning. Then, the cooperative path planning approach of multi-UAVs formation is analyzed to explore the difficulties of cooperative path planning, and the characteristics of various approaches are summed up, such as the advantages and disadvantages. Finally, the path planning problems worthy of further research in the future are prospected. A comprehensive grasp of path planning will help us to engage in innovative research in related fields.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-5845-8_27,en,An Efficient Algorithm for Multi Class Classification in Deep Neural Network,OriginalPaper,"In comparison to other machine learning techniques, deep neural networks are effective in classifying non-linearly separable data. Because of its simplicity, contemporary gradient-based algorithms such as momentum Stochastic Gradient Descent (SGD) are commonly employed in Deep Neural Networks (DNN). However, the process of convergence is slowed by the choice of an appropriate learning rate and the local minima problem. To address these issues, this research proposes a unique approach for training DNNs called Simulated Annealing Based Gradient Descent (SAGD), which involves optimizing weights and biases. The SAGD technique optimizes the function by combining gradient information with the simulated annealing notion. The learning rate does not need to be manually adjusted with this method. Instead, using the simulated annealing approach, the learning rate is modified automatically for each epoch. The approach is tested utilizing VGG16, ResNet 18 and InceptionV3 architectures on typical multi-class classification data sets such as Iris, MNIST, and CIFAR10. The performance of SAGD and other state-of-the-art gradient descent optimization methods is compared, and it is demonstrated that SAGD performs comparably to existing gradient descent methods.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3951-8_51,en,Optimal Planning of Renewable-Based Microgrid Using Bald Eagle Search Algorithm,OriginalPaper,"In this paper, the operational planning of microgrid consists of different type of distributed generators has planned to meet the load demand. The objective is to minimize the operating cost and output emission, which are mainly dependent on the type of distributed generators used in the microgrid. Therefore, load dispatch has been performed to evaluate the minimum operating cost and output emission, operating under their respective limits. For multi-objective optimization, the bald eagle search algorithm has been used to determine the best-compromised solution between operating cost and output emission. The bald eagle search algorithm is a new metaheuristic approach inspired by the behaviour and foraging methods of bald eagles, to hunt their food.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16868-0_12,en,Architecture Design for Analyzing Hyperspectral Images,OriginalPaper,"Denoising images is a key part of the process of images. The hyperspectral image (HSI) has three dimensions in addition to the natural 2D image to display spectral and spatial information. In forestry [ 2 , 2 ], agriculture, and urban planning, HSIs are commonly used. However, there are impacts with the multi-detector utilized to create the HSIs because of the harsh space environment, resulting in the noise of HSIs. The accuracy of consequent work, such as classification tasks [ 3 ], will be affected by the corrupted hyperspectral data with noise. As a result, in recent years have been an increase in interest in HSI denoising [ 4 , 5 ]. Numerous methods, for example the Tenser-SVD [ 6 ] as well as the K-singular value decomposition [ 7 ], have been proposed. In general, the HSI denoising techniques may be categorized into three major groups.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16489-7_11,en,"Rethinking the US Strategic National Stockpile for Future Pandemics with Inventory, Capacity, and Capability",OriginalPaper,"The response to COVID-19 as a public health emergency raised questions about preparedness against future pandemics. The US Strategic National Stockpile (SNS) with medically critical items such as ventilators and personal protection equipment for major public health emergencies proved to be inadequate in the first half of 2020. We seek to address how governments or other disaster management organizations should modify their stockpile approach for a more robust response to disasters than was the case in 2020. To this end, we argue that a “strategic reserve” against rare public health emergencies must not only have inventory but also backup capacity and standby capability. With a highly skewed “demand” distribution reflecting a rarely occurring pandemic or other disasters, we present a three-tiered approach comprising stockpile inventory as the first tier, backup capacity as the second, and standby capability to manufacture as the third.","['Business and Management', 'Supply Chain Management', 'Risk Management', 'Operations Research/Decision Theory', 'Operations Management']"
doi:10.1007/978-981-19-5217-3_74,en,Automatic Object Detection of Construction Workers and Machinery Based on Improved YOLOv5,OriginalPaper,"Automatic detection and localization of workers and machinery on construction sites through surveillance video is important to supervise on-site safety and construction process, which could develop civil construction management and services. However, it is difficult to detect all instances due to the extremely complex construction environment and numerous multi-scale objects. This paper proposes an improved YOLOv5 model to automatically detect and localize construction workers and 11 common types of construction machinery. Firstly, use the bidirectional feature pyramid network (BiFPN) layer for better multi-scale feature fusion ability; Secondly, 3 × 3 convolution layer is replaced by RepVGG block, which shows favorable accuracy-speed trade-off. The experimental results indicate that the mAP (mean Average Precision) of our proposed method is 87.32%, which is 2.12% higher, and inference time reduce to 5.7 ms per frame.","['Engineering', 'Civil Engineering', 'Public Policy', 'Arts']"
doi:10.1007/978-981-19-4193-1_42,en,Segmentation of Sidewalk for Visually Impaired Using Convolutional Network U-Net,OriginalPaper,Image segmentation can play the significant role in helping the visually impaired people to walk freely. We are proposing image segmentation on our custom dataset of tactile paving surface or blind sidewalk. The underlying model for the image segmentation is U-Net. We have used intersection over union (IoU) as a metric to know how our model is performing. We have achieved IoU score of 0.9391.,"['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-030-92989-3_12,en,Radioactive Source Localization Method for the Partially Coded Field-of-View of Coded-Aperture Imaging in Nuclear Security Applications,OriginalPaper,"Coded-aperture imaging (CAI) is an attractive collimator-based technique for the remote localization of radioactive sources in nuclear security applications. Such systems have high angular resolution and sensitivity, which make them well suited for reconstructing the detailed distribution of radioactive sources, but with limited field-of-view (FOV). Especially the sources within the partially coded field-of-view (PCFOV) of a coded-aperture camera will cause false reconstructed hotspots due to the geometric configuration of the collimator and detector. In this chapter, we introduce a detection and localization method for the radioactive sources within the partially coded field-of-view of a coded-aperture imager in nuclear security applications. The theory of coded-aperture imaging is introduced. The principle of the false reconstructed hotspots caused by the partial encoding effect is presented and its influence is experimentally studied with gamma sources. A deep neural network-based method is proposed and converts the source region identification to an image recognition problem, which can accurately identify and localize the radioactive sources within the PCFOV of a coded-aperture imager in real-time.","['Engineering', 'Circuits and Systems', 'Biomedical Engineering and Bioengineering', 'Microwaves, RF and Optical Engineering']"
doi:10.1007/978-981-19-4975-3_41,en,A Novel Arithmetic Optimization Algorithm-Based 2DOF Tilted-Integral-Derivative Controller for Restructured LFC,OriginalPaper,"During the last decade, the electrical power system has experienced several changes raised by increasing privatization and the newly deregulation policy. Further, the load–frequency control (LFC) job becomes more challenging due to the integration of non-conventional energy resources such as nuclear, wind, solar, and fuel into the power system. If the demand of consumers varies, then the voltage along with the frequency of the multiple area interconnected power system also changes. Therefore, these changes must be kept within a certain range to supply better quality of electricity to the consumers. The study in this article is focused upon the Load–Frequency Control (LFC) of a double area deregulated power system along with multiple generation sources using Tilted-Integral-Derivative Controller (TIDC) with 2DOF. LFC is the mechanism by which the power system tries to restore its nominal frequency after it has been subjected to load fluctuations. The control areas considered for this paper comprise a reheat turbine and gas unit in addition to the thermal generating unit. Considering the practical scenario of operation, an appropriate generation rate constraint (GRC) has been included for each unit. The gain parameters of the 2DOF Tilted-Integral-Derivative Controller have been optimized by Arithmetic Optimization Algorithm (AOA). Later, the supremacy of the suggested algorithm is verified among the other well-known meta-heuristic approaches such as particle swarm optimization (PSO), Teaching learning-based optimization (TLBO), and Artificial bee colony (ABC) by evaluating under the same test conditions. The dynamic response of the proposed controller to load disturbances has been compared with prevalent controller schemes to bring about the efficacy of the prospective work.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-3-031-21203-1_23,en,Time Series Predictive Models for Opponent Behavior Modeling in Bilateral Negotiations,OriginalPaper,"In agent-based negotiations, it is crucial to understand the opponent’s behavior and predict its bidding pattern to act strategically. Foreseeing the utility of the opponent’s coming offer provides valuable insight to the agent so that it can decide its next move wisely. Accordingly, this paper addresses predicting the opponent’s coming offers by employing two deep learning-based approaches: Long Short-Term Memory Networks and Transformers. The learning process has three different targets: estimating the agent’s utility of the opponent’s coming offer, estimating the agent’s utility of that without using opponent-related variables, and estimating the opponent’s utility of that by using opponent-related variables. This work reports the performances of these models that are evaluated in various negotiation scenarios. Our evaluation showed promising results regarding the prediction performance of the proposed methods.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-3-031-06780-8_7,en,Robust AI Driving Strategy for Autonomous Vehicles,OriginalPaper,"There has been significant progress in sensing, perception, and localization for automated driving, However, due to the wide spectrum of traffic/road structure scenarios and the long tail distribution of human driver behavior, it has remained an open challenge for an intelligent vehicle to always know how to make and execute the best decision on road given available sensing/perception/localization information. In this chapter, we talk about how artificial intelligence and more specifically, reinforcement learning, can take advantage of operational knowledge and safety reflex to make strategical and tactical decisions. We discuss some challenging problems related to the robustness of reinforcement learning solutions and their implications to the practical design of driving strategies for autonomous vehicles. We focus on automated driving on highway and the integration of reinforcement learning, vehicle motion control, and control barrier function, leading to a robust AI driving strategy that can learn and adapt safely.","['Engineering', 'Automotive Engineering', 'Computational Intelligence', 'Transportation Technology and Traffic Engineering', 'Signal, Image and Speech Processing']"
doi:10.1007/978-981-19-2821-5_33,en,Face Mask Detector Using Convolutional Neural Networks,OriginalPaper,"A convolutional neural network (CNN) has one or more layers and is mainly used for image processing, classification, segmentation. CNN is commonly used for satellite image capturing or classifying hand written letters and digits. In this particular project, a convolutional neural network is trained to predict whether a person is wearing a mask or not. The training is done by using a set of masked and unmasked images which constitutes the training data. The performance of the trained model is evaluated on the test dataset, and the accuracy of the prediction is observed.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6737-5_21,en,Deep Learning-Based COVID-19 Detection Using Transfer Learning Through ResNet-50,OriginalPaper,"Catering to the widespread COVID-19 pandemic, the authors aim to develop a system based on machine learning combined with the knowledge of medical science. Considering the prevailing situation, it becomes necessary to diagnose the COVID-19 at initial stages. The idea behind the described designed model is to identify the spread of infection in patients as fast as possible. The paper sketches two different approaches: K-fold cross-validation and deep network designer which are based on deep learning technology for the prediction of COVID-19 in the initial stages by using the chest X-rays. The performance evaluation of the cross-fold validation process is compared with the designed application in the deep network designer to find an effective and efficient methodology for classification which attained better accuracy.","['Computer Science', 'Computer Communication Networks', 'Computer Applications', 'Computer System Implementation', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-07322-9_52,en,Wave Propagation Modeling via Neural Networks for Emulating a Wave Response Signal,OriginalPaper,"Wave propagation in structures is generally computed by numerical methods such as finite element method, spectral element method, etc. In these numerical methods, spatio-temporal discretization of the partial differential equations is performed using a fine mesh which leads to high computation cost but precise results. A trade-off between accuracy and computational cost can be achieved by adopting deep learning-based approaches. This research demonstrates an alternative deep learning-based approach for predictive modeling of wave propagation signals within damaged structural elements. Our goal is to evaluate the wave propagation spatio-temporal solution matrix for a given crack depth and crack location within the structural element. In this framework, deep-learning-based surrogate modeling is proposed by utilizing a deep convolutional autoencoder (DCAE) to learn the wavefield representation and project it to a compressed domain called latent space. This latent space works as labels for a feed-forward neural network (FFNN) followed by DCAE. This process eliminates the need to solve the system’s governing equations each time, leading to significant savings in computational costs and making the method excellent for issues that require repeated model computations. In DCAE architecture, we integrated the squeeze-and-excitation (SE) block which works as a channel-wise attention mechanism and enhanced the performance of the model. The DCAE with SE block achieved the very good reconstruction accuracy. This deep learning-based wave propagation predictive model can be a valuable resource for generating data for a given crack depth and location, which can be used for inverse formulations and various structural health monitoring (SHM) application.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-6068-0_24,en,Ensemble Deep Learning Approach with Attention Mechanism for COVID-19 Detection and Prediction,OriginalPaper,"New coronavirus (COVID-19), which first appeared in Wuhan City and is now rapidly disseminating worldwide, may be predicted, diagnosed, and treated with the help of cutting-edge medical technology, such as artificial intelligence and machine learning algorithms. To detect COVID-19, we suggested an Ensemble deep learning method with an attention mechanism. The suggested approach uses an ensemble of RNN and CNN to extract features from data from diverse sources, such as CT scan pictures and blood test results. For image and video processing, CNNs are the most effective. RNNs, on the other hand, use text and speech data to extract features. Further, an attention mechanism is used to determine which features are most relevant for classification. Finally, the deep learning network utilizes the selected features for detection and prediction. As a result, data can be used to forecast future medical needs.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-18050-7_29,en,Identifying Places Using Multimodal Social Network Data,OriginalPaper,"Social media is an interesting source of information, specially when physical sensors are not available. In this paper, we explore several methodologies for the geolocation of multimodal information (image and text) from social networks. To this end, we use pre-trained neural network models for the classification of images and their associated texts. The result is a system that allows creating new synergies between image and text in order to geolocate information that has not been previously geotagged by any other way, which is a potentially relevant information for several purposes. Different experiments have been done revealing that, in general, text information is more accurate and relevant than images.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-3148-2_41,en,A Study for Sign Language Detection Using Deep Learning Methods,OriginalPaper,"In this paper, the image classification task for the objective of sign language has been done. We have created our own dataset and have used references of the ISL (Indian Sign Language) as well as looking into the ASL (American Sign Language), and our model will be trained on the Indian sign language. The pool of images while training and testing has been randomly sampled. We will be implementing and training from the scratch the state-of-the-art architectures such as VGGNet, ResNet, and other custom-made models. After fine-tuning and evaluating all the models, best performing model has been selected. For further process, various pre-processing techniques on the images such as flipping the images and segment capturing have been done. The resultant model classifies the input images with better performance.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-07254-3_13,en,Towards a Fleetwide Data-Driven Lifetime Assessment Methodology of Offshore Wind Support Structures Based on SCADA and SHM Data,OriginalPaper,"In recent years there has been an increased interest of the offshore wind industry to use structural health monitoring (SHM) data in the assessment of consumed lifetime and lifetime extension for an entire wind farm. In order for operators, certifying bodies, insurance entities and government agencies to agree on a lifetime extension, a commonly accepted lifetime assessment strategy with proven results is required. This paper aims to provide such an answer through a data-driven lifetime assessment approach using SHM and SCADA data. The research involves training neural network (NN) models using SCADA and SHM data to estimate the fore-aft damage equivalent moment (DEM) at the tower interface level on a 10-min basis for implementation in a data-driven lifetime assessment. The NN are trained and validated based on one instrumented turbine (the fleetleader ) and cross-validated based on another instrumented turbine. A DEM representative for the lifetime of the asset is calculated based on the 10-min DEM’s. An analysis of the NN models’ performance (error of 10-min DEM estimation in relation to DEM derived from SHM data) and accuracy (lifetime DEM error) is undertaken. The DEM representative for the lifetime of the assets is benchmarked with the as-designed DEM to assess the lifetime.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering', 'Monitoring/Environmental Analysis', 'Analytical Chemistry']"
doi:10.1007/978-981-19-3035-5_2,en,Opinion Mining of Movie Reviews Using Hybrid Deep Learning Technique,OriginalPaper,"Due to Internet, vast amount of data is generated day by day; from those data to find useful insights, there is need to identify and extract the subjective information. Today’s trends show that people are buying any products or watching any movie on Web sites, and they write the feedbacks related to that product or movie, which will be helpful to business in terms of profit. For that, the need is analysis of written reviews which will be done by sentiment analysis. It is a method which is used to gauge opinions of individuals or groups of persons related to their products or movies. This method will extract the meaningful insights from the written reviews in the form of positive, negative, or neutral. Analysis of sentiment is also known as opinion mining. In this paper, hybrid deep learning model (CNN + LSTM) is applied on IMDB movie review dataset and performs a comparison with CNN model.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Statistics, general']"
doi:10.1007/978-981-19-3391-2_21,en,MOOC-LSTM: The LSTM Architecture for Sentiment Analysis on MOOCs Forum Posts,OriginalPaper,"The massive open online courses (MOOCs) have been among the foremost energizing improvements in e-learning environment in recent days. As the number of MOOCs resources on each domain growing greatly, there is a necessity of evaluating MOOCs. Discussion forums are the key resources for MOOCS evaluation. Sentiment analysis is the famous mechanism to identify the opinion of the students on every particular MOOC. Long short-term memory architecture is used to avoid the issue of long-term dependencies in the text. In this paper, we propose a sentiment analysis system contains a new LSTM architecture and Ax hyperparameter tuner that can jointly performs well with large text for sequential analysis and sentiment classification. Proposed system is trained on two different datasets from different platforms using optimal hyperparameters. Experimental results shown that the proposed system outperforms other machine learning models in terms of accuracy and working well with different domains.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-19958-5_20,en,Transfer Learning Based Method for Classification of Schizophrenia Using MobileNet,OriginalPaper,"Schizophrenia is a serious mental disorder which makes a patient abnormal than other patient thinks he is not there and everyone is for his enemy. As a result, it is so much important to detect the disease at an early stage. If we can detect the disease at an early stage, we can make the patient’s life normal. CNN (Convolutional Neural Network) -based technique for classification of the disease is used many times. In our research, we are using two class one is normal class, another is Schizophrenia class which is used transfer learning approach for classifying Schizophrenia disease from brain MRI data. In our presented method, our technique, which is based on transfer learning theory, uses a pre-trained MobileNet method to identify brain MRI images by extracting features using the sigmoid classifier method with a mean classification accuracy of 93.95%. Our proposed method exceeds all previous strategies. We utilize the Kaggle dataset to evaluate our technique. One of the important performance indicators used in this study is precision, recall, and F-score. Our classification method got accuracy of 90.62%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3590-9_34,en,A Review of Swarm Intelligence-Based Feature Selection Methods and Its Application,OriginalPaper,"SI or swarm intelligence is considered to be one of the sound computational intelligence which deals with finding solutions for the issue related to optimization problem. The feature set is optimized by utilizing the feature selection technique, which reduces the number of features by eliminating those that are not essential or redundant. This improves the classification accuracy. This paper studies to examine the optimization/selection of significant features which is the most challenging part and it reduces the performance of algorithm time, complexity of calculations. It gives an overview of optimization techniques and their applications.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security', 'Mobile and Network Security']"
doi:10.1007/978-3-031-16174-2_7,en,Distilling Graph Neural Networks,OriginalPaper,"As shown in the previous chapters, Graph Neural Networks (GNNs) have been successfully used on many data mining tasks. Previous works mostly focus on the improvement of a single GNN model, and another idea is to design a universal framework for enhancing an arbitrary GNN. Knowledge distillation provides a general solution for this purpose, which injects the knowledge of a teacher model into a student model by aligning their outputs. In this chapter, we will introduce three typical knowledge distillation frameworks for graph neural networks.","['Mathematics', 'Graph Theory', 'Computer Science, general', 'Mathematical Applications in Computer Science', 'Mathematical Models of Cognitive Processes and Neural Networks', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-3-031-21062-4_15,en,Learning from the Past: Sequential Deep Learning for Gas Distribution Mapping,OriginalPaper,"To better understand the dynamics in hazardous environments, gas distribution mapping aims to map the gas concentration levels of a specified area precisely. Sampling is typically carried out in a spatially sparse manner, either with a mobile robot or a sensor network and concentration values between known data points have to be interpolated. In this paper, we investigate sequential deep learning models that are able to map the gas distribution based on a multiple time step input from a sensor network. We propose a novel hybrid convolutional LSTM - transpose convolutional structure that we train with synthetic gas distribution data. Our results show that learning the spatial and temporal correlation of gas plume patterns outperforms a non-sequential neural network model.","['Computer Science', 'Robotics', 'Robotics and Automation', 'Computational Intelligence']"
doi:10.1007/978-3-031-16868-0_9,en,Hybrid GA and PSO for Architecture Design,OriginalPaper,"In this chapter, a new approach based on EC is introduced for automatically searching for the optimal CNN architecture and determining whether or not to use shortcut connections between one layer and its forward layer. After that, a two-level encoding strategy is applied to a hybrid EC methodology that is composed of a GA and a PSO. This allows for the generation of both the network architecture and the shortcut connections within it. The technique is referred to as DynamicNet because to the fact that during the course of the evolutionary process, both the architecture and the shortcut connections are determined dynamically without any involvement from a human being. On three widely used datasets that have differing degrees of complexity, DynamicNet will be evaluated in comparison with one method that is based on EC and 12 methods that are considered to be state-of-the-art.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20650-4_8,en,"Wavelet Scattering Transform Depth Benefit, An Application for Speaker Identification",OriginalPaper,"This paper assesses the interest of the multiscale Wavelet Scattering Transform (WST) for Speaker identification (SI) applied in several depths and invariance scales. Our primary purpose is to present an approach to optimally design the WST to enhance the identification accuracy for short utterances. We describe the invariant features offered by the depth of this transform by performing simple experiments based on text-independent and text-dependent SI. To compete the state-of-the-art (SOTA), we propose a fusion method between WST and x-vectors architecture, we show that this structure outperforms HWSTCNN by $$7.57\%$$ 7.57 % on TIMIT dataset sampled at 8 kHz and makes the same performance in the SOTA at 16 kHz.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-07322-9_45,en,Site-Specific Defect Detection in Composite Using Solitary Waves Based on Deep Learning,OriginalPaper,"We propose a real-time non-destructive evaluation technique for defect detection in composites using highly nonlinear solitary waves (HNSWs) and a deep learning algorithm based on the convolution neural network (CNN). This technique implements deep learning to identify the presence of defects and classify the defect locations in the thickness direction of composites through HNSWs with strong energy intensity and non-distortive nature. To collect HNSW datasets for training and validation of the deep learning algorithm, AS4/PEEK composite specimens with artificial delamination are fabricated and HNSW datasets are generated from the experimental setup of a granular crystal sensor. Testing pretrained CNN based algorithms verifies the performance of detecting and classifying defects by location in composite plates.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-02063-6_19,en,Classifying COVID-19 Variants Based on Genetic Sequences Using Deep Learning Models,OriginalPaper,"The COrona VIrus Disease (COVID-19) pandemic led to the occurrence of several variants with time. This has led to an increased importance of understanding sequence data related to COVID-19. In this chapter, we propose an alignment-free k-mer based LSTM (Long Short-Term Memory) deep learning model that can classify 20 different variants of COVID-19. We handle the class imbalance problem by sampling a fixed number of sequences for each class label. We handle the vanishing gradient problem in LSTMs arising from long sequences by dividing the sequence into fixed lengths and obtaining results on individual runs. Our results show that one-vs-all classifiers have test accuracies as high as 92.5% with tuned hyperparameters compared to the multi-class classifier model. Our experiments show higher overall accuracies for B.1.1.214, B.1.177.21, B.1.1.7, B.1.526, and P.1 on the one-vs-all classifiers, suggesting the presence of distinct mutations in these variants. Our results show that embedding vector size and batch sizes have insignificant improvement in accuracies, but changing from 2-mers to 3-mers mostly improves accuracies. We also studied individual runs which show that most accuracies improved after the 20th run, indicating that these sequence positions may have more contributions to distinguishing among different COVID-19 variants.","['Engineering', 'Engineering Economics, Organization, Logistics, Marketing', 'Mathematical Modeling and Industrial Mathematics', 'Risk Management', 'Industrial Organization']"
doi:10.1007/978-981-19-5292-0_33,en,Deep Learning Applications to Detect Alzheimer’s—A Review,OriginalPaper,"The leading cause of dementia throughout the world is Alzheimer’s—an incurable neural condition that leads to a loss of cognitive abilities and progressive mental deterioration, with severity ranging from mild to severe. Such situations can affect a person’s ability to accomplish any task independently. Early diagnosis can be helpful in the prevention of Alzheimer’s progression. As per recent studies, for Alzheimer’s diagnosis, neuropsychological data by magnetic resource imaging are frequently used. Deep learning approaches to working with images have gained popularity in past years, and they have attracted an enormous amount of attention in detection. Deep learning techniques emerged more precisely, corresponding to other machine learning techniques. A considerable amount of research has previously been published on this topic. In the proposed work, we reviewed the admissible literature and discussed the necessary preprocessing techniques and various approaches to deal with neuroimaging data. We further critically inspected and summarized the performance of deep learning approaches and their findings in depth.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-05347-4_1,en,Deep Learning Approach Based on Fault Correction Time for Reliability Assessment of Cloud and Edge Open Source Software,OriginalPaper,"We discuss a method of machine learning in order to consider the characteristics reliability trends of edge open source project. Then, we focus on the method based on deep learning analysis. Thereby, the proposed method will be able to extract the characteristics data in order to comprehend the trend of fault big data recorded on the bug tracking system in edge open source project. Moreover, several numerical examples are shown by using actual fault big data in the edge open source project. Then, the illustrative results based on the deep learning are shown by using our methods discussed in this chapter. We discuss that our method by deep learning and prediction model are useful to assess the quality and reliability of the edge open source project.","['Mathematics', 'Mathematical Modeling and Industrial Mathematics', 'Risk Management', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/978-981-19-4606-6_82,en,Mobile Robot Path Planning Using Neuro-Sugeno-Fuzzy Gravitational Technique in a Cluttered Environment,OriginalPaper,"Paper shows the contribution of the neuro-Sugeno-fuzzy gravitational technique towards path navigation. Fire bird-V robot is used in the above approach. The work is done in both simulation and experimental environment using simulation tools. The results of the simulation and experiments are compared and expressed in tabular form. The percentage deviation in the result was within the acceptable limits. The reason for the deviation was various external reasons, and slippage at the contact between the wheel and the floor. The proposed controller was found to be effective in preventing collision with different obstacles at different locations. Arena selected was a static environment.","['Engineering', 'Industrial and Production Engineering', 'Machinery and Machine Elements', 'Materials Engineering']"
doi:10.1007/978-981-19-0312-0_20,en,"Perfomance Analysis of Text Extraction from Complex Degraded Image Using Fusion of DNN, Steganography, and AGSO",OriginalPaper,"In present time, any complex degraded image consists of very important and confidential information details which is recognized as a non-textual image and textual information. Due to diversity of text style in image, complicated background, and various interference factors makes detection of text (DOT) from complex degraded image as a field of research. The secure and accurate text in a complex degraded image is found useful for the audience to understand the complete situation. So, a fusion of DNN, adaptive galactic swarm optimization (AGSO), and steganography are applied in this proposed technique to securely, efficiently identify information in the form of text and thereafter, to recognize each character from degraded complex images. In general, images are affected by different type of noise such as structured noise, Poisson–Gaussian noise, periodic noise, and impulse valued noise, and to discard it in the initial preprocessing phase, the guided filter (GF) is used. A very important task in the text identification and recognition process is feature extraction, performed by using Gabor and stroke width transform. The extracted features of the image are required during the classification process. Thereafter, text identification and recognition is done by WNBA. Subsequently, performance comparison of various performance parameters such as precision, F1-scores, and recall was tested using the IIIT5K database for proposed algorithm along with other existing techniques.","['Engineering', 'Microwaves, RF and Optical Engineering', 'Wireless and Mobile Communication', 'Computer Communication Networks']"
doi:10.1007/978-3-031-13702-0_4,en,A Support Vector Machine Model for Rice (Oryza sativa L.) Leaf Diseases Based on Particle Swarm Optimization,OriginalPaper,"Our society is becoming increasingly reliant on technology every day. Agriculture, on the other hand, is critical to human survival. Rice is one of the most important grains of food. It feeds nearly half of the world’s population and supports a large number of jobs. Crop diseases cause significant yield losses around the world, particularly in Sub Saharan Africa. To implement effective disease management measures, pathogen detection and understanding spatiotemporal dynamics are critical, and this necessitates the use of molecular detection methods, particularly to discriminate between infections that cause similar symptoms. As a result, adequate disease mitigation for rice plants is critical. We used to our proposes a model for detecting three rice leaf diseases: brown spot, leaf smut, and bacterial leaf blight. This paper presents a unique model for the classification of rice leaf diseases by multi-class SVM, K—means clustering, and PSO. The disease was classified using an SVM classifier and the classifier accuracy is optimized using PSO. The exploratory results show that the proposed approach performed well in terms of disease detection accuracy, with a score of 98.89%.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Artificial Intelligence', 'Food Science']"
doi:10.1007/978-3-031-09835-2_12,en,A Comprehensive Review of the Firefly Algorithms for Data Clustering,OriginalPaper,"Separating a given data set into groups (clusters) based on their natural similar characteristics is one of the main concerns in data clustering. A cluster can be defined as a collection of objects which are “homogeneous” between them and are “heterogeneous” to the objects belonging to other clusters. Many areas encounter clustering based applications including the fields of medical, image processing, engineering, economics, social sciences, biology, machine learning and data mining. Clustering goes under unsupervised learning where no labels are given to the learning algorithm, leaving it on its own to find structure in its input. Even though many classical clustering algorithms can be found, most of such suffer from severe drawbacks such as sensitivity over initial cluster centroids and hence can be easily trapped in local optimum solutions. The other main problem with the data clustering algorithms is that it cannot be standardized. On the other hand, clustering can be considered under optimizations which goes to the category of NP hard optimization making more difficult in solving. Addressing such NP hard problems, meta-heuristics play a remarkable role in optimization. Since its appearance from more than a decade ago, Firefly Algorithm (FA), a stochastic meta- heuristic in nature inspired algorithms has shown significant performance in giving solutions to many optimization problems. Hence FA has been used in research addressing the problem of clustering optimization. This chapter forestalls the ability of firefly algorithm in solving data clustering problem. It presents an introduction to clustering and the performance of FA, briefly reviews and summarizes some of the recent firefly-based algorithms used for data clustering with the emphasis on how FA has been combined/ hybridized with other methods to contribute to the problem of data clustering. Further it discusses on different representations, initializations, and the used cluster validation criteria in FA based clustering methods. The chapter also discusses why FA is to be more useful for clustering over other methods and what features made it more suitable for handling the clustering problem compared with other meta-heuristics. Finally, it focuses on the limitations that have been found in the literature on clustering grounded on FA-based applications and discusses possible avenues in future.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4703-2_9,en,Improving Pathological Voice Detection: A Weakly Supervised Learning Method,OriginalPaper,"Deep learning methods are data-driven. But for pathological voice detection, it is difficult to obtain high-quality labeled data. In this work, a weakly supervised learning Method is presented to improve the quality of existing datasets by learning sample weights and fine-grained labels. First, A convolutional neural network (CNN) is devised as the basic architecture to detect the pathological voice. Then, a proposed self-training algorithm is used to iteratively run and automatically learn the sample weights and fine-grained labels. These learned sample weights and fine-grained labels are used to train the CNN model from scratch. The experiment results on the Saarbrucken Voice database show that the diagnosis accuracy improved from 75.7 to 82.5%, with a 6.8% improvement in accuracy over the CNN models trained with the original dataset. This work demonstrates that the weakly supervised learning method can significantly improve the classification performance to distinguish pathological voice and healthy voice.","['Engineering', 'Signal, Image and Speech Processing', 'Engineering Acoustics', 'Mathematics in Music', 'Music']"
doi:10.1007/978-981-19-2879-6_8,en,Huawei CLOUD Enterprise Intelligence Application Platform,OriginalPaper,"This chapter mainly introduces Huawei CLOUD Enterprise Intelligence (EI), including Huawei CLOUD EI service family. It focuses on Huawei ModelArts platform and Huawei EI solutions.","['Computer Science', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5221-0_36,en,Citation Recommendation Using Deep Learning Approach,OriginalPaper,"Citation recommendation is a technique that assists an academic publisher or scholar in identifying a selection of relevant works that can be cited whilst writing a paper. In comparison with prior years, it is clear that there is currently a deluge of articles being published every year. As a result, the traditional way to identify valid citations appears to be a very difficult process, as there appears to be massive flow of data. In this research, we will discuss an innovative way to solve the citation recommendation task. To begin, we employed a probabilistic model known as the Gaussian mixture model, to minimise the search space, which we subsequently integrated with other neural architectures such as ANN and a hybrid model, CNN-LSTM. Finally, we performed a thorough experimental investigation employing a variety of evaluation metrics.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2412-5_3,en,An Overview of Quantum Computing Approach in the Present-Day Energy Systems,OriginalPaper,"With the increase in global population and global heating, energy demand is also constantly increasing. The uprising demand to be fulfilled by taking care of environmental conditions’ protection to keep global warming in check. Significant efforts have been put into designing, controlling, handling, planning, and managing energy systems. In this regard, bio-inspired or nature-inspired evolutionary optimization schemes in the existing energy systems and innovative energy sources are inducted into the available resources. On the other hand, quantum computing has changed the classical computational approach with speed and efficiency. The assurance of quantum computing in the optimization of energy systems also gained a research attraction. The optimization techniques employed with the quantum advantage by quantum computers supersede classical approaches. This study explores the viability of quantum computing in energy system optimization and various challenges to tackle. This work will help the readers to plan for applying this approach in sustainability energy harvesting, intelligent power and energy systems, distribution network, and renewable energy. Security of the smart grid, intelligent energy systems, evaluation of the energy production process, and other similar or related applications may also be explored.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Renewable and Green Energy', 'Energy Storage']"
doi:10.1007/978-981-19-0968-9_23,en,"Systematic Literature Review on the Combination of Digital Fabrication, BIM and Off-Site Manufacturing in Construction—A Research Road Map",OriginalPaper,"According to the scientific literature, 98% of megaprojects in the construction sector suffer from cost overruns of more than 30%. This is due to several reasons, the most important of which is issues with productivity (Changali et al. The Construction Productivity Imperative, p 10, 2015, [ 1 ]). Off-site manufacturing in construction is one solution to increase productivity, but there is a current lack of studies linking it to the concept of design for manufacturing and assembly (Jin et al. J Cleaner Prod 202:1202–1219, 2018, [ 3 ]). Building information modeling (BIM) also offers new opportunities to underpin the computerized design and fabrication of industrialized buildings, providing greater productivity and cost-effectiveness (He et al. J. Cleaner Prod 278:123505, 2021, [ 18 ]). The combination of BIM and off-site construction (OSC) could maximize their benefits to the construction industry and their applications are highly recommended to improve construction efficiency (Yin et al. Autom Constr 101:72–91, 2019, [ 7 ]). However, even if the relationship between BIM and off-site construction has been identified and discussed among pioneer researchers, gaps were found in the literature concerning the integration of BIM and digital fabrication with off-site manufacturing (OSM) in construction. There has been insufficient research in integrating these practices. The aim of this literature review is to assess the current state of the combined use of digital fabrication, BIM, and OSM in the context of Construction 4.0. A bibliometric analysis was conducted to study the relationship between these practices. The contribution to the body of knowledge will be the outcome of this literature review including an in-depth discussion on main trends in off-site construction, research gaps, and recommendations for near-future perspectives in off-site construction.","['Engineering', 'Building Materials', 'Geoengineering, Foundations, Hydraulics', 'Transportation Technology and Traffic Engineering', 'Environment, general']"
doi:10.1007/978-981-19-3035-5_50,en,A Novel Scheme to Deploy the Throwboxes in Delay Tolerant Networks,OriginalPaper,"Delay Tolerant Networks, the heterogeneous wireless network, are having the ability to enable communication in intermittent connectivity. But, the network suffers from poor performance due to low contact opportunity between nodes. Contact opportunity between nodes must be increased to improve the performance of the network. For that we can deploy static relay nodes also called as throwboxes in optimistic places so that more number of nodes can communicate by that node. Hence, performance of the network can be improved. In this paper, we proposed a deployment technique using grey wolf optimization (GWO), a metaheuristic technique to increase delivery ratio in minimum delay. In this paper, the objective is to find the optimal places using GWO hence, delivery ratio of the network can be increased with least end-to-end delay. For simulation, we have used ONE simulator to find the result and comparison with previously proposed deployment schemes in DTN. By analysing results, we can say that proposed deployment technique performs better as compared to existing deployment technique.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Statistics, general']"
doi:10.1007/978-981-19-1607-6_26,en,MAGNeto: An Efficient Deep Learning Method for the Extractive Tags Summarization Problem,OriginalPaper,"In this work, we study a new image annotation task named Extractive Tags Summarization (ETS). The goal is to extract important tags from the context lying in an image and its corresponding tags. We adjust some state-of-the-art deep learning models to utilize both visual and textual information. Our proposed solution consists of different widely used blocks like convolutional and self-attention layers, together with a novel idea of combining auxiliary loss functions and the gating mechanism to glue and elevate these fundamental components and form a unified architecture. Besides, we introduce a simple but effective data augmentation technique dedicated to alleviate the effect of outliers on the final results. Last but not least, we explore a self-supervised pre-training strategy to further boost the performance of the model by making use of the abundant amount of available unlabeled data. Our model shows the good results as 90% $$F_{1}$$ F 1 score on the public NUS-WIDE benchmark, and 50% $$F_{1}$$ F 1 score on a noisy large-scale real-world private dataset. Source code for reproducing the experiments is publicly available at: https://github.com/pixta-dev/labteam .","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5574-7_9,en,Intelligent Virtual Reference Feedback Tuning Based Data Driven Control for Power Plant,OriginalPaper,Pulverizing system is an integral part of the coal-fired power plant. The safe and efficient operation of pulverizing system is very important to improve the economy of the plant.,"['Engineering', 'Control, Robotics, Mechatronics', 'Energy Systems', 'Computational Intelligence']"
doi:10.1007/978-3-031-19958-5_52,en,Brain-DeepNet: A Deep Learning Based Classifier for Brain Tumor Detection and Classification,OriginalPaper,"Brain tumor is one of the most hazardous disease that leads a man to gradual death. To ensure proper and effective treatment, this is very important to detect the brain tumor and predict this as cancerous or non-cancerous. Radiologists have shown interest to detect brain tumor and its category analyzing the MRI (Magnetic Resonance Image) of brain. This detection and classification task seems to be challenging because of different size, location and behavior of brain tumors. Deep learning based classifiers extract features from MRI and helps to diagnose brain tumor with the help of computer aided diagnosis system. In this paper, we have experimented this classification task on a publicly available dataset using transfer learning approach in InceptionV3 and DenseNet201 model. Data augmentation technique is performed to enrich the dataset for achieving a good classification result an to avoid over fitting.“Brain-DeepNet” a deep convolutional neural network has been proposed where six convolution layers are densely connected and extract features from dense layers. These dense layers extract features and all features are passed to a fully connected layer. Dense network extract features more efficiently from brain MRI. This work is experimented on MRI as MRI provides more details of cell structure and functions. Our proposed model has shown approximately 96.3% classification accuracy to differentiate among the three types of brain tumors most commonly encountered Glioma, meningioma, and pituitary. This model outperforms the classification performance in comparison with the pretrained models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3998-3_104,en,Research on Target and Firepower Allocation of Multi-platform Anti-ship Missiles Under Constraints,OriginalPaper,"According to the possible new situation in the future sea battlefield, under the cooperative operation of multi-platform anti-ship missiles. A route planning method based on geometric method is used for route planning, and a firepower distribution scheme that can maximize firepower as much as possible is put forward, and a model of firepower distribution optimization problem is established. Aiming at this model. An improved genetic algorithm is adopted to solve the problem that genetic algorithm is easy to fall into local optimal solution. Simulation results verify the effectiveness of this method.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-3-031-04435-9_6,en,A New Home Energy Management System for Smart Home Using Improved Multi-objective Antlion Optimization Algorithm,OriginalPaper,"Lack of knowledge on how to use energy properly has led to a significant increase in energy demand relative to its production in recent years. Electricity companies have also come up with various plans to solve this problem, such as consumption management. In this study, we present an optimized home energy management scheme to overcome major barriers to implementing demand response programs. The main goal is to find the minimum energy cost by considering variable pricing for at least one period during the day and based on shifting loads out of peak hours while maintaining user comfort. Due to the complexity of the problem, an improved meta-heuristic optimization approach is used, which we call OBLALO. The proposed scheme for CPP pricing is simulated and the results are compared with PSO, GOA and ALO algorithms. The simulation results show the advantage of using the proposed scheme in the home energy management system.","['Social Sciences', 'Science and Technology Studies']"
doi:10.1007/978-981-19-5574-7_3,en,Intelligent Segmentation of Furnace Flame Image,OriginalPaper,"In the edge detection process, it is Furnace  necessary to convert the color flame image into gray image, which will inevitably lead to the loss of some information of the flame image. For the actual furnace Furnace  safety monitoring system, only obtaining the edge position information of flame can not meet the production needs, nor can it realize the intelligent monitoring and diagnosis of combustion in the furnace Furnace . In order to realize the intellectualization of coal-fired power plants Coal-fired power plant , it is also necessary to mine the information of flame image in the furnace Furnace , such as color brightness information, color distribution, color difference between frames, and so on.","['Engineering', 'Control, Robotics, Mechatronics', 'Energy Systems', 'Computational Intelligence']"
doi:10.1007/978-3-031-19958-5_8,en,Bell Pepper Leaf Disease Classification Using Convolutional Neural Network,OriginalPaper,"In today’s agriculture, leaf disease is a major issue. It hinders the natural growth of plants. Stifles a country’s economic development. It will lower the quality of agricultural products. Leaf disease can develop as a result of bacterial, fungal, or other causes. Finding and detecting sick plants in the open eye takes a long time. As a result, automatically detecting and resolving plant disease is critical. Pepper Bacterial spot disease is generally caused by Xanthomonas campestris which reduces pepper production and quality. In this paper, we used the plant village dataset. The dataset contains 2080 image data of bacterial spotted pepper bell leaf and 1,881 healthy bell pepper leaf. We will study pepper belt bacterial spot disease. Using Conventional Neural Network(CNN), our suggested method will detect bell pepper bacterial spot plant disease. We’ll classify each plant image and look for illnesses. Our proposed CNN system has an accuracy rate of testing is 96.88% the accuracy rate of training and validation is 99.44% and 97.34% respectively.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-04098-6_18,en,A Hybrid-Attention-LSTM-Based Deep Convolutional Neural Network to Extract Modal Frequencies from Limited Data Using Transfer Learning,OriginalPaper,"Current computer video-based vibration modal analysis approaches typically decompose video frames into representations and then adjust them that allow to magnify motions to extract motion representations for vibration modal analysis. Their decomposition usually relies upon handcrafted designed kernels, such as the complex steerable kernels, which typically may not be optimally designed for the extraction of subtle motions specially in higher frequency domains. In this paper, optimal decomposition kernel is learned and designed directly from baseline dataset images using deep convolutional neural network (CNN) models. Each subpixel of an image obtained from a digital camera is included when computing the spatiotemporal information, which serves similar to an individual motion sensor to acquire the modal frequencies of a vibrating structure. A hybrid-attention-LSTM-based deep convolutional neural network architecture is developed to take advantage of attention and LSTM blocks to discover subtle motions from a specific source to visualize high resolution of dynamic properties of the structures in the existence of high amounts of noise. The idea of transfer learning is utilized to transfer the knowledge previously learned to new limited dataset. Transfer learning is used to take advantage of limited existing dataset to avoid underfitting in the training of the network, considering the current publicly available modal frequency datasets are insufficient to train a generalized network. The proposed deep learning architecture is designed in such a way that has capability of transferring the trained model from baseline dataset on a simple structure to a complicated structure using transfer learning perspective. After training, the model takes the video of a vibrating structure as input and outputs the fundamental modal frequencies. By showing reliable empirical results, the proposed model is autonomous, efficient, and accurate.","['Engineering', 'Machinery and Machine Elements', 'Mechanical Engineering', 'Measurement Science and Instrumentation', 'Aerospace Technology and Astronautics', 'Vibration, Dynamical Systems, Control']"
doi:10.1007/978-981-19-6004-8_19,en,Apple Leaf Disease Prediction Using Deep Learning Technique,OriginalPaper,"Agriculture is essential for human life on earth, providing food and income for people. Many diseases attack various fruits and crops. Appletree leaves are also susceptible to a variety of pathological conditions that affect their production which are Mosaic, Frogeye, etc. Whenever an apple crop is damaged by infections that harm apple production and the country's wealth, image processing is recommended for detecting apple leaf diseases. This approach permits an effective distinction between infected and non-infected apple leaves. Frequently, individuals tried to notice those ailments with gazes. Sometimes, people deliver bad decisions because the leaves look identical. In this way, they get false results as well as delays in achieving. They cannot expect the results on time. Moreover, manpower is required to discover these leaf diseases with the eye. Our paper provides the CNN model and an algorithm to detect such diseases in crop leaves. The sample we created was trained to analyze and understand the diseased leaf and then detect the leaf disease. We are using the Inception v3 algorithm.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-6004-8_7,en,Convolutional Neural Network Based Intrusion Detection System and Predicting the DDoS Attack,OriginalPaper,"In today’s world, cybersecurity is important as it secures data against damage and unauthorized access. In recent years, machine learning studies have made remarkable developments in cybersecurity and helped in deriving the pattern of user activities, checking for malicious activities in the network, and identifying the cyberattacks. This research intends to predict the DDoS attack that impedes network access by flooding a large amount of traffic, saturating the bandwidth of the network. The proposed work uses convolutional neural network (CNN) to train the machine learning model to anticipate the DDoS attack before it happens. The CNN model is trained using live data, where the data packets are captured using Wireshark and the KDD CUP 1999 dataset. The packet’s information is converted into a two-dimensional image and is trained using the CNN algorithm. The proposed system’s performance is evaluated and compared with the existing IDS systems, which attains a maximum accuracy of 95.8%.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2468-2_28,en,FIR Filter Design Using Grasshopper Optimization Algorithm,OriginalPaper,"In this paper, digital finite impulse response (FIR) low-pass filter (LPF) and high-pass filter (HPF) are designed using a novel meta-heuristic algorithm named grasshopper optimization algorithm (GOA). The GOA is meta-heuristic population-based optimization algorithm, which mimics the food searching behaviour of the grasshopper. The filter design aims to evaluate the optimal filter parameters and find the minimum objective function value so that the output of the designed filter matches with the output response of the ideal filter. Mean square error (MSE) is taken as the error objective function. The results obtained using GOA are compared with the other two algorithms, namely particle swarm optimization (PSO) algorithm and grey wolf optimization (GWO) algorithm. The simulated results reveal that GOA is best suited algorithm for FIR filter design problem.","['Engineering', 'Electronics and Microelectronics, Instrumentation', 'Quantum Physics', 'Measurement Science and Instrumentation']"
doi:10.1007/978-3-031-11748-0_12,en,From Theoretical to Practical Transfer Learning: The ADAPT Library,OriginalPaper,"In traditional machine learning, the learner assumes that the training and testing datasets are drawn according to the same distribution. However, in most practical scenarios, the two datasets are drawn according to two different distributions, the source distribution and the target distribution. In this context, the use of classical machine learning algorithms often fails as models trained on the source data provide poor performances on the target data. To solve this problem, many transfer learning techniques have been developed following one of the three main strategies: parameter-based transfer, instance-based transfer and feature-based transfer. The choice of the appropriate strategy is mainly determined by the nature of the shift between the source and target distributions. For example, to deal with the problem of sampling bias, when part of the population is over- or under-represented in the training set, instance-based approaches are useful to adequately reweight the source data in the training phase. If the shift is caused by a change in data acquisition, such as sensor drift, feature-based methods help to correct the shift by learning a common feature representation for the source and target data. For a real application, it is really a challenge to choose in advance the best transfer learning strategy and one often needs to evaluate different models in practice. As the different transfer methods were introduced by various contributors, no common framework is today available for a rapid development. To tackle this issue, we propose a Python library for transfer learning: ADAPT (Awesome Domain Adaptation Python Toolbox), which allows practitioners to compare the results of many methods on their particular problem. ADAPT is an open-source library providing the implementation of several transfer learning methods. The library is suited for scikit-learn estimator objects (objects which implement fit and predict methods) and tensorflow models. It allows to evaluate very easily the benefits of transfer learning methods on real data. In this chapter, we propose to illustrate the different features of the ADAPT library on both synthetic and real datasets.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning']"
doi:10.1007/978-3-031-09835-2_19,en,A Bumble Bees Mating Optimization Algorithm for the Discrete and Dynamic Berth Allocation Problem,OriginalPaper,"In maritime supply chain, there is a large number of significant optimization problems that have to be modeled and solved. One of them is the Berth Allocation Problem (BAP). In BAP, the assignment and scheduling of incoming ships to berth positions are studied. In literature, several variations and connections with other maritime problems can be found. In this paper, we present an efficient methodology for solving the discrete and dynamic version of BAP (DDBAP). We formulate the DDBAP as a heterogeneous vehicle routing problem with time windows (HVRPTW). In this formulation, berths correspond to vehicles, ships correspond to customers/nodes and the sequence of serviced ships at a particular berth represent a vehicle route. Each one of these sequences must start and end at a specific point such as the depot in VRP. The sequence of serviced ships starts at the origin and finishes at the destination point. Time windows can be imposed on each one of the ships. The time windows for each ship correspond to the availability time in order to be served properly in one of the available berths. In order to solve DDBAP, we apply a modified Bumble Bees Mating Optimization (BBMO) algorithm. BBMO belongs to the nature inspired optimization algorithms and it simulates the mating behavior of the bumble bees. It is an algorithm that its basis is the Honey Bees Mating Optimization (HBMO) algorithm but with a number of different steps that make the algorithm a completely different and competitive algorithm with other nature inspired algorithms. We applied the algorithm to benchmark instances from the literature and the obtained computational results proved the efficiency of BBMO with respect to the quality of solutions and the computational time needed to find these solutions.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16159-9_16,en,Estimation of Mass Flow Rates of Two-Phase Flow Using Convolutional Neural Networks,OriginalPaper,"We present the design and training process of a couple of convolutional neural networks (CNNs) to predict the mass flow rates of a glycerin-air mixture injected into an looped horizontal pressurized pipeline. The CNNs were trained with spectrogram images generated from pressure differentials, which were calculated from pressure measurements taken at different points along the pipeline. To obtain enough data for the conception of the CNNs, we performed a series of experiments in which several combinations of glycerin-air flow mass rates were supplied. To program the CNNs of this work, we used Anaconda tools for Python developers.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-3-031-17697-5_33,en,Predicting the Value of Cryptocurrencies Using Machine Learning Algorithms,OriginalPaper,"Cryptocurrencies are a type of unregulated, digital money, issued and most often controlled by its founders, used and accepted between members of a particular virtual community. Since values of cryptocurrencies can be presented as time series, the aim of this paper is to use recurrent neural networks (RNN) in the prediction model of cryptocurrencies values. Real-world data for three cryptocurrencies (Bitcoin, Ethereum, and Litecoin) were used in the experiments. Data were split for training and test set. Three different layer types of RNNs have been applied per each cryptocurrency: long-short term memory - LSTM, Simple, and gated-recurrent unit - GRU. Results of the experiment conducted within this paper show that machine learning algorithms can be used for cryptocurrencies values prediction. Values of mean square errors are very small (ranges from 0.03 to 0.42) which indicates promising model prediction capabilities.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5090-2_23,en,Image Retrieval Using Neural Networks for Word Image Spotting—A Review,OriginalPaper,"Borah, Naiwrita Baruah, Udayan Since the emergence of the Internet and low-cost digital image sensors, the number of image databases has grown tremendously. These image databases must have efficient image retrieval methods. One such technique is content-based image retrieval. In these databases, one will find charts, graphs, pictures, and even some text. Our main focus is on visual information retrieval by using this data. The idea is to bridge the semantic gap of high-level human perceptions and low-level features. This review was conducted based on an assessment and comparison of the most current CBIR approaches. Machine learning algorithms, similarity matching techniques, and performance assessment methodologies are also included in this study. This study provides an in-depth look into CBIR, covering its theory, concepts, techniques, difficulties, future directions, and performance.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-19958-5_37,en,Classifying Sentiments from Movie Reviews Using Deep Neural Networks,OriginalPaper,"Sentiment analysis has become crucial for the building of opinion mining systems due to the daily creation, sharing, and transfer of massive volumes of data and opinions via the Internet and other media. The sentiment analysis for movie recommendation is the main focus of this study. There are too many reviews and comments for movies to be manually processed. Therefore, to effectively process, we utilized user assessments of movies (whether they were favorable or unfavorable) to construct an overall assessment of the movie, which we then used to suggest it to other users. Even the most sophisticated review algorithms have been baffled by the enormous volume of reviews that are currently available. As a result, a strategy for extracting knowledge from the existing reviews and using it more skillfully needs to be created. This study utilized Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Long short-term memory (LSTM) to perform movie sentiment analysis on a dataset of 50k reviews from IMDB movies. Initially, the data was pre-processed using a GloVe word embedding algorithm. The testing outcomes showed that, with accuracy of 87.11% and an F1-score of 87%, LSTM beats other strategies. However, ANN recorded accuracy of 73.95%, while CNN reported accuracy of 85.17%. In addition, our models performed better than Naive Bayes (NB) and Support Vector Machine (SVM).","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18161-0_8,en,Smart and Sustainable Scheduling of Charging Events for Electric Buses,OriginalPaper,"The Irish transportation sector currently accounts for more than 30% of the energy related CO 2 emissions of the country. Therefore, in order to reach the sustainable goals, the Irish government is working on multiple incentives to promote Electric Vehicles (EV) and infrastructure to decarbonize the sector, e.g., free domestic charging points, tool reductions, and the implementation of electric Buses (eBuses) in the medium to long term. In particular, eBuses operate with rechargeable batteries with a capacity to store approximately 300 kWh (and up to 600 kWh), equivalent to around 29.9 L of diesel, while reaching approx. 200 km. In order to ensure a proper transition from regular diesel buses to eBuses, charging times must be coordinated to ensure each bus has adequate energy to complete their operational route. In this work, we present a framework for an efficient management of renewable energies to charge a fleet of eBuses without perturbing the quality of service. Our framework starts by building a deep learning model for wind power forecasting to predict clean energy time windows, i.e., periods of time when the production of clean energy exceeds the demand of the country. Then, the optimization phase schedules charging events to reduce the use of non-clean energy to recharge eBuses while passengers are embarking or disembarking. The proposed framework is capable of overcoming the unstable and chaotic nature of wind power generation to operate the fleet without perturbing the quality of service. As expected, the size of the batteries does have a positive impact on the percentage of clean energy required to operate large fleets of eBuses. Methods developed in this paper help to mitigate potentially inaccuracies derived the prediction models. Our extensive empirical validation with real instances from Ireland suggests that our solutions can significantly reduce non-clean energy consumed on large datasets.","['Political Science and International Relations', 'Public Policy', 'European Politics', 'Economic Growth', 'Business and Management, general', 'Economic Policy', 'Environmental Policy']"
doi:10.1007/978-3-031-08580-2_12,en,Shop Product Tracking and Early Fire Detection Using Edge Devices,OriginalPaper,"With the blooming of Internet of Things technology, edge device applications have become very popular in many areas. However, edge devices have constraints on power and resources that make edge device applications different from conventional applications. This paper proposes an architecture for tracking shop products and detecting early fire detection using computer vision that can be implemented in edge devices. Experimental results showed that the proposed architecture could be a feasible application that can be deployed to support staff in retail stores.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4863-3_32,en,Application of Adaptive Neuro-Fuzzy Inference System and Salp Swarm Algorithm for Suspended Sediment Load Prediction,OriginalPaper,"Due to the sheer importance of suspended sediment load (SSL) in watershed management and design of engineering structures and considering the impact of rainfall, temperature, and runoff parameters in quantifying and understanding nonlinear interdependence, it has been a crucial task to predict suspended sediment load based on these parameters. For this purpose of prediction, a soft computing model (adaptive neuro-fuzzy inference system (ANFIS)) is optimized with Salp swarm algorithm (SSA), and the results were validated against a well-established classical ANFIS model. Data from Jaraikela catchment area in Jharkhand with some part of it in Sundergarh district of Odisha were used in the analysis. The performance of the models was evaluated based on MSE and WI performance indicators. In comparing the results of the models used, it is evident that ANFIS-SSA model proved its ascendancy over ANFIS.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-18050-7_48,en,Complementing Direct Speed Control with Neural Networks for Wind Turbine MPPT,OriginalPaper,"The natural operation of wind turbines (WT) shows a nonlinear behavior which makes it difficult for the system to be controlled. Because of this, artificial intelligence techniques appear as promising control solutions. In this work, artificial neural networks (ANN) are used to complement the Direct Speed Control (DSC) of a wind turbine. Specifically, a neural network is used for the Maximum Power Point Tracking (MPPT) of a wind turbine model, controlling the generator speed and maintaining the active power into the correct levels to reach a power coefficient ( $${C}_{p}$$ C p ) within its optimum values. The real characteristics of a 1.5 MW wind turbine are considered. OpenFast and Matlab/Simulink software tools are used to model and simulate the non-linear WT and the controller, respectively. The intelligent proposed solution is compared with the standard control embedded in OpenFast with satisfactory results.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-15928-2_91,en,Topology Optimization for Thin-Walled Structures with Distributed Loads,OriginalPaper,"Additive Manufacturing (AM) is continuously increasing its appeal as a breakthrough production process due to well-established advantages compared to traditional manufacturing strategies based on chip removal or casting. The design of lightweight structures can exploit the AM advantages, thanks to the capability of shaping complex geometries where the constant level of stress can be achieved through Topology Optimization. Moreover, in transportation engineering and lightweight structures in general, thin-shell or thin-walled components are widely used for frames, fuselages, wings, car bodies, coaches, tanks or recipients. However, the application of topology optimization routines on thin-walled structures is not exempt from difficulties. This is true especially in the case of a distributed pressure load coming from fluid-structure interaction analysis. Coupling the benefits of TO methodology with the already good performances of thin-walled structures may lead to mechanically efficient shapes. This research addresses strategies to apply topology optimization on thin-walled structures. The effect of the local concentration of distributed load in a cloud of control points distributed along the surface of interest is considered and tested. Two case studies coming from industrial engineering have been carried out to show the capabilities of the proposed approach.","['Engineering', 'Engineering Design', 'Industrial and Production Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-981-19-3998-3_132,en,Cooperative 4D Penetration Path Planning of Multi-UAV Using MLS-EDA Algorithm,OriginalPaper,"Aiming at the path planning problem of unmanned aircraft vehicle (UAV) cooperative penetration, a 4D path planning method with the time and space constraints is developed. The simultaneous arrival time requirement of the end state is transformed into the state adaptation in the process, thus, the dimension reduction of the complexity of problem model is realized. Moreover, a reasonable cost function is developed to transform the cooperative penetration path planning problem into a single objective optimization problem, which is solved by MLS-EDA algorithm. The simulation test is carried out by using a UAV cluster with a scale of 20. The results show that the planned path of each UAV can effectively avoid the enemy fire threat and all member can reach the assemble points at the same time, which proves the rationality and feasibility of the proposed model as well as the algorithm.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-4052-1_22,en,Tomato Leaf Disease Recognition with Deep Transfer Learning,OriginalPaper,"Transfer learning has introduced us to a new aspect of sensitive image classification tasks, such as disease recognition from both flora and fauna. It enables us to achieve significant performance faster and more effectively. On the other hand, plant disease recognition is undoubtedly important from nutritional and financial aspects. In this manuscript, we have deployed ImageNet pre-trained ResNet152V2 along with a custom 10 layers densely connected neural network for automatic disease classification from ten types of tomato leaf images, nine diseased classes, and healthy class. Our approach demonstrates significant performance against other recent works on tomato leaf disease classification. Our model performed well with almost 97% accuracy on the testing set.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3015-7_17,en,AutoNav: A Lane and Object Detection Model for Self-Driving Cars,OriginalPaper,"The area of autonomous vehicles is of huge research interest and much has been accomplished in this area. This study involves three aspects: lane detection, object detection, and autonomous driving. Lane detection and object detection has been simulated in the CARLA simulator using TensorFlow and OpenCV libraries of Python. Canny edge detection algorithm and Hough line transform are then used to detect the lane lines. For object detection, image data is collected, labeled manually, and split into test and train data. SSD MOBNET 640 × 640 is used for training the model, and about 75% precision is obtained. Autonomous driving has been implemented in the Udacity simulator using behavioral cloning, a five-layer convolutional neural network (CNN) was used as the model and the data was trained for five epochs with 20,000 steps per epoch. Live predictions are made by the trained model which are used to run the car in autonomous mode.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Data Mining and Knowledge Discovery', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-2764-5_34,en,Obstacle Avoidance for a Swarm of AUVs,OriginalPaper,"This paper uses the bioinformatics-inspired technique for guiding a team of autonomous underwater vehicles (AUVs) towards the desired destination. Here, each AUV estimates the position of the neighbour AUVs while moving towards the destination. The proposed multi-AUV system constitutes a leader AUV and three follower AUVs. A distributed path consensus (DPC) is proposed that determines the distance constraint to ensure the neighbouring agent AUVs maintain a predefined distance between each other and are able to avoid static obstacles while moving towards the respective destinations. It is observed from MATLAB simulation that the co-operative motion control of multiple AUVs along the desired paths and obstacle avoidance is successively achieved. The proposed method solves coordination problems among multiple AUVs and increases the coverage of underwater missions like oceanographic surveys.","['Energy', 'Energy Systems', 'Artificial Intelligence', 'Machine Learning', 'Cyber-physical systems, IoT', 'Professional Computing', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/978-3-031-15191-0_46,en,Type 2 Fuzzy PID for Robot Manipulator,OriginalPaper,"This paper consists in developing a robust control device for a nonlinear dynamic robot manipulator subjected to external disturbances. We propose an approach combining the fuzzy logic type 2 with the proportional-integral-derivative (PID) controller to control the joint-angle position of the two-link robot manipulator in order to apply an optimization mechanism for the gains of the PID. The type-2 fuzzy system is applied to improve the efficiency and robustness of the traditional PID controller, allowing for improving the classical regulator, by introducing a certain degree of intelligence in the control strategy. For the purpose of verifying the effectiveness of the approach developed in this study, all the simulation results attest that the (T2FL-PID) compared with (T1FL-PID) and (PID) controller produces a better response. The feasibility, as well as the performances of this controller, have been validated in simulation in the control of the two-link robot manipulator.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security']"
doi:10.1007/978-981-19-4863-3_25,en,Early Identification of Diabetic Retinopathy Using Deep Learning Techniques,OriginalPaper,"Diabetic retinopathy (DR) is a rapidly spreading disease, which is caused in diabetic patients. Patients who have diabetic retinopathy may suffer from complete vision loss. Even after scientific and medical advancement it is still incurable and it is a big threat to humans. So, early detection of DR is important to provide treatment on time. Manual detection of DR is time, cost, and effort consuming. A convolutional neural network (CNN) is a method of deep learning and it is more widely used in the medical field. This paper presents an idea of building an automated system for detection and identification of DR using the Asia Pacific Tele-Ophthalmology Society (APTOS) 2019 Kaggle dataset. Two CNN models, ReseNet50 and VGG16, are used for training and classification. The accuracy of the ResNet50 has been calculated to be 81.7% and that of the VGG16 is calculated to be 80.5%.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-2004-2_15,en,Tracking of Maximum Power of Solar PV Array Under Partial Shading Condition Using Grey Wolf Optimization Algorithm,OriginalPaper,"In this paper, solar energy has been used to generate required maximum value of power from solar photovoltaic (PV) system under partial shading condition. A simulation study of Maximum Power Point Tracking (MPPT) controller for solar PV array under partial shading condition is shown in MATLAB using the two known techniques that is conventional Perturb and Observe (P&O) and a global search algorithm Grey Wolf Optimization (GWO). The result of both the techniques are compared to show effectiveness of GWO algorithm. The proposed GWO based MPPT controller extracts the maximum amount power from the solar photovoltaic (PV) system under partial shading condition by modifying the duty cycle of Boost Converter (DC-DC). It was observed that GWO gives oscillation free maximum value of power at a faster rate as compared to conventional P&O technique.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-981-19-6379-7_1,en,"The Scope and Applications of Nature-Inspired Computing in Bioinformatics
",OriginalPaper,"Charles Darwin postulated the concept of “survival-of-the-fittest” and evolution in general. He discussed how nature selects the best candidate under different situations who are fit enough to survive and reproduce. This analogy has inspired many computational scientists, bioinformaticians, and computational biologists to develop techniques that can learn, adapt, and evolve to find optimal solutions for complex problems. Biologists are heavily dependent on computational methods and strategies to analyze humongous biological and medical data. Nature-inspired computing (NIC) encapsulates an ensemble of myriad studies of computer science, statistics, mathematics, and biological sciences where the essence is to adapt and develop robust competing techniques just like nature. It is a novel approach to optimization algorithms that are motivated by the dynamics of the biological evolution of our natural milieu. Over the past decade, various nature-inspired optimization algorithms have been deployed to solve complex problems in bioinformatics, engineering, and other sciences. With the glorious artificial intelligence (AI) revolution in biological sciences, there have been times when some problems are nonlinear in nature with multiple constraints and some techniques are hard to deploy. To solve high dimensionality issues and time complexity in such cases, NIC algorithms are the best choice to be used to solve complex optimization problems. This chapter highlights the commonly used NIC algorithms and their applications in biological sciences and bioinformatics.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-981-19-6068-0_7,en,Chimp Optimization Algorithm-Based Feature Selection for Cardiac Image-Based Heart Disease Diagnosis,OriginalPaper,"In this paper, we propose chimp optimization algorithm (ChOA) for selection of feature to increase the classification accuracy of heart disease diagnosis. In this approach, noises contained in the cardiac image are removed using median filter initially. Then, GLCM features are extracted from the cardiac image. Among the extracted features, optimal features are chosen using ChOA algorithm. These selected features taken as input to the classifier. In this approach, support vector neural network (SVNN) is used as classifier. The classifier classifies the image into normal and abnormal. Simulation results depict that the ChOA-based SVNN performs superior than the conventional SVNN, ANN, KNN and SVM in terms of accuracy.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-16281-7_54,en,Detecting Ice on Wind Turbine Rotor Blades: Towards Deep Transfer Learning for Image Data,OriginalPaper,"Wind turbines, in particular, their rotor blades, are not only subjected to specific structural loads but also harsh weather conditions. There exists a risk of ice forming on the leading edge of the rotor blade depending on the location and, notably, at lower temperatures and high humidity. Some of the effects include significant power decreases, turbine damages and shutdowns. Ice detection systems for wind turbines operating in cold climates thus become important. Therefore, this paper proposes a method of ice detection that uses RGB-images, taken from rotating nacelles under different conditions, and pre-trained models of MobileNet, VGG-19 and Xception. The output is an icing prediction that is performed within milliseconds. The novelty of this research lies in utilizing network-based deep transfer learning with unfrozen backbones and learning schedulers. Results showed that the MobileNetV2 obtained up to 99% accuracy. The method outperformed previous research on ice detection by 3% and was evaluated in two different data sets, including near and far views of rotor blades, variety of ice densities and day phases.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-3951-8_75,en,Employing Generative Adversarial Network in Low-Light Animal Detection,OriginalPaper,"Animal detection has been an important research topic due to its demand in surveillance and protection of endangered species. Daylight images help in detecting animals during daytime. Low-light images are used for detection in low contrast environment which is quite challenging due to image quality issues like bad contrast, high noise, reflectance, lousy illumination, etc. Some recent algorithms using deep learning have potentials to enhance low-light images for better detection performance. This work proposes to employ a preprocessing technique to enhance low-light images before the detection network. A generative adversarial network-based enhancement algorithm, called EnlightenGAN, is considered here for preprocessing. This algorithm is chosen for enhancement because it uses unpaired data. The automatic rhino detection system presented in this work uses a modified version of YOLOv3 (single-stage) object detector for better detection accuracy and faster performance. This work also introduces a modified version of our earlier one-horned rhino dataset by including low-light images. These new images are annotated manually for using with deep learning systems. The performance of the EnlightenGAN enhancement model is evaluated in terms of BRISQUE and NIQE. The performance of the overall proposed detector model is studied in terms of mAP, F-measure, GIoU loss and objectness loss. It is observed that the use of EnlightenGAN improves the detection accuracy for low-light images.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-15030-2_12,en,"Deep Learning Quantile Regression for Robustness, Confidence and Planning",OriginalPaper,"It is well known that the estimator that minimizes the mean square error loss is the mean. However, the latter is prune to outliers and additionally, in many practical use cases, such as regression confidence bounds for risk, one would like to predict other statistics such as the median, or a certain percentile. In this chapter we will introduce the concept of Quantile Regression, a method based on a unique loss function which allows predicting quantiles of the conditional distribution of the target given the input. Albeit the method is useful and the loss is differential, thus fits modern deep neural network approach, most data science practitioners are not aware for its existence. We hope this chapter will reveal the theory and the know-hows of this methods and will allow building better machine learning systems. In addition, we discuss some recently developed approaches of how to optimize a general loss function under a quantile constraint.","['Computer Science', 'Artificial Intelligence', 'Privacy', 'Cryptology', 'Mobile and Network Security']"
doi:10.1007/978-981-19-5221-0_11,en,Environmental Characteristics Leveraging Crop Recommendation Based on Bayesian Optimisation-Support Vector Machine (BO-SVM) Approach,OriginalPaper,"Agriculture automation is a mechanical process that can be done with or without human intervention. Due to the limited space available on domestic lands, it has become critical to select the most appropriate crops based on the prevailing factors in the designated area. The most prevalent problem faced by Indian farmers is that they do not choose the appropriate crop for the prevailing agro-climatic conditions. As a result, they are experiencing difficulty for optimum production. Precision agriculture is a cutting-edge way to grow crops. It uses research data on soil attributes, soil types, and crop yield information to help farmers choose the best crops for their land. This article used machine learning to come up with a system that would help farmers grow more crops by taking into account things like temperature, humidity, pH, and rainfall. In the proposed model, the SVM with the Bayesian optimiser was the most important part. It achieved 98.8% accuracy with fivefold cross-validation.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-20241-4_38,en,Using a Novel Instrumented Roller to Estimate Soil Dry Density During Compaction,OriginalPaper,"Capturing evolution of density or void ratio during the compaction of geomaterials (soils and unbound granular materials) is essential for improved performance. This study developed a framework where the density evolution during compaction can be estimated using advanced instrumentation. The framework’s suitability was validated using a simulated large-scale soil box (dimensions: $$7.5\,{\rm m}\,\times\,4\,{\rm m}\,\times\,0.8\,{\rm m}$$ 7.5 m × 4 m × 0.8 m ) experiment mimicking the field conditions. Well-graded sand was compacted in 5 layers of 125 mm using a 1.5-tonne mini roller instrumented with Light Detection and Ranging (LiDAR) systems and a total station tracking system for positioning. The sand’s moisture content was homogenised at 8% (w/w) using a concrete truck. The in-situ sampling for measuring density was carried out using Nuclear Density Gauge (NDG) and sand cone test. The data from sensors were collected using a Data Acquisition (DAQ) system connected to a laptop. The measurement of the deformation in real-time provided an opportunity to estimate the density in real-time, and it was estimated using a machine-learning artificial neural network (ANN) model. The estimated density from deformation measured and NDG at the end of compaction shows that estimated density NDG density with an R = 0.9 for one layer, and for other layers, R was more than 0.8. This novel instrumentation allows the density to be measured during compaction with very high accuracy, which has a massive advantage over conventional approaches and contribute to the true Intelligent Compaction (IC) with an advancement of automation in construction.","['Engineering', 'Building Construction and Design', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general']"
doi:10.1007/978-3-030-93236-7_44,en,Machine Learning Enhanced Nonlinear Model Parameter Selection from HDR-S Cyclic Loading Test,OriginalPaper,"The accuracy of new types of seismic rubber bearing’s properties selection mainly depends on the engineer’s experience and might be subjected to bias, reliability, and some uncertainties. This was a trial-and-error process which takes a lot of time specifically on the assumption of the nonlinear model and initial parameters and might encounter problem on the parameter optimization depending on the optimization method used. The parameters from the HDR-S cyclic loading test data should be evaluated precisely to have a more accurate results on the bearing’s behavior on structure for nonlinear dynamic simulation. This study proposed a method that accelerated the selection process of the nonlinear parameters using machine learning and KH Method. The developed neural network model predicted the nonlinear parameters of an HDR-S bearing at three different temperature under bilinear model. The parameters was the initial input data for KH Method, which accelerated the process and solved the initial value problem. The proposed method imposes to use artificial intelligence which can serve as an initial guide and will greatly help the engineers in the evaluation of the nonlinear parameters of seismic isolators prior structural design.","['Engineering', 'Civil Engineering', 'Vibration, Dynamical Systems, Control', 'Mechanical Engineering', 'Structural Materials', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-15211-5_58,en,"Vehicle Routing for Municipal Waste Collection Systems: Analysis, Comparison and Application of Heuristic Methods",OriginalPaper,"Optimization refers to finding the optimal value or best possible option. With optimization, the resource utilization can be planned to be the most effective and cost-efficient, especially in the vehicles sector, where cost and quality are both important factors. However, when dealing with complex systems, findings the best solution is considered almost impossible due to the time and the resources consumed. Therefore, optimization algorithms are used to find an optimum solution as much as possible within a relatively short time. The optimization algorithms evolved from conventional mathematical approaches to modern developed methods that use heuristic and metaheuristic approaches. Within the frame of this paper, the authors present a study that describes the effectiveness of three metaheuristic algorithms and show a municipal waste collection case study in Miskolc. After an introduction and theoretical background about the optimization algorithms development, the authors describe three metaheuristic algorithms: genetic, particle swarm, and simulated annealing. Five benchmarks are used to compare the results and consumed time for the mentioned algorithms. A Traveling Salesman Problem case study is solved to find the shortest real route of twenty locations for a municipal waste collection system in Miskolc city center by using the analyzed three algorithms. After that. The results are compared with a random solution. Particle swarm showed the best results, while simulated annealing was the fastest algorithm in the average execution time.","['Engineering', 'Automotive Engineering']"
doi:10.1007/978-3-031-11814-2_1,en,"Value Creation, Valuation and Business Models in the Pharmaceutical Sector",OriginalPaper,"The chapter describes value creation and valuation under structural uncertainty in the healthcare and pharma industries. These risks and uncertainties can significantly influence organizational performance, value creation and long-term sustainability. The discussion continues by comparing traditional valuation concepts used in finance with the requirements posed by the current situation of healthcare business. In particular, patent valuation is a critical business issue, and the value of pharma patents and licensing deals has risen markedly in recent years. Existing evaluation approaches do not consider a patent’s life cycle, an important and unique characteristic of pharma and biotech patents. For this reason, the inherent uncertainty in a patent’s value is modelled as a stochastic process.","['Economics', 'Health Economics', 'Pharmaceutical Sciences/Technology', 'Economics, general', 'Biomedicine, general', 'Biotechnology', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1007/978-3-031-08246-7_5,en,Efficient Archiving Method for Handling Preferences in Constrained Multi-objective Evolutionary Optimization,OriginalPaper,"This chapter presents a method for Preferences-Handling in Multi-objective Evolutionary Algorithms called Archiving Solutions in Regions of Interest, which consists of archiving solutions during the evolutionary process which are in areas of interest from the Decision Maker without considering the algorithm as the base searching engine. The method requires three input parameters: (1) a Multi-objective Evolutionary Algorithm has been adapted by adding the proposed archiving method after the Environmental Selection; (2) a set of reference directions used to determine the areas of interest of the Pareto Front; and (3) a set of thresholds associated with each component from the reference direction vectors, which intuitively determine the boundaries from the area of interest being covered. Four representative evolutionary algorithms have been considered to analyse the effect of our proposal, one coevolution inspired algorithm paradigm (CCMO) which is a domineering sorting genetic algorithm (NSGA2), and the SMS-EMOA and SPEA2 belonging to techniques that incorporate Quality Indicators of Multi objective Optimization. The results suggest that our proposed archiving approach allows generating solutions within the regions of interest on unconstrained, constrained, and real-world problems.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6004-8_14,en,Generalization of Fingerprint Spoof Detector,OriginalPaper,"In the modern computerized biometric authentication system, a fingerprint spoof detector is utilized to differentiate the real and fake human finger. The efficiency of spoof detectors will be enhanced by introducing more number of testing and training set of images. When the spoof detector is exposed to bogus images that are not part of the training set, the performance of any such spoof detector will degrade. To address the security threat posed by new spoof attacks, this paper proposes a Weibull-calibrated SVM (W-SVM) approach for recognizing the robustness in the new material used in spoof generation detection and a method to detect how the interoperability across the classifiers gets automatically adapted to new spoof materials. Experiments have been conducted with new segments of the database, which are built for spoofs composed of new materials and later evaluated with existing spoof detectors. It was discovered that while testing with the existing method, the rate of error increases; however, when the recommended adaptive approach was used, the spoof detection and performance gets improved significantly.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-3951-8_3,en,Using Convolutional Neural Networks to Detect Compression Algorithms,OriginalPaper,"Machine learning is penetrating various domains virtually, thereby proliferating excellent results. It has also found an outlet in digital forensics, wherein it is becoming the prime driver of computational efficiency. A prominent feature that exhibits the effectiveness of ML algorithms is feature extraction that can be instrumental in the applications for digital forensics. Convolutional neural networks are further used to identify parts of the file. To this end, we observed that the literature does not include sufficient information about the identification of the algorithms used to compress file fragments. With this research, we attempt to address this gap as compression algorithms are beneficial in generating higher entropy comparatively as they make the data more compact. We used a base dataset, compressed every file with various algorithms and designed a model based on that. The used model was accurately able to identify files compressed using compress, lzip and bzip2.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-11748-0_2,en,Federated Learning for Resource-Constrained IoT Devices: Panoramas and State of the Art,OriginalPaper,"Nowadays, devices are equipped with advanced sensors with higher processing and computing capabilities. Besides, widespread Internet availability enables communication among sensing devices that results the generation of vast amounts of data on edge devices to drive Internet-of-Things (IoT), crowdsourcing, and other emerging technologies. The extensive amount of collected data can be preprocessed, scaled, classified, and finally, used for predicting future events with machine learning (ML) methods. In traditional ML approaches, data is sent to and processed in a central server, which encounters communication overhead, processing delay, privacy leakage, and security issues. To overcome these challenges, each client can be trained locally based on its available data and by learning from the global model. This decentralized learning approach is referred to as federated learning (FL). However, in large-scale networks, there may be clients with varying computational resource capabilities. This may lead to implementation and scalability challenges for FL techniques. In this paper, we first introduce some recently implemented real-life applications of FL underlying the applications that are suitable for FL-based resource-constrained IoT environments. We then emphasize the core challenges of implementing the FL algorithms from the perspective of resource limitations (e.g., memory, bandwidth, and energy budget) of client devices. We finally discuss open issues associated with FL for resource-constrained environments and highlight future directions in the FL domain concerning resource-constrained devices.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning']"
doi:10.1007/978-3-031-09909-0_11,en,Dark-Matter Search Optimiser,OriginalPaper,"A version of the Gravitational Search Algorithm that takes advantage of the repulsive forces of dark matter to explore the space is presented in this article. The classification of matter into dark and gravitational particles is used to balance between exploration and exploitation of the feasible set of solutions, respectively. The idea is to overcome some problems of the Gravitational Search Algorithm as, for instance, trapping into local optima and the choice of parameters. Three different problems are used to demonstrate the potential of the algorithm when compared with the Particle Swarm Optimiser. In the future, we plan to do a thorough assessment of the algorithm on benchmark problems as well as on some applications.","['Engineering', 'Robotics and Automation', 'Robotics', 'Engineering Design', 'Biomedical Engineering and Bioengineering']"
doi:10.1007/978-3-031-10015-4_19,en,Hand Gesture Recognition for Sign Languages Using 3DCNN for Efficient Detection,OriginalPaper,"Sign Language Recognition aims at providing an efficient and accurate mechanism for the recognition of hand gestures made in sign languages and converting them into text and speech. Sign language is a means of communication using bodily movements, especially using the hands and arms. With sign language recognition methods, dialog communication between the deaf and hearing society can become a reality. In this project, we carry out sign language recognition by building 3D convolutional neural network (3DCNN) models that can perform multi-class prediction on input videos containing hand gestures. On detection of the input, both text and speech are generated and presented as output to the user. In addition to this, we also implement real-time video recognition and continuous sign language recognition for multi-word videos. We present a method for recognition of words in three languages – Tamil Sign Language (TSL), Indian Sign Language (ISL), and American Sign Language (ASL), and outperform state-of-the-art alternatives in with accuracies of 97.5%, 99.75% and 98% respectively.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biological and Medical Physics, Biophysics', 'Machine Learning', 'Bioinformatics']"
doi:10.1007/978-3-031-18256-3_14,en,Detection of Breast Cancer in Mammography Using Deep Learning Models,OriginalPaper,"Due to the growing need for specialized care in communities that do not have a local radiology service, and the long response times of diagnostic departments, there is a need to provide technologies that help medical diagnosis. In this project, we implement convolutional neural network models for the identification of mammography’s with data suggestive of breast cancer, considering the BI-RADS scale system in a binary classification based on its scales. We considered BI-RADS scales from 1–3 as benign and from 4–5 as malign, binary interpretation for both models (VGG16 and ResNet50) obtained results of 0.87 in accuracy.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-3-031-22200-9_80,en,Optimization of Central Pattern Generator-Based Locomotion Controller for Fish Robot Using Deep Deterministic Policy Gradient,OriginalPaper,"This paper presents the optimization of a CPG-based locomotion controller for a fish robot using Deep Deterministic Policy Gradient (DDPG). Firstly, the rhythm of swimming of an elongated undulating fin-like black Knife fish is generated by Central Pattern Generator (CPG). In the CPG network, the Hopf oscillators are employed to provide the rhythmical output and ensure continuous sinusoidal oscillation even when the CPG parameters are abruptly changed. The smooth transition output of the CPG is dependent on an intrinsic parameter of the oscillator called the convergence speed. This parameter is optimized by a combination of Deep Q-Network (DQN) and Policy Gradient (PG), which overcomes the drawback of traditional DQN, such as providing stable learnings to adapt specifically to dynamic environments. The simulation results demonstrate that the convergence speed of the modified CPG network based on DDPG is improved by about 2.2%. It also indicates that the rhythmical output of the CPG integrated with the DDPG optimizer can provide higher accuracy of oscillatory amplitude (about 1,6%) than do the traditional DQN, leading to high efficiency in controlling the swimming gait of the robotic fish.","['Engineering', 'Mathematical and Computational Engineering', 'Mechanical Engineering', 'Electrical Engineering']"
doi:10.1007/978-3-031-16075-2_21,en,Design and Develop Hardware Aware DNN for Faster Inference,OriginalPaper,"On many small-scale devices, advanced learning models have become standard. The necessity of the hour is to reduce the amount of time required for inference. This study describes a pipeline for automating Deep Neural Network customization and reducing neural network inference time. This paper presents a hardware-aware methodology in the form of a sequential pipeline for shrinking the size of deep neural networks. MorphNet is used at the pipeline’s core to iteratively decrease and enlarge a network. Upon the activation of layers, a resource-weighted sparsifying regularizer is used to identify and prune inefficient neurons, and all layers are then expanded using a uniform multiplicative factor. This is followed by fusion, a technique for combining the frozen batch normalization layer with the preceding convolution layer. Finally, the DNN is retrained after customization using a Knowledge Distillation approach to maintain model accuracy performance. The approach shows promising initial results on MobileNetv1 and ResNet50 architectures.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3148-2_48,en,Movement Analysis of Mentally Challenged Individuals Using Cloud and Machine Learning Model,OriginalPaper,"Movement analysis of mentally challenged becomes more open and prevalent with the advancement of sensors embedded in mobile phone devices and cloud infrastructure for data streaming. In this paper, ‘movement analysis of mentally challenged individuals using cloud and machine learning model’ a framework for monitoring specially-abled multiple individuals is proposed. In the proposed framework, different conventional classification techniques of machine learning (ML) algorithms such as logistic classifier, ensemble method, and deep learning (DL) networks such as recurrent neural network (RNN) and dense neural network are used to recognize their activities. The popularly known KAGGLE-UCI datasets which contain smartphone accelerometer data are used to develop the proposed ML model. Performance analysis of classification-based activities recognition schemes is evaluated in terms of precision, recall, $$F_1$$ -score, and accuracy. Results show that DL model with dense and dropout layers after hyperparameter tuning has performed better when compared with rest of the classifiers.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-18458-1_51,en,Conducting Semantic Segmentation on Landcover Satellite Imagery Through U-Net Architectures,OriginalPaper,"Through the rise of satellite imagery, with terabytes of imagery collected each day, utilizing spatial imagery gives another source of data for entities to use. With such high amounts of data, manually analyzing each image to gain insights on land cover becomes laborious. In order to quickly and accurately analyze large amounts of data to obtain land cover insights, the author utilizes deep learning methods in order to classify, pixel by pixel, satellite imagery. Through a publicly available dataset of 2,135 512 × 512 RGB satellite images of Poland, the author semantically segments each image into classes of buildings, forestry, bodies of water, or roadways. This model predicted woodland land cover at 86.1%, buildings at 24.7%, bodies of water at 80.2%, and roads at 61.0% accuracy. This basic implementation of a U-Net architecture demonstrates the high capabilities of such an architecture to classify parts of such images.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-0105-8_25,en,U-Shaped Xception-Residual Network for Polyps Region Segmentation,OriginalPaper,"Segmenting the region of interest helps gastroenterologists for removing polyps during the surgery in the gastrointestinal tract. We propose a segmentation system to segment the area of the polyp from the informative frames. Informative frames are the frames that contain at least a polyp in colonoscopy still frames. Our proposed U-shaped convolution neural network model utilizes the concept of residual connection and Xception as a backbone structure. We consider colonoscopy still frames as input to the segmentation model and achieved a Dice and Jaccard score of 86.3 and 79, respectively. It outperformed the conventional U-net with a 3.7% performance gain with respect to Dice score. This proposed method can be used as a reliable alternative system to identify a region of polyps during colonoscopy analysis.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-981-19-5221-0_57,en,Nuclei Segmentation Using UNet with EfficientNetV2 as Encoder,OriginalPaper,"Nucleus segmentation of H &E-stained (hematoxylin and eosin) histopathology images is a crucial step in developing a computer-aided diagnostic (CAD) system for cancer prediction and diagnosis. Nucleus segmentation technology has made it possible to subjectively and quantitatively analyze thousands and thousands of nuclei in H &E-stained histopathology images. The segmentation of variable touching nuclei, on the other hand, is a substantial issue during nuclei segmentation. This paper proposes a deep learning model for automatic nuclei segmentation. Since UNet performs well in medical image segmentation areas, a modified version of UNet architecture is utilized for the segmentation of nuclei. Generally, UNet architecture has a contracting path and an expansion path. In this paper, the contracting path or encoder of UNet is replaced by EfficientNetV2-L architecture. By making this change, the proposed model achieved the Jaccard index of value 0.85 along with a dice coefficient of 0.91 on the MoNuSeg dataset. After nuclei segmentation with the proposed model, a post-processing method is utilized to filter out noise in predicted masks images by using the watershed method.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5221-0_65,en,Deep Learning Approach for Brown Spot Detection and Nitrogen Deficiency Estimation in Rice Crops,OriginalPaper,"More than half of the people in the world rely on rice as their primary energy source. Two main challenges in rice cultivation are plant diseases and nutrient deficiency. Brown spots on leaves caused by the fungus and Cochliobolus miyabeanus, are one of the predominant plant diseases. Nitrogen is the most significant nutrient required in rice crops. Farmers monitor the plant’s growth by matching the leaf colour with the colour as in the leaf colour chart (LCC) (an indicator of the amount of nitrogen concentration in leaves). LCC is mostly used manually in order to estimate the nitrogen content and in turn the fertilizer amount, resulting in overestimating or underestimating this required portion. But, the other crop health factors like diseases are mostly ignored. Hence, a novel model based on deep learning is proposed to simultaneously detect brown spots and interpret leaf colour, thus estimating nitrogen amount as well as the occurrence of disease. The dataset consists of 1607 training and 402 test images, divided into three classes by observing the LCC chart as well as the presence of brown spots. Image pre-processing (gaussian blur, colour conversion, thresholding, and augmentation), K-means segmentation and feature extraction using InceptionV3 followed by classification using VGG-19 were done, which gave an accuracy of approximately 75%. This novel model is an exception to the existing models which focuses on one particular feature, as the proposed model detects two of the most relevant issues seen in rice cultivation, i.e. brown spots and nitrogen deficiency.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-4208-2_34,en,Solution of Lubrication Problems with Deep Neural Network,OriginalPaper,"Reynolds equation is fundamental to model lubrication in bearing and tribological components. In this work, we concentrate on lubrication analysis and solve the governing Reynolds equation with deep neural network. The governing Reynolds equation is converted in deep collocation method. In this work, Deep Neural Networks (DNNs) works as a pressure flow filed approximation to obtain flow field. In this work, we have performed the lubrication analysis and analyzed the flow characteristics obtained from Reynold Equation. To get the assurance of the developed programme, we have taken with some bench mark lubrication problems and explore the abilities of this method in Lubrication analysis. In the present work hydrodynamics journal bearing is simulated with Deep Neural Network Model and compared with the results FEM.","['Engineering', 'Industrial and Production Engineering', 'Robotics and Automation', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/978-981-19-0105-8_13,en,Semantic Segmentation of Road Scene Using Deep Learning,OriginalPaper,"The semantic segmentation task is the process of predicting each pixel in the image to some class. Classes could be road, person, car, tree, etc. Road scene segmentation has a vast application in the autonomous driving system. Researchers emphasize autonomous vehicles to reduce the human effort of driving and reduce the chances of road accidents caused by human error. Much research has been done to segment and classify objects in the road scene. We humans can easily detect and understand the objects around the road scene, but it is quite a challenging task for any machine. This article presents some state-of-the-art deep learning-based semantic segmentation techniques, namely FCN, SegNet, and UNet. And a new architecture named PNet for road scene semantic segmentation is proposed. The proposed network is trained end-to-end and comparatively evaluated with the state-of-the-art by using segmentation performance matrices mean Intersection over Union (mIoU) and Dice Coefficient measure (DCM). The publicly available CamVid dataset is used to train and validate the proposed PNet. We have achieved 70.7% mIoU and 80.9% DCM, which is comparatively better than the state-of-the-art.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-981-19-3590-9_58,en,Stock Prediction System Using an Integrated Fine Tune Stacked and Ensembled Activation LSTM Network,OriginalPaper,"Money is the essential commodity for the survival of human beings, irrespective of their social economic status. People attract towards easy money, one of the easy money earning is spending their money on shares. Every coin has two sides, similarly people invested in shares may get profit or loss. So, stock prediction is gaining more attention by the business people to forecast the value based on time series and recommend a good share to the investors. Traditional approaches solve the stock prediction problem using artificial networks which deal only with features associated with the dataset but the major decision element of any stock prediction system is previous transactional records. The model addresses this issue by designing LSTM network by using ensemble activation functions. Existing neural networks use static architecture, i.e. standard values for all the parameters which are not suitable to for all types of stock markets. The proposed system identifies the hyper-parameter values for each estimator and designs the neural network with these values and ensemble activators to suit for generalized stock and financial data.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security', 'Mobile and Network Security']"
doi:10.1007/978-981-19-1607-6_39,en,Performance Evaluation of Boosted 2-Stream TCRNet,OriginalPaper,"Target detection in infrared imagery is a particularly challenging problem due to the presence of terrain clutter. The TCRNet-2 CNN architecture was introduced to combat this issue and has been shown to perform better than conventional networks such as faster RCNN and YOLOv3. In this paper, we evaluate the performance of the Boosted 2-Stream TCRNet in detail (including robustness to range variations, performance under day and night conditions) and compare it with that of YOLOv5. A MWIR dataset released by DSIAC is used for training and testing the network. We also propose the MWIR target classifier that recognizes the 10 classes in the NVESD dataset and achieves an accuracy of 65.72% which is state-of-the-art to date.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-7842-5_3,en,Deep Learning Approaches for Classroom Audio Classification Using Mel Spectrograms,OriginalPaper,"Active learning can be immensely beneficial for science, technology, engineering, and mathematics (STEM) students as well as instructors because they engage themselves with various activities in the classroom so that lectures will be more effective. Automatic audio classification for classroom activities can help us to improve the active learning strategies in classroom. In this paper, we compare the different deep learning approaches (e.g., convolutional neural network, long short-term memory (LSTM)) for classroom audio classification. We evaluate the models on three classes of activity labeled as “single voice”, “multiple voice” and “no voice” based on our classroom recording. The model is trained on the Mel spectrogram extracted from the recorded audio. We get the highest accuracy of 97% and F1 score of 0.97 with the 2D Convolutional Neural Network.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16072-1_49,en,Data Augmentation Methods for Electric Automobile Noise Design from Multi-Channel Steering Accelerometer Signals,OriginalPaper,"Noise, vibration, and harshness (NVH) of electric automobiles is important because the loud NVH can reduce the satisfaction of automobile drivers and passengers. Therefore, the effective machine learning models to alleviate NVH is required. Although a huge amount of data is needed to construct the reliable models, the number of training data is very scarce in practice. In this paper, we propose a deep learning model combined with data augmentation methods (dropout and SpecAugment) that predicts interior noise levels from steering accelerometer signals when only a small number of training data is available. The effectiveness of the proposed framework was demonstrated using steering automobile accelerometer signals and noise levels from real automobiles.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-13150-9_13,en,LightMobileNetV2: A Lightweight Model for the Classification of COVID-19 Using Chest X-Ray Images,OriginalPaper,"Deep Learning is most widely used in the area of medical imaging. Due to its success, many works of literature use deep learning methodsto classify COVID-19 related cases. Currently, the world is facing the second and third waves of this deadly disease. Therefore, the need of the hour is to develop some user-friendly lightweight model that could easily detect the disease with little effort. It is found that early diagnosis is the only way to defeat this deadly virus. This paper tried to classify the chest X-ray(CXR) images into different categories, such as normal lung opacity, viral pneumonia, and COVID-19 positive using a transfer learning mechanism. For this classification task, we have removed eight blocks of the pre-trained MobileNetV2model, and then it is fed to the classifier consisting of a dropout and linear layers. The proposed model is approximately 9times lighter than the original MobileNetV2 and performing almost with the same accuracy. Experimental evaluation proves that the proposed model gives satisfactory results compared with the original and other recent works.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Management']"
doi:10.1007/978-981-19-4162-7_25,en,Enhanced Video Classification System with Convolutional Neural Networks Using Representative Frames as Input Data,OriginalPaper,"Convolutional neural networks are extensively used in video classifications systems, which uses video action recognition data set which consists of 13,320 videos grouped to 101 classes. In this proposed method, we are using a promising way of speeding up the training by using representative frames from each video for the entire video data set. The advantage is that the training time can be drastically decreased with the compact input video data. Here we are using the inceptionv3 pretrained model for classification where the weights and architecture can be used for similar types of inputs. Instead of giving the entire video frames for video classification, the proposed method, selects representative frames only, still producing comparable results. When used with ‘action recognition data set’ having 101 classes and 13,320 videos, while using full frames, it gives a F 1 -score of 0.717 and while using representative frames, it results with a 0.704 value which is comparable. When all the frames of the videos are given as input to the pretrained model, the performance is considerably high. However, there is a direct implication on the cost as far as time is concerned. The main objective of this research work is to enhance the computational efficiency of the video classifier by using the representative frames for speeding up the process of training the data set.","['Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery', 'Systems and Data Security', 'Mobile and Network Security', 'Information Systems Applications (incl. Internet)']"
doi:10.1007/978-3-031-11058-0_57,en,Mathematical Modeling the H2SO4-Catalyzed Alkylation of Isobutane with Olefins,OriginalPaper,"The purpose of this work is to develop a mathematical model of sulfuric acid alkylation of isobutane by olefins, taking into account the physical and chemical laws of the process. Due to the increased environmental requirements for fuel, the relevance of sulfuric acid alkylation of isobutane with olefins, as it allows us to achieve good results in this matter. Significant features of the technology that need to be taken into account when modeling are identified. The main schemes of transformations of sulfuric acid alkylation of isobutane by olefins are considered. The probability of all reactions is estimated from the Gibbs energy value. The model is a system of ordinary nonlinear differential equations. The direct problem was solved by Radau IIA method. During the simulation, heuristic optimization methods were used, the Electromagnetism like Algorithm and the Harmony Search algorithm were considered to minimize the deviation of experimental data from theoretical ones. The rate constants of the reaction under consideration are found. The foundations for further modeling of the entire chemical-technological process are laid.","['Engineering', 'Control and Systems Theory', 'Control, Robotics, Mechatronics', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-2394-4_55,en,Image Recognition to Detect COVID-19 Violations: Saudi Arabia Use Case,OriginalPaper,"The upsurge in the number of criminal cases in Saudi Arabia is a cause for concern. More so, with the recent emergence of COVID-19, the government has forbidden specific social behaviors, which means that any breach of these prohibitions will be classified as a criminal. This work leverages the immense ability of deep learning architectures to develop and evaluate models to detect images of people or a person either violating or observing COVID-19 rules. For instance, an image of a person/s wearing a face mask would definitely fall under the category of non-violation, whereas an image of people hugging or shaking hands is an indication of a violation of COVID-19 rules. The model is trained and evaluated on a bunch of images that we have extracted from social media sites, and it produces exceptional results in the image classification assignment that we have performed.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-1457-7_72,en,Research on Operation Data Mining of Pulse Dust Collector,OriginalPaper,"By analyzing the filtering mechanism of the filter bag and the deficiency of static injection method based on fixed pressure difference, three models are selected to predict the pressure difference data. Experimental results show that all three models can predict the change trend of the pressure difference data. The MSE of BP neural network is 0.00994, indicating that the pressure difference data is predictable. The method of dynamic injection with pressure difference as control parameter is feasible. In order to solve the problem of flood of fault alarm information of pulse bag filter, the unsupervised learning method in machine learning was analyzed for the data set of fault type. By comparing the unity of the classification results of the three algorithms, it was found that 67% of the three algorithms have the unity 1, and the unity rate of the three algorithms was greater than 0.998 under different K values. It is proved that this classification algorithm is feasible and has good prediction accuracy.","['Engineering', 'Automotive Engineering', 'Engineering Fluid Dynamics', 'Energy Storage']"
doi:10.1007/978-3-031-21595-7_2,en,Rice Plant Disease Detection and Diagnosis Using Deep Convolutional Neural Networks and Multispectral Imaging,OriginalPaper,"Rice is considered a strategic crop in Egypt as it is regularly consumed in the Egyptian people’s diet. Even though Egypt is the highest rice producer in Africa with a share of 6 million tons per year [ 5 ], it still imports rice to satisfy its local needs due to production loss, especially due to rice disease. Rice blast disease is responsible for 30% loss in rice production worldwide [ 9 ]. Therefore, it is crucial to target limiting yield damage by detecting rice crops diseases in its early stages. This paper introduces a public multispectral and RGB images dataset and a deep learning pipeline for rice plant disease detection using multi-modal data. The collected multispectral images consist of Red, Green and Near-Infrared channels and we show that using multispectral along with RGB channels as input archives a higher F1 accuracy compared to using RGB input only.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Computer Communication Networks', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Machine Learning']"
doi:10.1007/978-981-19-3148-2_67,en,Recognition of Handwritten Digits Using Neural Networks: A Review,OriginalPaper,"Handwritten digit recognition is an ongoing and challenging research topic. In today’s world, the method for identifying handwritten digits is quite important, as there are plenty of methods to choose from. Handwritten digit recognition is an extremely common task and therefore requires techniques that are not only efficient but also that is extremely accurate. Currently, determining the correct meaning of handwritten characters is extremely difficult. The characters vary in their shapes and sizes depending on the writing style. There are numerous postal addresses for applications, as well as, bank checks that require handwritten digit recognition. Handwritten recognition techniques can help solve a variety of problems and make people’s jobs easier. The focus of this review article will be on several strategies for recognizing handwritten digits. The limitations and advantages of several neural network algorithms used for recognizing handwritten digits will be outlined in this review document. As a result, this paper provides an overview of various neural network algorithms for digit recognition as well as their limitations and precision rates.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-6068-0_10,en,Breast Cancer Detection Using Concatenated Deep Learning Model,OriginalPaper,"The research on cancer is taken a superior space for medical and technological professionals. Since a few years its research grows for both where maximum involvement of technological research is clearly visible. In the article, we analyze the earlier works for detection. Along with it, a model is proposed that performs well and will take the readers to next level of work. A concatenated model containing convolutional layers and long short term memory layers is proposed for cancer detection from the histopathological images. The Adam optimization algorithm is used for minimizing the error and to train the network that is one of the supervised learning methods. To check the practicability of the proposed method publicly available breast cancer dataset is taken to train, validate, and test the network. The proposed method resulted in 95.32% testing accuracy.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-030-99075-6_18,en,Rolling Bearing Remaining Useful Life Prediction Based on LSTM-Transformer Algorithm,OriginalPaper,"Bearings are the most critical components in modern industrial rotating machinery. If a bearing is damaged, it can lead to serious consequences such as an interruption to a production line and financial losses. It is important to monitor the bearing operation condition and to predict the remaining useful life (RUL) of bearings so that a scheduled maintenance can be planned ahead. In order to improve the accuracy of a bearing RUL prediction, a new data-driven RUL prediction technique based on Long Short-Term Memory (LSTM) network and Transformer network is proposed. Firstly, a total of 8 degradation characteristics in both time and frequency domains are extracted from the bearing data to be used as the input features. After the data preprocessing steps such as normalization and sliding window interception, the degradation characteristic dataset is obtained. Then, the proposed LSTM-Transformer technique is applied to the characteristic dataset for training and prediction. The prediction result shows that the proposed technique can effectively overcomes the information loss of LSTM network caused by the increase distance between the input and output sequences to produce a more accurate RUL prediction. The RUL prediction obtained using the proposed technique is compared with those using existing techniques such as GRU, LSTM and CNN networks for an evaluation of the effectiveness and efficiency of the proposed technique. It is confirmed that the proposed technique can yield a more accurate bearing RUL prediction than the existing techniques.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-3-031-16868-0_1,en,Evolutionary Computation,OriginalPaper,"EC is a class of nature-inspired algorithms that maintains a population of candidate solutions (individuals) and evolves toward the best answer(s). It has been frequently used to solve difficult real-world optimization problems since it evolves numerous solutions at the same time, which contribute to the notable characteristic of EC as being frequently insensitiveness to local minimal.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4162-7_35,en,An Effective Model for Malware Detection,OriginalPaper,"Malware detection and identification is important to protect an organization’s data and enable end-to-end monitoring of resources accessible by multiple users through Internet. Malicious users and Intruders usually try various methods to gain unauthorized access to data from remote locations. This paper proposes a model that helps in finding the malware characteristics by extracting features of the data provided. This model is also tested for unknown malware files generated using various available tools. This paper discusses the steps used in building an effective model, Model for Malware Detection (MMD) using EMBER dataset and Keras. The results obtained with model accuracy of 97.2% are presented.","['Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery', 'Systems and Data Security', 'Mobile and Network Security', 'Information Systems Applications (incl. Internet)']"
doi:10.1007/978-981-19-3938-9_6,en,A Multi-fidelity Aeroelastic Optimization of an Aircraft Wing Using Co-Kriging,OriginalPaper,"In the present study, a multi-fidelity, multi-objective and multi-disciplinary design problem statement for aeroelastic optimization of an aircraft wing has been posed. The problem minimizes the twin objectives of transonic drag and the weight of the structure during cruise flight at Mach 0.8 at 10 km altitude. The wing geometry was parametrized using 11 design variables. A multi-fidelity-based co-kriging metamodel was used to replace the multi-disciplinary analysis routine in the optimization problem. Reynolds-averaged Navier–Stokes (RANS) and Euler solvers were used as high and low fidelity aerodynamic solvers while refined and coarse meshes were used with a default Finite Element (FEM) solver for multi-fidelity structural analysis. Latin hypercube sampling was used to generate 100 design points, out of which 30 were used to perform high fidelity simulations and the rest were used for low fidelity simulations. A high fidelity MDA routine run had a computational time of 4 h while the low fidelity runs were of 30 min. A successful and computationally inexpensive coupled aeroelastic optimization methodology has been demonstrated using MDF and co-kriging. The aerodynamic coefficients $${C_\textrm{l}}$$ C l and $${C_\textrm{d}}$$ C d showed improvement of 4.69% and 17.9%, respectively, compared to the baseline values. The structural weight of the optimized geometry was reduced by 355.7 Kg, and there was 14.54% reduction in the maximum von-Mises stress in the optimized structure.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-981-19-2394-4_49,en,Analysis of an Independent Double Boost Interleaved Converter in a Renewable Energy Application,OriginalPaper,"In this paper a brief analysis has been carried out targeting a DC-DC step-up converter used within renewable energy applications. The converter topology involved is called as an “independent double boost interleaved converter” (IDBIC) with three-level output. The operation mode of the proposed power electronics conversion stage has been analyzed in the context of a photovoltaic application. The converter is used (in the present application) as an interfacing stage between three PV modules connected in series and the deserved load. An MPPT algorithm was also implemented as the control law that drives the application, in order to extract the maximum power from the solar panels array. A series of tests were performed in two study cases: In first case an off-line co-simulation using Plexim—PLECS and MATLAB—Simulink has been done. In second case an on-line Real-Time, Rapid Control Prototyping (RCP) simulation using C2000 F28069M Texas Instruments DSP with HOTLINK jTAG connection to the host computer that runs Altair/Solid Thinking Embed (formerly known as VisSim) as RCP development environment has been made. The resulting current and voltage waveforms for different cases of solar irradiation values were represented and analyzed. Also, the efficiency of the power stage was determined at different levels of solar irradiation.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2065-3_46,en,Reviewing Fake News Classification Algorithms,OriginalPaper,"Fake news is a terminology that has a different meaning to different people. At its bb in part because they are so easily and quickly shared online. In this paper, we propose a method of classifying fake news using Passive Aggressive classifier (PAC) [ 1 ] and afterward we also compare the results given by this model with machine learning classification algorithms like Naïve Bayes, Decision Trees Classifier (DTC), Random Forest Classifier (RFC), K-Nearest Neighbor (KNN), Support Vector classifier (SVC), Logistic Regression and Deep Learning algorithms like Long Short-Term Memory (LSTM) and Bi-Directional LSTM. ‘Bag of Words’ and TF-IDF technique are used to convert the textual data into vectors.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Machine Learning']"
doi:10.1007/978-981-19-3938-9_22,en,Optimization of Lot Size in Lubricant Blending Plants,OriginalPaper,"A common dilemma occurs when the plants want to maximize lubricant production lot sizes to improve overall equipment efficiency (OEE), reduce line changeover and flushing. However, this leads to higher working capital and logistics costs. We have developed a convex programming model to approach this problem by incorporating plant production data for the optimization of lot sizes of lubricant portfolio. In this model, we utilize data on production capacity, asset runtime, underlying costs such as raw material, labor, storage and the market demand drivers to find the lot size by optimizing the economic order quantity (EOQ). The whole problem is formulated into a convex model, and we used interior point method in solving the problem. Furthermore, an inventory sensitivity metric is applied to project the effect of inventory on lot sizes and all the costs. Initial deployment of this model has shown a reduction of 10% to 20% of the production cost and freeing up more than hundred hours of manual effort.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-981-19-4162-7_28,en,Fine-tuning for Transfer Learning of ResNet152 for Disease Identification in Tomato Leaves,OriginalPaper,"Plants provide a significant portion of the world’s food supply. Tomato is the most popular plant which is cultivated worldwide. The tomato leaf disease is the primary factor in productivity loss but can be avoided by monitoring regularly. Detection of tomato leaf diseases using pre-trained deep learning models can help to reduce the severity of the disease identification. However, instead of using a pre-trained model directly, there is an optional step to fine-tune the model in transfer learning, which improves the model performance. The examination of fine-tuning the model with four various scenarios of transfer learning and the art of employing pre-trained models were suggested in this work. Experiments were done using the pre-trained model ResNet152 on tomato leaf disease identification.","['Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery', 'Systems and Data Security', 'Mobile and Network Security', 'Information Systems Applications (incl. Internet)']"
doi:10.1007/978-981-19-4676-9_23,en,An Exploration of Machine Learning and Deep Learning-Based Diabetes Prediction Techniques,OriginalPaper,"Diabetes is now one of the world’s leading chronic diseases, affecting the middle-aged and elderly in most cases. This disease will gradually transform a person into death. There is an imbalance in blood glucose with the consequence of this disease that prompts the production of lower insulin. Medical science for the treatment of this disease is now advancing steadily. In addition to this, research focused on artificial intelligence (AI) is now advancing to define the stage of diabetes so that steps can be taken by everyone. A state-of-the-art analysis of various techniques for predicting diabetes is seen in this paper. For the last decade, several techniques based on machine learning (ML) and deep learning (DL) have been focusing on diabetes prediction. This research shows a summary of the published literature on the prediction of diabetes in the last six years. A recommendation system for observing the health of a patient through a web portal is proposed at the end of this article.","['Engineering', 'Computational Intelligence', 'Systems and Data Security', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-1540-6_7,en,Pill Defect Detection Using an Improved Convolutional Neural Network,OriginalPaper,"A novel effective method to detect pill defects during pill manufacturing is proposed in this study. We have developed an analysis program that incorporates deep learning convolutional neural networks to fully automate the image analysis of pills for internal crack detection. The deep learning tool based on YOLO algorithm is effectively implemented into the industrial pharmaceutical workflow. Firstly, we analyze Gauss filtering and smoothing techniques for pill detection. Secondly, Hog feature extraction is introduced to simplify the representation of the image that contains only the most important information about the image. Lastly, improved YOLO is designed for online detection of pill defects. The proposed approach obtains robust quantification of internal pill cracks.","['Engineering', 'Mechanical Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/978-981-19-2126-1_18,en,Anomalous Human Activity Detection Using Stick Figure and Deep Learning Model,OriginalPaper,"The population of the world is estimated to reach 8 billion by the year 2024. With the increase in human population, the crime rate keeps increasing rapidly. Law enforcers cannot be present in every corner of the world to monitor and avert crimes. But with the help of advanced computer vision techniques, advanced surveillance systems can be developed where the pose of a human is estimated to predict the action of the individual: normal or suspicious. This paper focuses on finding the key points on the human body and to develop and train a model using deep neural network which takes the key points as input and outputs the pose of the individual. This greatly reduces human surveillance and provides precise and accurate surveillance in every place required.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-981-19-5403-0_29,en,SMS Spam Detection Using Deep Learning Approach,OriginalPaper,"The popularity of mobile phones has increased drastically in the recent years which is making users vulnerable to various threats like SMS spam, where the user is deceived into revealing private information that could result in a security breach. The motivation of this research is to curb the attackers, hackers, etc., from using SMS spam to exploit mobile device users. Several researchers proposed various machine learning models to automatically detect spam, but they could not achieve a commendable accuracy rate. In this research, several machine learning and deep learning models are utilized to detect SMS spam. A dataset from UCI is used and deep learning models are developed to detect and classify SMS spam using LSTM and BERT. The results are compared with the previous models in SMS spam detection. The proposed deep learning approach obtained the highest accuracy of 99.28% using BERT and 98.84% using LSTM. We utilized Python for all implementations.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-3-031-16072-1_13,en,Self-supervised Learning for Predicting Invisible Enemy Information in StarCraft II,OriginalPaper,"In real-time strategy games such as StarCraft II, players gather resources, make buildings, produce various units, and create strategies to win the game. Especially, accurately predicting enemy information is essential to victory in StarCraft II because the enemy situation is obscured by the fog of war. However, it is challenging to predict the enemy information because the situation changes over time, and various strategies are used. Also, previous studies for predicting invisible enemy information in StarCraft do not use self-supervised learning, which is extracting effective feature spaces. In this study, we propose a deep learning model combined with a contrastive self-supervised learning to predict invisible enemy information to improve the model performance. The effectiveness of the proposed method is demonstrated by quantitatively and qualitatively.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2004-2_38,en,Comparative Analysis of Machine Learning Models to Predict Stock Market Price,OriginalPaper,"Stock market forecasting is viewed as one of the most interesting is of study for many researchers. The crucial data that can be accessed is thought to have predictive correlations with future stock performance which could provide information to investors so that they may make better decisions when purchasing equities. The paper tries to present a comparative analysis of four machine learning models to predict stock market price. The methods that we have considered are: support vector machine (SVM), artificial neural network (ANN), and hybrid models like PCA + SVM and PCA + ANN to predict stock market state. We have experimented using Vanguard Total Stock Market ETF (VTI) dataset for last 10 years which shows that SVM-based predictive model performed well among all the models for predicting the stock market status.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-981-19-5331-6_51,en,Ensemble Deep Learning Model for Breast Histopathology Image Classification,OriginalPaper,"Breast cancer is a life-threatening disease that affects individuals all over the world. As a consequence, effective and precise breast cancer screening is crucial. Early detection of breast cancer allows patients to obtain the best treatment available, boosting their probability of surviving. Several studies have resulted in the development of computational algorithms for predicting breast cancer progression using a variety of diagnostic imaging modalities. In this paper, a deep learning approach for the classification of breast cancer histopathology images is carried out. The proposed model is hybrid combination of Inspection-ResNetv2 and EfficientNetV2-S with pretrained weights as ImageNet. The proposed model was validated on BreakHis and Breast Cancer Histology (BACH) dataset. For concatenation of both networks, top layer was removed and global average pooling was added, followed by dense layer, dropout and final classification layer. The proposed model was evaluated in comparison with individual results obtained by Inspection-ResNetv2 and EfficientNetV2 model results. The final classification layer consists of dense layer of four neurons of BACH dataset classification and for BreakHis 8 neurons. The experimentation of proposed model showed good results by achieving overall accuracy of 98.15% for the BACH dataset and 99.036% for BreakHis dataset.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2225-1_23,en,Convolutional Neural Networks for Audio Classification: An Ensemble Approach,OriginalPaper,"Machine perception has steadily become more accurate with deep learning methodologies. Abundant multimedia data sources have made navigating audio data essential. This work performs environmental sound classification as a step toward integrating artificial intelligence in audio data. Audio files are converted to tensors, resampled and then converted to mel spectrograms to account for human sensitivity to different audio frequencies. Pre-trained and high-performing convolutional neural networks (CNNs) are leveraged to train the ResNet-152 and DenseNet-121 architectures for transfer learning. The custom ensemble model uses these models for inference. The outputs of the models are combined and passed through a dense layer to generate an ensemble capable of inferring correct weightage for each of the models without manual interference. The ensemble model achieves promising results with an accuracy of 91%, and precision and recall of 0.91 and 0.93, respectively. The results demonstrate that a CNN-based ensemble method is adept at extracting and generalizing temporal information from audio signals.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Statistics, general', 'Mobile and Network Security']"
doi:10.1007/978-981-19-4676-9_33,en,Steering Wheel Angle Prediction from Dashboard Data Using CNN Architecture,OriginalPaper,"Rath, Manas Kumar Swain, Tanmaya Samanta, Tapaswini Banerjee, Shobhan Swain, Prasanta Kumar Various innovations on self-driving cars are trending in the automobile industry these days. The general approach for AI applications is to collect the data through various sensors that are fit in a car, process them through appropriate techniques, and then train a model upon which one can try and test the efficiency of the model. Many companies like Tesla, Uber, Waymo (a subsidiary of Google), and Mercedes are already working with a lot of sensor-captured data and high computation power. The sensors range from normal cameras to high-end ultrasonic and LIDAR sensors. Whatever be the data that we capture, the basic intention is to move swiftly through a path given various twists and turns, and other traffic conditions, where the car needs to be correctly steered as per the external environment. We should know that more the data we capture, more shall be the complexity of the system, hence more will be the required computational power for that. In this paper, we present a simple model where we capture data through a front dashboard camera, process it through a CNN, and predict the appropriate steering angles based on the external traffic conditions, where we have acquired a significant level of accuracy. We assume automatic transmission system in our vehicle without clutch and gears.","['Engineering', 'Computational Intelligence', 'Systems and Data Security', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-3571-8_27,en,Comparative Study of CNN-Based Multi-Disease Detection Models Through X-Ray Images,OriginalPaper,"Adaptation of computer-aided techniques in health care is continually improving diagnosis and treatment using chest X-ray images. Deep learning approaches are proving to be effective in offering more accurate disease detection However, there are still significant hurdles in medical imaging. In this paper, presented an experiential comparative analysis of popular deep learning-based convolutional neural networks (CNN’s) models such as ResNet50, Xception, VGG16, and VGG19 using transfer learning for multi-disease detection. Although experimented with several deep convolution architectures but presented here top most only. This paper addresses four classes (chest disease) classification using chest X-ray, namely COVID, Normal, Pneumonia, and Tuberculosis. All four models are trained, tested, and validated using the same chest X-ray dataset which consists of 700 images for each disease. The comparative result presented, accuracy, predict output, training and validation loss, confusion matrices, error rate, and F 1-score.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-15191-0_31,en,Managing Spatial Big Data on the Data LakeHouse,OriginalPaper,"The objective of this paper is to propose some of the best storage practices for using Spatial Big data on the Data Lakehouse. In fact, handling Big Spatial Data showed the limits of current approaches to store massive spatial data, either traditional such as geographic information systems or new ones such as extensions of augmented Big Data approaches. Our article is divided into four parts. In the first part, we will give a brief background of the data management system scene. In the second part, we will present the Data LakeHouse and how it responds to the problems of storage, processing and exploitation of big data while ensuring consistency and efficiency as in data warehouses. Then, we will recall the constraints posed by the management of Big Spatial Data. We end our paper with an experimental study showing the best storage practice for Spatial Big data on the Data LakeHouse. Our experiment shows that the partitioning of Spatial Big data over Geohash index is an optimal solution for the storage.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security']"
doi:10.1007/978-981-19-5331-6_28,en,Temperature Dependency of TGS822 Model on PSPICE,OriginalPaper,"The Metal Oxide Gas Sensor is one of the general, accessible choices for the detection of gases. Its high sensitivity nature makes it even more popular among the other choices. Here, TGS822 Model is implemented on the PSPICE to study its characteristics. In this work, by using PSPICE simulation we have presented novel FNN technique to show its dependency on temperature and concentration. The plots show the deviation in voltages that occurs due to change in various types of gas temperatures when the other factors such as relative humidity and concentration are kept constant.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-17091-1_64,en,Food Carbon Footprint: Challenges and Opportunities,OriginalPaper,"“Carbon Footprint” (CF) term has become popular over the last decade due to the high rate of climate change. Food related GHG emissions is a main cause of climate change, which needs to be reduced. Following the raising awareness of the food impact on carbon footprint of consumers, a number of recent studies have analysed how the citizen contribute to the reduction of emission of carbon ‘footprint’. This paper identifies the challenges and opportunities in being able to evaluate carbon ‘footprint’ (named CF) of food choices and we reviewed some current approaches of CF calculator tools at dish ingredient level, or recipe level, and finally, applications that help to reduce Carbon Footprint of foods.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-0105-8_55,en,Bengali POS Tagging Using Bi-LSTM with Word Embedding and Character-Level Embedding,OriginalPaper,"Part-of-speech tagging (POS) is an important and very fundamental process in natural language processing (NLP). POS tagging is required as a preprocessing task in many types of linguistic research such as named entity recognition (NER), word sense disambiguation, information extraction, natural language translation, and sentiment analysis. In this paper, we propose a practical Bengali POS tagger, which takes as input a text written in Bengali and gives a POS tagged output. In recent times, Bi-LSTM networks have been proven effective in sequential data processing but not very much tested on resource-poor and inflectional languages like Bengali. This paper addresses the issues of the POS tagging task for the Bengali language using Bi-LSTM with transfer learning by applying pre-trained word embedding information. The POS tagged output from our proposed model can be used directly for other applications of Bengali language processing as our proposed tagger can also handle out-of-vocabulary (OOV) words. Our experiment reveals that Bi-LSTM with transfer learning is effective for tagging Bengali documents.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-981-19-6068-0_31,en,Sentiment Analysis of Stress Among the Students Amidst the Covid Pandemic Using Global Tweets,OriginalPaper,"Covid-19 pandemic has affected the lives of people across the globe. People belonging to all the sectors of the society have faced a lot of challenges. Strict measures like lockdown and social distancing have been imposed several times by governments throughout the world. Universities had to incorporate the online method of teaching instead of the regular offline classes to implement social distancing. Online classes were beneficial to most of the students; at the same time, there were many difficulties faced by the students due to lack of facilities to attend classes online. Students faced a lot of challenges, and a sense of anxiety was prevalent during the uncertain times of the pandemic. This research article analyzes the stress among students considering the tweets across the globe related to students stress. The algorithms considered for classification of tweets as positive or negative are support vector machine (SVM), bidirectional encoder representation from transformers (BERT), and long short-term memory (LSTM). The accuracy of the abovementioned algorithms is compared.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-15191-0_16,en,Combining Static and Contextual Features: The Case of English Tweets,OriginalPaper,"In recent years, social media networks have emerged as a veritable source for gathering information in various disciplines. This enables the extraction and analysis of users’ feelings, opinions, emotions and reactions to different topics. Pre-trained Neural Language Models (LMs) have yielded outstanding scores on a variety of Natural Language Processing (NLP) related tasks across different languages. In this paper, we propose that the efficiency of contextual word vectors using LMs for social media can be further boosted by incorporating static word embeddings that are specifically trained on social media (e.g., Twitter). We demonstrate that the combination of static word embeddings with contextual word representations is indeed effective for building an enhanced Sentiment Analysis system for English tweets.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security']"
doi:10.1007/978-981-19-4863-3_11,en,DeepLeaf: Analysis of Plant Leaves Using Deep Learning,OriginalPaper,"A growing number of scientists are examining the issue of the survival of plant species under adverse climate conditions caused by global warming. The extinction of some plant species is a more concern and such they must be saved one must have experience and expertise with the species before one can assess it which is manual and time consuming. Various Scientific methods are being evolved such as image processing, digital camera, mobile devices, pattern recognition but it is lagging by accuracy. For such problems, solution could be to identify the correct species of plant by identifying recent methods like Convolutional Neural Network (CNN) and Visual Geometry Group-16 (VGG16), deep learning, machine learning. The proposed System is comprised CNN and VGG16 for Feature fusion extraction which extracts shape, texture, Contour, Margin. Finally, the results of each feature were combined and classified using Hyper Parameter Tuned Gradient Descent (HPTGD) classifier with dimension reduction method PCA. This paper represents collection of images, preprocessing, and extraction of features using deep learning methods and Classification on Flavia dataset. The preprocessing was done on images, Augmented and forwarded for CNN + VGG16 and Classifier. Our model achieved an accuracy up to 97%. It has been observed that the VGG16 architecture with HPTGT classifier achieved better accuracy at a similar execution time compared to other methodologies.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-4786-5_49,en,Near-Infrared Vascular Image Enhancement Using Deep Convolutional Neural Network,OriginalPaper,"Near-infrared vascular images play an important role in the diagnosis and treatment of vascular diseases. However, near-infrared vascular images often have problems such as low image quality and unclear vascular patterns. To solve these problems, we propose a Deep Convolutional Neural Network (DCNN) auto-encoder for image enhancement to enhance vascular structures and suppress non-vascular structures. We also collect a datasets of 156 images for the training and validation testing of the model; and further we use the full-reference image quality assessment metrics, i.e., Peak Signal to Noise Ratio (PSNR) and Structural SIMilarity (SSIM) to quantitatively evaluate the image enhancement effect of this model. The experimental results show: compared with the traditional image enhancement algorithm, the enhanced image quality of the Residual Convolutional Auto-Encoder (RCAE) model is better and more similar to the original image.","['Engineering', 'Manufacturing, Machines, Tools, Processes', 'Engineering Economics, Organization, Logistics, Marketing', 'Industrial and Organizational Psychology', 'Artificial Intelligence', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-981-19-1976-3_12,en,Recognition of Hand Gesture-Based Sign Language Using Transfer Learning,OriginalPaper,"Sign languages are the natural languages of the deaf and dumb community and provide access to communication. The discipline of sign language recognition and translation is exploding. Hand gesture-based sign language recognition is a fortunate application of human–computer interaction. A developed model detects sign language gestures involving feature extraction and classification. This work presents transfer learning-based image recognition models using VGG16 and ResNet50 to translate static hand gestures into text with 57 gesture classes. Fine-tuning hyperparameters of VGG16 and ResNet50 are used to further improve image recognition accuracy. The test results are compiled and evaluated on the large-scale dataset and showed that the recognition rate was significantly improved compared with Support Vector Machine, Random Forest, Logistic Regression, and XGBoost.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5224-1_71,en,Sentinel-2 Data Processing for Pichavaram Mangrove Forest Using Convolutional Neural Network,OriginalPaper,"Image classification is commonly utilized in computer vision tasks like remote sensing, scene analysis, object detection, and image retrieval. We use the Sentinel-2 satellite dataset in our study to classify land cover using the convolutional neural network (CNN) method and determine different plant indices, water indices, and geology features in the Pichavaram mangrove forest. Mangrove woods protect the shoreline from ocean waves, Tsunami storms, and soil erosion. They are efficient at sequestering and storing carbon and mitigating climate change. It is critical to map the extent of the mangroves to protect them. We need an automation solution because the geographic expansions are so large. For automatic feature extraction, CNN is used. Our datasets have been divided into water bodies, vegetation, and marshy plains. Sentinel-2 has a variety of uses, including land monitoring, yield prediction, land cover, flood volcanic eruption detection, and landslide detection.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-981-19-3998-3_99,en,Self-adversarial Network Based on MADDPG for Emergency Power Dispatch,OriginalPaper,"A self-adversarial network based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG) is proposed for emergency power dispatch. This paper proposed two methods to improve the performance of MADDPG in mixed cooperative-competitive environments. The self-adversarial mechanism sat an adversarial network to improve the convergence and training speed. For the relations of cooperation, a self-attention layer is set for improving communication efficiency. Experiment results on the MPE environment show that the proposed model method has better performance and greater learning speed than MADDPG.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-5331-6_62,en,Patch-Based Autoencoder for Document Image Denoising with Smoothening,OriginalPaper,"Documents can be stored in digital formats permanently thanks to the ease of digitization. While scanning or capturing documents, noises such as low resolution, printing noise, and compression noise might be introduced. Noise has a negative impact on document analysis activities such as OCR and text identification. Denoising and restoration of noisy scanned document images are a classical image processing problem that aims to eliminate such type of artifacts from given noisy images. We propose a novel deep learning pipeline for denoising followed by smoothening, ultimately enhancing the quality of scanned documents images. We train an end-to-end patch-based convolutional autoencoder network along with ResNet skip connections. We introduce a noisy-clean paired dataset by synthetically adding noises to corresponding clean images. Our model is shown to effectively remove both physical noises like coffee stains and pencil scribbles, as well as digitally introduced noises like printer noise, compression noise, and low resolution, among others.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5331-6_71,en,Insect Image Semantic Segmentation and Identification Using UNET and DeepLab V3+,OriginalPaper,"Semantic image segmentation, which has become one of the most important applications in image processing and computer vision, has been used in a wide range of fields. This study aimed to construct a novel large-scale soybean insect dataset in unconstrained environment. It includes 2558 images of three species and their ground truth. Firstly, this paper summarizes the dataset development which includes reformatting, annotation, bounding box, masking, and splitting dataset. Furthermore, two deep learning methods (UNET and DeepLab V3+) have been applied for semantic segmentation of insect. Later, it compared the predicted labels of image segmentation models with respect to its ground truth in terms of IoU, loss, accuracy, precision, recall, and $$F_1$$ -score. The results are a witness in favor of the adopted feature engineering methods, where both the methods have demonstrated competitive performance, however, DeepLab V3+ slightly outperforms.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-1607-6_32,en,Learn to Ask What You Don’t Know,OriginalPaper,"Asking questions relates to the cognitive ability of language comprehension and context understanding. For that reason, question generation is a challenging topic in Natural Language Understanding. In this work, we propose a task called “question generation with masked target answer,” which emphasizes asking questions from text passages without providing a target answer. Compared to other related question generation tasks, our task demands rigorous language comprehension and closely resembles the question asking ability of humans. We then propose various sequence to sequence-based models leveraging additional information about the text, such as its part of speech and named entity recognition(NER) tags. Results show that the proposed models perform on par with other related question generation tasks, despite lacking the key answer phrase.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3387-5_149,en,Apple Leaf Disease Identification Method Based on Improved YoloV5,OriginalPaper,"In this paper, an improved YoloV5s based identification model, named as GSE_YoloV5s, is proposed for apple leaf disease identification, which can efficiently address the problem of high storage and computational resource consumption for the YoloV5s model. Firstly, the GhostBottleneck module is used to replace the original CSPBottleneck module to reduce the parameters and computation of the YoloV5s; meanwhile, by adding the channel attention module SE (Squeeze-and-Excitation), the model’s detection performance for small target lesions is improved. The experimental results show that, the improved GSE_YoloV5s model can reduce the number of parameters and computational effort by 40%, as compared with the YoloV5s model, and the AP (Average Precision) achieves 83.4%, which can effectively detect apple leaf diseases.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-19032-2_2,en,Towards Reliable Solar Atmospheric Parameters Neural-Based Inference,OriginalPaper,"Methods of Stokes profile inversion based on spectral polarization analysis represent a powerful tool for obtaining information on magnetic and thermodynamic properties in the solar atmosphere. However, these methods involve solving the radiation transport equation. Over the past decades, several approaches have been developed to provide an analytical solution to the inverse problem, but despite its advantages, in many cases it requires large computing resources. Neural networks have been shown to be a good alternative to these methods, but in general they tend to be overly confident in their predictions. In this paper, the uncertainty estimation of atmospheric parameters prediction is presented. It is shown that deterministic networks containing partially-independent MLP blocks allow one to estimate uncertainty in predictions achieving the high accuracy results.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-981-19-4676-9_37,en,Improvement of Speaker Verification Using Deep Learning Techniques,OriginalPaper,"Deep learning (DL) has been used to solve a range of real-time artificial intelligence (AI) challenges with great success. This is a cutting-edge area of machine learning (ML) that has been rapidly evolving. As a result, deep learning is quickly becoming one of the most popular and well-defined machine learning techniques, with applications in a wide range of fields, including image processing, computer vision, speech and speaker recognition, emotion recognition, natural language processing, hand-written character recognition, cyber-security, and many others. Over other prevalent methods, DL approaches have demonstrated superior performance in speech processing areas like as voice recognition and speaker recognition. We describe an experimental setup for speaker verification (SV) utilizing DL techniques, and discuss its performance and findings, as well as how it outperformed established approaches such as HMM, GMM-UBM, and SVM. In this research works, we analyse and review deep neural network (DNN) approaches employed in SV systems. With a 1.51% equal error rate (EER), the final result is the best performance of the SV systems of restricted Boltzmann machine (RBM)-based DNN.","['Engineering', 'Computational Intelligence', 'Systems and Data Security', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-3148-2_51,en,Forecasting of COVID-19 Cases in India Using Machine Learning: A Critical Analysis,OriginalPaper,"COVID-19 is an infectious disease that has spread over the world since the first case was discovered in China in December 2019. Multiple variants of COVID-19 have been discovered in the last two years, indicating that it is highly mutable. The most recent variant is omicron, which has similar transmissibility to the delta variant and so has a significant risk of producing a third wave in India. This study analyzes five distinct time series forecasting models: autoregression (AR), exponential smoothing (ES), multilayer perceptron (MLP), long-short term memory (LSTM), autoregressive integrated moving average (ARIMA), and their hybrid models. The purpose of this research is to find the best machine learning model for forecasting COVID-19 cases, as the number of novel variant omicron cases in India is on the rise. As a result, it is necessary to forecast COVID-19 cases to make appropriate precautionary actions in order to avert the third wave of COVID-19 in India.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-4606-6_83,en,Gravity Search Algorithm-Based Path Planning of Single Humanoid Based on the Study of Different Artificial Intelligence Techniques,OriginalPaper,"The paper presents a rigorous study of the different path planning approaches for humanoid navigation in an unknown space. Different reactive techniques along with classical techniques are discussed in the paper for navigation of the humanoid robot in different terrain. The paper deals with a comprehensive discussion of the above methods to identify the best intelligent technique for the navigation-based planning of humanoids. It was found that the convergence of the classical method is dependent upon the navigation environment and the level of selection of parameters. The paper introduces a new controller (based on the gravity search algorithm, GSO) to model humanoid navigation in cluttered terrain. The controller was tested in both simulation and experimental environments. It was found that the designed controller was effective in achieving optimized path navigation and the percentage error was within the acceptable limits.","['Engineering', 'Industrial and Production Engineering', 'Machinery and Machine Elements', 'Materials Engineering']"
doi:10.1007/978-981-19-2145-2_43,en,"Prediction of BOD from Wastewater Characteristics and Their Interactions Using Regression Neural Network: A Case Study of Naidu Wastewater Treatment Plant, Pune, India",OriginalPaper,"Analysis of variance (ANOVA) results were used to analyze the behavior of various water quality parameters like Total Suspended Solids, Biochemical Oxygen Demand, and Turbidity. The turbidity is not a recommended water testing parameter for wastewater testing, still it was measured on experimental basis. The datasets used in this work were derived from a detailed experimental investigation of inlet and outlet water parameters of Naidu wastewater treatment plant, a major conventional treatment plant in Pune City, India. It has an average flow rate of 115 MLD. The samples were collected daily for over three months. Statistical data analysis methods such as correlation coefficient, regression analysis, and graphical representation of the data were used to find the interrelations between selected parameters. The regression neural network model obtained from the analysis of data was used to predict BOD parameter considering standard error. The p -value obtained by ANOVA analysis was observed to be significant at p  < 0.05. The ANN model predicted the results at maximum accuracy of 96% and average accuracy of 94%. Good interrelation between the selected parameters was observed and it was observed that BOD can be predicted using suspended solids and turbidity of water sample.","['Engineering', 'Building Construction and Design', 'Construction Management', 'Transportation Technology and Traffic Engineering']"
doi:10.1007/978-981-19-6068-0_35,en,COVID-19 Detection Using CNN-ResNet-50 Model,OriginalPaper,"COVID-19 is a deadly virus that originated in 2019 and could be easily transmitted from one geographical area to another. It affected the integral world, resulting in severe mortality due to its contagious effect on human life. The infection rate is continuously growing and it is becoming unmanageable since the virus moves easily from one human to another. Once we detect the COVID-19 virus in its early stages, we can easily reduce the death rate. The most common and widely used method of diagnosing COVID is through reverse transcription polymerase chain (RT-PCR). But the RT-PCR test is time consuming, inaccurate, and expensive. In this situation, the time period for the detection of viruses is valuable. Keeping these limitations in mind, we use an X-ray image of the chest to identify the COVID-19 infected patient. This procedure is achieved by using convolution neural network (CNN) in deep learning.","['Computer Science', 'Artificial Intelligence', 'Computational Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-4193-1_46,en,Plant Species Recognition from Leaf-Vein Structures Using ResNets,OriginalPaper,"Leaf-vein structures are equivalent to fingerprints for a plant species. Each species of plant has a unique vein structure and this structure identifies species. In this paper, we have used a leaf-vein dataset that contains 64 × 64 pixel green-channel center-focused images for four species, two from monocotyledon and the other two from dicotyledon categories. We have trained two state-of-the-art Residual Neural Network (ResNet) models with a recently introduced leaf-vein image dataset. We also introduced two extended versions of those models. Our study shows that ResNets are efficient in recognizing vein structures from those partial leaf-vein images with 78.98% accuracy for ResNet50 and 81.63% accuracy for ResNet101. Also, our proposed extended versions of the ResNets prove to be more efficient than the existing ones with around 82% accuracy for DenseResNet50 and approximately 83% accuracy for DenseResNet101 models.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-04435-9_41,en,Evaluation of Adjustment Models for Taking Body Temperature with a Thermal Imaging Camera in the Context of COVID-19,OriginalPaper,"In the context of the COVID-19 health emergency, it is necessary to have devices that help identify symptoms that indicate whether a person has COVID-19. As one of the main symptoms is fever, which can be identified by measuring body temperature, different non-contact measurement methods are being widely used as an alternative to traditional contact thermometers. However, readings with thermographic cameras present limitations in terms of high dependence on the environment. For this reason, this article aims to validate the temperature measurements of a thermographic camera by comparing different models using as a reference standard the readings of a calibrated medical infrared thermometer. For this purpose, 463 measurements were analyzed using an infrared thermometer and a thermographic camera simultaneously. As significant differences were observed between the measurements made using statistical analyses with $$ (p <0.05) $$ ( p < 0.05 ) , models were developed establishing weighting and compensation criteria to obtain similar readings between the values measured from the thermal images and the infrared thermometer. For uncertainty estimation, linear and non-linear regression models such as artificial neural networks were tested, selecting the best model that allows reducing the variation among the readings.","['Social Sciences', 'Science and Technology Studies']"
doi:10.1007/978-981-19-5331-6_65,en,Stock Market Prediction Using Recurrent Neural Network and Long Short-Term Memory,OriginalPaper,"The aim this paper is to make the trader’s life easy with all kinds of exploratory data analysis and to bring the forecast with the deep learning model. We have used a different kind of analysis to come to a point on which we have built a model completely based on long short-term memory and recurrent neural network. We have also performed different types of graphs to explain the trend of the company user searches. The trend is a line graph depicting years and months based on user selection. Since forecasting has highly emerged, there has been a lot of research on this topic. The stock market is always the scientist’s favourite category to show their skills. This paper attention on the analysis and prediction of stock values that do not take care of party-political tenures, and financial tension which disturbs the stock market. This model will assist a stockholder, distinct user or the general public to make protected investments.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-2394-4_41,en,EfficientNeXt: EfficientNet for Embedded Systems,OriginalPaper,"Convolutional neural networks have come a long way since AlexNet. Each year the limits of the state-of-the-art are being pushed to new levels. EfficientNet pushed the performance metrics to a new high and EfficientNetV2 even more so. Even so, architectures for mobile applications can benefit from improved accuracy and reduced model footprint. The classic Inverted Residual block has been the foundation upon which most mobile networks seek to improve. EfficientNet architecture is built using the same Inverted Residual block. In this paper we experiment with Harmonious Bottlenecks in place of the Inverted Residuals to observe a reduction in the number of parameters and improvement in accuracy.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18256-3_11,en,CNNs for ISCI Stage Recognition on Video Sequences,OriginalPaper,"Intracytoplasmic sperm injection (ICSI) is one of the most commonly applied techniques for in vitro fertilization. This technique consists of the single selection of a spermatozoon followed by the injection of this sample into the oocyte’s plasma. The embryologists perform the ICSI by using their judgment to select the spermatozoon to inject. Additionally, they decide the best technique to penetrate the oocyte with the needle. Therefore, the success of an ICSI procedure can be affected by subjective decisions such as the characteristics of the sperm selected, the angle at which the needle pierces the oocyte, and the speed at which it is performed. The main objective of this project is to develop a computational tool that can automatically identify the different stages of the ICSI procedure. A tool like this will automate the activation of other artificial intelligence tools that can assist the embryologist while performing the ICSI (e.g., assistants to select the best sperm to inject and guides for the injection technique). Our results indicate the feasibility of employing a deep neural network architecture to determine the stage of the ICSI procedure from a video stream from a camera attached to a microscope.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-981-19-3632-6_36,en,Deep Dense Autoencoder Using Modulation Spectrogram for Machine Unsupervised Anomaly Detection,OriginalPaper,"The purpose of this paper is to design an abnormal sound detection system to detect abnormal sound during mechanical operation. The system uses the modulation spectrogram as a feature of the sound signal to train a dense autoencoder. Using the development set provided by DCASE2021 to compare 7 machine types with its baseline system, the result is better than the baseline system. Among the 7 machine types, the effects of Fan, Gearbox, Pump, Slider and Valve are significantly better than the baseline system provided by DCASE2021. By comparison, we believe that the method of feature extraction has an impact on the training of the neural network. In addition, the number of layers of the neural network should not be too large.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing']"
doi:10.1007/978-981-19-5221-0_37,en,A Study and Analysis of Algorithms on Intelligent Systems to Recognize Hand Written Digits and Characters,OriginalPaper,"The article includes a vast survey on the algorithms applied for hand written digit and character recognition and analysis of the best effective algorithm that can be used, and the effectiveness here is measured with accuracy. The major uses of recognition of handwritten characters and digits are interpreting postal addresses, check amounts of bank, tax documents, and filled forms. In case of reading the bank check amount, the accuracy of the recognition should be high or should be 100% as it is a very sensitive information and any mistakes in recognizing that will lead to complications within the bank system. So, recognizing the handwritten digit and character accurately plays a very important role in the real-world application, and this paper includes a survey and analysis on the most effective algorithm that can be used. The algorithms considered in this study are random forest, multilayer perceptron, random tree, support vector machine, Naïve Bayes, Bayes Net, J48, CNN, hybrid CNN-SVM, KNN, NN, RFC, single layer network with PCA, multilayer network -LeNet-5 CCN architecture. Analyzing the algorithms that can be used to recognize the handwritten digit and character, it is observed that CNN algorithm produces more accuracy in recognizing the hand written digits and characters.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5224-1_59,en,Recognition of Struck Out Words Using a Deep Learning Approach,OriginalPaper,"Handwritten document contains a lot of unreadable texts or elements which have no meaning such as struck out words or characters. If such words are fed into a handwriting recognition system, there can be a drop in the accuracy of the system, and it may result into predicting false words. In this paper, we propose an approach to detect these struck out words. With the help of CNN model, we train the model to recognize and differentiate the normal words from the struck out words. For this purpose, some common types of struck out strokes were handled. In order to obtain a handwritten text free of these words, this method may identify strike-through text, locate the word/character, and erase these words. The model was trained on a set of English words and characters that we generated, and it was then put to the test on a range of texts that contained words that had been struck out. The experimental results demonstrate the accuracy of the proposed approach, where the models achieved accuracy levels of 100%.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-981-19-0105-8_30,en,BUS-Net: A Fusion-based Lesion Segmentation Model for Breast Ultrasound (BUS) Images,OriginalPaper,"Breast cancer is the most common cancer(s) among women worldwide. The survival rate decreases if the cancer is not detected at an early stage. Breast ultrasound (BUS) is emerging as a popular modality for breast cancer detection owing to its several advantages over other modalities. We proposed a novel deep learning framework named BUS-Net for automated lesion segmentation in BUS images in this work. However, every deep learning framework has disadvantages of its own; however, the drawbacks associated with individual models can be overcome when combined. Our proposed BUS-Net is an ensemble of three popular deep learning frameworks, namely attention U-net, U-Net and SegNet. The final segmentation map generated by BUS-Net is a pixel-level fusion on the outputs of each of the individual frameworks. The potentiality of BUS-Net was tested on a publicly available dataset named BUSI dataset. This dataset consists of 647 tumor images collected from 600 different female patients. To prevent biased results, the training and test set were separate. BUS-Net framework achieved an accuracy—93.19%, precision—93.18%, recall—88.75%, dice—90.77%, and volume similarity—95.55% for lesion segmentation in the test set. The degree of correlation between the lesion region segmented by the medical experts and that segmented by BUS-Net was high ( $$R^2 = 0.9131$$ R 2 = 0.9131 ). Further, the performance of BUS-Net was also compared with the state-of-the-art techniques. This comparison showed that BUS-Net maintains a tradeoff between precision and recall, proving the robustness, efficiency, and reliability of the framework.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-981-19-3632-6_37,en,Multi-dimensional Convolutional Neural Network for Speech Emotion Recognition,OriginalPaper,"Speech Emotion Recognition (SER) is a difficulty of deep learning algorithms. The difficulty is that people’s own understanding of emotions is not absolute. Different people may also have different judgments on the same speech. And speech emotion recognition plays a huge role in many real-time applications. With the continuous development of deep learning in recent years, many people use convolutional neural networks (CNN) to extract high-dimensional features in speech from speech spectrograms, thereby improving the accuracy of speech emotion recognition. In contrast, we propose a new model of speech emotion recognition. The model uses the eGeMAPS feature set extracted through the openSMILE toolkit to input into our model. The model learns the correlation and timing between features. In addition, we perform intra-class normalization on the input features to ensure more accurate recognition and faster data fitting. In our model, the key speech segments can be selected through the characteristics of convolutional neural network (CNN), so that the recognition accuracy of the model can achieve a better effect. Our model was evaluated experimentally in the IEMOCAP dataset. Experimental results show that our unweighted accuracy (UA) and weighted accuracy (WA) on the test set reached 60.9% and 63.0%.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing']"
doi:10.1007/978-981-19-2397-5_56,en,Sign Language Recognition of Selected Filipino Phrases Using LSTM Neural Network,OriginalPaper,"The use of Filipino Sign Language (FSL) has contributed to the improvement of communication of deaf people; however, the majority of the population in the Philippines does not understand FSL. The study explored computer vision in obtaining the images and deep learning techniques in building the automated FSL recognition model to bridge the communication gap between the deaf community and the hearing majority. The model has been trained using LSTM neural network using the features extracted using MediaPipe Holistic from video files of Filipino phrases performed by three (3) FSL signers. The SLR system developed could recognize (15) continuous Filipino words. The model evaluation has shown an impressive result wherein the average accuracy achieved on the test set is 94%. In the experimentation conducted on 10 participants using the SLR system, the overall accuracy obtained on two trials is 72.38%, with an average prediction time of 0.3 s. Based on the analysis of the experiment results, the developed SLR system is robust, is time efficient, is signer independent, and can detect both manual and non-manual features of the gesture. More data will be collected for future directions to enable a conversational SLR system with more FSL vocabularies and sentences.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3938-9_38,en,Optimization of Fused Filament Fabrication for Maximum Stiffness Considering Anisotropy,OriginalPaper,"Build orientation (BO) is the spacial arrangement of an object with respect to the build plate of the additive manufacturing (AM) machine. Many researchers have shown that BO has significant influence on the properties of the printed part. In this work, we study the effect of BO on the structural properties of the fused filament fabrication (FFF) part. First, we characterize the anisotropy in the part using orthotropic material model. Tensile tests were carried out to extract the elastic constants. An FE analysis is performed using the part model and the extracted elastic constants. Then, we perform an optimization to find the BO corresponding to minimum structural compliance. The results show that there is significant reduction in compliance of the structure.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-981-19-5224-1_68,en,PROPHETESS: A Tool for Prediction of Prophage Loci in Bacterial Genomes,OriginalPaper,"This paper describes the design, development, and implementation of a standalone bioinformatic tool for the prediction of putative prophage loci in bacterial host genomes using statistical measures, based on the algorithm published as the “Prophage Loci Predictor for Bacterial Genomes” and described as the loci predictor algorithm. This algorithm proposed a novel approach to the problem of detecting prophage regions in bacterial genomic information using particle swarm optimization, using a fixed size pattern lookup table to detect virus-like pattern distributions in the host/bacterial genome. As this algorithm was designed with the intension of providing highly consistent and fast performance, the time-to-process sequence is the primary metric for evaluating the performance of the tool, and the processing speed was expected to scale only with the size of the genome under consideration and not on the size of the pattern database as is the case with other algorithms in its class. The implemented tool was evaluated using both the algorithms test and training sets and was shown to obtain a linear co-related performance as expected in both training and prediction phases of the performance testing.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-3-031-19032-2_34,en,Learning Various Locomotion Skills from Scratch with Deep Reinforcement Learning,OriginalPaper,Proficiency in locomotion skills will help robots to navigate over challenging terrains. This task is hard to solve programmatically due to the wide variety of terrains and motion patterns. Here we present a framework to learn an agent capable of solving the task of moving with desired linear and angular velocity. The agent learns the task in a curriculum which gradually increases the difficulty of the learned task. We carefully tune the reward function for the agent. The training process is performed in a simulator with domain randomization which forces the agent to learn a robust policy. We tested the proposed framework on the quadruped robot and achieved competitive results.,"['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-3-031-18409-3_3,en,Hand SOS Gesture Detection by Computer Vision,OriginalPaper,"Computer vision has been applied widely in cybersecurity, specially for authentication purposes like iris or fingerprint recognition. Image processing techniques also allow to understand hand gestures of the sign language alphabet, among others. Combining both approaches, in this paper, a system to detect the hand SOS gesture is proposed. By training a model to understand hand gestures, the detection of a certain sequence of hand gestures make possible to identify the SOS signal. The proposed method can be deployed in surveillance systems and others devices with a camera such as social robots. So, victims can ask for help silently and alarms can inform the authorities automatically.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Education, general']"
doi:10.1007/978-3-031-08580-2_13,en,SDNs Delay Prediction Using Machine Learning Algorithms,OriginalPaper,"The transmitting time of a packet between two devices is an essential factor in evaluating the network quality. Previous studies have applied machine learning to predict the connection delay value between two devices in traditional networks. However, there is little research using Software-Defined Networks (SDNs) because of the lack of SDNs traffic datasets. A method for collecting SDNs traffic data with delay values is proposed in this manuscript. In addition, this paper also evaluates different learning algorithms for SDNs delay prediction using the collected data. Experimental results showed that the Bidirectional LSTM had the lowest losses and the Recurrent Neural Networks had the shortest inference time.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-1906-0_36,en,Use of Deep Neural Networks in Detecting Breast Cancer Lesion,OriginalPaper,This paper presents a deep neural network module for breast cancer lesion detection which is trained and tested over 569 datasets. The model produces an accuracy of 0.95 in predicting the benign and malignant lesions in the breast. This model gives high accuracy to few advancement in technology with multiple input views and optimised amongst many choices. A thorough analysis of the model’s performance on various populations is conducted. The python code used in the model uses tensor flow library and a seaborn visualisation library. The data sets used in the code are segregated as training and testing datasets. Training sets contain 80% of data and testing sets contain 20% of data. Histogram images are created to view all the parameters separately for training and testing. Later the accuracy and cost are found so that it can be implemented in real-world applications without any wrong predictions.,"['Engineering', 'Electronics and Microelectronics, Instrumentation', 'Computer Systems Organization and Communication Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-1906-0_52,en,Home Automation Using Brain–Computer Interface,OriginalPaper,"Brain–Computer Interface (BCI) is a link whichconnects the brain of humans with the outside peripheral devices and machines, and it is a mechanism that allows the users to interact with the outside environment. BCI can be considered an extension of human–computer interaction because BCI is the control of computers by brain waves. The brain waves produced inside the brain can be analyzed with the help of BCI after which simple machine learning algorithms can be used to classify the signals into different classes and hence, map the brain signals into computer commands to operate different external devices. This simple idea is used for home automation to create a basic domotic (Corralejo et al. in A domotic control system using brain-computer interface (BCI). Springer, Berlin, Heidelberg, 2011 [ 1 ]) system. This will also be very beneficial for people suffering from neuromuscular disorders like stroke, cerebral palsy and who can’t operate their voluntary muscles like those in the hands and legs.","['Engineering', 'Electronics and Microelectronics, Instrumentation', 'Computer Systems Organization and Communication Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3938-9_4,en,Supply Chain Optimization for Mainstreaming SAF in the Indian Aviation Sector,OriginalPaper,"Sustainable Aviation Fuel (SAF) will play a key role in the coming decades not only in decarbonizing the aviation sector but also in improving the rural economy. Nevertheless, the challenge remains in achieving competitive pricing of the SAF blends in a well-established, conventional Aviation Turbine Fuel (ATF) market. Consequently, the objective of this work is to propose an optimal supply chain network, with focus on optimizing the locations of multiple SAF production facilities. A Mixed Integer Linear Programming (MILP) model is formulated and solved that enables the selection of optimal spatial distribution of a centralized supply chain configuration. The optimization framework is demonstrated for facilities in southern India to evaluate the effect of regional biomass availability on the overall transportation cost associated with different configurations. We describe the methodology and present various case studies for supplying SAF to five airports situated in the chosen area of study.","['Engineering', 'Mathematical and Computational Engineering', 'Optimization', 'Machine Learning']"
doi:10.1007/978-981-19-0095-2_18,en,Traffic Sign Detection and Recognition,OriginalPaper,"Surge in the amount of automobiles on street imposes the consumption of automatic systems for driver aid. These structures form significant instruments of self-driving automobiles as well. Traffic Sign Recognition remains such an automatic structure which affords the relative responsiveness aimed at self-driving automobile. In this work we are able to perceive and identify traffic signs in video classifications detailed by an onboard automobile camera. Traffic Sign Recognition (TSR) stands used to control traffic signs, inform a driver and facility or proscribe definite actions. A debauched real-time and vigorous instinctive traffic sign finding and recognition can upkeep and disburden the driver and ominously upsurge heavy protection and ease. Instinctive recognition of traffic signs is also important for automated intellectual driving automobile or driver backing structures. This paper presents a study to identify traffic sign via OpenCV procedure and also convert the detected sign into text and audio signal. The pictures are mined, perceived and recognized by preprocessing through numerous image processing methods. At that time, the phases are accomplished to identify and identify the traffic sign arrangements. The structure is trained and endorsed to find the finest network architecture. Aimed at the network exercise and assessment we have generated a dataset containing of 1012 images of 8 diverse classes. The tentative results demonstrate the exceedingly accurate groupings of traffic sign patterns with composite contextual images and the computational price of the planned system. Though, numerous features make the road sign recognition tricky and problematic such as lighting state changes, occlusion of signs due to hitches, distortion of signs, gesture blur in video images.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Systems and Data Security', 'Artificial Intelligence', 'Computational Intelligence']"
doi:10.1007/978-3-030-96314-9_2,en,Challenges and Benefits for Detecting Soon-to-Fail Drives in Industry 4.0,OriginalPaper,"Data storage and processing is an integral component of Industry 4.0 applications. Failures of storage devices may lead to system corruptions and malfunctioning of physical infrastructures. In this chapter, we explore a methodology to monitor the storage devices that are used in an Industry 4.0 environment and investigate mechanisms for early detection of failing devices. In particular, we describe the usage of machine learning models over data that describe the physical condition of the storage devices to determine whether a device is ill-functioning. We support our proposed models with experimental results over a large dataset, and we provide an analysis on the performance of our algorithms.","['Engineering', 'Industrial and Production Engineering', 'Operations Research/Decision Theory', 'Facility Management']"
doi:10.1007/978-3-031-15928-2_86,en,Design for Additive Manufacturing Tools: Are They an Effective Support for Designers?,OriginalPaper,"Additive manufacturing (AM) is currently one of the most promising industrial technologies that allow designers to operate with more degrees of freedom to create shapes without overthinking restrictive manufacturing constraints. Products must be conceived with the “AM on mind” to exploit AM potentialities. Design for AM (DfAM) methods and tools, such as topology optimization and generative design, are crucial for this aim. The present paper aims to understand how existing DfAM tools can effectively support the DfAM process. The study is based on the definition and application of a systematic evaluation protocol consisting of quantitative and qualitative metrics. The case studies involved four commercial DfAM tools tested on three mechanical components. Results confirmed that most of the tools lead to very similar solutions from the technical point of view since they are based on analogous optimization algorithms. The consideration of manufacturability constraints and the availability of advanced functionalities for geometry reconstruction after the optimization phase are relevant issues observed. Finally, regarding tools functionalities, notable differences have been registered.","['Engineering', 'Engineering Design', 'Industrial and Production Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-981-19-5077-3_32,en,Prediction of Interface Friction Angle Between Landfill Liner and Soil Using Machine Learning,OriginalPaper,"This study employs machine learning (ML) techniques and artificial neural networks (ANN) to predict the interface friction angle between the landfill liner and the soil. The interface behavior is majorly affected by the thickness of landfill liner (t), mass of landfill liner (m), tensile strength of landfill liner (T), cohesion of soil (cu), angle of shearing resistance of soil ( Φ ), shear strength ( τ ), and normal stress ( σ ). As the stability of landfill liner varies significantly from that of the soil due to the non-homogeneity and anisotropic character of the soil, it is critical to comprehend the interface behavior between the landfill liner and the soil. However, no prior research employing machine learning techniques to analyze the interface behavior between landfill liners and soil has been reported; a study using machine learning algorithms and artificial neural networks is carried out on 66 datasets to probe the interface behavior with the help of an ANACONDA navigator. Further, to understand the impact of input variables on the output variable, Pearson’s correlation coefficients were determined. Mean absolute error (MEA) is considered as a loss function, and the best model was chosen based on the r 2 -value. Random forest regressor (RFR) model is determined to be the best model among the available models with an r 2 -score of 0.99 and a minimum mean absolute error of 0.46.","['Environment', 'Environment, general', 'Geoengineering, Foundations, Hydraulics', 'Sustainable Development', 'Environmental Engineering/Biotechnology']"
doi:10.1007/978-981-19-3391-2_29,en,Hybrid Classification Algorithm for Early Prediction of Alzheimer’s Disease,OriginalPaper,"Alzheimer's disease is the most common type of dementia found. Dementia is actually a syndrome related to an ongoing decline of brain functioning. Alzheimer’s is caused due to increase in age, genes inherited, depression, and factors related to lifestyles. AD at its final stage cannot be treated. The intermediate stages like mild cognitive impairment (MCI), can be treated, so that the risk of developing AD can be decreased. In this work, structural MRI images are used and hybrid approach is introduced to detect MCI with good accuracy. ADNI dataset is used for project work for classification of cognitive normal (CN) and mild cognitive impairment (MCI). ADNI provides MRI data along with demographic information such as age, gender, physical examinations, and other neurobiological data. Initially, the MRI data is subjected to segmentation using K-means clustering in order to extract 2D images and gray matter. These segmented images are preprocessed using discrete wavelet transform (DWT), which is then further classified into MCI and CN classes. Random forest (RF) classifier, artificial neural network (ANN) is discretely implemented and the accuracy of prediction is calculated. Further, these algorithms are hybridized in order to achieve improved accuracy of prediction. By hybridizing the algorithms, an accuracy of 93.47% was achieved.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-2012-7_8,en,Big Data Analytics in Industry 4.0,OriginalPaper,"With the unpredictable development of technology, a wide variety of data is produced in a very short time from countless sources. Industry 4.0 is a revolution in new technology for the digital world of digital factories and smart products. The ‘Big Data’ used by machines to communicate with each other using the internet of things is the building block of Industry 4.0. Big data is not just data to be stored or accessed. It is also data that is made sense and analyzed. In this chapter, the characteristics of big data, its applications, and its place in industry 4.0 are presented. Firstly, history and definitions of the big data are introduced in this chapter, then some applications are explained by using big data methods and techniques. Finally, opportunities and challenges are mentioned for the future aspects of using big data analytics.","['Engineering', 'Industrial and Production Engineering', 'Professional Computing', 'Statistics, general', 'Cyber-physical systems, IoT']"
doi:10.1007/978-981-19-3951-8_44,en,Dermoscopic Image Analysis for Skin Lesion Classification Using Dense Convolutional Network,OriginalPaper,"Skin cancer has become a more common disease in most parts of the world and accounts for high-mortality rates. Accurate diagnosing of skin cancer or lesion by observing or with hand-crafted methods is a more challenging task for expert dermatologists. Therefore, an automated and intelligent system is essential for the accurate detection of lesions. In this work, we have used a dense convolutional network (DenseNet) for automated skin lesions classification of seven types of skin lesions comprising of basal cell carcinoma, melanoma, melanocytic nevi, actinic keratosis/bowens disease, dermatofibroma, benign keratosis, and vascular lesion. The dense convolutional network used comprised of 5 dense blocks and each dense block comprised of batch normalization layer, ReLU, and two convolution layers which empower the maximum information flow between layers. The Softmax loss combined with image feature similarity restraint is further used to minimize the misclassification loss and to preserve the intra-class feature similarity scatter. We evaluated our model on the HAM10000 dataset, which consists of 10,015 images of seven skin lesion types. Our method achieved AUC of 0.96 and a precision of 0.92 for melanocytic nevi. Classification accuracy of the model is 88.86% in validation set which is approximately equivalent to trending performance of traditional models in skin lesion classification.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16159-9_9,en,Controller Modelling as a Tool for Cyber-Attacks Detection,OriginalPaper,"This paper aims to develop a method for detecting intrusions into the control system, consisting of malicious modifications of the control algorithm or its parameters. Data from the control loops are routinely collected and archived. Therefore we propose a data-based diagnosis method consisting of modelling the controller using data from normal operation and calculating residual as a difference between model output and real control signal. Larger residual values indicate a change in the controller behaviour, possibly resulting from the malicious intervention. We use two modelling approaches: linear models and neural networks. The linear model additionally gives estimates of PID controller parameters. The methods are evaluated and compared on simulated and real control data. Finally, a superheaters system case study presents the method’s effectiveness for detecting intrusions into the control system.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-3-031-05918-6_1,en,Effect-Engineering by Additive Manufacturing,OriginalPaper,"With the help of effect-engineering, highly efficient additively manufactured products with a high-power density can be designed. The potential of product development lies in the conceptualization design and embodiment design phases, which have, however, only been methodically analyzed to a limited extent. Effect-engineering offers the possibility to resolve constructive contradictions and to influence disturbance variables. The research question answered in this article describes how a methodical procedure for effect-engineering must look to design highly efficient products for additive manufacturing. Simulation and multi-criteria optimization are particularly challenging in this context. For this purpose, a framework of effect engineering will be developed and the effects that offer significant added value for additive manufacturing will be shown. Furthermore, new system technologies in additive manufacturing are presented, which serve as enablers of the various effects. As a result of the contribution, the method of effect-engineering is successfully applied to two application examples.","['Engineering', 'Industrial and Production Engineering', 'Business and Management, general', 'Simulation and Modeling']"
doi:10.1007/978-3-031-16213-8_6,en,Deep Learning–Based Bathymetry Mapping from Multispectral Satellite Data Around Europa Island,OriginalPaper,"Bathymetry studies are important to monitor the changes occurring in coastal topographies, to update navigation charts, and to understand the dynamics of the marine environment. Satellite-derived bathymetry enables rapid mapping of large coastal areas through measurement of optical penetration of the water column. In this study, bathymetry prediction is investigated using Pleiades multispectral satellite data. This research work explores the possibility of using very-high-resolution multispectral satellite data with a deep learning U-Net-inspired neural network architecture to infer bathymetry estimates around Europa Island (22 o 20′S, 40 o 22′E), which is a coralline island in the Mozambique Channel. This study is among the first to provide an overview suitable for bathymetry mapping using a deep learning approach based on optical satellite data. An airborne light detection and ranging (LiDAR) dataset of 1 m resolution is used as ground truth to train the model. From experiments, the overall accuracy evaluation of the model shows a good relationship ( R 2  = 0.99, standard error = 0.492) between the predicted and reference depth values that satisfy the International Hydrographic Organization (IHO) S-57 Category of Zone of Confidence (CATZOC) levels A1, A2, B, and C (IHO, 2014). These predicted bathymetry values could potentially be incorporated into electronic navigational charts. The image reconstruction shows accurate results to estimate bathymetry in the shallow waters with mean absolute error not exceeding 1.5 m in that case. The U-Net-inspired deep learning technique exhibits promising outcomes to predict water depth from very-high-resolution satellite data to operate bathymetry mapping automatically over a wide area.","['Earth Sciences', 'Oceanography', 'Computer Applications', 'Geography, general', 'Water, general', 'Pollution, general', 'Ecology']"
doi:10.1007/978-981-19-4052-1_70,en,Study on Optimizing Feature Selection in Hate Speech Using Evolutionary Algorithms,OriginalPaper,"Hate speech is an important problem while dealing with user-generated content on online social media platforms. The huge amount of data generated makes it nearly impossible to manually moderate hate speech content and take appropriate measures. In this paper, we utilize various optimization algorithms to enhance the feature extraction and vectorization, of various techniques like TF-IDF, Word2Vec, and Bag of Words and appertain on the machine learning models for two-fold classification. We gauge and visualize the conclusion of the propounded methodology of the hate speech problem about Twitter tweets. We examine our suggested technique on three datasets; out of which, two of the datasets were highly unbalanced, and SMOTE was used for class balance. Our experiments indicate the random behavior of particle swarm optimization and genetic algorithm and the decrease in accuracy when applied individually to the experiments. The results also indicate that the accuracy can be achieved back by applying particle swarm optimization and genetic algorithm parallels, countering their random behavior.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3391-2_3,en,Dynamic Multi-objective Optimization Using Computational Intelligence Algorithms,OriginalPaper,"Multi-objective optimization problems (MOPs) have multiple, often conflicting objectives where an improvement in one objective leads to the worsening of at least one other objective. The goal of a multi-objective algorithm (MOA) is to find a set of optimal trade-off solutions that is both accurate and diverse. However, many real-world problems are dynamic in nature where at least one objective and/or constraint changes over time. A dynamic multi-objective algorithm (DMOA) must therefore be able to track the changing set of optimal trade-off solutions over time. This chapter highlights issues that have to be addressed when evaluating the performance of DMOAs. It discusses areas that require further research, including decision making and analyzing the behavior of DMOAs. Emerging areas, and how they can impact on research in the field of dynamic multi-objective optimization (DMOO), are also highlighted.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-16868-0_5,en,Architecture Design for Variational Auto-Encoders,OriginalPaper,"Most VAEs were developed with symmetrical architecture in mind, which means that the encoder and decoder must have the same number of layers. However, when completing the step of unsupervised pre-training step, the decoder portion is deprecated and will never be employed when fine-tuning image classification problems. As a result, maintaining a symmetrical architecture is not nearly necessary [ 1 ]. However, new complications develop when the asymmetrical architecture is used, despite the fact that it is a quite appealing decision. This shows that if the asymmetrical architectures of CVAEs are planned to improve, one needs to evaluate the architectures of each partition on its own before making any changes to the architectures as a whole. In addition, for the correct forwarding of the asymmetrical CVAEs, it is essential to make certain that the decoder appropriately fits the resolution of the raw input data in the appropriate manner. This also adds another layer of complication to the manual design process for VAE designs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5292-0_38,en,Multi-class Classification of COVID-19 in Chest X-Ray Using Deep Convolutional Neural Networks,OriginalPaper,"Lanka, Naga Sai Deep Arunkumar, C. COVID-19 has principally affected everybody within the world in a way or another and thousands of individuals are becoming infected daily. In Present ways for checking COVID positive or negative, is taking a lot of time for results and these results are giving low specificity and sensitivity. Because of that the computer science—Artificial Intelligence (AI) is necessary in finding the positive COVID-19 cases. With Image processing and machine learning and deep learning techniques the researchers are able to achieve high accuracy and sensitivity and specificity from Chest X-ray (CXR) and Computed tomography (C.T) images. In this paper, we have proposed different deep neural networks like CNN, Alexnet, ResNet, Inception-v3 and ResNeXt-101-32x8d (all of those belong to the CNN family) with around 20,000+ CXR pictures of 3 classes. CXR is the initial technique which is important in diagnosing the Covid-19 patients. For verifying the strength of the models we compared validation accuracies, inception V3 achieved the best accuracy of 95%, however, we must always conjointly take into account the training time and complexity of the model. When the models accuracy, specificity, and sensitivity are higher, then it is really helpful for non-radiologist medical staff to diagnoses and quarantine faster when hospitals are flooded with patients, It reduces screening time for COVID-19 greatly.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-3951-8_33,en,Inference of a Gene Regulatory Network by Applying OMOPSO to the S-System and the Half-System,OriginalPaper,"Various approaches are used to reconstruct gene regulatory networks from gene expression data. This work applies a multi-objective particle swarm optimization algorithm to two different formulations of the problem model, the S-system and the recently introduced half-system. Two methods to set a threshold for distinguishing between existing and non-existing gene–gene interactions are tested. The S-system and the half-system show similar performance although the tested implementations applied to a gene expression benchmark dataset could not sufficiently outperform the performances of other methods previously reported in the literature.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-1-4842-8925-9_6,en,Fine-Tuning Deep Learning Models Using PyTorch,OriginalPaper,Deep learning models are becoming very popular. They have very deep roots in the way biological neurons are connected and the way they transmit information from one node to another node in a network model.,"['Computer Science', 'Python', 'Big Data', 'Big Data/Analytics']"
doi:10.1007/978-981-19-6032-1_20,en,Academic Performance Prediction of Postgraduate Students Using Artificial Neural Networks,OriginalPaper,"Institutions of higher learning operate in a highly competitive environment. To compete with world-class institutions, institutes must adapt their strategy to increase overall performance. Academic achievement of students is one of the most important factors in improving an institution's ranking and recognition. Performance of students in an academic program depends upon several aspects of their previous academic performance and family background. In the present study, an artificial neural network (ANN) is developed using Python programming language to predict students’ performance and to determine the outcome of students’ performance. Students’ data were collected through a questionnaire-based survey from postgraduate students of technical education institutions all over India. The appropriate ANN model is identified, and the Python code for the same is developed with the help of Keras library. The developed model did not have the expected accuracy due to lack of adequate number of responses required for deep learning techniques, but still valuable results are obtained such as that of identifying some crucial factors.","['Engineering', 'Mechanical Engineering', 'Engineering Fluid Dynamics', 'Vibration, Dynamical Systems, Control', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-01129-0_9,en,A Scalable Controller Synthesis Method for the Robust Control of Networked Systems,OriginalPaper,"This chapter discusses a scalable controller synthesis method for networked systems with a large number of identical subsystems based on the $$\mathcal {H}_{\infty }$$ H ∞ -norm control framework. The dynamics of the individual subsystems are described by identical linear time-invariant delay differential equations and the effect of transport and communication delay is explicitly taken into account. The presented method is based on the result that, under a particular assumption on the graph describing the interconnections between the subsystems, the $$\mathcal {H}_{\infty }$$ H ∞ -norm of the overall system is upper bounded by the robust $$\mathcal {H}_{\infty }$$ H ∞ -norm of a single subsystem with an additional uncertainty. This chapter will therefore briefly discuss a recently developed method to compute this last quantity. The resulting controller is then obtained by directly minimizing this upper bound in the controller parameters.","['Engineering', 'Solid Mechanics', 'Complexity', 'Control and Systems Theory', 'Functional Analysis']"
doi:10.1007/978-3-031-15509-3_5,en,An Imprecise Label Ranking Method for Heterogeneous Data,OriginalPaper,"Learning to rank is an important problem in many sectors ranging from social sciences to artificial intelligence. However, it remains a rather difficult task to perform. Therefore, in some cases, it is preferable to perform cautious inference. For this purpose, we look into the possibility of an imprecise probabilistic approach for the Plackett-Luce model, a popular probabilistic model for label ranking. We aim at extending current Bayesian inference techniques for the Plackett-Luce model to an imprecise probabilistic setting so that we can deal with heterogeneous data by means of cautious mixture modelling. To achieve this, we perform a robust Bayesian analysis over a set of imprecise Dirichlet priors, which allows us to perform cautious label ranking. Finally, we use a synthetic dataset to illustrate our imprecise estimation method.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-12011-4_82,en,Hyper-parameter Optimised Artificial Neural Network Model for Failure Mode Identification of RC Shear Wall,OriginalPaper,"A reinforced concrete (RC) shear wall is a popular lateral-force-resisting element in high-rising structures for better seismic performance and damage reduction. Hence, it is essential to predict the shear wall’s failure modes and collapse resistance. The mechanics-based approach is very involved and time-consuming to accurately predict failure modes of RC shear walls subjected to seismic loading. Neural networks can describe complex input–output functional relationships with their general nonlinear mapping capacity. This study investigates the efficiency of the Artificial Neural Network (ANN) model to predict the failure modes of RC shear walls employing a limited experimental dataset. The dataset consists of 393 one-story, one-bay reinforced concrete shear walls with sliding shear failure, shear failure, flexure-shear failure, and sliding shear flexural failure as the four classes of failure modes. For each data sample in the database, nine input parameters are considered as the input features, including the wall configuration, reinforcement index, and cross-section shape. The study also optimises the performance of the model by investigating the hyper-parameters. Various hyper-parameter optimisation methods such as random search, grid search, hyperband, genetic algorithm (GA), Bayesian Optimization (BO), and particle swarm optimisation (PSO) are considered to improve the baseline model. These hyper-parameter tuned models are evaluated and compared using various performance parameters such as accuracy and f1 score. This study can contribute to developing improved ANN models by effectively identifying the proper hyper-parameter configurations and their subsequent optimisation.","['Engineering', 'Construction Management', 'Building Construction and Design', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/978-3-031-07322-9_73,en,Automation in Documentation of Ageing Masonry Infrastructure Through Image-Based Techniques and Machine Learning,OriginalPaper,"Visual inspection and manual documentation of masonry is a time consuming and subjective process. This paper aims to improve automation in documentation and assessment of ageing masonry infrastructure through image-based techniques and machine learning. A large dataset of annotated images has been developed to train a deep learning model. Different convolutional neural networks were investigated to identify the most suitable for the task. The results presented are combined with previous work to generate high quality geometrical and numerical models of masonry infrastructure. This implementation of deep learning, for segmentation and localisation of bricks in masonry, highlights the potential of recent technologies for the automation of structural inspection, documentation, and analysis of cultural heritage.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-7264-5_5,en,Design of a Centralized PI Controller for Three-Tank Hybrid System Based on Optimization Methods,OriginalPaper,"In this study, a method of designing a centralized controller for the Multiple-Input and Multiple-Output (MIMO) non-linear process is presented. A Three-Tank Hybrid (TTH) is taken as a non-square multivariable non-linear system. The real-time TTH setup is subjected to a Pseudo-Random Binary Signal (PRBS) and the obtained excitation data is used to validate the first principle model (FPM) of the system. The model is linearized around the nominal values to obtain the state-space model. The Davison method is used to design a PI controller which in turn needs Non-Square Relative Gain Array (NRGA). Since the Davison method provides an initial guess of the controller parameters, an optimization technique, viz., Manta Ray Foraging Optimization (MRFO) technique is implemented to fine tune the controller parameters to obtain better performance of the controller. The proposed controller performance is compared with another existing popular optimizing technique, viz., Particle Swarm Optimization (PSO) for a change in multiple set points.","['Engineering', 'Materials Engineering', 'Mineralogy', 'Industrial Chemistry/Chemical Engineering']"
doi:10.1007/978-3-031-16072-1_47,en,Artificial Vision Algorithm for Behavior Recognition in Children with ADHD in a Smart Home Environment,OriginalPaper,"Artificial vision has made a great advance in the recognition of visual patterns that are not perceptible by humans or that are biased in their interpretation. Among its applications, artificial vision or computer vision has served in the support of people with some kind of disability. In this work, an image classification algorithm is developed to complement a pervasive therapy support system for children with Attention Deficit Hyperactivity Disorder (ADHD) during the development of their homework. For this purpose, a camera is adapted within a smart environment made up of Smart objects and a robotic assistant. In the system, a convolutional neural network (CNN) is implemented for the classification of the child’s status (doing or not doing his/her homework). An experiment of this implementation is carried out in which the results of the environment without the camera are compared with the results obtained by using the camera and the implemented CNN. The latter results are also compared with the information collected through observation by the therapist during the session. The results show that what the camera identifies as the child not doing homework matches what the smart objects identify as distractions and pauses at 82.70% and what the therapist identifies as distractions and pauses at 98.21%. This approach will help the smart home environment have new and more accurate data to process and make better decisions, just like a therapist would do.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3679-1_60,en,Supervised Question Classification on SelQA Dataset Using Variational Quantum Classifiers,OriginalPaper,"Machine learning and quantum computing fuse together to form quantum machine learning. Although the phenomenon is new, it has already proved its worth in various fields like finance and chemistry. The potential of quantum computing and its extraordinary properties enable us to process data in a way classical computer can never think of. When machine learning gets the power of quantum computing, information processing is enhanced significantly. In this paper, we have used variational quantum classifiers to classify questions from two domains of SelQA dataset. We keep the focus on the implications of circuit-depth in different experiments and analyze the results. VQC performs well with 11 features on lowest circuit depths and gives a testing accuracy of 58%.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-15226-9_23,en,Design of a Minimalistic Torque Actuated Variable Rolling SLIP Leg for Robust Locomotion,OriginalPaper,"In this paper we investigate the use of an optimal control coupling applied to a continuously jumping robot utilizing a single spiral-shaped leg. Exploiting multiple controllers simultaneously can increase robustness over uneven terrain. However, this can be difficult to actualize. By designing a spiral-shaped leg, we can couple leg length and stiffness and optimize leg retraction rate and torque. Through this embodied design we can achieve a robust minimalistic running robot with a simple feedforward controller.","['Engineering', 'Control, Robotics, Mechatronics', 'Robotics', 'Computational Intelligence']"
doi:10.1007/978-3-031-20650-4_1,en,Graph Augmentation for Neural Networks Using Matching-Graphs,OriginalPaper,"Both data access and data collection have become increasingly easy over the past decade, leading to rapid developments in many areas of intelligent information processing. In some cases, however, the amount of data is still not sufficiently large (e.g. in some machine learning applications). Data augmentation is a widely used mechanism to increase the available data in such cases. Current augmentation methods are mostly developed for statistical data and only a small part of these methods is directly applicable to graphs. In a recent research project, a novel encoding of pairwise graph matchings is introduced. The basic idea of this encoding, termed matching-graph, is to formalize the stable cores of pairs of patterns by means of graphs. In the present paper, we propose to use these matching-graphs to augment training sets of graphs in order to stabilize the training process of state-of-the-art graph neural networks. In an experimental evaluation on five graph data sets, we show that this novel augmentation technique is able to significantly improve the classification accuracy of three different neural network models.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-21333-5_9,en,Convolutional Neural Network for Monkeypox Detection,OriginalPaper,"Machine learning has been implemented in medical applications, especially in classification models to support diagnosis. In dermatology, it is of great relevance, due to the high difficulty in differentiating between pathologies that are similar, such is the case of its wide application in skin cancer. One of the diseases that has recently become relevant due to a recent outbreak is monkeypox, which is an exanthematic disease; these types of pathologies are very similar if you are not an expert, so diagnostic support would favor their identification, mainly for adequate epidemiological control. Therefore, the objective of this work is use a public database of monkeypox and control group images. These images were preprocessed, divided into 80/20 for training and testing set respectively. Implementing MiniGoggleNet, 6 experiments were carried out, with different number of epoch. The best model was the one of 50 epochs with accuracy of 0.9708, a loss function of 0.1442, an AUC for class 0 of 0.74, AUC for class 1 of 0.74, AUC for micro-average of 0.76 and AUC for macro-average of 0.74.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4162-7_31,en,Fish Classification System Using Customized Deep Residual Neural Networks on Small-Scale Underwater Images,OriginalPaper,"Recent improvements in marine science research have increased the importance of underwater fish species identification. Using technology to automate fish species identification would positively impact marine biology. Since deep learning techniques, image classification problems have become increasingly popular. Wild natural habitats make it harder to identify fish species because of the complex background and noise in the raw images. Some of the most advanced approaches for categorizing fish species in their natural habitats have been developed in the previous decade. This paper demonstrated an automated approach for classifying fish species based on deep residual networks. Existing transfer learning models do a good job working with smaller datasets, but not so effectively. A novel RESNET model (SmallerRESNET) is developed to reduce the overfitting generated by the standard pre-trained RESNET model. Convolutional and fully linked layers are used in the more straightforward form of the RESNET model. We evaluated and compared six different versions of the RESNET model. In addition to the number of convolutional and fully connected layers, the number of iterations required to achieve 80.56% accuracy on training data, batch size, and the dropout layer is examined. Compared to the original RESNET model, the proposed and modified RESNET model with fewer layers obtained 90.26% testing accuracy with a validation loss of 0.0916 on an untrained benchmark fish dataset. The inclusion of a dropout layer enhanced our proposed model's overall performance. It is more efficient with less memory, fewer training photos, and less computing complexity than its predecessor.","['Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery', 'Systems and Data Security', 'Mobile and Network Security', 'Information Systems Applications (incl. Internet)']"
doi:10.1007/978-981-16-8154-7_37,en,Siamese U-net with Attention Mechanism for Building Change Detection in High-Resolution Remote Sensing Images,OriginalPaper,"Building change detection in high-resolution remote sensing images is very important for illegal building management and urban supervision. Recently, with the development of neural network and the increase of RS data, there are more and more change detection methods based on deep learning. Most of the existing change detection algorithms based on deep differential feature analysis which detect all semantic changes in two-temporal images, not specifically designed for building change detection and unable to give an accurate mask for building changes area. In this paper, we propose a Siamese U-net with attention mechanism for building change detection in high-resolution bi-temporal remote sensing images. By introducing scene-level building segmentation, we improve the boundary integrity and internal compactness of the final changed building. Our method was applied to WHU dataset and have outstanding building change detection results.","['Engineering', 'Aerospace Technology and Astronautics', 'Communications Engineering, Networks', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Control and Systems Theory']"
doi:10.1007/978-3-031-16075-2_13,en,Firearm Detection Using Deep Learning,OriginalPaper,"Surveillance Cameras (SCs) are great support in crime investigation and proximity alarms which play a critical role in public safety and peace. The major drawback is their limited use in producing evidences in judicial court but they were not used in providing early firearms detection to stop attacks in crime scenes. Traditional firearm detection techniques use X-Rays for detecting weapons in limited coverage area that fail in detecting non-metallic weapons. The main motivation of this research is developing a deep learning object detection model to early detect firearms in crime scenes and alert the corresponding authorities. Designing an efficient and accurate object detection model that localize and classify the firearm classes with overcoming variations in shape, size, appearance and occlusions is a real challenge. We collected a dataset of different firearm classes from Kaggle and Google Open Images and manually annotated about 2300 samples. The dataset consists of variety of firearms classes handgun, revolver, rifle along with knife and person classes. YOLOv5 (You Only Look Once) is a unified object detector which detects objects without losing their precision and accuracy. All the YOLOv5 models are built from scratch and generate final models that achieved 89.4%, 70.1%, 80.5% of precision, recall and mAP@0.5 respectively and achieved the highest 94.1% precision per individual class.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2273-2_48,en,Accident Prediction Modeling for Collision Types Using Machine Learning Tools,OriginalPaper,"Road accidents are one of the most significant concerns being addressed by developing countries. It is highly imperative to scientifically analyze road accidents to reduce the increasing number of accidents and victims. This study aimed to develop accident prediction models for the predominant collision types on a selected road stretch in Calicut City of a length of 7 km. The main objectives of the project were to identify predominant collision types of accidents and develop prediction models using machine learning algorithms. Rear-end collisions and pedestrian hit collisions were the most predominant types of collision on the study stretch. The developed artificial neural network gave a satisfactory prediction for rear-end collisions, and the major influencing factors were maximum gradient and traffic volume. The random forest model showed promising results for predicting pedestrian hit collisions, and the significant factors that contributed were pedestrian volume, traffic volume, presence of bus stop, and maximum gradient.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Building Construction and Design', 'Mechanical Engineering']"
doi:10.1007/978-981-19-3998-3_58,en,A Fusion Method of 3D Object Detection Graph Neural Network Based on Local and Global Data Augmentation,OriginalPaper,"LiDAR-based 3D object detection is an important task for autonomous driving because it provides the location information of objects on the road. However, the existing methods perform poorly in the detection of distant objects since they suffer from sparse and incomplete point clouds. To overcome the problems caused by multi-scale and occupancy, we propose a novel graph-based framework, i.e. named a fusion method of 3D object detection graph neural network based on local and global data augmentation (LGDA-GNN), for accurate 3D object detection from the point clouds. Firstly, we summarize seven characteristics of point cloud data, and point out a new way to improve accuracy of this task. Secondly, to select effective vertices of graph neural network and to enhance local features and the proportion of point cloud at medium and long distances, we propose methods including self-adaptive multi-scale voxel downsampling and multi-step graph construction. Thirdly, to improve the accuracy of difficult category, a fusion method of self-attention module and visibility property is presented for global feature reinforcement. Finally, we integrate our refinement modules into a graph-based pipeline, and extensive experiments on the KITTI benchmark show that we achieve a great performance on the difficult car object for the bird eye view detection and 3D object detection task.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-3-031-19032-2_57,en,On the Similarities Between Denoising Diffusion Models and Autoencoders,OriginalPaper,"Denoising diffusion probabilistic models (DDPMs) have recently emerged as a powerful paradigm for generative modelling, outperforming adversarial methods in various domains and applications using a comparable amount of computation resources. Recent findings demonstrate that there is a connection between them and more venerable methods like denoising autoencoders. In this article, we present a simplified description of this connection that maintains all essential components. We also demonstrate empirically that, at least on toy datasets, one can indeed obtain a similar performance to DDPM by training a timestep-aware denoising autoencoder using an instance noise trick, which doesn’t require any reparameterizations.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-3-031-20029-8_10,en,Improving Classification Accuracy by Optimizing Activation Function for Convolutional Neural Network on Homomorphic Encryption,OriginalPaper,"A secure machine learning technology that performs prediction while encrypting data using homomorphic encryption is being developed. However, Convolutional Neural Networks (CNN) on homomorphic encryption cannot use general non-linear activation functions, Thus, the classification accuracy is low. We proposed a novel method to create an activation function that improves the classification accuracy by performing a pre-training optimization on the coefficients of the polynomial approximation of the Mish function. We confirmed the improvement of classification accuracy for MNIST, Fashion-MNIST, and CIFAR-10 by optimizing the Mish function through pre-training. The classification accuracy can be improved by 4.27% for CIFAR-10. Furthermore, we showed that classification accuracy improves for Fashion-MNIST and CIFAR-10 even when different networks and datasets are optimized by pre-training the activation function. These results show that the activation function of CNNs on a homomorphic encryption can be optimized to improve classification accuracy.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence']"
doi:10.1007/978-3-031-10015-4_3,en,Could an Exoskeleton-Driven Rehabilitation Treatment Improve Muscle Forces Generation in PD? - a Pilot Study,OriginalPaper,"Research focusing on optimal rehabilitation methods has been directed towards powered lower-limb exoskeletons which combines the advantages delivered from the grounded robotic devices with the ability to train the patient in a real-world environment. In this context literature has highlighted the benefit of coupling gait analysis and musculoskeletal modeling for treatment planning. Recently, this combined approach has been successfully applied to detect the alterations in motor control related to Parkinson’s Disease (PD). However, no study has reported about the effects of an overground wearable exoskeleton in terms of both gait analysis and musculoskeletal modeling-derived parameters in people with PD. The aim of this study was to quantitatively assess the effect of an overground rehabilitation treatment on a PD participant both in terms of gait parameters and muscle forces. One people with PD has been enrolled and gait analysis was performed before and after a 4-weeks gait training intervention with an overground exoskeleton. Inverse kinematics, inverse dynamics, and static optimization were performed in OpenSim. Results from joint moments and muscle forces were compared with a group of healthy controls. Preliminary results showed that after the therapy joint loads in both ankle and knee joints were reduced during the stance phase and muscle forces displayed an increased magnitude in their peak after the treatment. For the best of our knowledge, the presented case study is the first attempt to track rehabilitation improvement via muscle forces assessment. Further studies should focus on increasing the sample size to generalize the outcomes.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biological and Medical Physics, Biophysics', 'Machine Learning', 'Bioinformatics']"
doi:10.1007/978-981-19-4960-9_24,en,Named-Entity Recognition in Medical Records Using Transfer Learning,OriginalPaper,"Based on government regulations, eighteen attributes from medical records are classified as protected health information (PHI) which needs to be de-identified prior to sharing the records to a secondary source such as research. The de-identification task is classified under natural language processing task considering the obstacles in manual de-identification. Many pretrained models are implemented for de-identification task. But there is no comparison to determine which is the most efficient one. Hence, this is an attempt to compare efficiency (recall, precision and F 1-score) of few potential transfer learning models by implementing them using same dataset—Informatics for Integrating Biology and Bedside (I2B2). Through this experiment, it is found that NeuroNER is the most efficient model identifying almost 20 categories with accuracy of 99.68%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-2394-4_45,en,Camera and LiDAR Fusion for Point Cloud Semantic Segmentation,OriginalPaper,"Perception is a fundamental component of any autonomous driving system. Semantic segmentation is the perception task of assigning semantic class labels to sensor inputs. While autonomous driving systems are currently equipped with a suite of sensors, much focus in the literature has been on semantic segmentation of camera images only. Research in the fusion of different sensor modalities for semantic segmentation has not been investigated as much. Deep learning models based on transformer architectures have proven successful in many tasks in computer vision and natural language processing. This work explores the use of deep learning transformers to fuse information from LiDAR and camera sensors to improve the segmentation of LiDAR point clouds. It also addresses the question of which fusion level in this deep learning framework provides better performance.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-1-0716-2687-0_8,en,Prediction of Dynamic RBP–RNA Interactions Using PrismNet,OriginalPaper,"A capacity to detect the binding profiles of RNA targets for an RNA-binding protein (RBP) under different cellular conditions is essential to understand the functions of the RBP in posttranscriptional regulation. However, the prediction of RBP binding sites in vivo remains challenging. Tools that predict RBP–RNA interactions using sequence and/or predicted structures cannot reflect the exact state of RNA in vivo. PrismNet, which uses both sequences and in vivo RNA structure information from probing experiments, can accurately predict RBP binding under different cellular conditions by deep learning, and can be applied for functional studies of RBPs. Here, we provide a detailed protocol showing how to train a PrismNet model of RBP–RNA interactions for an RBP, and how to apply the model for predictions of the RBP binding under different conditions.","['Life Sciences', 'Genetics and Genomics', 'Analytical Chemistry', 'Materials Science, general']"
doi:10.1007/978-981-19-5090-2_10,en,AUTCD-Net: An Automated Framework for Efficient Covid-19 Diagnosis on Computed Tomography Scans,OriginalPaper,"Ghosal, Palash Kumar, Amish Kundu, Soumya Snigdha Srivastava, Utkarsh Prakash Datta, Ashis Sarma, vs The coronavirus pandemic has caused one of the biggest global crises. With an inevitable need for fast screening of the disease, deep learning-based segmentation of Covid-19 infected lung regions in computed tomography (CT) scans gained significant attention. The automated screening procedure generated results significantly faster than the manual screening techniques and directly helped provide a wider outreach to patients. Therefore, to aid in computer-aided diagnoses, this paper presents AUTCD-Net ( AUT omated framework for efficient C ovid-19 D iagnosis-Network), based on hierarchical resolution steps, to efficiently segment Covid-19 infected lung regions in CT scans. The approach results in a 0.71 dice score and rivals all previous state-of-the-art approaches. The overall evaluation combined with our in-depth model analysis, and critical inferences can be further extended for developing a computer-aided diagnostic (CAD) tool to assist the CT image reading process for detecting Covid-19 infected regions in the near future.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-05405-1_5,en,Generative Adversarial Networks for Labelled Vibration Data Generation,OriginalPaper,"As Structural Health Monitoring (SHM) being implemented more over the years, the use of operational modal analysis of civil structures has become more significant for the assessment and evaluation of engineering structures. Machine Learning (ML) and Deep Learning (DL) algorithms have been in use for structural damage diagnostics of civil structures in the last couple of decades. While collecting vibration data from civil structures is a challenging and expensive task for both undamaged and damaged cases, in this paper, the authors are introducing Generative Adversarial Networks (GAN) that is built on the Deep Convolutional Neural Network (DCNN) and using Wasserstein Distance for generating artificial labelled data to be used for structural damage diagnostic purposes. The authors named the developed model “1D W-DCGAN” and successfully generated vibration data which is very similar to the input. The methodology presented in this paper will pave the way for vibration data generation for numerous future applications in the SHM domain.","['Engineering', 'Mechanical Statics and Structures', 'Building Construction and Design', 'Complexity', 'Vibration, Dynamical Systems, Control', 'Mechanical Engineering', 'Statistical Theory and Methods']"
doi:10.1007/978-981-19-1844-5_3,en,Classification of Breast Cancer Using CNN and Its Variant,OriginalPaper,"Deep learning comes under machine learning. It includes statistics and predictive modeling, which plays vital role in data science. It helps in acquiring and analyzing vast amount of data quick and easier. This technique is employed in image recognition tools and natural language processing. Carcinoma is one other frequently occurring cancer in women. Carcinoma can be identified in two variants: One is benign, and another one is malignant. Automatic detection in medical imaging has become the vital field in many medical diagnostic applications. Automated detection of breast cancer in magnetic resonance imaging (MRI), and mammography is very crucial as it provides information about breast lesions. Human inspection is the conventional method for defect detection in magnetic resonance images. This method is impractical for large amount of data. So, cancer detection methods are developed as it would save radiologist time and also the risk faced by woman. Various machine learning algorithms are used to identify breast cancer. Deep learning models have been widely used in the classification of medical images. To improvise the accuracy in the model various, deep learning approaches are to be used to detect the breast cancer. The proposed approach classifies the breast cancer not just as benign or malignant, but it will classify the subclasses of breast cancer. They are Benign, Lobular Carcinoma, Mucinous Carcinoma, Ductal Carcinoma, and Papillary Carcinoma. To classify the subclasses of tumor, we use DenseNet Architecture. Image preprocessing is done using histogram equalization method.","['Engineering', 'Communications Engineering, Networks', 'Mobile and Network Security', 'Artificial Intelligence', 'Big Data']"
doi:10.1007/978-1-0716-2609-2_17,en,Structural Considerations in Affinity Maturation of Antibody-Based Biotherapeutic Candidates,OriginalPaper,"Affinity maturation is an important stage in biologic drug discovery as is the natural process of generating an immune response inside the human body. In this chapter, we describe in silico approaches to affinity maturation via a worked example. Both advantages and limitations of the computational methods used are critically examined. Furthermore, construction of affinity maturation libraries and how their outputs might be implemented in an experimental setting are also described. It should be noted that structure-based design of biologic drugs is an emerging field and the tools currently available require further development. Furthermore, there are no standardized structure-based strategies yet for antibody affinity maturation as this research relies heavily on scientific logic as well as creative intuition.","['Biomedicine', 'Antibodies', 'Bioinformatics', 'Pharmaceutical Sciences/Technology']"
doi:10.1007/978-3-031-13702-0_3,en,Explainable AI and Slime Mould Algorithm for Classification of Pistachio Species,OriginalPaper,"The safety and quality of the food are considered an essential issue in the entire world. This is due to food being the basis of human health. Nowadays, machine learning algorithms have embodied the recent technology in all stages of food processing such as food grading, food quality determination, and food classification. Pistachio nuts have an important role in the agricultural economy. To increase the efficiency of post-harvest industrial processes, there is a need to introduce technologies for classifying different species of pistachio. This study considers an automated model to separate pistachio species. The proposed pistachio species classification consists of three main phases; features selection based on slime mould algorithm phase, feature interpretation based on explainable artificial intelligence phase, and finally classification of pistachio species using logistic regression phase. The proposed pistachio species classification model obtained overall 90% classification accuracy, 90% precision, and 91% f1-score.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Artificial Intelligence', 'Food Science']"
doi:10.1007/978-3-031-14771-5_15,en,Predicting Drug Functions from Adverse Drug Reactions by Multi-label Deep Neural Network,OriginalPaper,"Drug functions classification plays a vital role in the area of drug development. Drug discovery and development is a complex and tedious process that is costly. Misleading drug functions are one of the reasons for drug failure in the market. The use of deep learning in drug functions classification for unseen drugs helps in the drug development process by minimizing time and cost. The classification of drug functions is a multi-label classification task and therefore, a multi-label prediction supported deep neural network methodology is employed to solve this issue. The common obstacle of multi-label classification tasks is class imbalance which is addressed using Multilabel Synthetic Minority Over-sampling Technique (MLSMOTE). Most of the existing research works has focused on the 1D chemical structure, the 2D chemical structure and the transcriptomic data (i.e., Gene Expression Information) of drugs to predict drug functions derived from Medical Subject Headings (MeSH). However, the capability of adverse drug reactions have not yet been utilized to classify drug functions prediction. The current work proposed a deep neural network methodology with a class imbalance handling technique to predict drug functions from adverse drug reactions. The proposed methodology gives promising results and it achieved the highest accuracy of 98.43%.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2188-9_95,en,Computational Study on R134a Refrigerant for Vortex Tube Condenser for Automotive Air-Conditioner,OriginalPaper,"This paper provides the importance of vortex tube, when it is employed in HVAC closed system, with different working fluids. The vortex tube condenser (VTC) can provide the better heat transfer rate and may help to diminish the carbon emissions by reducing the HVAC parasitic load on engine. During the numerical studies, two different VTC geometries employed with 2 and 6 nozzles were analysed under two working refrigerant fluids (R134a and air). The prototype was assumed to be fitted in between the compressor and condenser refrigeration cycle. Further, their cooling and heating effect, temperature distribution and heat flux were analysed under adiabatic and non-adiabatic conditions. The results illustrate that Proto-II leads to higher temperature difference as compared to Proto-I for both the refrigerants unless would reach a peak value when the chamber inlet/nozzle outlet velocity gets chocked. Finally, non-adiabatic condition is considered by providing fins towards the outer side of the VTC, and for both refrigerant, the performance of the VTC is decreased. Although the temperature difference between the outlet hot and cold refrigerant was largest for Proto-II. However, the temperature drop at the outlet cold temperature was larger than the temperature rises at the high temperature outlet. Therefore, it became clear that the heat transfer to the surroundings is larger in Proto-I, and it is suitable performance for the refrigerator, which has a smaller temperature difference of outlet hot and cold refrigerant.","['Engineering', 'Industrial and Production Engineering', 'Mechatronics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Energy Storage', 'Materials Engineering']"
doi:10.1007/978-3-031-18461-1_49,en,An Annotated Caribbean Hot Pepper Image Dataset,OriginalPaper,"The Caribbean region is home to, and widely known for, its many “hot"" peppers. These peppers are now heavily researched to bolster the development of the regional pepper industry. However, accurately identifying the different landraces of peppers in the Caribbean has since remained an arduous, manual task that involves the physical inspection and classification of individual peppers. An automated approach that uses machine-learning techniques can help with this task; however, machine learning approaches require vast amounts of data to work well. This paper presents a new multi-label annotated, image-dataset of Capsicum Chinense peppers from Trinidad and Tobago. The paper also presents a benchmark for image-pepper classification and identification. It serves as a starting ground for future work that can include the compilation of larger datasets of regional peppers that can include more morphological features. It additionally serves as the starting ground for a Caribbean-based hot-pepper ontology.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2308-1_5,en,Optimization of Subthreshold Parameters of Graded-Channel Gate-Stack Double-Gate (GC-GS-DG) MOSFET Using PSO-CFIWA,OriginalPaper,"This work optimized the performance parameters of the Graded-Channel Gate-Stack Double-Gate (GC-GS-DG) MOSFET using a meta-heuristic technique. The meta-heuristic algorithm applied for this study is PSO with Constriction Factor and Inertia Weight Approach (PSO-CFIWA). The drawbacks of PSO are the premature convergence and stagnation problem. The PSO-CFIWA eliminates these drawbacks. Both the current at OFF-state ( I OFF ) and subthreshold swing (SS) are considered in the formulation of overall objective/cost function (CF). The weighted sum approach method is used to obtain the overall CF. Compared with the previous literature, the PSO-CFIWA shows much better results in device design.","['Engineering', 'Circuits and Systems', 'Electronics and Microelectronics, Instrumentation', 'Nanotechnology and Microengineering']"
doi:10.1007/978-3-031-18516-8_20,en,Autoencoders and Ensemble-Based Solution for COVID-19 Diagnosis from Cough Sound,OriginalPaper,"We propose a new method for COVID-19 screening from cough sound, which is based on the extraction of Low-Level Descriptors from cough sound and make use of a Stacked Autoencoder to extract some specific non-linear features, and then, utilize Random Forest an ensemble learning technique to build a Machine Learning model that classifies a cough sound as a likely negative or positive case. Stacked Autoencoder and Random Forest were trained on the largest publicly available dataset, COUGHVID. SMOTE method was employed to address the lack of positive examples. The performance of the proposed diagnosis system is that can correctly classify more than 90% of unseen data, and also boost the ability to identify positive cases, from 81% reported in our recent study, to more than 86%.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-3-031-04536-3_12,en,Haptic Software Design,OriginalPaper,"This chapter reviews design concepts of haptic modeling and rendering software. The main focus lies in realistic kinesthetic and tactile haptic models for virtual and augmented reality based on the data collected from physical objects. We consider both data-driven algorithms providing a black-box action-response mapping and measurement-based approaches identifying parameters of physics-based models. To show the research landscape and highlight ongoing research challenges, we introduce a series of state-of-the-art methods including data-driven models with deterministic and stochastic responses, physics-based simulation using optimization-based FEM solver, and hybrid approaches of combining the concepts of both data-driven and physics-based methods. These examples also cover a wide range of haptic properties, i.e., modeling and rendering of elasticity and plasticity, tool deformation, and haptic textures.","['Computer Science', 'User Interfaces and Human Computer Interaction', 'Control, Robotics, Mechatronics', 'Special Purpose and Application-Based Systems']"
doi:10.1007/978-3-031-12127-2_10,en,Improvement of DGA Long Tail Problem Based on Transfer Learning,OriginalPaper,"As the number of classes increases in traditional multiple classification and recognition tasks, there is often the problem of a long tail: the sample data is mainly distributed in a few classes. In the detection of domain names generating malware (DGA - domain generation algorithm), due to the variability of malware, the number of classes of DGA is also increasing and shows a long tail nature. However, in previous DGA detection research focused on the classes of a large amount of data so they do not address the long tail characteristics. We propose an effective knowledge transfer DGA detection model that transfers the knowledge learned in the previous stage of training to the next stage, and optimizes the impact of the long tail problem on the detection model. In order to inherit the continuity of the model, we propose a data balance review method, which can alleviate the catastrophic forgetting problem of transfer learning and detect new classes without retraining the whole model. Finally, the macro average F1 score of our model is 76.6%, 8.74% higher than ATT_BiLSTM and 6.34% higher than ATT_CNN_BiLSTM. So our model optimizes the long tail problem and better predicts all classes.","['Engineering', 'Computational Intelligence', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/978-3-031-21065-5_26,en,AttDLNet: Attention-Based Deep Network for 3D LiDAR Place Recognition,OriginalPaper,"Place recognition has often been incorporated in SLAM and localization systems to support autonomous navigation of robots and intelligent vehicles. With the increasing capacity of DL approaches to learning useful information from 3D LiDARs, place recognition has also benefited from this modality, which has led to higher re-localization and loop-closure detection performance, particularly, in environments with significantly changing conditions. Despite the progress in this field, the efficient extraction of invariant descriptors from 3D LiDAR data is still a challenging problem in this domain. In this work, we propose a novel 3D LiDAR-based deep learning network that resorts to a self-attention mechanism to, on one hand, leverage the computational efficiency of these operations and, on the other, reweigh relevant local features and thus create discriminative descriptors. The proposed network is trained and validated on the KITTI dataset and an ablation study is presented to assess the components of the novel network. Results show that adding attention to the network improves performance, leading to efficient loop closures, and outperforming an established 3D LiDAR-based place recognition approach. From the ablation study, results indicate that the middle encoder layers have the highest mean performance, while deeper layers are more robust to orientation change. The code is publicly available at: https://github.com/Cybonic/AttDLNet","['Engineering', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Robotics']"
doi:10.1007/978-981-19-4052-1_67,en,COVID-19 Disease Classification Model Using Deep Dense Convolutional Neural Networks,OriginalPaper,"Preventing the transmission of COVID-19 necessitates diagnosis and identification. Researchers have developed algorithms to detect the presence of COVID-19 in X-ray and CT scans and images. These methodologies produce skewed data and incorrect disease detection. So, in the case of COVID-19 forecasting utilizing CT scans in an IoT setting, the current study paper established an oppositional-based deep dense convolutional neural network (DDCNN) and chimp optimization algorithm. The framework proposed is divided into two stages: preprocessing and estimation. Previously, a CT scan pictures generated from anticipated COVID-19 are acquired utilizing IoT devices from an open-source system. After that, the photos are preprocessed with a Gaussian function. A Gaussian filter can be used to remove undesirable noise from CT scan pictures that have been obtained. The preprocessed photos are then transmitted to the prediction process. DDCNN is applied to the images preprocessed in this step. The recommended classifier is designed to be as efficient as possible using the oppositional-based chimp optimization algorithm (OCOA). This approach is used to choose the best classifier parameters under consideration. Furthermore, the suggested method is applied to forecast COVID-19 and categorizes the findings as COVID-19 or non-COVID-19. The proposed technique was used in Python, and results were assessed using statistical analysis. CNN-EPO and CNN-FA were compared to the new method. The results proved that the proposed model was optimal.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-11295-9_17,en,Diagnostic Value of Knee Osteoarthritis Through Self-learning,OriginalPaper,"Osteoarthritis (OA) is the most common chronic and progressive musculoskeletal disorder. These chronic disorders are not diagnosed early nor is the treatment adequate, resulting in challenges for health care systems. Radiography is the most widely used imaging method for OA diagnosis since it allows a two-dimensional evaluation, whose main advantages its low cost and wide availability. We present a computer-assisted diagnostic (CAD) system for OA based on the analysis of X-ray images of the knee using deep learning to automatically score the Knee OA. The scoring is based on the Kellgren-Lawrance (KL) scale. The model was implemented in PyTorch and was based on Deep Siamese convolutional neural networks and fine-tuned ResNet-34 through transfer learning for the classification task. A public dataset was used for training and validating, and a private dataset for testing. The results indicate a multiple-class accuracy of the test set of 61%. The highest accuracy was obtained with KL-3 at 89%. It is expected that this software will be useful for training of medical students and can be used as a second opinion for the correct prediction of OA knee diagnosis. Early diagnosis is necessary to alleviate symptoms, delay the evolution of the disease and improve the functional capacity and quality of life of the patient.","['Social Sciences', 'Sociology, general', 'Urban Studies/Sociology']"
doi:10.1007/978-3-030-99075-6_24,en,Prediction of Sensor Values in Paper Pulp Industry Using Neural Networks,OriginalPaper,"The economic sustainability of any industry is directly linked to the management and efficiency of its physical assets. The maintenance of these assets is one of the key elements for the success of a company since it represents a relevant part of its Capital and Operational Expenses (CAPEX and OPEX). Due to the importance of maintenance, a lot of research has been done to improve the methodologies aiming to maximize physical assets’ availability at the most rational costs. The introduction of Artificial Intelligence in the world of maintenance increased the quality of prediction on equipment failures, namely when associated to continuous equipment monitoring. This paper presents a case study where a neural network is proposed to predict the future values of various sensors installed on a paper pulp press. Data from the following variables is processed: electric current; pressure; temperature; torque; and speed.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-3-031-15160-6_9,en,Room Occupancy Prediction Leveraging LSTM: An Approach for Cognitive and Self-Adapting Buildings,OriginalPaper,"Energy consumption of heating, cooling, ventilation, lighting, and appliances is deeply influenced by human presence in buildings. Accurate room occupancy prediction is a key to making buildings cognitive and self-adapting in order to achieve energy efficiency and wastage cut. Instead of using cameras or human tracking devices, a predictive model based on indoor non-intrusive environmental sensors allows mitigating privacy concerns. In such direction, this study aims to develop a data-driven model for occupancy prediction using machine learning techniques based on a combination of temperature, humidity, CO2 concentration, light, and motion sensors. The approach has been designed and realized in a real scenario by leveraging the COGITO platform. The experimental results show that the proposed long short-term memory neural network is well suited to account for occupancy detection at the current state and occupancy prediction at the future state, respectively, with an overall detection rate of 99.5% and 92.6% on a literature dataset and 99.6% and 94.2% on a real scenario. These outcomes indicate the ability of the proposed model to monitor the occupancy information of spaces both in a real-time and in a short-term way.","['Engineering', 'Cyber-physical systems, IoT', 'Communications Engineering, Networks', 'Computer Applications']"
doi:10.1007/978-981-19-4960-9_33,en,An Exploratory Study-Based Analysis on Loan Prediction,OriginalPaper,"A good amount of people seeking loans in India has enlarged to a greater extent and the reasons for it could be many. The Employees working in banking sectors are deprived of knowledge to judge or foresee whether a customer (good or poor) will be able to pay the debt of the loan at the stipulated interest rate. Throughout the financial system, banking institution offer a variety of services, but lines of credit remains their primary and the biggest source of income. As a consequence, banking businesses will prosper from the revenue produced on the mortgages they make. Lending, or whether consumers return the money or default on one’s loans, actually impact a banking institutions’ financial statement. The banking institutions non-performing investments will be diminished by estimating mortgage. Consequently, more exploration into this event happening is needed. Because detailed estimations are crucial and essential for sufficient service, various methodologies must be assessed and analyzed. Our study and work research seeks to provide a comprehensive review of lending estimation systems and structures that employ prediction methods and techniques flourished and developed after recent years. In this study and paper, researchers studied the learning techniques as well as the raw datasets utilized for training and test sets. The system model’s precision is also discussed. Our work also provides a quick overview of a few datasets that can be used to anticipate loan/mortgage analysis. Recent and future trends are also spotlighted.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-4863-3_52,en,Popularity of Optimization Techniques in Sentiment Analysis,OriginalPaper,"In today’s scenarios, online marketing and social networking require sentiment for opinion mining to understand its customers and users. The sentiment analysis involves extracting information from the text and symbols shared by the individuals over the website reflecting their opinions. It describes various emotions of the customers based on any product. Sentiment analysis is applicable to monitor social media that recognized the mood of customers against the brand or any other product. It has been observed that a variety of techniques were used to optimize the features extracted during sentiment analysis. In the present paper, the author has presented a detailed literature survey to outline the popularity of optimization techniques used in the field of sentiment analysis. The literature review conducted over the authenticated research published in the last decade had illustrated that most of the researchers had implemented Ant Colony Optimization (ACO) and Particle Swarm Intelligence (PSO) as optimization techniques. In addition to this hybrid, optimization had also been emerging in recent years. The work outcomes are supported by the graphical illustrations to show the rising popularity of optimization techniques in the field of sentiment analysis.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-19032-2_12,en,Low-Bit Quantization of Transformer for Audio Speech Recognition,OriginalPaper,"The automatic speech recognition is a challenging deep learning problem and transformer architectures have gained an immense improvement in the performance on that task. However, transformer-based models are computationally expensive and comparatively large, which creates issues on deploying them on the memory-constrained devices. Quantization is one of the most promising approaches in reducing the neural network’s size and latency. In this paper, we mainly focus on the optimization of the ASR transformer model by applying quantization and knowledge distillation. We apply the SotA quantization methods on the baseline ASR model and examine the sensitive layers which make significant contribution to the performance drop. We’ve come up with the improvements to accelerate the convergence of quantization methods and to enhance the quantization representation quality. Our modified 2-bit model has shown less than 1% drop in WER in comparison to the float model on the LibriSpeech dataset.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-981-19-5403-0_5,en,Workload Prediction of Virtual Machines Using Integrated Deep Learning Approaches Over Cloud Data Centers,OriginalPaper,"Exponential growth in the use of cloud computing services makes it difficult to forecast loads of virtual machines (VMs). Accurate virtual machine (VM) workload forecasting is the most critical task in appropriately managing cloud resources such as memory and central processing units while minimizing energy usage. To address this problem, an integrated deep learning model is proposed in this research paper. The model employs two popular neural networks: a bidirectional Long Short-Term Memory Network (BiLSTM) with a convolutional neural networks (CNN). The CNN component pulls high-level attributes from all VM workload data, whereas the BiLSTM component forecasts future VM workload. The experimental results reveal that the suggested model outperforms commonly used workload prediction methods in terms of forecasting accuracy of VMs workloads in cloud computing environments.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/s00256-022-04160-z,en,A deep learning algorithm for detecting lytic bone lesions of multiple myeloma on CT,"['OriginalPaper', 'Scientific Article']","Background Whole-body low-dose CT is the recommended initial imaging modality to evaluate bone destruction as a result of multiple myeloma. Accurate interpretation of these scans to detect small lytic bone lesions is time intensive. A functional deep learning) algorithm to detect lytic lesions on CTs could improve the value of these CTs for myeloma imaging. Our objectives were to develop a DL algorithm and determine its performance at detecting lytic lesions of multiple myeloma. Methods Axial slices (2-mm section thickness) from whole-body low-dose CT scans of subjects with biochemically confirmed plasma cell dyscrasias were included in the study. Data were split into train and test sets at the patient level targeting a 90%/10% split. Two musculoskeletal radiologists annotated lytic lesions on the images with bounding boxes. Subsequently, we developed a two-step deep learning model comprising bone segmentation followed by lesion detection. Unet and “You Look Only Once” (YOLO) models were used as bone segmentation and lesion detection algorithms, respectively. Diagnostic performance was determined using the area under the receiver operating characteristic curve (AUROC). Results Forty whole-body low-dose CTs from 40 subjects yielded 2193 image slices. A total of 5640 lytic lesions were annotated. The two-step model achieved a sensitivity of 91.6% and a specificity of 84.6%. Lesion detection AUROC was 90.4%. Conclusion We developed a deep learning model that detects lytic bone lesions of multiple myeloma on whole-body low-dose CTs with high performance. External validation is required prior to widespread adoption in clinical practice.","['Medicine & Public Health', 'Imaging / Radiology', 'Orthopedics', 'Pathology', 'Nuclear Medicine']"
doi:10.1007/978-981-16-8154-7_39,en,SatMVS: A Novel 3D Reconstruction Pipeline for Remote Sensing Satellite Imagery,OriginalPaper,"Recently, 3D reconstruction based on satellite imagery has been a hot topic in the remote sensing community. Its output called the digital surface model (DSM) can be widely used in urban planning, military navigation, and so on. Nowadays, almost all satellite image 3D reconstruction pipelines are based on traditional stereo matching algorithms which have low accuracy and long runtime. In contrast, the neural networks based on multi-view stereo (MVS) have shown great reconstruction performance in the computer vision community. To transfer the advanced MVS neural networks to the remote sensing community, we propose a novel 3D reconstruction pipeline called SatMVS. First, the input satellite images and their rational polynomial camera parameters (RPC) are cropped into small tiles according to the designated output DSM region. Second, the RPC parameters are converted to the projection matrix for the homography transform which is the core step in MVS neural networks. Third, the advanced MVS neural network is applied to estimate height maps from satellite images. At last, all inferred height maps from small tiles are converted to 3D points in Universal Transverse Mercator (UTM) coordinate system and fused to get the final complete DSM. In order to train and test SatMVS, we build a novel satellite imagery 3D reconstruction dataset called SatMVS3D dataset, which contains satellite images, RPC parameters, and height map ground truth that covers about 3km 2 . The experimental results on the SatMVS3D dataset demonstrate that our proposed pipeline can provide robust reconstruction performance.","['Engineering', 'Aerospace Technology and Astronautics', 'Communications Engineering, Networks', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Control and Systems Theory']"
doi:10.1007/978-981-19-8222-4_13,en,A Spiking Neural Network for Brain-Computer Interface of Four Classes Motor Imagery,OriginalPaper,"Spiking neural networks (SNN) has the advantages of low power consumption and high efficiency in processing temporal information. However, due to the difficulty of network training, there exist few studies about the applications of SNN in brain-computer interface (BCI), especially in the four-classification task of motor imagery (MI). In this study, we develop a four-layer SNN structure to solve the MI four-classification problem. Firstly, an improved optimization algorithm for Ben’s spiker algorithm (BSA) is presented to convert EEG signals into spike signals, which obtains about 50 times higher efficiency than the commonly used optimizing algorithms. Secondly, a SNN combined with spike long-short-time-memory (LSTM) module is proposed to perform four-classification tasks in MI. Finally, we introduce the channel-wise normalization strategy to facilitate the training of deeper layers. Our experiment on the publicly released dataset achieves the accuracy that is comparable to the previous work of one-Dimension convolution neural network (1D-CNN). Meanwhile, the number of parameters of proposed network is about 1/10 of that in 1D-CNN. This study reveals the great potential of the SNN in developing a low-power and wearable BCI system.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-19-4182-5_23,en,Extractive Long-Form Question Answering for Annual Reports Using BERT,OriginalPaper,"This paper suggests a strategy to obtain extractive long-form answers from Annual Reports. Perceiving most previous usage, BERT has been used to get short answers with either a phrase or a sentence. Annual reports are simply not sufficient and most queries require a more well-rounded answer. In this paper, TF-IDF, FinBERT, and BERT are used to build extractive long answers from annual reports—which is a very large context. The BERT model that is used in the pipeline is pre-trained on unsupervised textual data from annual reports and fine-tuned on FiQA.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-3-031-21333-5_76,en,ThermalYOLO: A Person Detection Neural Network in Thermal Images for Smart Environments,OriginalPaper,"Nowadays, low-resolution thermal cameras are gaining relevance in smart environments due to keeping user privacy by recording images and videos in domestic environments. Many neural networks obtain outstanding results from visible spectrum devices for human activity and event detection, such as fall detection, object detection or pose estimation. However, these state-of-the-art neural networks are trained in datasets that do not contain thermal images, so their performance on them is not good. The main objective of this work is human body recognition and segmentation from thermal cameras. For this purpose, we propose ThermalYOLO, a neural network based on the YOLO neural network and fine-tuned with thermal images. For the generation and auto-labelling of the thermal dataset, an IoT device with two cameras, a visible camera and a thermal camera, is used. Therefore, the user does not have to manually annotate the dataset. As a result, ThermalYOLO outperforms YOLO in thermal images from two different smart environments.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16072-1_7,en,A Semi-supervised Vulnerability Management System,OriginalPaper,"With the advent of modern network security advancements, computational resources of an organization are always at a threat from external entities. Such entities may be represented by hackers or miscreants who might cause significant damage to data and other software or hardware resources of an organization. A Vulnerability is a general way of representing a weakness in the software or hardware resources of the computational infrastructure of an organization. Such vulnerabilities may be either minor software issues, or in some cases may expose vital computational resources of the organization to external threats. The first step is to scan the entire computational infrastructure for such vulnerabilities. Once they are ascertained, a patching process is carried out to mitigate the threats. In order to perform effective mitigation, the most serious vulnerabilities should be given a higher priority. In order to create this priority list, a scoring mechanism is required for all scanned vulnerabilities. We present an end to end deployed vulnerability management system which can score these vulnerabilities using a natural language description of the same.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-8222-4_7,en,Emotion Recognition from EEG Using All-Convolution Residual Neural Network,OriginalPaper,"Emotion recognition has become a research hotspot due to the rapid development of machine learning and neuroscience. One of the most challenging tasks in the Brain Computer Interface (BCI) is to recognize human emotions by electroencephalography (EEG) signals. Motivated by the excellent performance of deep learning approaches in recognition tasks, we proposed an All-Convolution Residual Neural Network (ACRNN), which is a hybrid neural network that combines convolution neural network (CNN) and residual network (ResNet). The ACRNN solves the problem of information loss between convolution layer and full connection layer to some extent, and the time hardly increase. Meanwhile, instead of pooling layer, we increased the convolution step to reduce the size of the feature map, so there was no pooling layer in ACRNN. We conducted extensive experiments on the DEAP dataset to demonstrate the performance of the emotional recognition of the ACRNN. The experimental results demonstrate that the proposed method achieved an excellent performance with a recognition accuracy of 92.46% and 91.68% on arousal and valence classification task. It was verified that the ACRNN for emotion recognition is effective.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-19-4052-1_68,en,Forecasting COVID-19 Confirmed Cases in China Using an Optimization Method,OriginalPaper,"COVID-19, a brand-new coronavirus, was found in Wuhan, China, in December 2019 and has since spread to 24 additional nations as well as numerous locations in China. The number of confirmed cases continues to rise every day, reaching 34,598 on February 8, 2021. We present our findings a new method was used in this investigation, predictive framework, for such number of reported COVID-19 cases in the China. During the next 10 days, predicated on recently known cases in China. The suggested upgraded adaptable neuro-fuzzy powerful instrument (ANFIS) with an updated floral modeling is used in this model. The salp swarm algorithm (SSA) was used to implement the pollination algorithm (FPA). Generally, SSA is used to enhance FPA in order to minimize its shortcomings. The fundamental theme of the essay FPASSA-ANFIS seems to be a proposed paradigm of improving ANFIS effectiveness through determining FPASSA which was used to determine the ANFIS specifications. The world is also used to analyze the FPASSA-ANFIS model. Statistical figures from the World Health Organization (WHO) on the COVID-19 pandemic for forecast the cases reported these following are indeed the cases for the next 10 days. Most specifically, the FPASSA-ANFIS model in comparison to such a number of other models outperformed them in terms of computing time, root mean squared error (RMSE), and mean absolute percentage (MAP). Researchers also put the suggested model to the tests utilizing two distinct datasets of week pandemic confirmed cases from two or more countries: the USA and China. These results also indicated incredible performance.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6153-3_11,en,Resource-Aware Capsule Network,OriginalPaper,"Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as a resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets’ training and testing are slow and resource hungry. In this chapter, we propose two methods to reduce PCs to make CapsNet resource-efficient. In our first approach, we introduce Light and Enhanced Capsule Network (LE-CapsNet). In LE-CapsNet we modify the CapsNet architecture by introducing the Primary Capsule Generator (PCG) module. We further compress this network by optimizing the feature extraction and introduce LE-CapsNet-T as a tiny variant of the network. Using 3.8M weights, LE-CapsNet obtains 77.21% accuracy for the CIFAR-10 dataset while performing inference 4x faster. In our second approach, we investigate the possibility of pruning PCs in CapsNet. We show that a pruned version of CapsNet performs up to 9.90x faster than the conventional architecture by removing 95% of Capsules without loss of accuracy. Also, our pruned architecture saves on more than 95.36% of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-3-031-16281-7_25,en,Optimal Robot Workpiece Placement for Maximized Repeatability,OriginalPaper,"When choosing an industrial robot, repeatability is often one of the most important decision factors. But repeatability is not a global property, instead, it strongly depends on the robot’s workspace position. This is currently ignored in robot cell planning leading to a loss of actual repeatability. This paper not only quantifies this loss but presents a new algorithm that finds the optimal position of a workpiece such that repeatability is optimized. The predictions of this algorithm are verified using digital twin simulations on different robots which show up to nine times higher repeatability than with the unoptimized position.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-3998-3_20,en,Online Task Assignment Method of Multi-aircraft Based on Decreasing Pheromone Ant Colony Algorithm,OriginalPaper,"This paper focuses on the online task assignment of multi-aircraft. The constraint condition model and cost model are established by considering aircraft capability, target value, forbidden zone. In order to expand the search capability of solution space and improve the quality of solution, an improved ant colony algorithm combining decreasing pheromone and roulette is proposed to solve online task assignment problem, which is proved to have good real-time performance and reliability.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-2126-1_23,en,Manipulating Muscle Activity Data from Electromyography for Various Applications Using Artificial Intelligence,OriginalPaper,"Electromyography is used to capture electrical signals produced by skeletal muscle fibers. It is widely used in the medical field for evaluating problems in the nerves and muscles. It has been proven to provide accurate results that can even differentiate movements on a millimeter scale. New applications for EMG are being researched in numerous sectors. Uniqueness in any form of data is a boon for AI applications, and muscle activity generates a unique pattern of electrical signals for different movements like hand pinch or flexing. Artificial intelligence can be leveraged for the data generated by the human body for classification, prediction, and or analysis. Pairing the two together may lead to applications in diverse fields. In this paper, we experimented with applications like detecting motor neuron disease and hand gesture recognition using artificial intelligence with EMG data. We show improvement over the results of previous studies on gesture recognition and ALS detection using one-dimensional time series classification and CNNs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-981-19-5403-0_26,en,Disguise Face Classification Using EfficientNet Deep Learning,OriginalPaper,"With the rise in popularity of social media and intelligent gadgets, one essential biometrics for identifying people is their face. The efficiency of existing automatic face recognition systems has decreased due to factors such as face ageing, conceals, and pose variations. Face recognition algorithms must be more accurate for recognizing faces hidden behind masks and makeup, as security and surveillance requirements become more stringent. Disguise face classification acts as a standalone early warning system in such scenarios. The major goal of this research was to see how the EfficientNet family of models compares to the current state-of-the-art architecture in terms of disguise face classification. For disguise face classification, the EfficientNet deep learning architecture was proposed in this paper. The models were trained and tested using the Disguise Faces in Wild (DFW) 2018 data set. The transfer learning method was used to train EfficientNet and other deep learning models. Results obtained proved that EfficientNet-B3 has outperformed other EfficientNet architectures with 92.2% precision and 93.9% accuracy.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-981-19-2535-1_3,en,Plant Disease Classification Using Siamese Convolutional Neural Network,OriginalPaper,"Through the years, plant diseases have been a consistent risk to food security. Hence, their rapid identification could significantly mitigate the economic losses around the world, also reducing the harmful effect of manures and pesticides on the climate. When the disease is recognized, matching the characteristic trait, appropriate supervision measures can be applied. The idea of Precision Agriculture provides well-timed automation of agricultural processes by applying the methods of computational engineering in the agronomical domain, machine learning being the most researched and deployed technology. Furthermore, the authors have implemented a customized siamese neural network (SNN) for the originally collected tomato leaves dataset of 155 images with the achieved accuracy of 83.749% and 80.4% for training and the testing set respectively.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20601-6_16,en,A Proposed Model for Fake Media Detection Using Deep Learning Techniques,OriginalPaper,"Recent advances in visual media technology have led to new tools for processing and, more importantly, generating multimedia content. Modern AI-based technologies, in particular, have made it simple to create highly realistic manipulated videos. These synthetic videos, called Deep Fakes, can pose a serious threat to damage the reputation of public bodies or to shape public opinion about a particular event. For this, being able to identify this type of false information becomes essential. To extract features at the frame level, the system employs a convolutional neural network (CNN). These skills are then used to train a Long short-term memory (LSTM) that learns to classify whether a video has been manipulated or not. We ran tests on two datasets to assess the quality of the model. DeepFake Detection Dataset (DFDC) with the model receives 0.968 AUC and 0.937 F1score, compared to SOTA model, which receives 0.972 AUC and 0.906 F1score, and the CelebDF dataset, which receives 0.988 AUC and 0.9628 F1score.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09909-0_10,en,Comparative Study of MPC and PID Controllers in Autonomous Vehicle Application,OriginalPaper,"The main challenging task in autonomous navigation is path-following control where the vehicle required to track a given path. Which can be interpreted by the error between the reference path and the desired one is to converge to zero and it gets more complicated when the vehicle dynamics is considered, such as non-linear vehicle dynamics, fast sampling time and limited computing resources. In this paper, we compare and describe the performance of the control law based on PID and MPC controllers for vehicle path tracking. PID controller is used for controlling the speed, stability and precision of the response, and eliminating the static error. MPC controller is used because of its known advantages such as the explicit handling of constraints explicit use of a model, well understood and defined tuning parameters and the most interesting advantage is the ability to predict future behavior of the system and take it into consideration.","['Engineering', 'Robotics and Automation', 'Robotics', 'Engineering Design', 'Biomedical Engineering and Bioengineering']"
doi:10.1007/978-3-031-08580-2_17,en,Disease Diagnosis Based on Symptoms Description,OriginalPaper,"When one feels unwell, it is crucial to arrange a time as soon as possible to meet a doctor for early detection of potential health-related problems. However, a relatively large number of Vietnamese people usually avoid going to the hospital as they are afraid of long waits at such crowded places, while the current COVID-19 pandemic means being at those places poses a higher risk of contracting the disease. For simpler health problems, people would prefer a solution that, given their symptoms, provides a reliable diagnosis in a shorter time. This study presents an approach in building a deep-learning-based disease predictor of health conditions conducted from given symptoms in Vietnamese. The proposed method combines a tokenizer and bi-directional recurrent neural networks and achieved an accuracy of 98.96% (compared to a certified doctor’s diagnosis) in selected test cases, demonstrating its promising capabilities in the task. The application is expected to easily be integrated into a mobile application and open the way for other deep-learning-based solutions which analyze people’s symptoms to help them have their health conditions diagnosed at home.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2225-1_5,en,Easy Detect—Web Application for Symptom Identification and Doctor Recommendation,OriginalPaper,"It is not very safe to go to hospitals for regular check-ups as we are used to. Patients can get medical advice from the comfort of their homes from specialized medical professionals. Patient data can also be digitalized, so that it can be used at any hospital. Symptoms are taken as input, and our deep learning model will give the possible disease, which the person might be suffering from, and the doctor he/she needs to consult for further medication.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Statistics, general', 'Mobile and Network Security']"
doi:10.1007/978-981-19-0098-3_59,en,Digitization of Measuring Scales in Social Science Research Area,OriginalPaper,"Development in the space of computing technologies opened the doors for organizing large scale population studies on Internet where psychometric assessments are involved. This is resulting in higher speed of research, large datasets, and optimal costing to conduct research. However, mode of test conduction should not change the ethical responsibilities of researcher. Online mode of conduction should not have negative impact of it on its psychometric properties of measuring scale (MS). Over the period, Internet survey/testing became boon for people living in rural area away from physical testing facilities. During this period, we observed that Internet base survey conduction is having its own advantages and challenges. The aim of this paper is to explore and provide inputs to maintain psychometric properties of MS on Internet while taking technological advancement in this space. This paper should help while creating new research tools and/or MS like online surveys on Internet. It should help in creating complex experimental psychological models with enhanced statistical power of it. We will take help of technology advancement in the field of machine learning and artificial intelligence.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Statistics, general', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6032-1_16,en,A Model for Prediction of Water Level and Pressure in an Industrial Boiler Using Multivariate Regression,OriginalPaper,"During the operation of an industrial boiler, it is subjected to huge variations in pressure as well as water level which lead to a decrease in the performance of a boiler. This decline in performance can be avoided by minimizing these variations with the help of a control system. However, the conventional control system used by a boiler accounts for a significant time lag which in turn affects the performance negatively. Hence, in this study, predictive models of water level and pressure developed using mass and energy balance equations of the boiler are presented, which can be utilized in minimizing these variations by determining the fluctuations beforehand. The predictions made by these models can be used in adjusting the inputs such as feedwater rate and fuel input rate to avoid the impending changes in pressure and water level. The concept of multivariate regression has been implemented in developing these models that have an average error of 0.0243% in water level predictions and 8.0525% in pressure predictions. This regression algorithm deals with a water level range of 64.525%, pressure range of 0.467 MPa, and steam load range of 70%.","['Engineering', 'Mechanical Engineering', 'Engineering Fluid Dynamics', 'Vibration, Dynamical Systems, Control', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-05491-4_20,en,Automobile Theft Detection by Driving Behavior Identification Using Deep Autoencoder,OriginalPaper,"Modern vehicles consist of an on-board detection unit that can record a driver’s driving behavior. Detecting anomaly in the driving behavior can be used for theft detection. There are many supervised learning models to detect driving behavior. However, it is impractical to collect the behavior of possible thieves beforehand for training. In this work, we design an unsupervised deep autoencoder model, which can learn the driving behavior only from one or few vehicle owners and it can recognize non-owner driving behavior as the vehicle theft. The model is lightweight and it can achieve high accuracy up to around 98% like a supervised model. The analysis results also show that each driver has different important features for the detection when compared with the other drivers.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Big Data', 'Mathematical and Computational Engineering']"
doi:10.1007/978-981-19-7083-2_2,en,Communication Efficient Federated Learning,OriginalPaper,"Federated learning, as a branch of distributed learning, suffers from the high cost and concurrency of communication. The situation gets even worse in training deep models on large-scale mobile devices. Hence, communication efficiency is an avoidable topic in multi-party federated learning systems. Focusing on this issue, this chapter first discusses the background, the main methodologies and potential directions for communication efficient federated learning. Then, we introduce two methods for reducing the communication cost of federated learning, which are layer-wise asynchronous update and model quantization. Extensive experiments are performed to show the efficiency of these two approaches.","['Computer Science', 'Machine Learning', 'Privacy', 'Cryptology']"
doi:10.1007/978-981-19-2397-5_21,en,Scheduling of Cloud Computing Tasks via Intelligent Optimization Methods,OriginalPaper,"Distributed green cloud datacenters (DGCDs) are increasingly deployed around the world. DGCDs integrate many renewable sources to provide clean power and decrease their operating cost. They are spread over multiple locations, where renewable energy availability, bandwidth prices and grid electricity costs have high geographical diversity. This paper focuses on delay-bounded applications in DGCDs and performs cost and energy-effective scheduling of multiple heterogeneous applications subject to delay-bound constraints. The minimization problem of operational cost of DGCDs is formulated and successfully solved by using Firefly, bat, and simulated annealing-bat algorithms. Data-driven experiments are conducted to assess and compare their effectiveness to solve it. The Firefly algorithm is shown to well outperform its peers.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18461-1_59,en,Dual-Statistics Analysis with Motion Augmentation for Activity Recognition with COTS WiFi,OriginalPaper,"In recent years, WiFi signal based activity recognition attracts attention in the community. One traction is the ubiquity of WiFi devices. The challenge is to achieve sufficient accuracy with minimal infrastructure cost without compromising user experience, e.g., no device attachment on body. In this work, we propose a novel design paradigm called WiSen, to enhance the performance of the status quo. WiSen is able to fully utilize the channel information in received signals. Behind the scenes, WiSen exploits the diversity across subcarriers in the WiFi band while solving the challenge of dual-statistics analysis. With extensive experiments in typical environments, the dual-statistics scheme enhances the accuracy by 36% over the traditional approach. While, integration with motion augmentation further improves the overall accuracy by 5.2%, achieving 98% of overall accuracy.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17091-1_45,en,Personality Traits of Twitter Bullies Using Deep Learning,OriginalPaper,"Research in the phycology domain pointed out a relationship between the personality traits and the ability of a person to bully or be bullied. This paper aims at analyzing the personality traits, in terms of the Big Five model (BFM), of cyberbullies on Twitter communities. To achieve our objective, two deep learning-based models were trained to detect the BFM traits from a text; the first of them is a multi-label CNN based classifier while the second includes five independent binary CNN classifiers (one for each trait). The trained models were applied to a tweeter dataset of cyberbullying people, in order to identify their BFM personality traits. Experimental results showed that the characteristics of cyberbullies are high neuroticism, high extroversion, and low agreeableness. Moreover, modelling the BFM personality prediction problem using five binary classifiers achieves higher accuracy than utilizing one multi-label classifier.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-16281-7_3,en,Machine Learning Based Reconstruction of Process Forces,OriginalPaper,"During milling, process forces are acting on the cutting tool, causing tool deflection and subsequently a shape deviation of the workpiece. To compensate these effects, knowledge of the process forces is required. In this work, machine learning (ML) methods are applied to reconstruct process forces from the drive signals of two different milling centers. The results of a linear regression, bagged trees and a stacked LSTM are presented. The approaches show different results depending on the milling center. Only for the LSTM an error lower than 30 N is achieved for both machine tools. Independent of the ML approach, the results strongly depend on the selection of milling processes used for training.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-3-031-18409-3_18,en,Obfuscating LLVM Intermediate Representation Source Code with NSGA-II,OriginalPaper,"With the generalisation of distributed computing paradigms to sustain the surging demands for massive processing and data-analytic capabilities, the protection of the intellectual property tied to the executed programs transferred onto these remote shared platforms becomes critical. A more and more popular solution to this problem consists in applying obfuscating techniques, in particular at the source code level. Informally, the goal of obfuscation is to conceal the purpose of a program or its logic without altering its functionality, thus preventing reverse-engineering on the program even with the help of computing resources. This allows to protect software against plagiarism, tampering, or finding vulnerabilities that could be used for different kinds of attacks. The many advantages of code obfuscation, together with its low cost, makes it a popular technique. This paper proposes a novel methodology for source code obfuscation relying on the reference LLVM compiler infrastructure that can be used together with other traditional obfuscation techniques, making the code more robust against reverse engineering attacks. The problem is defined as a Multi-Objective Combinatorial Optimization (MOCO) problem, where the goal is to find sequences of LLVM optimizations that lead to highly obfuscated versions of the original code. These transformations are applied to the back-end pseudo-assembly code ( i.e. , LLVM Intermediate Representation), thus avoiding any further optimizations by the compiler. Three different problem flavours are defined and solved with popular NSGA-II genetic algorithm. The promising results show the potential of the proposed technique.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Education, general']"
doi:10.1007/978-3-031-16078-3_38,en,RAVSSNet: Recurrent Audio Visual Speech Separation,OriginalPaper,"The main objective of speech separation is to differentiate between overlapping speech and gain better insight about the information being conveyed. In this paper, we present a time domain approach to separate speech from audio-visual input to improve model accuracy, thereby making the approach ‘multi-modal’. We introduce a novel architecture which integrates visual component with the audio-only ‘Dual Path RNN’ model. Dual-Path RNN model achieves better performance compared to previously reported systems like TasNet, Conv-TasNet, etc. even though the model size is about 20 times smaller. Adding video encoder into DPRNN architecture makes for a new efficient time domain multi-modal speech separator. The proposed architecture performs better than audio-only models in terms of Si-SNR. Future work on this architecture can potentially lead to real-time implementation of audio-visual speech separation for various applications.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6153-3_3,en,Transfer Learning for Depression Screening from Follow-Up Clinical Interview Questions,OriginalPaper,"Depression is a common mental health disorder with large social and economic consequences. It can be costly and difficult to detect, traditionally requiring hours of assessment by a trained clinical. Recently, machine learning models have been trained to screen for depression with patient voice recordings collected during an interview with a virtual agent. To engage the patient in a conversation and increase the quantity of responses, the virtual interviewer asks a series of follow-up questions. However, asking fewer questions would reduce the time burden of screening for the participant. We, therefore, assess if these follow-up questions have a tangible impact on the performance of deep learning models for depression classification. Specifically, we study the effect of including the vocal and transcribed replies to one, two, three, four, five, or all follow-up questions in the depression screening models. We notably achieve this using unimodal and multimodal pre-trained transfer learning models. Our findings reveal that follow-up questions can help increase F1 scores for the majority of the interview questions. This research can be leveraged for the design of future mental illness screening applications by providing important information about both question selection and the best number of follow-up questions.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-3-031-21062-4_6,en,Robot Navigation Anticipative Strategies in Deep Reinforcement Motion Planning,OriginalPaper,"The navigation of robots in dynamic urban environments, requires elaborated anticipative strategies for the robot to avoid collisions with dynamic objects, like bicycles or pedestrians, and to be human aware. We have developed and analyzed three anticipative strategies in motion planning taking into account the future motion of the mobile objects that can move up to 18 km/h. First, we have used our hybrid policy resulting from a Deep Deterministic Policy Gradient (DDPG) training and the Social Force Model (SFM), and we have tested it in simulation in four complex map scenarios with many pedestrians. Second, we have used these anticipative strategies in real-life experiments using the hybrid motion planning method and the ROS Navigation Stack with Dynamic Windows Approach (NS-DWA). The results in simulations and real-life experiments show very good results in open environments and also in mixed scenarios with narrow spaces.","['Computer Science', 'Robotics', 'Robotics and Automation', 'Computational Intelligence']"
doi:10.1007/978-981-19-3951-8_23,en,Double U-Net a Deep Convolution Neural Network for Tongue Body Segmentation for Diseases Diagnosis,OriginalPaper,"A robust automatic tongue diagnosis system greatly relies on accurate segmentation of the tongue body from the image. Traditional methods that involve usage of snakes active contours, gradient vector flow, level set methods, etc., are now superseded by end-to-end trainable deep convolution networks like fully connected networks (FCNs), U-Net, residual network (ResNet). In this paper looking into the tremendous capability of deep neural networks, we have employed double U-Net for tongue image segmentation and compared the results with that of U-Net, and Res-U-Net architectures. Qualitative analysis of the three reveals a superior performance of double U-Net especially in images with additional dominant features of the face such as lips, teeth, and spaces with the tongue image. In addition, quantitative analysis indicates Res-U-Net to have a slight upper hand, but validation results push double U-Net toward higher performance.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20650-4_11,en,Transformer-Encoder Generated Context-Aware Embeddings for Spell Correction,OriginalPaper,"In this paper, we propose a novel approach for context-aware spell correction in text documents. We present a deep learning model that learns a context-aware character-level mapping of words to a compact embedding space. In the embedding space, a word and its spelling variations are mapped close to each other in a Euclidean space. After we develop this mapping for all words in the dataset’s vocabulary, it is possible to identify and correct wrongly spelt words by comparing the distances of their mappings with those of the correctly spelt words. The word embeddings are built in a way that captures the context of each word. This makes it easier for our system to identify correctly spelt words that are used out of their contexts (e.g., their/there, your/you’re). The Euclidean distance, between our word embeddings, can thus be deemed as a context-aware string similarity metric. We employ a transformer-encoder model that takes character-level input of words and their context to achieve this. The embeddings are generated as the outputs of the model. The model is then trained to minimize triplet loss, which ensures that spell variants of a word are embedded close to the word, and that unrelated words are embedded farther away. We further improve the efficiency of the training by using a hard triplet mining approach. Our approach was inspired by FaceNet [ 18 ], where the authors developed a similar approach for face recognition and clustering using embeddings generated from Convolutional Neural Networks. The results of our experiments show that our approach is effective in spell check applications.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-4863-3_46,en,An Ensemble Model to Detect Parkinson’s Disease Using MRI Images,OriginalPaper,"Parkinson’s disease (PD) is a highly common progressive central nervous system disorder caused by the decrease in neurons that produce dopamine in the basal ganglia and substantia nigra regions of the brain that control the body’s movement. To diagnose this disorder in the early stages, an extensive analysis of Magnetic Resonance Imaging (MRI) capable of capturing the pathophysiological changes in the brain that can determine the deficiency of dopamine in Parkinson’s disease affected patients is required. In our study, an ensemble of deep neural networks has been implemented to accurately detect and classify the brain MR images of patients into Parkinson’s disease and healthy control (HC). These deep learning models help clinicians use models with good classification performances of specific feature sets and better classify images in the early diagnoses of Parkinson’s disease. An ensemble of popular convolutional neural networks VGG16 and ResNet50 is performed. The model helps combine the best performance of the two models in extracting specific features of the images and is tested on a large dataset to observe an overall high classification performance compared to the individual performance of the models. A weighted average ensemble is used, which takes the ideal weights of the two models based on their contribution to classification. An accuracy of 96.09% is observed.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-16072-1_11,en,Stochastic Feed-forward Attention Mechanism for Reliable Defect Classification and Interpretation,OriginalPaper,"Defect analysis in manufacturing systems has been crucial for reducing product defect rates and improving process management efficiency. Recently, deep learning algorithms have been widely used to extract significant features from intertwined and complicated manufacturing systems. However, typical deep learning algorithms are black-box models in which the prediction process is difficult to understand. In this study, we propose a stochastic feed-forward attention network that consists of input feature level attention. The stochastic feed-forward attention network allows us to interpret the model by identifying the input features, dominant for prediction. In addition, the proposed model uses variational inferences to yield uncertainty information, which is a measure of the reliability of the interpretations. We conducted experiments in the field of display electrostatic chuck fabrication process to demonstrate the effectiveness and usefulness of our method. The results confirmed that our proposed method performs better and can reflect important input features.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2689-1_37,en,Aerothermodynamic Shape Optimization of a Hypersonic Lifting Body,OriginalPaper,"Hypersonic lifting vehicles experience severe aerodynamic heating while on hypersonic flight regime during atmospheric reentry or unpowered glide. Complex flow interaction is observed in a certain range of flight conditions that results in high heat flux banded region on the windward surface, which should be taken into consideration for thermal protection designing. To obtain desirable surface heat flux distribution and aerodynamic efficiency of a lifting body, shape optimization to reduce the interaction intensity and increase the spanwise distance of the banded heating region on the windward surface was performed in this study. Both single- and multi-objective designs were developed with multiple constraints, including volume, lift-to-drag ratio, the heating rates in stagnation region, and the base section profile of the body. The free-form deformation method was employed for evaluating surface deformation followed by the transfinite interpolation method to update the computational mesh. The range of Pareto front obtained was consistent with the results of single-objective optimization. Additionally, a contradictory and non-linear correlation between the two objectives was elucidated. The optimal solutions exhibited superior comprehensive aerodynamic performance, and the aerothermodynamic shape optimization method was demonstrated to be efficient and practicable.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-3-031-12409-9_5,en,Generalized Linear Models,OriginalPaper,"This chapter discusses state-of-the-art statistical modeling in insurance and actuarial science, which is the generalized linear model (GLM). We discuss GLMs in the light of claim count and claim size modeling, we present feature engineering, model fitting, model selection, over-dispersion, zero-inflated claim counts problems, double GLMs, and insurance-specific issues such as the balance property for having unbiasedness.","['Mathematics', 'Applications of Mathematics', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Machine Learning', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-0105-8_9,en,Light U-Net: Network Architecture for Outdoor Scene Semantic Segmentation,OriginalPaper,"We present a novel and efficient semantic segmentation architecture termed Light U-Net in this paper. Light U-Net is less complex with regard to the parameter and model size than the original U-Net. We also propose a network and training strategy based on geometric data augmentation for efficient use of the annotated images from small and limited datasets. Following improvements, we show the use of Light U-Net for semantic segmentation of outdoor scenes. Results reveal that our improvements and data augmentation remarkably enhance the proposed network’s performance. Results from experiments demonstrate that the proposed architecture outperforms some recent segmentation models achieving mIoU scores of 70.06% and 79.84% on the Camvid and Stanford Background datasets respectively.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Computational Intelligence', 'Bioinformatics']"
doi:10.1007/978-3-031-11814-2_7,en,Life Events that Cascade: An Excursion into DALY Computations,OriginalPaper,"A problem frequently encountered in point process modeling is that event times are usually not known. The only available information is the number of events over a given interval. Calculus and regularization present a convenient framework to perform inference in these circumstances through explicit formulas for this class of mixed doubly point processes. As an application, we present a novel way of dealing with uncertainties in the computation of disability-adjusted life year, which is a measure of overall disease burden in populations.","['Economics', 'Health Economics', 'Pharmaceutical Sciences/Technology', 'Economics, general', 'Biomedicine, general', 'Biotechnology', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1007/978-981-19-8222-4_10,en,Transfer Learning to Decode Brain States Reflecting the Relationship Between Cognitive Tasks,OriginalPaper,"Transfer learning improves the performance of the target task by leveraging the data of a specific source task: the closer the relationship between the source and the target tasks, the greater the performance improvement by transfer learning. In neuroscience, the relationship between cognitive tasks is usually represented by similarity of activated brain regions or neural representation. However, no study has linked transfer learning and neuroscience to reveal the relationship between cognitive tasks. In this study, we propose a transfer learning framework to reflect the relationship between cognitive tasks, and compare the task relations reflected by transfer learning and by the overlaps of brain regions ( e.g. , neurosynth). Our results of transfer learning create cognitive taskonomy to reflect the relationship between cognitive tasks which is well in line with the task relations derived from neurosynth. Transfer learning performs better in task decoding with fMRI data if the source and target cognitive tasks activate similar brain regions. Our study uncovers the relationship of multiple cognitive tasks and provides guidance for source task selection in transfer learning for neural decoding based on small-sample data.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-16-9967-2_24,en,Human Activity Recognition Using LSTM with Feature Extraction Through CNN,OriginalPaper,"Human activity recognition is important for detecting anomalies from videos. The analysis of auspicious activities using videos is increasingly important for security, surveillance, and personal archiving. This research paper has given a model which can recognize activities in random videos. The architecture has been designed by using BiLSTM layer which helps to learn a system based on time dependencies. To convert every frame into a featured vector, the pre-trained GoogLeNet network has been used. The evaluation has been done by using a public HMDB51 data set. The accuracy achieved by using the model is 93.04% for ten classes and 63.96% for 51 classes from same data set only. Then, this network is compared with other state-of-the-art method, and it proves to be a better approach for the recognition of activities.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Wireless and Mobile Communication', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-20650-4_9,en,Sequence-to-Sequence CNN-BiLSTM Based Glottal Closure Instant Detection from Raw Speech,OriginalPaper,"In this paper, we propose to frame glottal closure instant (GCI) detection from raw speech as a sequence-to-sequence prediction problem and to explore the potential of recurrent neural networks (RNNs) to handle this problem. We compare the RNN architecture to widely used convolutional neural networks (CNNs) and to some other machine learning-based and traditional non-learning algorithms on several publicly available databases. We show that the RNN architecture improves GCI detection. The best results were achieved for a joint CNN-BiLSTM model in which RNN is composed of bidirectional long short-term memory (BiLSTM) units and CNN layers are used to extract relevant features.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-1-4842-8864-1_7,en,Table Optimizations,OriginalPaper,"During the life cycle of your data-tier applications, you may need to perform a number of maintenance tasks and performance optimizations against the tables that hold your application’s data. These operations may include partitioning a table, compressing a table, or migrating data to a memory-optimized table. In this chapter, we will explore these three concepts in detail.","['Computer Science', 'Microsoft and .NET', 'Database Management']"
doi:10.1007/978-981-19-3387-5_158,en,Location-Independent Human Activity Recognition Using WiFi Signal,OriginalPaper,"The WiFi sensor constrains the Human activity recognition scheme to an immovable orientation when providing training samples, drastically reducing practical applications. In addition, the empirical investigation through the ubiquitous experience of participants in environments with wireless signals makes it imperatively challenging to evaluate a location-independent method to recognize human activity. In this research, a 1D CNN-LSTM model was built to comparatively analyze location-independent human activity recognition through WiFi CSI. Thus, the method reduces the impact of human activity recognition on location independent. Nevertheless, the experimental results demonstrate that our method can achieve over 94.9% coverage accuracy for location-independent human activity recognition and 90% coverage accuracy for CNNs using our proposed method. The location-independent model developed can be used possibly for practical applications in any situation, and data from each antenna is processed independently, making it applicable in any location or antenna setup. A variety of input sample lengths has been tested to overcome the measurement sensors’ sampling rate limitations. The results show that human activities can be recognized in real-time through WiFi signaling. Therefore, the CSI WiFi connectivity is potentially enhancive in building an excellent platform to locate human activities in modern age.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-16684-6_12,en,Planning the Electricity Transactions Based on AI,OriginalPaper,"In Romania, the electricity market in 2018 and the gas supply market in 2021 have been liberalized. We are currently facing a global energy crisis with an unprecedented rise in the price of oil, electricity, and gas. The correct substantiation of the portfolio framework contracts concluded between producers, suppliers, and energy consumers is an essential condition from a financial point of view regarding transactions. They must provide a benefit to the producers, within reasonable limits, so that the consumer price is not artificially increased. This process becomes optimal if a forecast based on artificial intelligence-AI is used, assuming the recently recorded consumption is mediated annually. In the case of the EU countries, it must reflect both the domestic consumption and international trade to be achieved. The forecast must be made in the form of load curves with hourly levels for each day of the selected period. The adopted solution uses a hybrid model, optimized statistically, with neuronal networks, built based on selected preliminary data. The modeling used in the daily task schedules is presented, using self-organizing processes with Kohonen type neural networks. AI and Fuzzy logic is used in estimating energy consumption for a different scenario of average daily and monthly consumption. The results obtained by forecasting are compared with the values recorded in the months selected as a model. The errors obtained are in the range of 2.8–5.1%, validating the elaborated model.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-3-031-20650-4_3,en,Minimizing Cross Intersections in Graph Drawing via Linear Splines,OriginalPaper,"The generation of aesthetically pleasing graph layouts is the main purpose of Graph Drawing techniques. Recent contributions delved into the usage of Gradient-descent (GD) based schemes to optimize differentiable loss functions, built to measure the graph layout adherence to given layout characteristics. However, some properties cannot be easily expressed via differentiable functions. In this direction, the recently proposed Graph Neural Drawer (GND) framework proposes to exploit the representational capability of neural models in order to be able to express differentiable losses, specifically for edge intersection, that can be subsequently optimized via GD. In this paper, we propose to improve graph layout readability leveraging linear splines. We exploit the principles behind GND and use a neural model both to identify crossing edges and to optimize their relative position. We split crossing edges introducing linear splines, and threat the control points as novel “fake” vertices that can be optimized via the underlying layout optimization process. We provide qualitative and quantitative analysis over multiple graphs and optimizing different aesthetic losses, that show how the proposed method is a viable solution.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-5845-8_24,en,"Comparative Analysis of Machine Learning and Deep Learning Algorithms for Real-Time Posture Detection to Prevent Sciatica, Kyphosis, Lordosis",OriginalPaper,"People must incorporate a “work from home” strategy because of the COVID-19 outbreak. In today's pandemic situation, due to working from home, employees are working for long hours, and spending long hours is a pretty challenging task. Nowadays, irrespective of age concerns, sciatica, Kyphosis, and lordosis are becoming a significant problem even for youngsters. The longest nerve in our body is sciatica, which causes severe pain due to stress applied while sitting in the wrong posture. It gets compressed with our lower back discs, which may lead to severe radiating pain from our lower back disc to the entire right leg, and a person can't even perform his daily activities comfortably. To prevent these problems, sitting posture while working should be maintained correctly. This work mainly focuses on preventing employers, and students from sciatica, Kyphosis, and Lordosis health issues. We used all kinds of sitting postures that interact while working with a laptop, classified which posture was good, and predicted which stance led to health issues. We used Convolutional Neural Network and K-Nearest Neighbor machine learning algorithms to predict the correct sitting postures. In KNN, we followed two techniques to improve the performance: using Edge detection. The other method we used was detecting facial landmark detection and plotting their respective rotational angles. So by using this technique, we improved the accuracy and precision rate compared to the classical Edge detection. We also trained the model with CNN, which gives good results. We performed a comparative analysis to pick the best model to integrate with OpenCV to make it real-time.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5240-1_5,en,Automated Paragraph Detection Using Cohesion Network Analysis,OriginalPaper,"The ability to express yourself concisely and coherently is a crucial skill, both for academic purposes and professional careers. An important aspect to consider in writing is an adequate segmentation of ideas, which in turn requires a proper understanding of where to place paragraph breaks. However, these decisions are often performed intuitively, with little systematicity in sequencing ideas. Thus, an automated method of detecting the optimal hierarchical structure of texts using quantifiable features could be a valuable tool for learners. Here, we aim to define a framework grounded in Cohesion Network Analysis to establish the structure of a text by modeling paragraphs as clusters of sentences. The analogy to clustering enables us to identify paragraph breaks that maximize inter-paragraph separation while ensuring high intra-paragraph cohesion. Our approach consists of two steps acted on texts without paragraph breaks. First, the number of paragraphs is automatically inferred with an absolute error of 1.02 using a Recurrent Neural Network, which relies on text features and cohesion flow. Second, paragraph splits are detected using two algorithms: top k which selects the largest cohesion gaps between adjacent utterances, and divisive clustering which iteratively splits the text into paragraphs. Silhouette scores are used to assess performance and the obtained values denote adequately inferred structures.","['Engineering', 'Computational Intelligence', 'Sociology, general', 'Big Data', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20241-4_37,en,Structural Health Monitoring with Artificial Neural Network and Subspace-Based Damage Indicators,OriginalPaper,"In recent years, different structural health monitoring (SHM) systems have been proposed to assess the actual conditions of existing bridges and effectively manage maintenance programmes. Nowadays, artificial intelligence (AI) tools represent the frontier of research providing innovative non-invasive and non-destructive evaluations directly based on output-only vibration measures. This is one of the key aspects of smart structures of the future. In the current study, an artificial neural network (ANN) method has been proposed in order to perform damage detection based on subspace-based damage indicators (DIs) and other statistical indicators. A numerical case study example has been analysed with simulated damaged conditions. Based on a comparison between a reference situation and a new one, the greatest advantage in adopting these particular DIs is because they are able to point out significant changes, i.e. possible damage, without requiring a beforehand modal identification procedure, which may introduce further noise and modelling errors inside the traditional damage detection process.","['Engineering', 'Building Construction and Design', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general']"
doi:10.1007/978-981-19-3951-8_24,en,Performance Efficacy of Cost-Sensitive Artificial Neural Network: Augmenting the Results of Imbalanced Datasets in Supervised and Unsupervised Learning,OriginalPaper,"In any type of classification model, successful implementation of the balanced dataset is required so as to obtain the optimal result in machine learning. Imbalance dataset enforces many challenges for achieving and implementing the analytical data in the real world. Such situation occurs when there is high variation in one sample class as compared to other sample class. Performance of any classification model is totally dependent on the dataset quality as well as its quantity. Imbalanced training data has a significant detrimental influence on overall performance. The most advanced machine learning approach always fights an unbalanced dataset by focusing on two major components as avoiding the minority class and decreasing inaccuracy for the majority. In this paper, an attempt is made to compare the results of SMOTE-NC, SMOTE-ENC, and cost-sensitive ANN (Artificial Neural Network) to alleviate the imbalance data and to generate the better efficiency than existing methods and models. Furthermore, it is clearly demonstrated that ‘cost-sensitive ANN’ technique generates better results using deep learning model for prediction with versatile datasets. Finally, the results of the experiment conducted in the study verified that ‘cost-sensitive ANN’ technique performs more effectively when there is a presence of extensive data and fundamentally the case of high ratio exists.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20322-0_21,en,Speaker Identification in Noisy Environments for Forensic Purposes,OriginalPaper,"The speech is a biological or physical feature unique to each person, and this is widely used in speaker identification tasks like access control, transaction authentication, home automation applications, among others. The aim of this research is to propose a connected-words speaker recognition scheme based on a closed-set speaker-independent voice corpus in noisy environments that can be applied in contexts such as forensic purposes. Using a KDD analysis, MFCCs were used as filtering technique to extract speech features from 158 speakers, to later carry out the speaker identification process. Paper presents a performance comparison of ANN, KNN and logistic regression models, which obtained a F1 score of 98%, 98.32% and 97.75%, respectively. The results show that schemes such as KNN and ANN can achieve a similar performance in full voice files when applying the proposed KDD framework, generating robust models applied in forensic environments.","['Engineering', 'Computational Intelligence', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/978-3-031-20650-4_12,en,Assessment of Pharmaceutical Patent Novelty with Siamese Neural Networks,OriginalPaper,"Patents in the pharmaceutical field fulfil an important role as they contain details of the final product that is the culmination of years of research and possibly millions of dollars of investment. It is crucial that both patent producers and consumers are able to assess the novelty of such patents and perform basic processing on them. In this work, we review approaches in the literature in patent analysis and novelty assessment that range from basic digitisation to deep learning-based approaches including natural language processing, image processing and chemical structure extraction. We propose a system that automates the process of patent novelty assessment using Siamese neural networks for similarity detection. Our system showed promising results and has a potential to improve upon the current patent analysis methods, specifically in the pharmaceutical field, by not just focusing on the task from a Natural Language Processing perspective, but also, adding image analysis and adaptations for chemical structure extraction.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-13249-0_3,en,Structural Form-Finding Enhanced by Graph Neural Networks,OriginalPaper,"Computational form-finding methods hold great potential for enabling resource-efficient structural design. In this context, the Combinatorial Equilibrium Modelling (CEM) allows the design of cross-typological tension-compression structures starting from an input topology diagram in the form of a graph. This paper presents an AI-assisted design workflow in which the graph modelling process required by the CEM is simplified through the application of a Graph Neural Network (GNN). To this end, a GNN model is used for the automatic labelling of edges of unlabelled topology diagrams. A synthetic topology diagram data generator is developed to produce training data for the GNN model. The trained GNN is tested on a dataset of typical bridge topologies based on real structures. The experiments show that the trained GNN generalises well to unseen synthetic data and data from real structures similar to the synthetic data. Hence, further developments of the GNN model have the potential to make the proposed design workflow a valuable tool for the conceptual design of structures.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Industrial Design', 'Interaction Design', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/978-3-031-07322-9_100,en,Numerical Modeling of a Pyroshock Test Plate for Qualification of Space Equipment,OriginalPaper,"Over their life, space equipment needs to withstand strong high-frequency shocks, which could cause mission and safety critical damages. In order to verify the compliance with safety standards, pyroshock tests are employed. Based on launch vehicle characteristics, the requirements for the qualification of space equipment are usually established following the NASA-STD-7003A international standards in terms of a Shock Response Spectrum (SRS) representing the damage potential of the shock. Laboratory tests should then match the actual stress conditions reached during a real launch. Historically, this was obtained by means of explosive charges (hence the name “pyroshock”). Nevertheless, to foster repeatability and safety in laboratories, hammers or bullets are commonly used in nowadays shock testing machines. In this work, a resonant fixture test bench is considered. In this very common layout, a resonant metallic plate is interposed between the impact location and the test component so as to better simulate the shocks. The response of the resonant plate - which determines the required shock response spectrum - is currently empirically tuned by adding masses, damping, stiffness, or by varying the nature of the impact. This study aimed at developing a numerical model able to completely simulate a pyroshock test. Such a model can be used both for designing and for tuning the test bench so as to easily match different SRS requirements for different components under test. This leads to great economical advantages as can cut the calibration times leading to more efficient and effective testing.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-4204-4_19,en,Application of On-Board Diagnostics (OBD) Data for Vehicle Trajectory Prediction,OriginalPaper,"This study explores the use of On-Board Diagnostics (OBD) data in the analysis and prediction of vehicle dynamics. Though various data sources are available for traffic data collection, these conventional approaches may not work for the complex traffic system in India, with its heterogeneity and lack of lane discipline. On-board units such as GPS and OBD are some devices, which perform independent of the traffic conditions. This study focuses on the use of OBD data along with GPS data for individual vehicle trajectory prediction. A machine learning tool, namely Long–Short-Term Memory (LSTM) model is employed and the prediction of speed and bearing for the next 1 s is done. Results obtained showed the OBD as a potential source of data that can be used for various real-time and offline applications.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Construction Management', 'Sustainable Development']"
doi:10.1007/978-3-031-16868-0_14,en,End-to-End Performance Predictors,OriginalPaper,"In fact, common optimization problems in ENAS are computationally expensive and are usually handled using surrogate-assisted EAs(SAEAs) [ 1 ], employing inexpensive approximation regression and classification models, such as the Gaussian process model [ 2 ], radial basis network (RBN), etc., to replace the costly fitness evaluation [ 3 ]. SAEAs have proven to be useful and efficient in a variety of practical optimization applications [ 1 ].","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5221-0_13,en,A Novel Approach for Implementation of Software Requirement Specifications Using the Humpback Whale Optimization Model,OriginalPaper,"The requirements elicitation process is the foundation of every software system, and it is focused with software requirement specification procedures. In terms of limitations and rules, constituents are the prime source of these requires. Software requirement prioritizing is regarded as among the most important approaches in the requirements elicitation since its being used to prioritize the order of execution of specifications based on the perspectives of participants. Whales’ optimization model, on either side, has lately been employed in optimization methods because it resembles humpback whale feeding behaviour by utilizing the bubbles net scavenging approach. The whale optimizer (WOA) has been used in the article to prioritize system specifications by presuming the specifications in the problem space and implementing scavenging behaviour to prioritize these specifications. The proposed method is assessed in terms of execution times with analytical hierarchy process (AHP) to prioritize various sizes requirement’s pairs, and the findings are summarized with 40% efficiency.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-18461-1_2,en,Face Generation from Skull Photo Using GAN and 3D Face Models,OriginalPaper,"Generating face images from skull images have many applications in fields such as archaeology, anthropology and especially forensics, etc. However, face/skull images generation remain a challenging problem due to the fact that face image and skull image have different characteristics and the data on skull images is also limited. Therefore, we consider this transformation as an unpaired image-to-image translation problem and research the recently popular generative models (GANs) to generate face images from skull images. To this end, we use a novel synthesis framework called U-GAT-IT, a new framework for unsupervised image-to-image translation. This framework use AdaLIN (Adaptive Layer-Instance Normalization), which a new normalization function to focus on more important regions between source and target domains. Furthermore, to visualize the generated face in many other aspects, we use an additional 3D facial generation model called DECA (Detailed Expression Capture and Animation), which is a model for 3D facial reconstruction that is trained to robustly produce a UV displacement map from a low-dimensional latent representation. Experimental results show that the proposed method achieves positive results compared to the current unpaired image-to-image translation models.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3494-0_4,en,Multi-Branch Traffic Flow Prediction Based on Temporal Speed,OriginalPaper,"Efficient traffic management is a major issue for developing countries. Traffic flow prediction is an important problem in intelligent transportation system (ITS). Various studies have been reported in the literature for traffic flow prediction in which combined models have been proposed only using traffic flow data. It is evident from the traffic flow theory that speed and flow are inter-related. Therefore, considering speed to predict flow in a model can help in improving the performance of a prediction method. Keeping this in mind, we propose a traffic flow prediction method consisting of two branches. First branch predicts traffic flow using past flow data through long short-term memory (LSTM) neural network. Second branch predicts volume using Gaussian process regression (GPR) based on temporal speed. Finally, prediction from both the branches was combined through weighted average. The mean squared error (MSE), root mean squared error (RMSE), and Pearson’s correlation coefficient ( r ) were used to evaluate the effectiveness of the proposed model. Based on these measures, it was found that results of our proposed model are promising.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Construction Management', 'Sustainable Development']"
doi:10.1007/978-981-19-4578-6_2,en,Short-Term Wind Power Prediction Based on DP-RVESN Model,OriginalPaper,"Recently, with the increasing lying of environmental pollution and energy consumption, renewable energy, and clean energy have been paid more and more attention [ 1 ]. Wind energy has gradually become the most promising new energy because its abundant source, safe, renewable, little pollution and low cost. In recent decades, the wind power has developed rapidly all over the world [ 1 ]. However, the volatility and intermittent of wind energy lead to the uneven and random nature of wind power, which will affect the power system adversely. Especially, the large-scale integration of wind power will make grid scheduling difficult inevitably, leading to reduce the reliability of the grid [ 2 ]. The prediction of wind power is considered an effective way to solve this problem. Therefore, accurate wind power prediction is essential for the integration of wind power and the stable operation of the power system [ 3 ].","['Engineering', 'Control, Robotics, Mechatronics', 'Energy Systems', 'Optimization', 'Control and Systems Theory', 'Power Electronics, Electrical Machines and Networks', 'Renewable and Green Energy']"
doi:10.1007/978-3-031-18050-7_3,en,Estimation of Lamb Weight Using Transfer Learning and Regression,OriginalPaper,"Meat production needs of accurate measurement of livestock weight. In lambs, traditional scales are still used to weigh live animals, which is a tedious process for the operators and stressful for the animal. In this paper, we propose a method to estimate the weight of live lambs automatically, fast, non-invasive and affordably. The system only requires a camera like those that can be found in mobile phones. Our approach is based on the use of a known Convolutional Neural Network architecture (Xception) pre-trained on the ImageNet dataset. The acquired knowledge during training is used to estimate the weight, which is known as transfer learning. The best results are achieved with a model that receives the image, the sex of the lamb and the height from where the image is taken. A mean absolute error (MAE) of 0.58 kg and an $$R^{2}$$ R 2 of 0.96 were obtained, improving on current techniques. Only one image and two values specified by the user (sex and height) allow to estimate with a minimum error the optimal weight of a lamb, maximising the economic profit.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-16075-2_26,en,A Deep Reinforcement Learning Algorithm Using A New Graph Transformer Model for Routing Problems,OriginalPaper,"Routing problems, which belong to a classical kind of problem in combinatorial optimization, have been extensively studied for many decades by researchers from different backgrounds. In recent years, Deep Reinforcement Learning (DRL) has been applied widely in self-driving, robotics, industrial automation, video games, and other fields, showing its strong decision-making and learning ability. In this paper, we propose a new graph transformer model, based on the DRL algorithm, for minimizing the route lengths of a given routing problem. Specifically, the actor-network parameters are trained by an improved REINFORCE algorithm to effectively reduce the variance and adjust the frequency of the reward values. Further, positional encoding is used in the encoding structure to make the multiple nodes satisfy translation invariance during the embedding process and enhance the stability of the model. The aggregate operation of the graph neural network applies to transformer model decoding stage at this time, which effectively captures the topological structure of the graph and the potential relationships between nodes. We have used our model to two classical routing problems, i.e., Traveling Salesman Problem (TSP) and Capacitate Vehicle Routing Problem (CVRP). The experimental results show that the optimization effect of our model on small and medium-sized TSP and CVRP surpasses the state-of-the-art DRL-based methods and some traditional algorithms. Meanwhile, this model also provides an effective strategy for solving combinatorial optimization problems on graphs.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-0151-5_8,en,Neoplastic—Nuclei Cell Labelling Using Mask R-CNN,OriginalPaper,"Neoplastic cells are tumorous cells that damage the cells around them and are the prologue for cancer development in organs. However, identifying these cells poses a bottle-neck in the research of cancer cure as it is an extremely tedious job to manually isolate these from the rest of the cells in the tissue. Hence, the automation of this process using deep learning (DL)-based object detection and segmentation techniques such as Mask R-CNN will allow researchers and pathologists to save valuable time otherwise consumed in manually identifying these nuclei. The main objective of this research paper is to provide an instance segmentation technique to label and segment neoplastic cell nuclei from multiple instances of whole-slide images (WSI). For this process, a contemporary neural network architecture called the mask region-based convolutional neural network (Mask R-CNN) was used. This proposed technique generates a pixel-wise binary mask. These masks are capable of segmenting these instances and facilitating the advancement of intelligent systems in medical imaging and computational pathology. This time can instead be devoted to developing better cures by conducting more research. The paper also highlights the best techniques and practices that can be employed while training a model for a task of such complexity. The results of these techniques provide a mean average precision (mAP) score of 0.756 and a binary panoptic quality (bPQ) score of 0.675.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/978-3-031-18458-1_56,en,An Efficient Deep Learning Technique for Detecting and Classifying the Growth of Weeds on Fields,OriginalPaper,"The agriculture cycle needs to be expanded in the next decades to meet the demand of the world population. Weeds are one of the main challenges that severely affect the agricultural production and its quality. An accurate, automatic, low cost, little environmental impacts and real-time weeds detection technique is required to control weeds effectively on fields. In addition, automating the classification process of weeds based on their growth stages is crucial for using appropriate weeds-controlling techniques. In this paper, we fly a drone to collect a dataset of four different weed (Consolida Regalis) growth stages. As well, we developed and trained deep learning object detector (YOLOv5) to detect weed (Consolida Regalis) and to classify its four growth stages in real-time with a sufficient accuracy. The results show that the generated YOLOv5 small model succeeds to detect and classify the weed’s growth stages in real-time with highest recall 0.794 at 156 FPS. However, YOLOv5 large model depicts efficient detection and classification precision of 0.827 at 70 FPS.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19945-5_7,en,Model for Preventing DDoS Attacks Using a Hypervisor,OriginalPaper,"Distributed denial of service (DDoS) attacks aim to deny service, and these attacks have long been a serious threat to networks. DDoS attacks can quickly drain the network resource server or server capacity, prevent access to resources, and cause negative effects such as a loss of profits and the inability to use services and perform transactions. Network users currently encounter an unprecedented range of attacks and threats that lead to large-scale losses. Therefore, timely and reliable detection of various attacks has become urgent. Many solutions have been proposed to develop intrusion detection systems (IDSs). These solutions are widely used to search for unauthorized sources. However, IDSs must be constantly improved because species attacks on various network entities evolve continuously. Intrusion detection is divided into two levels depending on the intrusion source: host-based detection and network intrusion detection. This study proposes a model designed using DDoS dataset analysis, which is practical for large datasets subject to DDoS attacks, and an intelligent metaheuristic algorithm for training and classification based on the selected attack intent and in which one attack is included among blacklisted IPs. The common goal of the proposed approach and related approaches is to detect DDoS attacks. The metric scores are presented concisely in a confusion matrix to easily calculate other representative evaluation measures, such as precision, accuracy, and sensitivity.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Computer Applications']"
doi:10.1007/978-981-19-6153-3_14,en,Multi-layer Wavelet Transformations for Image Super-Resolution: Applications to Voxel-Based Deep Learning and Areal Density Maps of Carbon Nanotube Sheets,OriginalPaper,"Miralon sheets are an advanced carbon-based product which enables environmentally resistant solutions for some of the toughest industrial problems. One of the challenges for quality control is to ensure the uniformity of density distribution. However, current areal density maps obtained from a beta transmission equipment are insufficient to identify variation and defects at the required level. As areal density maps can be interpreted as images, image super-resolution convolutional neural networks (CNNs) promise to reconstruct high-resolution areal density maps with fine texture details. However, for this promise to be realized two challenges need to be overcome. First, generating areal density maps is expensive, and only limited training data is available. Second, current 2D CNNs cannot provide sufficient accuracy in depth. Therefore, we propose a 3-layer voxel representation to adapt 2D CNNs to the particular application of Miralon sheets. Moreover, we also study the transform domains to provide more distinguishable features for density distribution patterns. Our results demonstrate that appropriate data augmentation is essential to achieving accurate results and transform domains could provide special information based on image contents. Using our proposed techniques, a high-resolution areal density map, which is rich in 3D information, can be obtained by leveraging 2D image super-resolution CNNs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-3-031-16684-6_13,en,Integrated Solutions and Computerized Human Gesture Control,OriginalPaper,"Recognition of human actions is a topical issue due to its applicability in different domains such as security, medicine, behavioral analysis, and education. The development of the computer processing power, as well as the quality of image capture equipment, has increased the accessibility of larger and more accurate data sets. This permanently improves the performance of the developed programs. Convolutional neural networks are the starting point for identifying facial analysis and gesture recognition. By their nature, they can recognize ideal Spatio-temporal features for RGB image processing. The proposed model with 3D Kernel is useful in certain situations where the gestures are very different from each other and must be recognized by a system that reacts quickly and with high processing speed (emergencies or critical situations). The main objective of this paper is to build a reliable and easy-to-implement human gesture recognition classifier. It must be able to easily classify a large number of images so that it can operate in real-time, processing at least 30 frames per second. This is possible by using other hardware components of a shelf computer connected to a normal CCTV-Closed Circuit TV infrastructure. The selection of the gesture class must be not related to the environment, person, background, appearance, or viewing angle. The decision should be related only to the person to whom the gesture is attributed and to the context in which he reacts.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-3-031-18050-7_38,en,A GAN Approach for Anomaly Detection in Spacecraft Telemetries,OriginalPaper,"In spacecraft health management a large number of time series is acquired and used for on-board units surveillance and for historical data analysis. The early detection of abnormal behaviors in telemetry data can prevent failures in the spacecraft equipment. In this paper we present an advanced monitoring system that was carried out in partnership with Thales Alenia Space Italia S.p.A, a leading industry in the field of spacecraft manufacturing. In particular, we developed an anomaly detection algorithm based on Generative Adversarial Networks, that thanks to their ability to model arbitrary distributions in high dimensional spaces, allow to capture complex anomalies avoiding the burden of hand crafted feature extraction. We applied this method to detect anomalies in telemetry data collected from a simulator of a Low Earth Orbit satellite. One of the strengths of the proposed approach is that it does not require any previous knowledge on the signal. This is particular useful in the context of anomaly detection where we do not have a model of the anomaly. Hence the only assumption we made is that an anomaly is a pattern that lives in a lower probability region of the data space.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-981-16-9967-2_8,en,Offline Handwritten Signature Forgery Verification Using Deep Learning Methods,OriginalPaper,"Offline signature verification is one of the most challenging tasks in biometric authentication. Despite recent advances in this field using image recognition and deep learning, there are many remaining things to be explored. The most recent technique, which is Siamese convolutional neural network, has been used a lot in this field and has achieved great results. This paper presents an architecture that combines the power of Siamese Triplet CNN and a fully-connected neural network for binary classification to automatically verify genuine and forgery signatures even if the forged signature is highly skilled. On the challenging public dataset for signature verification BHSig260, the proposed model can achieve a low False Acceptance Rate = 13.66, which is slightly better than the reference model. Based on this approach, the one-shot learning should make it possible to determine if the input image is genuine or fraudulent just from one base image. Therefore, our model is expected to be extremely suitable for practical problems, such as banking systems or mobile authentication applications, in which the amount of data for each identity is limited in quantity and variety.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Wireless and Mobile Communication', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-662-65625-9_1,en,"Logistics, Operations, and Supply Chain Management",OriginalPaper,"Logistics, operations, and supply chain management deal with the management of systems that determine the performance of an enterprise or among companies as well as with the corresponding planning & control of daily business operations. These tasks are still managed by human beings who, through intuition and based on experience, find creative solutions. People have unique strategic and operational management abilities, in that they can fill in the blanks accurately and react flexibly to specific situations. However, if processes become more complex, frequent, and rapid, intuition alone does not suffice. Prior experience can also be misleading. In large companies and in transcorporate supply chains, moreover, there are many people involved in the processes, both simultaneously and in sequence. They differ in their experience, knowledge, and intuition. Therefore, logistics, operations, and supply chain management stand in the field of tension of the various stakeholders and contradictory objectives of the company or supply chain.","['Engineering', 'Engineering Economics, Organization, Logistics, Marketing', 'Operations Management', 'IT in Business', 'Industrial Organization', 'Organization']"
doi:10.1007/978-981-19-3951-8_17,en,A Deep Learning Model for Air Quality Forecasting Based on 1D Convolution and BiLSTM,OriginalPaper,"Particulate matter has a significantly larger impact on human health than other toxins which makes air pollution a highly serious problem. The air quality of a given region can be utilized as a primary determinant of the pollution index, as well as how well the industries and population are controlled. With the development of industries, monitoring urban air quality has become a persistent issue. At the same time, the crucial effect of air pollution on individuals’ healthiness and the environment and monitoring air quality is becoming gradually important, mainly in urban areas. Several computing methods have been studied and compared to verify the accurateness of air quality forecasting requirements to date, ranging from machine learning to deep learning. This paper introduced a deep learning air quality forecasting approach based on the convolutional bidirectional long short-term memory (CBLSTM) model for PM 2.5, which combines 1D convolution and bidirectional LSTM neural networks. The experiment findings demonstrate that the suggested approach outperforms the LSTM, CBLSTM, and CBGRU comparison models and achieves a high accuracy rate (MAE = 6.8 and RMSE = 10.2).","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3796-5_1,en,Deep Learning Model for Arrhythmia Classification with 2D Convolutional Neural Network,OriginalPaper,"Cardiac arrhythmia, an irregular heart rhythm, causes a massive number of sudden casualties due to delays in diagnosis and treatment. Furthermore, early automated arrhythmia diagnosis could help the physician identify types of arrhythmia which help in precise and personalized treatment. Traditional 1D ECG signal is converted to images since the artifacts in the 1D signal may perturb the accurate classification. Diagnosis of arrhythmia in the ECG signals has taken a new dimension of transforming signals to images, which is versatile. In this paper, we have proposed two models, ARBC and AMBC, for binary and multi-class classification of ECG signals using convolutional neural networks (CNN). Experimental effects show that the proposed models reached an accuracy of 96.88% and 98.98% for binary and multi-class classification, respectively.","['Engineering', 'Communications Engineering, Networks', 'Systems and Data Security', 'Artificial Intelligence', 'Software Engineering/Programming and Operating Systems', 'Computational Intelligence']"
doi:10.1007/978-3-031-14771-5_28,en,Using Nursing Notes to Predict Length of Stay in ICU for Critically Ill Patients,OriginalPaper,"Managing resource critical facilities like Intensive Care Units (ICUs) is an important task for hospital management officials. Predicting how long a patient is going to stay in ICU is considered an important problem for managers. Several attempts have been made to solve this problem using different types of clinical data that are available from the past. While a number of studies have deployed classification models that use structured clinical variables, recent advances in Natural Language Processing models have opened up the possibilities of using unstructured text data like nursing notes, discharge summaries, etc. for prediction. In this work, we have proposed the use of CNN and LSTM based prediction networks along with transformer-based language models for representing the notes data. The proposed model can predict with a much higher accuracy rate than any other existing model. The dataset used for the experiment is MIMIC, which is an anonymized dataset that contains detailed records of around 40,000 patients most of whom were critically ill. We use the first day’s nursing notes for prediction since that can provide most relevant and valuable input to planning.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2397-5_16,en,PSO-Based Improved Surface Roughness Measuring Approach of Manufactured Product Within CP Factory Using T6 6068 Aluminium,OriginalPaper,"This paper presents a methodology to obtain improved quality of surface roughness during production of mobile case cover inside a cyberphysical (CP) factory using micro-CNC end milling with aluminium alloy T6 (6068). The said machining is done with different machining parameters such as cutting velocity, spindle speed and cut depth. Three profile parameters ( R a , R z and R z max ) are projected as response variables. Thereafter, Taguchi’s orthogonal array design is considered with smaller-is-better signal-to-noise ratio, and linear regression is performed to get optimal process parameter settings combination. This result is further verified using a particle swarm optimization (PSO) technique, and validation is done on CNC machining centre.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16832-1_9,en,Fitting Curves of Ruminal Degradation Using a Metaheuristic Approach,OriginalPaper,"The use of the modeling process in animal nutrition has found many applications. Therefore, using artificial intelligence and optimization techniques is inevitable to pay attention to all the needs and nutrient degradability and degradation fraction. This study aimed to investigate the use of particle swarm optimization (PSO) to describe the disappearance curves of oats, and beans cuts. This paper uses the first-order kinetic model to describe the disappearance of rumen dry matter (DM) and crude protein (CP) for oats and bean cuts. The results showed that the model fitted the disappearance data well ( R 2  > 0.98), with minor differences in statistical evaluation. Also, reducing the number of iterations increased the merit of this method. To conclude, the utilization of PSO for degradability curve fitting is recommended.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering']"
doi:10.1007/978-3-031-16281-7_11,en,Resource-Constrained Implementation of Deep Learning Algorithms for Dynamic Touch Modality Classification,OriginalPaper,"Integrating Machine Learning (ML) algorithms with tactile sensing arrays yield sophisticated systems capable of performing intelligent tasks. Such systems can be used in prosthetic devices and robotics applications, enabling conducting daily tasks and manipulations. This paper presents low-cost and resource-constrained implementations of deep learning algorithms for the classification of dynamic touch modality based on alphabetic letter patterns. This work provides a comparison between two types of deep neural networks: 1-D Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Moreover, the models providing the best performance in terms of accuracy and computational cost have been deployed on a resource-constrained embedded system. Experimental results show that 1-D CNN outperforms RNNs in terms of both accuracy and computational cost achieving a classification time of 242 ms using 32-bit floating point on the Arduino Nano BLE hardware device.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-4208-2_1,en,Application of Jaya Algorithm for Minimizing Surface Finish in MQL-Based Green Machining of Titanium Alloy,OriginalPaper,"“Sustainable Manufacturing” is a recent concept to obtain efficiency in the manufacturing process in terms of economy and environmental friendliness with social aspects. Green manufacturing (GM) is a sub-set of sustainable manufacturing and aims to obtain environmentally friendly machining. Dry machining, minimum quantity lubrication (MQL) machining, and nanofluid machining are different approaches employed for minimizing the use of cutting fluid during machining, aiming toward the achievement of the green machining concept. Also, the use of cutting fluids causes several health hazards for human operators during the machining and handling processes. Machining of hard aero-space materials like titanium and nickel-based alloys with better surface finish requires a flood coolant system. This work aims to minimize the surface finish and cutting temperature and obtain optimum machining parameters during nanofluid-based MQL machining of titanium alloy Ti6Al4V. Turning experiments, conducted by Zaman et al. (Adv Mater Process Technol 1–21, 2020), are used for optimizing the process. A Box Behnken design of experiments with four factors at three levels is used, with 27 experimental runs. Model for Ra and T is developed with response surface methodology (RSM) as a function of four process variables (i.e., v , f , d, and C ). The process is optimized using the Jaya algorithm, a popular parameter-free method. The results are compared with the conventional desirability function (DF) approach and found to be closer; the source codes developed were run in MATLAB and obtained optimum parameters with a fewer iterations.","['Engineering', 'Industrial and Production Engineering', 'Robotics and Automation', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/978-3-031-16078-3_9,en,Boundary-Based Fake Face Anomaly Detection in Videos Using Recurrent Neural Networks,OriginalPaper,"With the surge in the videos over the internet and the spread of digital communications, the attackers are greatly benefited by creating and publishing the fake distorted videos and images known as Deepfakes. The recent trends, Machine Learning powered applications are highly implied to create deepfakes that leads to fallacious arguments. To detect such an instance, there are several existing techniques that uses convolutional neural networks and machine learning methods which are limited to identify the larger details of the deepfake created. We propose a novel anomaly detection technique for face areas in videos using recurrent neural networks based on boundary marking and hashing technique which is efficient in detecting the deepfakes over a smaller face boundary regions. The proposed method is evaluated, and the results show promising performance for deepfake detection in the face area based on the boundaries. Our proposed method revealed good accuracy in detecting the smaller frame 8  $$\times $$ ×  8 raster based anomalies in videos.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17544-2_4,en,Multi-Objective Optimization Algorithms in Medical Image Analysis,OriginalPaper,"Color correction is important part for medical image preprocessing. The proposed method provides color correction with minimum error for the human visual perception. The principal feature of proposed method: the color error is calculated according to perceptual metric CIEDE2000. Using the loss function based on CIEDE2000 metric leads to necessity transfer from least square method using for transformation function parameters identification to multi-objective optimization. Each color of palette is represented as separate target function. As algorithm for palette matching, the 3rd order polynomial with 11 coefficients for each color channel was used. Algorithm of transformation function coefficients estimation based on multi-objective optimization includes three steps: evaluation of starting point by least square method, line search by BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm and solution refinement by Nelder—Mead Algorithm. Our experiments show that for all colors from palette error according CIEDE2000 is less than 1. If error of CIEDE2000 for colors after matching is more than 1 the difference between color will be visible for observer.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-981-19-6153-3_15,en,Deep Semi-supervised Learning for Time-Series Classification,OriginalPaper,"While deep semi-supervised learning has gained much attention in computer vision, limited research exists on its applicability in the time-series domain. In this work, we investigate the transferability of state-of-the-art deep semi-supervised models from image to time-series classification. We discuss the necessary model adaptations, in particular, an appropriate model backbone architecture and the use of tailored data augmentation strategies. Based on these adaptations, we explore the potential of deep semi-supervised learning in the context of time-series classification by evaluating our methods on large public time-series classification problems with varying amounts of labeled samples. We perform extensive comparisons under a decidedly realistic and appropriate evaluation scheme with a unified reimplementation of all algorithms considered, which is yet lacking in the field. Further, we shed light on the effect of different data augmentation strategies and model architecture backbones in this context within a series of experiments. We find that these transferred semi-supervised models show substantial performance gains over strong supervised, semi-supervised and self-supervised alternatives, especially for scenarios with very few labeled samples.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-981-19-1610-6_17,en,A Novel Video Prediction Algorithm Based on Robust Spatiotemporal Convolutional Long Short-Term Memory (Robust-ST-ConvLSTM),OriginalPaper,"Recently, video prediction algorithms based on neural networks have become a promising research direction. Therefore, a new recurrent video prediction algorithm called “Robust Spatiotemporal Convolutional Long Short-Term Memory” (Robust-ST-ConvLSTM) is proposed in this paper. Robust-ST-ConvLSTM proposes a new internal mechanism that is able to regulate efficiently the flow of spatiotemporal information from video signals based on higher-order Convolutional-LSTM. The spatiotemporal information is carried through the entire network to optimize and control the prediction potential of the ConvLSTM cell. In addition, in traditional ConvLSTM units, cell states, that carry relevant information throughout the processing of the input sequence, are updated using only one previous hidden state, which holds information on previous data unit already seen by the network. However, our Robust-ST-ConvLSTM unit will rely on N previous hidden states, that provide temporal context for the motion in video scenes, in the cell state updating process. Experimental results further suggest that the proposed architecture can improve the state-of-the-art video prediction methods significantly on two challenging datasets, including the standard Moving MNIST dataset, and the commonly used video prediction KTH dataset, as human motion dataset.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4193-1_32,en,Chaos-Based Image Encryption with Salp Swarm Key Optimization,OriginalPaper,"Substantial data is being transferred across the unsecured channel; with this considerable data transfer comes the need to protect this data. Thus, to achieve security during transmission, several encryption algorithms have been proposed. Chaos-based maps are widely employed for multimedia encryption due to their characteristics, like pseudo-randomness sensitivity to initial conditions. Inspired by researchers, we proposed an image security algorithm based on a chaotic tent map integrated with the Salp Swarm Algorithm (SSA) for key generation and optimization, for grayscale images. A diffusion and permutation are carried out in each round to make it secure. A simple XOR function is applied to encrypt and decrypt the data. Different statistical analysis has been applied to images, and results has been discussed to justify proposed techniques effectiveness.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2126-1_27,en,Aerial Object Detection Using Different Models of YOLO Architecture: A Comparative Study,OriginalPaper,"Aerial object detection is a key to many functionalities like security systems, pedestrian counting, animal population estimation, security surveillance and many more. Previously used traditional machine learning approaches made use of handmade features and algorithms that failed to generalise on a larger data set. Therefore, deep learning models have outperformed the traditional machine learning models, especially in the computer vision field. Aerial object detection is a subdomain of object detection that has been a hot topic of interest in recent years. Transfer learning has emerged as one of the go-to methods to adapt models well on a small data set. This paper proposes a comparative study of the performance of three state-of-the-art object detection models on an aerial images data set using Transfer learning. The paper aims at studying how well a model can adapt to aerial images using transfer learning. The three models used are the YOLOv5m6, YOLOv5x6 and the YOLOv5l6 models based on the YOLO architecture. The data set used is the Vehicle Detection in Aerial Imagery (VEDAI) data set. The study finds out that the YOLOv5m6 model outperforms the other two models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-981-19-2535-1_8,en,Pose Driven Deep Appearance Feature Learning for Action Classification,OriginalPaper,"In this work, we propose to learn the fusion process between the dominant skeletal features and the RGB features. This is in contrast to the previous fusion methods that simply fused these multimodal features, without learning the fusion process to exploit the semantic relationship between them. Here, we propose a gated feature fusion (GFF) of multimodal feature data which provides attention to the appearance stream of RGB data using the temporal skeletal data. Initially, the features from RGB and skeletal frames are extracted using CNN models. Subsequently, the gated fusion network fuses the features from pose and appearance domains using temporal convolutions which are further combined into a latent subspace. Finally, the latent subspace features are classified using fully connected layers with the combined loss embeddings. The proposed architecture has performed better than the state-of-the-art models on RGB-D action datasets.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16237-4_2,en,Machine Learning Construction: Implications to Cybersecurity,OriginalPaper,"Statistical learning is the process of estimating an unknown probabilistic input-output relationship of a system using a limited number of observations. A statistical learning machine (SLM) is the algorithm, function, model, or rule, that learns such a process; and machine learning (ML) is the conventional name of this field. ML and its applications are ubiquitous in the modern world. Systems such as Automatic target recognition (ATR) in military applications, computer aided diagnosis (CAD) in medical imaging, DNA microarrays in genomics, optical character recognition (OCR), speech recognition (SR), spam email filtering, stock market prediction, etc., are few examples and applications for ML; diverse fields but one theory. In particular, ML has gained a lot of attention in the field of cyberphysical security, especially in the last decade. It is of great importance to this field to design detection algorithms that have the capability of learning from security data to be able to hunt threats, achieve better monitoring, master the complexity of the threat intelligence feeds, and achieve timely remediation of security incidents. The field of ML can be decomposed into two basic subfields: construction and assessment . We mean by construction designing or inventing an appropriate algorithm that learns from the input data and achieves a good performance according to some optimality criterion. We mean by assessment attributing some performance measures to the constructed ML algorithm, along with their estimators, to objectively assess this algorithm. Construction and assessment of a ML algorithm require familiarity with different other fields: probability, statistics, matrix theory, optimization, algorithms, and programming, among others. To help practitioners, specially those of cyberphysical security, to understand the theoretical foundations of ML, before they delve into whole books, we compile the very basics of the first of these two subfields ( construction ) in this chapter. In addition to explaining the mathematical foundations of the field, we emphasize the intuitive explanation and concepts.","['Engineering', 'Cyber-physical systems, IoT', 'Data Engineering', 'Computational Intelligence', 'Big Data', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_20,en,Applying the Population-Based Ant Colony Optimization to the Dynamic Vehicle Routing Problem,OriginalPaper,"The population-based ant colony optimization (P-ACO) algorithm is a variant of the ant colony optimization metaheuristic specifically designed to address dynamic optimization problems. Whenever a change in the environment occurs, P-ACO repairs the pheromone trails affected by the change using previous solutions maintained in a population-list. Typically, change-related information are utilized for repairing these solutions. The change-related information for this dynamic vehicle routing problem (DVRP) case are the nodes removed and inserted when a change in the environment occurs. In this chapter, the operators of the unstringing and stringing (US) heuristic are utilized for repairing the solutions. Experimental results demonstrate that P-ACO embedded with the US heuristic outperforms other peer methods in a series of DVRP test cases.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-1142-2_1,en,Enhancement of Energy Efficiency in Wireless Sensor Network with Mobile Sink: A Survey,OriginalPaper,"The energy consumed by any activity taking place in WSN should be controlled such that limited energy in terms of battery backup remains focus throughout. In the case of dying nodes, battery discharge may cause the network to get disconnected. WSN design issues, e.g., location of sensor nodes, scheduling activities, routes of data flow, mobile sink route, should be dealt with keeping energy limitation in mind. The sensor nodes sense the data from the area of concern and communicate the same to the sink for processing. Sensor nodes deployed in various application areas have limited memory, computational power, and battery backup. There is no defined topology of such network and frequently changing environment, very less amount of battery, and limited storage capability of the nodes. It is essential that each node in the network has knowledge about the routing path to the sink which is energy efficient. Since random placement of the nodes restrains coders from presuming routing table data at the sensor nodes, numerous methods have been suggested to create a dynamic path up to sink. Numerous researches are performed for WSN using the mobile sink. Most of the research activities focused on energy conservation in the background while proposing approaches for clustering, data flow paths, trajectory design, etc. In the WSN with a mobile sink, the trajectory of the sink node plays a vital role. Designing of trajectory is an NP-hard problem. With the use of nature-inspired techniques, e.g., particle swarm optimization (PSO), genetic algorithm (GA), etc., can be used for generating a nearly optimal paths for the mobile sink. In this current article, the authors make attempt to present the summary of various strategies for energy-efficient data collection methodology and energy-efficient path planning of mobile sink in wireless sensor networks.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Mobile and Network Security', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20141-7_4,en,Phi-Functions for Objects Bounded by the Second-Order Curves and Their Application to Packing Problems,OriginalPaper,For non-oriented convex 2D objects whose frontiers are defined by the second order curves an approach of constructing the non-intersection and containment conditions analytically is proposed. For pairs of ellipses or objects bounded by a parabola mutual non-intersection conditions are constructed. For these objects including the case of ellipses being circles the containment conditions are provided as well. The equations of the objects frontiers are used to describe the placement conditions as a system of equalities and inequalities. Solving the systems allows to construct corresponding phi -functions for their use in a various packing problems. A problem of packing circles in an ellipse of minimal size saving constant eccentricity is considered as an application.,"['Engineering', 'Computational Intelligence', 'Transportation Technology and Traffic Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/978-981-19-3951-8_26,en,Optimal Sizing of Stand-Alone Hybrid Energy System Using Black Widow Optimization Technique,OriginalPaper,"The renewable energy sources are clean energy sources, and their role and contribution are increasing day by day. In this paper, optimal sizing has been carried out for stand-alone hybrid energy system (HES) with solar PV, wind turbine (WT), diesel generator (DG), and energy storage. Black widow optimization (BWO) is one of the recent and powerful algorithms which is implemented for finding the optimal sizing of HES through energy management based on sample day load data using annualized model. The main goal of this study is to minimize the cost of energy such that all practical constraints are satisfied. A comparison of BWO has also been carried out with differential evolutionary (DE), particle swarm optimization (PSO), and a traditional solver (TS). It is concluded that the BWO performs better than the other optimization technique. Different configurations were tested for finding the best combination of units to be installed.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-20650-4_14,en,Medical Deepfake Detection using 3-Dimensional Neural Learning,OriginalPaper,"In recent years, Generative Adversarial Networks (GAN) have underlined the necessity for exercising caution in trusting digital information. Injection and removal of tumorous nodules from medical imaging modalities is one method of maneuvering deepfakes. The inability to acknowledge medical deepfakes can result in a substantial impact on healthcare procedures or even cause of death. With a systematic case study, this work seeks to address the detection of such assaults in lung CT (Computed Tomography) scans generated using CT-GANs. We experiment with machine learning methods and a novel 3-dimensional deep neural architecture on the topic of differentiating between tampered and untampered data. The proposed architecture on the CT-GAN dataset attained a remarkable accuracy of 91.57%, sensitivity of 91.42%, and specificity of 97.20%. Sectioned data cubes containing the affected region of interest seem to perform better compared to raw CT slices with a gain of approximately 20%. Furthermore, 3DCNN outperforms its 2-dimensional counterpart as it extracts temporal features unlike the spatial relationship insufficient for medical data processing. The outcomes of this research reveal that nodule injection and removal manipulations in complicated CT slices may be recognized with a high degree of precision.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-4017-0_6,en,Combinational Logic-Based Implementation of PUF,OriginalPaper,"In the earlier chapters, we have noted that a standalone arbiter PUF is vulnerable to machine learning (ML) attacks; however, multiple instances of arbiter PUF can be combined to create more robust PUF variants.","['Engineering', 'Circuits and Systems', 'Artificial Intelligence', 'Mathematics, general', 'Special Purpose and Application-Based Systems', 'Computer Science, general']"
doi:10.1007/978-981-19-2004-2_39,en,Hybrid Features-Based Ensembled Residual Convolutional Neural Network for Bird Acoustic Identification,OriginalPaper,"Bird audio identification is one of the challenging fine-grained tasks due to various complexity in the signal. In the current work, we present a new bird audio dataset from the Indian subcontinent and propose a novel hybrid features-based ensembled residual convolutional neural network to identify bird audios from the Indian subcontinent. We utilized mel-frequency cepstral coefficients (MFCC) and melspectrogram features to train the neural network. We compared the results of the proposed model with other machine learning and deep learning models. The results show that our proposed model achieved the best accuracy of 92% and best F1-score of 91% on using modified ResNet50 model. The dataset and the experiemental codes are available at GitHub ( https://github.com/Theivaprakasham/iccdn-2021-birdcall-id ).","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Materials Science, general']"
doi:10.1007/978-3-031-12127-2_7,en,Hierarchical Medical Classification Based on DLCF,OriginalPaper,"Medical classification is affected by many factors, and the traditional medical classification is usually restricted by factors such as too long text, numerous categories and so on. In order to solve these problems, this paper uses word vector and word vector to mine the text deeply, considering the problem of scattered key features of medical text, introducing long-term and short-term memory network to effectively retain the features of historical information in long text sequence, and using the structure of CNN to extract local features of text, through attention mechanism to obtain key features, considering the problems of many diseases, by using hierarchical classification. To stratify the disease. Combined with the above ideas, a deep DLCF model suitable for long text and multi-classification is designed. This model has obvious advantages in CMDD and other datasets. Compared with the baseline models, this model is superior to the baseline model in accuracy, recall and other indicators.","['Engineering', 'Computational Intelligence', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/978-3-031-18050-7_54,en,Feature-Aware Drop Layer (FADL): A Nonparametric Neural Network Layer for Feature Selection,OriginalPaper,"Neural networks have proven to be a good alternative in application fields such as healthcare, time-series forecasting and artificial vision, among others, for tasks like regression or classification. Their potential has been particularly remarkable in unstructured data, but recently developed architectures or their ensemble with other classical methods have produced competitive results in structured data. Feature selection has several beneficial properties: improve efficacy, performance, problem understanding and data recollection time. However, as new data sources become available and new features are generated using feature engineering techniques, more computational resources are required for feature selection methods. Feature selection takes an exorbitant amount of time in datasets with numerous features, making it impossible to use or achieving suboptimal selections that do not reflect the underlying behavior of the problem. We propose a nonparametric neural network layer which provides all the benefits of feature selection while requiring few changes to the architecture. Our method adds a novel layer at the beginning of the neural network, which removes the influence of features during training, adding inherent interpretability to the model without extra parameterization. In contrast to other feature selection methods, we propose an efficient and model-aware method to select the features with no need to train the model several times. We compared our method with a variety of popular feature selection strategies and datasets, showing remarkable results","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-18516-8_13,en,Auto-Diversified Ameliorated MultiPopulation-Based Ensemble Differential Evolution,OriginalPaper,"Stagnation and premature convergence are considered the most known problems of differential evolution (DE). To address this issue, we propose a multipopulation differential evolution with an automatic re-diversification mechanism that consists of combining our previously proposed approach (AMPEDE) with a detection/re-diversification process. In the proposed AD-AMPEDE, the objective function values’ mean and the coefficient of variance are used to detect whether the population has been stagnant or has converged to probably a local optimum. After confirming the existence of stagnation and premature convergence, a re-diversification mechanism, with the strategy proposed for AEPD [ 4 ], is applied on the population. A set of 24 Black-Box Optimization Benchmarking (BBOB2009) functions is used to evaluate the AD-AMPEDE efficiency. Results display that AD-AMPEDE is very competitive and provides an excellent performance in dealing with some categories of optimization problems.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-981-19-2689-1_77,en,Research on Bearing Fault Recognition Based on PSO-MCKD and 1D-CNN,OriginalPaper,"This paper proposes a bearing fault feature extraction and recognition method based on particle swarm optimization optimization Maximum Correlated Kurtosis Deconvolution (MCKD) and one-dimensional convolutional neural network to solve the non-stationarity of rolling bearing fault signals, Non-linear and complex characteristics, as well as the problems of noise interference and unclear fault characteristics in the process of fault identification. First, the multi-channel signals of the rolling bearing is analyzed, in order to select the signal containing the impact component as the fault feature. Next, the signal containing the fault feature is filtered through MCKD, where the best parameters of MCKD are obtained by improving the particle swarm algorithm to achieve the feature enhancement of the main signal. Finally, a one-dimensional convolutional neural network (One Dimensional Convolutional Neural Network, 1D-CNN) is used to model the characteristic signals under different damage conditions in order to obtain the fault recognition model of the rolling bearing.The experimental results show that the method can effectively extract the main characteristic signals of the faulty bearing, and realize the accurate identification of the bearing fault in the noisy environment.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-3-031-19945-5_25,en,Towards AI-powered Cybersecurity Attack Modeling with Simulation Tools: Review of Attack Simulators,OriginalPaper,"Cybersecurity currently focuses primarily on defenses that detect and prevent cyber-attacks. However, it is more important to regularly verify an organization’s security posture to reinforce its cybersecurity defenses as the IT environment becomes more complex and competitive. Confronted with an increasing use of artificial intelligence (AI) in cyber attacks, attack simulation platforms need to allow software vulnerabilities to be found against AI-powered attacks too. Such simulators will enable defenders to maintain a basic safety level and gain control over their security posture. Gradually, we are moving towards smart and autonomous platforms. This paper reviews established cyberattack simulation scientific research techniques with the goal of presenting a selection of tools and platforms that minimize the biases and inaccuracies inherent in traditional, isolated ad hoc research on A-powered cyberattacks.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Computer Applications']"
doi:10.1007/978-3-031-16281-7_12,en,Human Recognition for Resource-Constrained Mobile Robot Applied to Covid-19 Disinfection,OriginalPaper,"The global COVID-19 pandemic has stimulated the use of disinfection robots: in September 2021, following a European Commission’s action, 200 disinfection robots were delivered to European Hospitals. UV-C light is a common disinfection method, however, direct exposure to UV-C radiation is harmful and disinfection can be operated only in areas strictly forbidden to human personnel. We believe more advanced safety mechanisms are needed to increase the operational flexibility and safety level. We propose a safety mechanism based on vision and artificial intelligence, optimised for execution on mobile robot platforms. It analyses in real-time four video streaming and disables UV-C lamps when needed. Concerning other detection methods, it has a relatively wider and deeper range, and the capability to operate in a dynamic environment. We present the development of the method with a performance comparison of different implementation solutions, and an on-field evaluation through integration on a mobile disinfection robot.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-3-031-17697-5_25,en,PV - Battery - Diesel Standalone Hybrid System for Campus of International Burch University,OriginalPaper,"Today renewable energy is playing a key role in the production of power. These sources are used more and more because they are unlimited and environmentally friendly. Plants of such sources are much smaller and are placed close to consumers, which directly leads to reduced losses and the production of better power in the system. For the past ten years, photovoltaic systems have had more growth compared with the others. Solar energy is the energy that occurs from the sun and is stored in the battery. In this case study, a PV-battery hybrid system for International Burch University will be modeled by taking into account yearly loads from the year 2019. Thus, the study evaluates the techno-economic aspects of using this hybrid system at the university. The system consists of a photovoltaic module, a converter, and a battery. The main goal is to make a favorable solution, which is efficient and environmentally friendly, by integrating all these elements into the system. After all of the analysis was done in the HOMER software, which has different optimization algorithms, it is concluded that the best solution is the configuration made exactly of a PV-Diesel-Battery hybrid system. Its return on investment is 99% in 1 year, with an internal rate of return of 100%. Initial capital costs are 225,000 USD, while net present costs are about 3.67 million USD.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18461-1_15,en,On the Role of Depth Predictions for 3D Human Pose Estimation,OriginalPaper,"Following the successful application of deep convolutional neural networks to 2D human pose estimation, the next logical problem to solve is static 3D human pose estimation from monocular images. While previous solutions have shown some success, they do not fully utilize the depth information from the 2D inputs. With the goal of addressing this depth ambiguity, we build a system that takes 2D joint locations as input along with their estimated depth value and predicts their 3D positions in camera coordinates. Our system out performs comparable frame-by-frame 3D human pose estimation networks on the largest publicly available 3d motion data set, Human 3.6M. To provide further evidence for the usefulness of predicted depth values in the 3D pose estimation problem, we perform an extensive statistical analysis showing that even with potentially noisy depth predictions there is still a statistically significant correlation between the predicted depth value and the true depth value.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17544-2_1,en,Nature-Inspired Optimization Algorithms: Past to Present,OriginalPaper,"Nature-inspired algorithms are class of novel methods and processes for computing, analyzing, and solving various optimization problems. Nature-Inspired Optimization Algorithms (NIOAs) are bio inspired computational intelligence techniques gives an enormous drive for solving many complex problem as it exploits an exceptionally unique, strong, convincing and engaging behavior which is competent to give ideal outcomes. In the past few decades, several Nature-Inspired Optimization Algorithms has been proposed. However, very limited efforts have been made to provide a comprehensive investigation of NIOAs. In this chapter we present an overview of most significant NIOAs established from past to present days and their role in resolving complex computationally hard problems in various field of application. This overview endeavors to give a more extensive point of view and significant illumination to comprehend NIOAs. This also features the achievement, challenges and future research direction concerning recent NIOAs.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/978-3-031-18050-7_35,en,Convolutional Neural Networks for Structured Industrial Data,OriginalPaper,"Regression methods aim to predict a numerical value of a target variable given some input variables by building a function $$f:\mathbb {R}^n \rightarrow \mathbb {R}$$ f : R n → R . In Industry 4.0 regression tasks, tabular data-sets are especially frequent. Decision Trees, ensemble methods such as Gradient Boosting and Random Forest, or Support Vector Machines are widely used for regression tasks with tabular data. However, Deep Learning approaches are rarely used with this type of data, due to, among others, the lack of spatial correlation between features. Therefore, in this research, we propose two Deep Learning approaches for working with tabular data. Specifically, two Convolutional Neural Networks architectures are tested against different state of the art regression methods. We perform an hyper-parameter tuning of all the techniques and compare the model performance in different industrial tabular data-sets. Experimental results show that both Convolutional Neural Network approaches can outperform the commonly used methods for regression tasks.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-15030-2_11,en,Artificial Intelligence Enabled Radio Signal Intelligence,OriginalPaper,"Radio signal intelligence has very important roles in the world of wireless communications. Automatic modulation classification (AMC) as a crucial part of radio signal intelligence has been evolving with the development in machine learning (ML) and artificial intelligence. Being a pattern recognition problem, AMC provides a good entry point for developing ML based solutions for radio signal intelligence where ML algorithms have great potential competing against traditional methods. Yet, to release the potential of ML algorithms, some basic understanding of a wireless system and the common constraints imposed on a system is required. In this chapter, essential background information for developing ML based AMC solution is provided. AMC features and feature based ML classifiers are listed. Recent developments in deep learning based AMC solutions are also discussed.","['Computer Science', 'Artificial Intelligence', 'Privacy', 'Cryptology', 'Mobile and Network Security']"
doi:10.1007/978-3-031-17091-1_49,en,Adaptive Weighted Sum Bi-objective Bat for Regression Testing Optimization,OriginalPaper,"Regression testing is a type of testing carried out during the software maintenance phase, to confirm the validity of a software system after any modifications. However, regression testing is expensive, and sometimes it cannot be carried out within the testing budget, due to the large size of a test suite. In order to reduce regression testing cost, the test suite should be reduced without losing its efficiency in terms of pre-defined criteria such as its capability of fault detection; this problem is known as test suite reduction problem (TSR). In this paper, the TSR problem was formulated as a bi-objective optimization problem using an adaptive-weighted (AW) sum method. Then an adapted binary Bat algorithm (AW-ABBA) was utilized to search for a Pareto-optimal set of solutions; allowing the decision-maker under various circumstances to choose the best solution from the proposed set. The efficacy of the AW-ABBA was assessed using three metrics, Cardinality ratio, $${IGD}^{+}$$ IGD + and Diversity, over five test suites of different sizes. Experimental results showed that the AW-ABBA was able to efficiently approximate a reference Pareto-front.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-3895-5_15,en,"Process Parameter Optimization in Friction Stir Spot Welding of High-density Polyethylene Sheets Using GA, PSO, and ABC Algorithm",OriginalPaper,"The promising applications of high-density polyethylene (HDPE) in automotive and aerospace industries demand for more reliable and durable fabrication techniques. Friction stir spot welding is one such fabrication process which has proved to be a favorable joining technique for HDPE. In this paper, mathematical model has been developed using response surface methodology (RSM) for the friction stir spot welding (FSSW) process on commercial 4 mm HDPE sheets. The welding parameters, namely tool rotational speed, axial feed rate, plunging depth, and dwell time, were taken as decision variables for the resulting joint properties, namely the lap shear strength (LSS) and the temperature variations, are checked across the weld based on these parameters. The equations developed for LSS from experimental results with the help of RSM were used as objective function for the meta-heuristic optimization algorithms, namely genetic algorithm (GA), particle swarm optimization (PSO), and artificial bee colony (ABC) algorithm individually. The global optimum that has been deduced from GA, PSO, and ABC algorithm has come out to be consistent for LSS.","['Materials Science', 'Structural Materials', 'Nanotechnology', 'Materials Science, general']"
doi:10.1007/978-981-19-4975-3_32,en,Real-Time Power Quality Monitoring System in LabVIEW Using Wavelet Transform and Stockwell Transform,OriginalPaper,"Power quality (PQ) standards have become a concern, which in turn becomes intensified due to day-to-day increase usage of power electronic and electricity demand devices. Now, it has become a challenge for engineers to identify and analyze qualitatively and quantitatively these issues. Hence, it has become a significant concern for developing hardware, PQ monitoring systems for real time. Some of these problems have been cited in this paper. Continuous use of wavelet and Stockwell transforms and monitoring PQ are solutions to some of the main issues. Using LabVIEW, we can compare both the advantages and disadvantages of the above-said schemes. In LabVIEW, PQ monitoring is designed and implemented, which help to display result like harmonic components, values of root mean square (RMS), power quality factor (PQF), values of total harmonic distortion (THD), and 3D S-transform plots and plots of wavelet transform (WT) in the proposed real-time system.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-981-19-4975-3_5,en,The Review Paper on Different Topologies of LNA,OriginalPaper,"This paper describes the different topologies of low-noise amplifiers from the last twenty years. Furthermore, each topology has been described with its diagram. Apart from it, all the topologies have been compared in the form of a table and a conclusion arises that a low-noise amplifier without an inductor is more suitable than others, and also in place of resistance a metal oxide semiconductor field-effect transistor can be used. Apart from it, the table has been compared on different parameters such as Gain, Power Consumption, Power supply, Noise Figure, S11, and IIP3. Apart from it, all the technologies have been compared.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-3-031-10507-4_6,en,A Blockchain-Based Machine Learning Intrusion Detection System for Internet of Things,OriginalPaper,"The Internet of Things (IoT) is the major evolution of Internet also known as Internet of Everything which made a network with smart sensors heterogeneous devices. Nowadays, the usability of IoT networks is increasing very rapidly from smart home, smart industry to smart everything. But, these smart devices like as traditional Internet are vulnerable to various attacks such as denial of service (DoS), spoofing attacks, ransomware attacks, and many more. There are also various protocols such as DTLS, IPv6, and many other lightweight protocols used for IoT data security. But despite these, these attacks are also occurred via sniffing or manipulating of header information to both encrypted and non-encrypted protocols. Attacks generated via header information can be mitigated by various methods as ML-based intrusion detection systems (IDSs) is one of them. These IDSs security depends on the accuracy/integrity of training data (IoT data) and trust on the ML/DL algorithms. Recently, blockchain, a new advanced technology, is emerged, which has several use cases in the IoT domain for providing security. Due to the various advantages of blockchain and ML/DL methods in IoT data security, we combine these technologies and provide a secure blockchain-ML-based framework for heterogeneous IoT data security environment.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Systems and Data Security']"
doi:10.1007/978-981-19-4960-9_63,en,Multi-network for Joint Detection of Dynamic and Static Objects in a Road Scene Captured by an RGB Camera,OriginalPaper,"This study presents a unified network to localize dynamic, static traffic objects and pedestrians, classify traffic light colors, detect drivable area and lane line simultaneously. In the network architecture, traffic object branch is created to classify dynamic objects such as cars, trucks, buses, motorcycle, and bicycle. Static objects are categorized by traffic sign and traffic light objects. Pedestrians are also localized as a separate traffic object group. Traffic light is classified correctly when it is visible. The network design has a unified architecture, one shared encoder for feature extraction and three decoders for three tasks. For benchmarking purposes, the BDD100K dataset is used. The presented model is ranked in the second place for drivable area segmentation, lane line detection, and inference speed while benchmarking with publicly available multi-networks. In comparison with respect to state-of-the art segmentation models re-trained with BDD100K dataset, the task of dynamic object localization’s MIoU metric is reached to the level of 73.54%, which is 40% higher than the results of re-trained segmentation methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-4017-0_3,en,Machine-Learning Basics,OriginalPaper,"In Chap.  1 , we discussed how machine-learning algorithms differ from traditional algorithms. This chapter will introduce some machine-learning terminologies as well as some details about the working of ML algorithms.","['Engineering', 'Circuits and Systems', 'Artificial Intelligence', 'Mathematics, general', 'Special Purpose and Application-Based Systems', 'Computer Science, general']"
doi:10.1007/978-981-19-7648-3_15,en,Cooperative Data Transaction in Mobile Networks,OriginalPaper,"Mobile data traffic is experiencing unprecedented increases due to the proliferation of highly capable smartphones, laptops and tablets, and mobile data offloading can be used to move traffic from cellular networks to other wireless infrastructures such as small-cell base stations. This work addresses the related issue of data allocation, by proposing a novel infrastructure independent method based on the hotspot function of smartphones. In the proposed scheme, smartphones transfer data allowances among mobile users, so that users with excess data allowances act as accessible Wi-Fi hotspots, selling their data allowance to other users who need extra data allowances. To achieve this objective, we propose to use auctions with single and multiple data sellers. Efficient schemes based on auction models are discussed to sell the data allowances over successive days in a month, and over different time slots during a single day. Overall system performance is considered based on the behavior of mobile users, such as changing demands for the sale or purchase of data allowances. Together with the analytical results presented, our simulation experiments also indicate that knowledge of user behavior can significantly improve the performance of data allowance transactions, leading to highly efficient allocations among users.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Communications Engineering, Networks', 'Wireless and Mobile Communication']"
doi:10.1007/978-981-19-2225-1_11,en,Lightweight Model for Waifu Creation Using Deep Convolutional Generative Adversarial Network (DCGAN),OriginalPaper,"The inceptions of generative adversarial networks have made it possible for machines to mimic creativity, one of the most unique and sophisticated human characteristics. Due to the rapid advancements in the field of generative adversarial models, lots of approaches have been proposed in the past. One of the most efficient GANs is deep convolutional generative adversarial network (DCGAN), which uses convolutional layers in the generator model to generate more realistic fake images. In this paper, we propose a lightweight implementation of the DCGAN that can be productive for the animation industry. Our model can be used by animators and designers to innovate ideas about creative anime avatars that have never existed before. This novel approach not only saves a lot of time on creative thinking but also provides brand new character designs for anime and manga avatars production.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Statistics, general', 'Mobile and Network Security']"
doi:10.1007/978-3-031-16281-7_62,en,Comparison of Machine Learning Models for Predictive Maintenance Applications,OriginalPaper,"In the field of industry 4.0, one of the sectors in which research is particularly active is the area of Predictive Maintenance(PdM), the purpose of which is to improve the industrial production process. This type of maintenance aims to predict a possible failure event, reduce non-production times and increase the quality of the processing result. The objective of this paper is to select the best Machine Learning models for a PdM application. In particular, such a model should allow making a prediction based on a real dataset, obtained by monitoring a turning process, with the aim of making the classification of the chip shape. The criteria used to choose the best model are accuracy and prediction speed (to reduce the inference time). Indeed it is crucial to spot any potential machine fault in the shortest time possible, in order to intervene before the machine fails. Hence, our goal is to choose the ML models with a lesser inference time while still maintaining high accuracy.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-3-031-16868-0_17,en,Distribution Training Framework for Architecture Design,OriginalPaper,"As discussed in Part I, for the time-consuming issue of the ENAS methods, there are two primary categories of available acceleration methods. First, various acceleration approaches for DNN evaluation are proposed, including weights sharing [ 1 ], low fidelity [ 2 ], and prediction model [ 3 ]. However, the reduction of training in these methods introduces bias in the estimation of DNN performance. Second, many well-known distributed parallelism techniques have been developed to accelerate large-scale DNN training, the most common ones include model parallelism and data parallelism.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-8222-4_6,en,SE-1DCNN-LSTM: A Deep Learning Framework for EEG-Based Automatic Diagnosis of Major Depressive Disorder and Bipolar Disorder,OriginalPaper,"As two typical subtypes of depression, bipolar disorder (BD) is often misdiagnosed as major depressive disorder (MDD) in the early stage. Accurate diagnosis can provide effective treatment for patients. In this paper, we propose a deep learning framework namely SE-1DCNN-LSTM to automatically learn the latent EEG features of the two subtypes. Firstly, a SE block was used as a channel attention module to adaptively learn the weight of each electrode. Subsequently, a 1DCNN-LSTM network was applied to learn discriminative and effective patterns of EEG for MDD and BD. The noteworthy performance of proposed method was verified in 44 MDD and 26 BD patients with 81.10% and 83.16% classification accuracy in epoch-level and subject-level respectively. An extensive investigation of ablation analysis and window size of EEG epoch were conducted. Through visual analysis of electrode weights, we found that the weights of Fp1, Fp2, O1 and O2 electrodes were slightly greater. It demonstrated that the prefrontal lobe and occipital lobe may be possibly important brain regions for MDD and BD recognition. Overall, this study shows the effectiveness of the proposed model in EEG-based automatic diagnosis for MDD and BD.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-3-031-07258-1_105,en,Deep Autoencoders for Unsupervised Damage Detection with Application to the Z24 Benchmark Bridge,OriginalPaper,"Structural Health Monitoring (SHM) of ageing infrastructures represents one of the most important and challenging issues in modern society. In this framework, the increasing number of bridge collapses has progressively fueled the interest towards the development of reliable monitoring strategies, able to efficiently ensure real-time bridge assessment and early-stage damage detection. To this aim, recent advancements in sensor technologies and data science have strongly encouraged the use of Machine Learning (ML) algorithms. Within the unsupervised learning context, this paper proposes an innovative convolutional autoencoder-based damage detection technique applied to the Z24 benchmark bridge. Firstly, raw acceleration sequences of user-defined length are converted into images using the Gramian Angular Field Approach (GAF). Then, during the training, the model learns how to correctly reconstruct healthy data acquired by SHM systems. The trained network is afterwards tested with unknown data and the mean absolute error (MAE), considered as a damage-sensitive feature, is adopted to quantify the errors between the original input and reconstructed output. Finally, damage detection is performed when the percentage of damaged sequences within a pre-defined macro-sequence exceeds a properly fixed threshold. Results prove that the implemented ML algorithm, working at the level of single sensor, is effective to enable continuous assessment of large-scale monitored bridges as new data is collected, involving limited computational efforts.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering', 'Monitoring/Environmental Analysis', 'Analytical Chemistry']"
doi:10.1007/978-3-031-18516-8_14,en,Plant Recognition Using Data Augmentation and Convolutional Neural Network,OriginalPaper,"In this paper, the utility of data augmentation in plant recognition is presented. For this, the Swedish Leaf dataset is used. It covers 15 species with 75 images per species. After augmenting each of the images used for training by flipping, rotating, and random cropping, the total number of training images increased from 855 to 3420. The augmented dataset is then tested on a convolutional neural network developed from scratch. The test results showed 97.78% accuracy with the augmented dataset, against 88.52% on the original dataset.","['Engineering', 'Complexity', 'Computational Intelligence', 'Control and Systems Theory']"
doi:10.1007/978-981-19-2188-9_54,en,Predictive Modeling of Surface Roughness Using Machine and Deep Learning Frameworks from Experimental Data of Chemically Etched Polished Silicon Wafer with DDMAF,OriginalPaper,"The newly developed double disk magnetic abrasive finishing (DDMAF) process has been recently executed to improve the finishing efficacy of polished silicon wafers. It is obvious to have induction of errors in the experimental processes for predicting the output response in form of surface roughness of polished silicon wafers. This work has been presented to model the actual experimental values of surface roughness of polished mono-crystalline silicon wafer with the predicted data of surface roughness by utilizing machine and deep learning libraries in Python’s JupyterLab. The experimental data of 31 experiments for surface roughness of polished mono-crystalline silicon wafer with chemically accompanied unconventional machining process known as DDMAF has been used here. The machine learning algorithm chosen for the specific data distribution was a polynomial regression from scikit-learn API. The data was split into 85% training and 15% testing for the same. The data was further modeled in Keras sequential neural nets, with the experimental data spitted into 85% training and 15% validation.","['Engineering', 'Industrial and Production Engineering', 'Mechatronics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Energy Storage', 'Materials Engineering']"
doi:10.1007/978-981-19-3032-4_8,en,Introduction to Huawei Cloud Database GaussDB,OriginalPaper,"Database plays an important role in enterprises, and Huawei GaussDB database is one of the “main forces” in Kunpeng ecology.","['Computer Science', 'Database Management']"
doi:10.1007/978-3-031-16075-2_53,en,Brand Recommendations for Cold-Start Problems Using Brand Embeddings,OriginalPaper,"This paper presents our work to recommend brands to customers that might be relevant to their style but the brands are new to them. To promote the exploration and discovery of new brands, we leverage article-embeddings , also known as Fashion DNA, a learned encoding for each article of clothing at Zalando, that is utilized for product and outfit recommendations. The model used in Fashion DNA’s work proposed a Logistic Matrix Factorization approach using sales data to learn customer style preferences. In this work, we evolved that approach to circumvent the cold-start problem for recommending new brands that do not have enough sales or digital footprint. First, we computed an embedding per brand, named Brand DNA, from the Fashion DNA of all articles that belong to a given brand. Then, we trained a model using Logistic Matrix Factorization to predict sales for a set of frequent customers and brands. That allowed us to learn customer style representations that can be leveraged to predict the likelihood of purchasing from a new brand by using its Brand DNA. Customers are also able to further explore Zalando’s assortment moving from the more popular products and brands.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18326-3_9,en,Deep Anomaly Detection for Endoscopic Inspection of Cast Iron Parts,OriginalPaper,"Detecting anomalies in image data plays a key role in automated industrial quality control. For this purpose, machine learning methods have proven useful for image processing tasks. However, supervised machine learning methods are highly dependent on the data with which they have been trained. In industrial environments data of defective samples are rare. In addition, the available data are often biased towards specific types, shapes, sizes, and locations of defects. On the contrary, one-class classification (OCC) methods can solely be trained with normal data which are usually easy to obtain in large quantities. In this work we evaluate the applicability of advanced OCC methods for an industrial inspection task. Convolutional Autoencoders and Generative Adversarial Networks are applied and compared with Convolutional Neural Networks. As an industrial use case we investigate the endoscopic inspection of cast iron parts. For the use case a dataset was created. Results show that both GAN and autoencoder-based OCC methods are suitable for detecting defective images in our industrial use case and perform on par with supervised learning methods when few data are available.","['Engineering', 'Robotics and Automation', 'Industrial Chemistry/Chemical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-3-031-18050-7_36,en,Classification of Polymers Based on the Degree of Their Transparency in SWIR Spectrum,OriginalPaper,"Detection, classification and sorting of polymeric particles is a common task required in recycling industry. In the proposed work, an innovative method for detection of polymeric particles and their classification is introduced. The method is based on evaluation of images of polymeric particles, obtained from short-wavelength infrared (SWIR) camera, by convolutional neural network (CNN). Compared to conventionally used spectroscopes or hyper-spectral imaging, this method utilizes single wavelength (1 050 nm) and a degree of polymer transparency serves as the main descriptor. Five different polymers (ABS, ABS-T, Nylon, PETG, PLA) in form of regular blocks (size 15 $$\times $$ × 15 $$\times $$ × 0.3 mm) were used in the experiment. In total 203 images (size 288 $$\times $$ × 288 px) were prepared for CNN training and 67 for testing. Scalable ASP U-Net was tested in 6 combinations and their outputs were compared. According to used intersection over union metrics over all outputs, the topology with 64 filters and depth of 3 exhibited the best results.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-3035-5_22,en,Text Summarization Using Combination of Sequence-To-Sequence Model with Attention Approach,OriginalPaper,"In daily life, we come across tons and tons of information which can be related to news articles or any kind of social media posts or customer reviews related to product. It is difficult to read all the content due to time constraint. Being able to develop the software that can identify and automatically extract the important information. There are two types of summarization methods. Extractive text summarization is the method where it picks the important content from the source text and gives same in the form of short summary, and on the other hand, abstractive summarization is the technique where it gets the context of the source text, and based on that context, it regenerates small and crisps summary. In this paper, we use the concept of neural network with attention layer to deal with abstractive text summarization that generates short summary of a long piece of text using review dataset.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Statistics, general']"
doi:10.1007/978-3-031-11051-1_107,en,Icebreaker Fleet Management in Simulation Models of the Arctic Marine Transport Systems,OriginalPaper,"The report raises the issue of reflecting the processes of planning and dispatching the icebreaker fleet exploitation in the simulation models of the Arctic transport systems. Due to the theory of agent and discrete-event simulation, icebreakers can be considered as resources that help entities (cargo vessels) to move along to the process diagram. It means, that any such simulation model must include a certain intelligent algorithm for operational planning of icebreakers’ work, which would adequately reflect the logic of their dispatching in real icebreaking fleet management practice. Several alternative approaches for assigning support tasks to icebreakers are considered: ad hoc “greedy” algorithms, a heuristic algorithm for placing icebreakers on areas of responsibility, the use of built–in as well as third party optimization engines. So, we attempt to view the subject from different points of view and to present a range of specific solutions from different development teams. The analysis and comparison of these methods are carried out on test scenarios and configurations of the simulated transport system.","['Engineering', 'Control and Systems Theory', 'Control, Robotics, Mechatronics', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-11748-0_9,en,Rethinking Importance Weighting for Transfer Learning,OriginalPaper,"A key assumption in supervised learning is that training and test data follow the same probability distribution. However, this fundamental assumption is not always satisfied in practice, e.g., due to changing environments, sample selection bias, privacy concerns, or high labeling costs. Transfer learning (TL) relaxes this assumption and allows us to learn under distribution shift. Classical TL methods typically rely on importance weighting —a predictor is trained based on the training losses weighted according to the importance (i.e., the test-over-training density ratio). However, as real-world machine learning tasks are becoming increasingly complex, high-dimensional, and dynamical, novel approaches are explored to cope with such challenges recently. In this chapter, after introducing the foundation of TL based on importance weighting, we review recent advances on joint and dynamic importance-predictor estimation. Furthermore, we introduce a method of causal mechanism transfer that incorporates causal structure in TL. Finally, we discuss future perspectives of TL research.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning']"
doi:10.1007/978-3-031-16485-9_6,en,AI Recruitment System Using EEG to Explore the Truth of Interviewers,OriginalPaper,"COVID-19 pandemic is behind the implementation of the “AI recruitment system.” The number of companies trying to introduce AI recruitment systems is increasing because the non-face-to-face method is recommended due to the COVID-19 pandemic and the management change of the organization comes with the development of IT technology. Behind the positive evaluation that the development of AI technology improves the efficiency of work, the demand for fair and transparent recruitment procedures has been increasing as controversy over fairness and objectivity has increased due to various hiring irregularities. This study aimed to approach in a more systematic and scientific way to maximize the effect of recruiting talent. In the previous study, voice and video were identified based on ML. In situations where the problem of truth and falsehood is raised, this study conducted EEG-based biological experimental studies with a deep learning method to explore more objectively. Also, the experimental design applied biological experiments between brain activity patterns and brain regions as signals from EEG-based 14 channels to explore the truth/false authenticity of the experimenters. As a result of the experiment, the best performance and effect were shown in the CNN model with an accuracy of 91% truth and 89% false among the comparative analysis of Decision Tree, Random Forest, and CNN.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-21062-4_33,en,Blueberry Row Detection Based on UAV Images for Inferring the Allowed UGV Path in the Field,OriginalPaper,"Blueberries (Vaccinium corymbosum) have become one of the most popular fruits in Europe, being ranked as the top-berry fruit species. Due to the high cost and poor availability of the workforce, agricultural production of blueberries is becoming heavily reliant on robots. Within the EU-supported Flexigrobots project, unmanned ground vehicle (UGV) solutions are being developed for weeding, soil analysis, and spraying. However, to detect the problematic regions (regions of interest) and plan the UGV trajectories, fields are first scanned using unmanned areal vehicles (UAVs). In this paper, we propose a procedure for processing UAV images for inferring the allowed UGV trajectories in blueberry fields. First, the image dataset was acquired using a UAV equipped with a multispectral camera. Afterward, we utilized U-Net-based deep neural network (DNN) models for semantic segmentation of blueberry rows and enhanced the results with Radon transform and additional image post-processing steps. The quality of the obtained segmentation mask is analyzed through the ranking of trained models in terms of the intersection over union (IoU) and F1-score metrics. Lastly, based on the detected rows and the inter-row regions, we generated the trajectory along which the UGV is allowed to move to perform the weeding, soil analysis, or spraying tasks.","['Computer Science', 'Robotics', 'Robotics and Automation', 'Computational Intelligence']"
doi:10.1007/978-3-031-20029-8_26,en,Generating Personalized Phishing Emails for Social Engineering Training Based on Neural Language Models,OriginalPaper,"To prevent phishing attacks, social engineering training is a practical way by reinforcing the concepts of being aware of phishing emails. However, existing social engineering training relies on manual planning and artificially design, which suffers from scale and cost concerns. In this paper, we explore the idea of using natural language generation techniques for automatic social engineering training phishing mail generation. We present AI-Phishing, a novel phishing mail generation to facilitate personalized social engineering training planning. Users can utilize AI-Phishing to generate personalized phishing emails according to a given title and keywords.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence']"
doi:10.1007/978-981-19-1610-6_51,en,Adaptive Multi-attention for Image Sentence Generator Using C-LSTM,OriginalPaper,"Capturing image feature and multi-object region of an image and transferring it into a Natural Language Sentence is a research issue needs to be addressed with natural language processing. Technically, the attention mechanism will force every word representation to an corresponding image region, however at times it do neglect certain words like ‘the’ in the description text, as it misleads the text interpretation. The captioning of an image involves not only detecting the features from various images, but also decoding the collaborations between the items into significant image text. The focus of the suggested work, predicts the image sentence in a more detailed way for every region/frame of an image. To overcome, an image feature extraction is carried out using CNN and LSTM for the image text generation with the help of adaptive attention mechanism, which will be add in the layer of LSTM to predict better image sentence is constructed. The above mentioned deep network methods have been analyzed using two output combination. Experiments have been implemented using Flickr8k dataset. The implementation analysis illustrates that adaptive attention performs significantly better than without adaptive attention of image sentence model and generates more meaningful captions compared to any of the individual models used. From the results on test images, the suggested network gives the accuracy, bleu score with and without using adaptive attention in the LSTM of 81.53, 61.94 and 73.53, 57.94%.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5292-0_31,en,Reliable Incorporation of Robust Fractional-Order Nonlinear PID Controller into Integrated Power System Using Gray Wolf Optimization Method,OriginalPaper,"The first and foremost goal of this research article is to explore the efficient trajectory tracking performance of the integrated power system (IPS) using a fractional-order nonlinear proportional plus integral plus derivative (FONPID) control strategy for fulfilling the demand power. The various energy-generating components such as wind, solar, and diesel and storage components such as batteries, flywheels, and ultracapacitors are just a few of the components that make up the IPS. Due to the integration of such stochastic energy components, the grid frequency varies continuously affecting the fulfillment of power demand. To mitigate the variations in grid frequency, a FONPID controller is proposed which has a greater ability to withstand the changes that occur in the system parameters and the nonlinearity of the rate limitation. The gains of the proposed controller are modified using the gray wolf optimization (GWO) technique based on the sum of the integral of the square in frequency deviation (ISFD) for greater performance. The trajectory tracking for the demand power is showcased for the proposed controller. To validate the obtained simulation results, the suggested controller, i.e., FONPID, is compared with PID and FOPID controller whereas it has been observed that the FONPID controller outperforms the classical PID and the FOPID controller.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-20650-4_16,en,Utilization of Vision Transformer for Classification and Ranking of Video Distortions,OriginalPaper,"The perceptual quality of video surveillance footage has impacts on several tasks involved in the surveillance process, such as the detection of anomalous objects. The videos captured by a camera are prone to various distortions such as noise, smoke, haze, low or uneven illumination, blur, rain, and compression, which affect visual quality. Automatic identification of distortions is important when enhancing video quality. Video quality assessment involves two stages: (1) classification of distortions affecting the video frames and (2) ranking of these distortions. A novel video dataset was utilized for training, validating, and testing. Working with this dataset was challenging because it included nine categories of distortions and four levels of severity. The greatest challenge was the availability of multiple types of distortions in the same video. The work presented in this paper addresses the problem of multi-label distortion classification and ranking. A vision transformer was used for feature learning. The experiment showed that the proposed solution performed well in terms of F1 score of single distortion (77.9%) and F1 score of single and multiple distortions (69.9%). Moreover, the average accuracy of level classification was 62% with an average F1 score of 61%.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-662-65625-9_9,en,ERP and SCM Software,OriginalPaper,"In the world of systems at operational level, a system for planning & control is known as an information system because it contains information in a structured form concerning future, current, and past events associated with the provision of goods.","['Engineering', 'Engineering Economics, Organization, Logistics, Marketing', 'Operations Management', 'IT in Business', 'Industrial Organization', 'Organization']"
doi:10.1007/978-3-031-16072-1_32,en,Combining Rule-Based System and Machine Learning to Classify Semi-natural Language Data,OriginalPaper,"Command-line commands form a special kind of semi-natural language. Analyzing their structure and classifying them is a useful approach in the field of cyber security to detect anomalous commands used by malicious actors. Without any contextual knowledge, commands’ analysis is a difficult task as similar-looking commands might be performing different tasks, and commands with different aliases might be performing the same tasks. To understand command-line commands’ structure and their syntactic and semantic meanings, we created a rule-based system based on expert opinions. Using this system, we classified command-line commands into similar and not-similar classes. This rule-based system transformed command-line commands’ data into a binary classified form. We trained three machine learning models (a logistic regression document classifier, a deep learning document classifier, and a deep learning sentence-pair classifier) to learn the set of rules created in the rule-based system. We used Mathews Correlation Coefficient (MCC) score for the models’ performance comparison. The logistic regression model shows an MCC score of 0.85, whereas both the Deep Learning (DL) models scored above 0.98. DL document classifier and DL sentence-pair classifier achieved an accuracy of 0.943 and 0.983 respectively on unseen data. Our proposed hybrid approach solves the complex problem of classifying semi-natural language data. This approach can be used to create a domain-specific set of rules, and classify any semi-natural language data into multi-classes.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5217-3_119,en,Long Short-Term Memory Network and Ordinary Kriging Method for Prediction of PM2.5 Concentration,OriginalPaper,"When building a smart city, one must pay attention to the complex and diverse environmental problems in the city. Air quality prediction is an indispensable part of smart cities as it is conducive to guiding the development of air pollution prevention and control and alleviating urban air pollution. Based on the PM2.5 concentration data of 10 state-controlled air quality monitoring sites in Changsha City, Hunan Province, China from January 1, 2017 to December 30, 2021 and the meteorological data of Changsha city, a long short- term memory recurrent neural network model applicable to PM2.5 concentration prediction of Changsha city is formulated according to actual conditions with reference to related machine learning methods, and is used to predict the daily average concentration of PM2.5 at 10 air quality monitoring sites. Using the predicted PM2.5 concentration at the air quality monitoring sites in Changsha city, the spatial distribution simulation map of PM2.5 mass concentration within Changsha Third Ring Road is made by ordinary kriging method. Suggestions for PM2.5 pollution protection measures are put forward based on the predicted results. In addition, the interpolation analysis map of PM2.5 concentration distribution based on the measured value of each monitoring station is made. This is used to compare with the interpolation analysis map based on the predicted values so as to illustrate the rationality of the PM2.5 concentration prediction method.","['Engineering', 'Civil Engineering', 'Public Policy', 'Arts']"
doi:10.1007/978-3-031-18050-7_8,en,Deep Learning Approach for the Prediction of the Concentration of Chlorophyll ɑ in Seawater. A Case Study in El Mar Menor (Spain),OriginalPaper,"The goal of this research is to develop accurate and reliable forecasting models for chlorophyll ɑ concentrations in seawater at multiple depth levels in El Mar Menor (Spain). Chlorophyll ɑ can be used as a eutrophication indicator, which is especially essential in a rich yet vulnerable ecosystem like the study area. Bayesian regularized artificial neural networks and Long Short-term Memory Neural Networks (LSTMs) employing a rolling window approach were used as forecasting algorithms with a one-week prediction horizon. Two input strategies were tested: using data from the own time series or including exogenous variables among the inputs. In this second case, mutual information and the Minimum-Redundancy-Maximum-Relevance approach were utilized to select the most relevant variables. The models obtained reasonable results for the univariate input scheme with $$\overline{\sigma }$$ σ ¯ values over 0.75 in levels between 0.5 and 2 m. The inclusion of exogenous variables increased these values to above 0.85 for the same depth levels. The models and methodologies presented in this paper can constitute a very useful tool to help predict eutrophication episodes and act as decision-making tools that allow the governmental and environmental agencies to prevent the degradation of El Mar Menor .","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-6153-3_2,en,Evaluation of Sequential and Temporally Embedded Deep Learning Models for Health Outcome Prediction,OriginalPaper,"Deep learning sequential models are increasingly being used to predict patients’ health outcomes by analyzing their medical histories. In this paper, we investigate the design decisions and challenges of using deep learning sequential models for predictive health modeling. Our results show that the most successful deep learning health models to date, called transformers, lack a mechanism to analyze the temporal characteristics of health records. To address this gap, we propose and evaluate a new model called DTTHRE: Decoder Transformer for Temporally Embedded Health Records Encoding. DTTHRE analyzes patients’ medical histories, including the elapsed time between visits. We also evaluate the performance of DTTHRE on a real-world medical dataset for two health outcomes: (1) diagnostic and (2) readmission prediction. DTTHRE successfully predicted patients’ final diagnosis (78.54 ± 0.22%) and readmission risk (99.91 ± 0.02%) with improved performance compared to existing deep learning sequential models in the literature. DTTHRE predicts the health outcome for each medical visit, which increases the training examples available from limited medical datasets with no additional training time.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-981-19-1844-5_56,en,Kannada Handwritten Character Recognition Techniques: A Review,OriginalPaper,"Handwritten character recognition is a very interesting and challenging branch of pattern recognition. It has an impressive range of applications, from filling up banking applications to digitizing a text document. Handwritten character recognition is difficult because of the huge variety of writing styles, the similarity between different handwritten characters, the interconnections, and overlapping between characters. The main motive of this study/review is to compare and summarize the performance of different models and techniques used for the recognition of Kannada handwritten characters. The paper focuses on three main classifiers—Convolutional Neural Network (CNN), Support Vector Machine (SVM), and the K-nearest neighbour classifier (KNN). This review paper also highlights the various pre-processing and feature extraction techniques that were implemented by other authors and identifies their respective research gaps. The subsequent aim of this survey is to develop a new powerful and efficient model that provides improved accuracy, efficiency, and the least error rates which can be applied over a large character set of the Kannada language.","['Engineering', 'Communications Engineering, Networks', 'Mobile and Network Security', 'Artificial Intelligence', 'Big Data']"
doi:10.1007/978-3-031-15226-9_15,en,Design Optimization of a Four-Bar Leg Linkage for a Legged-Wheeled Balancing Robot,OriginalPaper,"Balancing legged-wheeled robots have gained popularity in recent years due to their locomotive efficiency while still being able to conquer rough terrain and obstacles. Furthermore, as this type of robot maintains ground contact with its wheels for most of the time, passive gravity compensation mechanisms can greatly minimize power consumption. Various designs with different leg configurations have emerged, whereby a 1-DOF mechanism per leg already showed sufficient compliance to adapt to most outdoor terrain. We propose a design optimization procedure for a 1-DOF four-bar leg linkage to ensure minimum pitch angle correction of the robot’s base while varying the leg extension. Gravity compensation is further achieved through an optimized torsional spring. Finally, we evaluate the performance of the leg linkage and gravity compensation mechanism on real hardware.","['Engineering', 'Control, Robotics, Mechatronics', 'Robotics', 'Computational Intelligence']"
doi:10.1007/978-3-031-18326-3_36,en,AI-Based Engineering and Production Drawing Information Extraction,OriginalPaper,"The production of small batches to single parts has been increasing for many years and it burdens manufacturers with higher cost pressure. A significant proportion of the costs and processing time arise from indirect efforts such as understanding the manufacturing features of engineering drawings and the process planning based on the features. For this reason, the goal is to automate these indirect efforts. The basis for the process planning is information defined in the design department. The state of the art for information transfer between design and work preparation is the use of digital models enriched with additional information (e.g. STEP AP242). Until today, however, the use of 2D manufacturing drawings is widespread. In addition, a lot of knowledge is stored in old, already manufactured components that are only documented in 2D drawings. This paper provides an AI(Artificial Intelligence)-based methodology for extracting information from the 2D engineering and manufacturing drawings. Hereby, it combines and compiles object detection and text recognition methods to interpret the document systematically. Recognition rates for 2D drawings up to 70% are realized.","['Engineering', 'Robotics and Automation', 'Industrial Chemistry/Chemical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-3-031-20029-8_27,en,Stock Price Trend Prediction Using LSTM and Sentiment Analysis on News Headlines,OriginalPaper,"To simulate the trading behavior of investors in the stock market, this study adopts parameters including technical, fundamental, and chip to build a LSTM model, and also observes the ability of news sentiment to predict stock prices. Influential stocks such as TSMC, Fulgent Sun, and HTC are chosen as the target of our experiment. Four common natural language processing packages are used to label news sentiment. Then the combined sentiment labels along with the LSTM model are used for backtesting. The results of the study found that FinBERT's ability to predict the price trend outperforms other methods, with an accuracy of 41.6%. In addition, combining news sentiment labels with the LSTM model generally leads to better outcome than using either the news label or the LSTM model alone. However, in certain extreme cases, traditional technical indicators or even buy-and-hold strategy have better performances.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence']"
doi:10.1007/978-981-19-4960-9_49,en,Semi-supervised Learning with In-domain Pre-training and Deep Co-training,OriginalPaper,"Scarcity of labelled data places a significant limitation on the power of any supervised machine learning model. This challenge is further amplified by the domain-specific expertise and hours of manual labour involved in labelling. In the case, when even the available data is not enough, a strategy of transfer learning using in-domain pre-training can be of help. In this paper, we formulate a dual strategy of semi-supervised deep neural co-training technique along with transfer learning based on in-domain pre-training to vastly improve model efficiency relative to its supervised counterpart for the task of review polarity prediction. We additionally investigate and implement a co-teaching strategy as a possible use case of countering the problem of noisy labels in addition to scarce labels. This is part of an on-going investigation that we also document in this paper.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-5845-8_45,en,Virtual Machine and Container Live Migration Algorithms for Energy Optimization of Data Centre in Cloud Environment: A Research Review,OriginalPaper,"The growing pace of cloud computing technology needs to optimize the energy consumption of the data centres. Virtualization is one such technology that helps in live migration of Virtual Machines and Containers and thus help reduction in energy consumed. An effort is made in this paper to explore different approaches for energy optimized live migration using Virtual Machine and Container. Majority of the work has been carried out by researchers considering CPU utilization as the parameter to optimize the energy consumption and some works have used other parameters like memory, disk space, application execution time for active server along with CPU utilization. The survey also depicts majority of the work are implemented using CloudSim and also there is more scope for developing solutions for optimal migration in Virtual Machine and Containers.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5845-8_61,en,A Hybrid Approach on Conditional GAN for Portfolio Analysis,OriginalPaper,"Over the decades, the Markowitz framework has been used extensively in portfolio analysis though it puts too much emphasis on the analysis of the market uncertainty rather than on the trend prediction. While generative adversarial network (GAN), conditional GAN (CGAN), and autoencoding CGAN (ACGAN) have been explored to generate financial time series and extract features that can help portfolio analysis. The limitation of the CGAN or ACGAN framework stands in putting too much emphasis on generating series and finding the internal trends of the series rather than predicting the future trends. In this paper, we introduce a hybrid approach on conditional GAN based on deep generative models that learns the internal trend of historical data while modeling market uncertainty and future trends. We evaluate the model on several real-world datasets from both the US and Europe markets, and show that the proposed HybridCGAN and HybridACGAN models lead to better portfolio allocation compared to the existing Markowitz, CGAN, and ACGAN approaches.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2126-1_1,en,Visually Guided UGV for Autonomous Mobile Manipulation in Dynamic and Unstructured GPS-Denied Environments,OriginalPaper,"A robotic solution for the unmanned ground vehicles (UGVs) to execute the highly complex task of object manipulation in an autonomous mode is presented. This paper primarily focuses on developing an autonomous robotic system capable of assembling elementary blocks to build the large 3D structures in GPS-denied environments. The key contributions of this system paper are (i) designing of a deep learning-based unified multi-task visual perception system for object detection, part detection, instance segmentation, and tracking, (ii) an electromagnetic gripper design for robust grasping, and (iii) system integration in which multiple system components are integrated to develop an optimized software stack. The entire mechatronic and algorithmic design of UGV for the above application is detailed in this work. The performance and efficacy of the overall system are reported through several rigorous experiments.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-981-19-3391-2_34,en,A Comparative Study of Hierarchical Risk Parity Portfolio and Eigen Portfolio on the NIFTY 50 Stocks,OriginalPaper,"Portfolio optimization has been an area of research that has attracted a lot of attention from researchers and financial analysts. Designing an optimum portfolio is a complex task since it not only involves accurate forecasting of future stock returns and risks but also needs to optimize them. This paper presents a systematic approach to portfolio optimization using two approaches, the hierarchical risk parity algorithm and the Eigen portfolio on seven sectors of the Indian stock market. The portfolios are built following the two approaches on historical stock prices from January 1, 2016, to December 31, 2020. The portfolio performances are evaluated on the test data from January 1, 2021, to November 1, 2021. The backtesting results of the portfolios indicate that the performance of the HRP portfolio is superior to that of its Eigen counterpart on both training and the test data for the majority of the sectors studied.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-2065-3_11,en,A Comparative Study of Time Series Models for Blood Glucose Prediction,OriginalPaper,"Diabetes is a serious metabolic disorder which itself is a cause of many diseases. The only way to cure diabetes is to control blood glucose levels at regular intervals. Machine learning models play a significant role in the prediction of blood glucose levels. Data collected by blood glucose monitoring systems at regular intervals forms a time series data which requires the sequential layered network to predict the level of blood glucose. Various time series models like ARMA, ARIMA, LSTM and Bi-LSTM have been critically examined in this study. This research work compares the performance of state-of-the-art time series models for the prediction of blood glucose values for T1D patients. The experiment is conducted on the UVA/Padova dataset which consists of fifteen days of data of 30 patients (10 adolescents, 10 adults and 10 children) at four different meal timings. Results show that the Bi-LSTM model outperforms the LSTM, ARMA and ARIMA models.","['Engineering', 'Communications Engineering, Networks', 'Artificial Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Machine Learning']"
doi:10.1007/978-981-19-4182-5_8,en,Gunshot Detection and Classification Using a Convolution-GRU Based Approach,OriginalPaper,"Rising demand of firearms all over the globe for various reasons has created a strong demand for uninterrupted surveillance systems to monitor different regions. Personal firearms availability inciting civil violence, poaching which has been a major contributor to the ever-increasing number of animal species facing threat of extinction are a few areas demanding immediate attention. Different infrastructure-based solutions like video surveillance and infrared thermography have been proposed to monitor and investigate any illicit activity, but these are not cost-effective, especially in large dense areas with compact viewing angles. We propose a model for acoustic-based approach to detect gunshot and further classify it based on its type/caliber. The proposed solution uses a deep learning convolutional-GRU based model to detect a gunshot in an audio stream and classify it based on type/caliber. Gunshot data used contains 830 instances of eight different types of guns which are overlapped on noisy background to create genuine instances. The audio, either real-time or pre-record, after computing Mel-Frequency Cepstral Coefficients (MFCCs), is converted into 2D images that are fed to the model. In case the model recognizes any gunshot, the timestamp of the gunshot and weapon caliber is returned, alerting the concerned authorities. Gunshots were detected correctly with an average classification accuracy of over 80%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-981-19-3032-4_2,en,Basic Knowledge of Database,OriginalPaper,"Various database products have different characteristics, but they share some common ground in the main database concepts, that is, they can achieve various database objects and different levels of security protection measures, and they also emphasize the performance management and daily operation and maintenance management of the database.","['Computer Science', 'Database Management']"
doi:10.1007/978-981-19-3387-5_141,en,Information Bottleneck Attribution Based Retinal Disease Classification Using OCT Images,OriginalPaper,"The existence of deep learning’s “black box” makes it difficult to understand how the algorithms analyze patterns and make image-level predictions. A representation of the pixels contributing the most to the algorithm’s classification will require new insights. Classification methods for neurodegenerative ocular disorders have been developed using machine learning and image processing techniques with considerable efficacy. However, the techniques’ robustness and transferability remain uncertain. We have developed a new classification method based upon information bottleneck to analyze the attribution of each feature and the information provided for the network prediction in each input area for a clear understanding of the affectability of a black-box model. In this research, we apply the attribution information bottleneck and limit the information flow and assess the amount of information image areas produce in bits by allowing noise to intermediate feature maps. Our studies indicate that the information bottleneck for attribution (IBA) has increased model interpretability and gives a more reliable estimation through a publicly available dataset.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-3-031-18256-3_1,en,Device for the Fall Detection in Older Adults Through Neural Networks,OriginalPaper,"For an elderly adult, a fall can be fatality by the fact that his bones are fragile and propense to fracture. The present device is intended to detect a change of posture prior to a fall by using a sensor composed of an accelerometer and a triaxial gyroscope. The signal processing used multiple classification neural networks with hyper-parameters of five hidden layers and 500 interactions, training and validation showed an accuracy of 83% respectively. In the results, the device detects fall postures efficiently and with a minimum margin of error.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-3-031-16281-7_15,en,Towards a Trade-off Between Accuracy and Computational Cost for Embedded Systems: A Tactile Sensing System for Object Classification,OriginalPaper,"The deployment of the inference phase in self–standing systems, which have resource–constrained embedded units, is faced with many challenges considering computational cost of the elaboration unit. Therefore, we propose using a learning strategy based on a loss function that leads to finding the best configuration of the prediction model balancing the generalization performance and the computational cost of the whole elaboration system. We validate our proposal by integrating a tactile sensing system on a Baxter robot to collect and classify data from five daily–life objects using four different algorithms. Results show that the best performance, when the computational cost is not relevant, is achieved by the fully–connected neural network using 16 features, while, when the computational cost matters, the loss function showed that the kernel SVM with 4 features has the best performance.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-4815-2_6,en,A-DDPG: Attention Mechanism-Based Deep Reinforcement Learning for NFV,OriginalPaper,"Chapters 3–5 transform the VNF placement and traffic routing proble into some well-known NP-hard problems, and then a heuristic or approximation method is proposed to solve it, at the expense of ignoring the network state dynamics. To bridge that gap, in this chapter, we formulate the VNF placement and traffic routing problem as a Markov Decision Process model to capture the dynamic network state transitions. In order to jointly minimize the delay and cost of NFV providers and maximize the revenue, we devise a customized Deep Reinforcement Learning (DRL) algorithm, called A-DDPG, for VNF placement and traffic routing in a real-time network. A-DDPG uses the attention mechanism to ascertain smooth network behavior within the general framework of network utility maximization (NUM). The simulation results show that A-DDPG outperforms the state-of-the-art in terms of network utility, delay, and cost.","['Engineering', 'Communications Engineering, Networks', 'Graph Theory', 'Operations Research, Management Science', 'Theory of Computation', 'Algorithm Analysis and Problem Complexity']"
doi:10.1007/978-3-031-07322-9_53,en,Delamination Identification Using Global Convolution Networks,OriginalPaper,"In this paper, we present a deep learning technique for image segmentation known as the global convolutional network, which we employ for delamination detection and localisation in composite materials. The model was trained and validated on our previously generated dataset that resembles full wavefield measurements acquired by a scanning laser Doppler vibrometer. Additionally, the model was verified on experimentally acquired data with a Teflon insert representing delamination, showing that the developed model can be used for delamination size estimation. The achieved accuracy in the current implemented model surpasses the accuracy of previous models with an improvement of up to $$22\%$$ 22 % for delamination identification.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-20650-4_2,en,A Novel Representation of Graphical Patterns for Graph Convolution Networks,OriginalPaper,"In the context of machine learning on graph data, graph deep learning has captured the attention of many researcher. Due to the promising results of deep learning models in the most diverse fields of application, great efforts have been made to replicate these successes when dealing with graph data. In this work, we propose a novel approach for processing graphs, with the intention of exploiting the already established capabilities of Convolutional Neural Networks (CNNs) in image processing. To this end we propose a new representation for graphs, called GrapHisto, in the form of unique tensors encapsulating the features of any given graph to then process the new data using the CNN paradigm.","['Computer Science', 'Artificial Intelligence', 'Computers and Education', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Computer Appl. in Social and Behavioral Sciences', 'Image Processing and Computer Vision']"
doi:10.1007/978-981-19-2600-6_9,en,Breast Cancer Prediction Using Auto-Encoders,OriginalPaper,"Breast cancer is one of the most common forms of cancer that is diagnosed in most women and some rare cases even men. In recent years, breast cancer survival rates have increased significantly, due to factors such as earlier detection. Treatment for breast cancer largely depends on identifying the type of mass of tissue formed, which is known as a tumor. If normal cells grow in an uncontrollable manner the tumor is called benign (non-cancerous). But, if the cells’ growth is out of control and their behavior is abnormal, then the tumor is called malignant (cancerous). During the invasive, (i.e. curable) stage of cancer, only 10–15% part of the breast contains cancerous cells. Therefore, it is difficult to diagnose it using mammography. However, the development of machine learning techniques has allowed early detection of breast cancer in clinical trials. This paper presents a comparative study of different machine learning techniques for detecting breast cancer. It also presents an auto-encoder model that performs breast cancer detection in an unsupervised manner. It attempts to identify a compact representation of features that are strongly related to breast cancer. The techniques are tested on the Breast Cancer Wisconsin (Diagnostic) Dataset, which is publicly available on Kaggle. The auto-encoder beats its competitors with a precision and recall of 98.4%.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-3998-3_28,en,Extended ADMM with Paillier Cryptosystem for Nonsmooth Decentralized Optimization,OriginalPaper,"This article studies a nonsmooth multiagent decentralized optimization problem where the agents aim at minimizing a sum of local strongly-convex smooth components plus a common nonsmooth term. We propose an algorithm based on ADMM framework which introduces a proximal term. We establish linear convergence of the proposed algorithm to the exact optimal solution in the presence of the nonsmooth term. Moreover, Paillier cryptosystem has been combined with our algorithm to ensure the privacy preserving in decentralized optimization in the absence of any third party or aggregator. We further provide a numerical example for a least squares problem equipped with $$L_1$$ L 1 regular term to show the linear convergence rate and how the step size influences the convergence.","['Engineering', 'Signal, Image and Speech Processing', 'Computational Intelligence', 'Multiagent Systems', 'Systems Theory, Control']"
doi:10.1007/978-981-19-4960-9_66,en,Multitask Learning for Joint Grading of Diabetic Retinopathy and Macular Edema Using Transformer,OriginalPaper,"Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) are the two major reasons of vision loss. These diseases occur due to high levels of sugar in the blood, which leads to damage to the retina. Recently, some computer-aided detection methods for DR and DME have been proposed to help ophthalmologists with early detection and better treatment for patients. However, earlier research either graded DR or DME neglected the relation between DR and DME. In this paper, two modules have been proposed. The individual disease grading module for learning specific features of each disease. Moreover, the multitask learning module for joint grading DR and DME, to inspect the intrinsic relation between DR and DME, generates disease reliant features and enhances the overall accuracy of DR and DME grading. The proposed method’s efficiency was evaluated using the Messidor-1 benchmark dataset. The evaluation revealed that the model’s accuracy exceeded that of the state-of-the-art model by >10%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-3-031-21062-4_16,en,A Novel Odor Source Localization Method via a Deep Neural Network-Based Odor Compass,OriginalPaper,"Mobile robot-based odor source localization (OSL) has broad applications in industrial and daily-life scenarios. However, subject to the limited sensing capacity of common metal oxide semiconductor (MOS) sensors, the OSL robots still lag far behind their biological counterparts. In this paper, we rethink the odor-source direction estimation paradigm of odor compass and propose a deep neural network (DNN) based method to improve both the accuracy and the generalization ability. The odor compass is composed of four wireless MOS sensors, and a DNN model, which contains a convolutional neural network (CNN) module and a long short-term memory (LSTM) module. An OSL strategy is further designed based on the proposed odor compass. Experimental results validate the feasibility of the proposed method.","['Computer Science', 'Robotics', 'Robotics and Automation', 'Computational Intelligence']"
doi:10.1007/978-981-19-3951-8_46,en,"A PSO-Optimized Fixed and a PSO-Optimized Neural Network-Adaptive Traffic Signal Controllers for Traffic Improvement in Santo Domingo, Dominican Republic",OriginalPaper,"Satisfying the mobility demand is one of the biggest concerns arising with the increase of urban population. With many people in the road network, traffic congestions are present in most of the cities in the world. The Distrito Nacional in Santo Domingo, capital city of Dominican Republic, is a notorious example of this phenomenon. Unfortunately, all the efforts to improve traffic experience there have had little success. With this work, two models have been developed using Particle Swarm Optimization (PSO): a PSO-Optimized Fixed Traffic Signal Control (PSO-FTSC) and a PSO-Optimized Neural Network-Adaptive Traffic Signal Control (PSO-NN-ATSC) that uses 4 neural networks to predict phase times. The intersection of 27 de Febrero Avenue corner with Winston Churchill Avenue was simulated using Simulation of Urban Mobility (SUMO), minimizing the time loss per vehicle during optimization. These models, PSO-FTSC and PSO-NN-ATSC, present reductions of 17% and 24% of mean time loss, respectively. These promising results may lead to a decrease of fuel consumption, reducing the consequent air pollution, as well as to an improvement of businesses and people’s productivity and mental health.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19958-5_50,en,Developing a Classification CNN Model to Classify Different Types of Fish,OriginalPaper,"Identifying any fish type can be difficult for people who are not familiar with fish. Implementation of a fish classification machine learning model can become helpful in this scope. The purpose of this paper is to build such a fish classification machine learning model. With this classification model, people will be able to identify the class or type of fish even without much experience with fish. Different types of fish have different nutrition, vitamin, and fat content. Thus, this model can be helpful to ensure better nutrition intake as well. As we have to classify types of fish, we implemented a Convolutional Neural Network (CNN) with Keras along with a modified VGG16 transfer learning model. With the CNN model, the accuracy of our training is 96.67%, and classification accuracy with the modified VGG16 is 97.44%. For validation, with the CNN model, accuracy is 99.92%, and classification accuracy with the VGG16 is 99.76%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3015-7_9,en,Improved LeNet Model for Flower Classification Using GPU Computing,OriginalPaper,"Convolutional neural networks can be used for recognizing and categorizing wide range of image datasets. In the paper, we have proposed an improved LeNet model for flower classification. This revised LeNet is an improved version of modified LeNet model in paper (Mitrovic and Milosevic, Flower classification with convolution neural network). This model can be used for recognizing and classifying five different types of flowers. The dataset (flower recognition) is taken from Kaggle. CNN algorithms require a lot of mathematical convolutional computations which take a lot of time and has high computational demand. GPUs can be used for accelerating image processing. Also, we can parallelize CPU cores to process images faster. Along with the implementation of the improved LeNet model, this paper also differentiates between the performance of parallelized CPUs and a GPU. In the paper, we will be using Ray library in Python for parallelizing the multicore CPUs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Data Mining and Knowledge Discovery', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-13249-0_35,en,Matter as Met: Towards a Computational Workflow for Architectural Design with Reused Concrete Components,OriginalPaper,"Over the past decades computational design, digital fabrication and optimisation have become widely adapted in architectural research, contemporary practice as well as in the construction industry. Nevertheless, current design and fabrication process-chains are still stuck in a linear notion of material use: building components are digitally designed, engineered, and ultimately materialised by consumption of raw materials. These can be defined as digital-real process chains. But these parametric design logics based on mass customisation inhibit the reuse of building components. In contrast to these predominant and established digital-real process chains, we propose a real-digital process chain: departing from our real, already materialised built environment. We digitise and catalogue physical concrete components within a component repository for future reuse. Subsequently, these components are reconditioned, enhanced if necessary and transitioned into a modular building system. The modularised components are then recombined to form a new building design and, eventually, a new building by combinatorial optimisation using mixed-integer linear programming (MILP). An accompanying life cycle assessment (LCA) complements the process and quantifies the environmental potential of reused building components. The paper presents research towards a feasible workflow for the reuse of structural concrete components. Furthermore, we suggest a digital repository, storing geometric as well as complementary data on the origin, history, and performances of the components to be reused. Here, we identify core data to be integrated in such a component repository.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Industrial Design', 'Interaction Design', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/978-3-031-11150-1_4,en,Optimization of Plasma-Assisted Surface Treatment for Adhesive Bonding via Artificial Intelligence,OriginalPaper,"In this research, Artificial Intelligence (AI) was used to support the optimization of six bonding process parameters for maximal joint strength and minimal production costs. Two industrial bonding processes were investigated, one from electronic potting and another from the manufacturing industry. The focus was on optimizing the plasma treatment of the substrate materials. Two approaches for optimization were compared, namely the traditional approach where the adhesive expert proposes experiments and interpret the results, and an AI approach with Bayesian optimization and Gaussian process models. Similar joint strengths could be achieved via the Bayesian optimization approach with 40% less budget to find the optimum compared to the traditional approach. Additionally, in the electronic potting process, the AI approach resulted in 18% reduction in production cost, while achieving a similar joint strength, compared to the traditional approach. Ageing of the samples did not result in a significant drop in joint strength nor changes in failure type or mechanism. This indicates that AI can support adhesive experts to find the optimal bonding process settings and manufacture robust and cost-efficient adhesive bonds.","['Engineering', 'Mechanical Engineering', 'Industrial Chemistry/Chemical Engineering', 'Solid Mechanics', 'Inorganic Chemistry', 'Organic Chemistry', 'Physical Chemistry']"
doi:10.1007/978-3-031-06340-4_4,en,Modeling the Mechanics of Single Polymer Chains in the Finite-Element Framework,OriginalPaper,"The Kirchhoff-Love beam and its finite element formulation presented in Sects. 2 and 3 are employed to model the mechanical behavior of polymers. In the remainder of this section, a model for a single polymer chain is presented including both contributions, intrinsic stiffness and temperature effects. A solution procedure within the finite-element framework is devised and used to fit the model to experimental data using a surrogate model based on a Gaussian process. Obtained material parameters are compared to the literature.","['Engineering', 'Mechanical Engineering', 'Classical and Continuum Physics', 'Materials Science, general']"
doi:10.1007/978-981-19-1540-6_15,en,Data Analysis and Predictive Control for a Pusher-Type Billets’ Reheating Furnace,OriginalPaper,"In this paper, the reheating process and the initial milling process, i.e. the roughing phase of steel billets, are analyzed. An Advanced Process Control has been developed and installed on pusher-type billets’ reheating furnace of an Italian steel industry. Data analysis is focused on the proof that the installed measurement tools are reliable and suitable for control purposes. A Model Predictive Control approach is exploited based on an adaptive model of the plant that takes into account the validated measurements. The main purpose of the work is to find a model capable of correlating the absorptions of the first six rolling stands with process variables and field measurements taking into account the different operating conditions of the furnace. The designed control system has been installed on the real plant providing a significant support for plant operators and optimizing the furnace conduction. The developed system received Industry 4.0 certification.","['Engineering', 'Mechanical Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/978-981-19-2397-5_45,en,Filipino Sign Language Recognition Using Long Short-Term Memory and Residual Network Architecture,OriginalPaper,"Filipino sign language (FSL) has improved communication among deaf people. However, the majority of the population in the Philippines does not understand FSL. This study explored computer vision in obtaining the data and deep learning in training the continuous SLR model. The model has been trained using ResNet and LSTM model using the features extracted using MediaPipe Holistic from video files of Filipino phrases performed by three (3) FSL signers. The SLR system developed can recognize (15) continuous Filipino phrases. Based on the comparison of both deep learning models, the best performing model is from the LSTM setup, wherein the accuracy achieved is 94%. In comparison, ResNet produced 87% accuracy on the test set. The feature importance analysis has ranked the facial component in this order (eyebrows, eyes, and mouth). The analysis shows that the exclusion of such features has negatively affected the performance of the SLR model compared to the model with full facial features. Based on the analysis of the experimentation results, the developed SLR system is robust, time-efficient, signer-independent, and can detect both manual and non-manual features of the gesture.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3035-5_25,en,Convolutional Neural Network-Enabling Speech Command Recognition,OriginalPaper,"The speech command recognition system based on deep image classification is the key that would tremendously promise to revolutionize research and development by overcoming the communication barrier between human and machine or computer. We are all aware of challenges in identifying the voice command in noise and variability in speed, pitch, and projection. This paper has developed an efficient and highly accurate speech command recognition for smart and effective speech processing applications like modern telecommunication. In particular, a novel convolutional neural network (CNN) is presented that works with a one-second audio clip consisting of one specific word including ten speech commands and other words labeled as “unknown,” and model implementations were operated in the noisy environment. The CNNs are structurally fully developed in such a way to recognize the speech commands with the utilization of deep learning (DL) for image classification concepts. Thus, this research used the concept of DL for image classification to translate the problem of speech command recognition into the image domain.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Cyber-physical systems, IoT', 'Professional Computing', 'Wireless and Mobile Communication', 'Statistics, general']"
doi:10.1007/978-3-031-18458-1_30,en,Target Speaker Detection with EEG/EOG Glasses: A Pilot Study,OriginalPaper,"Recent studies have examined the use of electroencephalography (EEG) and electrooculography (EOG) for target speaker detection in multi-speaker paradigms. The use of small, mobile and low-channel EEG and EOG-based devices is a recent trend due to the user-friendly nature of such solutions and their deployment outside the lab. The current study is focused on exploring if a low-channel EEG/EOG device can be used for target speaker detection. The performance of a low-channel EEG/EOG device was compared with a research-grade EEG headset for target speaker identification in a dichotic listening paradigm. 14 users took part in a study where a cocktail party problem was simulated and two audio streams were played simultaneously. Participants were asked to attend to one audio stream at a time. Audio exempts belonging to various genres of stories were simultaneously presented via the left and right audio streams as stimuli. Convolutional Neural Networks (CNNs) were used for classification of the target speaker in a dichotic listening paradigm. Low-channel EEG/EOG device identified the target speakers with an average accuracy of 74% and the high-channel EEG system achieved an average accuracy of 79%. The results provide encouraging results to continue exploring the feasibility of using wearable, low-channel EEG/EOG devices for auditory attention tasks.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6004-8_17,en,Forecasting Prediction of Covid-19 Outbreak Using Linear Regression,OriginalPaper,"Coronavirus these days becomes an alarming topic as the loss of lives is increasing every day. Coronavirus was born in China (Wuhan) and has spread globally within no time. Machine learning is used to forecast the future outcomes almost in all areas like computers, medical, business, security, and many more. In this study, various machine learning techniques such as Linear Regression, SVM, Random Forest, Logistic Regression, and KNN are used to forecast the future tendency of SARS-CoV-2. The polynomial feature with degree 3 is used to enhance the accuracy of the model. Dataset is collected from Johns Hopkins University dashboard. It contains 3 separate files of Covid-19 such as confirmed case, recovered case, and death case dataset. This article has four phases, i.e., data preprocessing, data classification, training, testing, and parametric evaluation such as RMSE, MSE, MAE, and R 2 -score. In this study, Linear Regression (LR) shows excellent outcome in contrast to other approaches. LR has a minimum mean squared error value and an obtained accuracy of 99.92% for Covid-19 confirmed cases, 99.73% for recovered cases, and 99.60% for death cases. Results show that if humans become unsocial and the administration imposes strict lockdown, there is a great chance to overcome this disease within a small period.","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-1610-6_46,en,Improving Arabic Hate Speech Identification Using Online Machine Learning and Deep Learning Models,OriginalPaper,"Due to the rising use of social media platforms on a global scale to interact and express thoughts freely, the spread of hate speech has become very noticeable on these platforms. Governments, organizations, and academic institutions have all spent substantially on discovering effective solutions to handle this issue. Numerous researches have been performed in several languages to find automated methods for identifying hate speech, but there has been minimal work done in Arabic. The findings of a performance evaluation of two machine learning models, namely the passive-aggressive classifier (PAC) and the Bidirectional Gated Recurrent Unit (Bi-GRU) augmented with an attention layer, are investigated in this work. Proposed models are developed and evaluated using a multi-platform Arabic hate speech dataset. We employ term frequency-inverse document frequency (TF-IDF) and Arabic word embeddings for feature extraction techniques after running a variety of pre-processing steps. The experimental results reveal that the two proposed models (PAC, Bi-GRU with attention layer) provide an accuracy of 98.4% and 99.1%, respectively, outperforming existing methods reported in the literature.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-8202-6_24,en,Application Research on Water Body Extraction of Gaofen-3 Polarimetric SAR Based on Deep Learning,OriginalPaper,"Quickly and accurately obtaining the area, volume and distribution of surface water is of great significance to the resource survey, urban planning, economic development, flood disaster monitoring, etc. In this paper, the domestic Gaofen-3 (GF-3) dual-polarization radar satellite data is used, combined with the semantic segmentation algorithm of the deep learning attention mechanism, to construct a polarized radar surface water information extraction model based on TransUnet, which has been carried out in the Poyang Lake area. The results show that the model can not only give full play to the advantages of radar water body information extraction, but also better ensure the accuracy of water body edges and reduces the interference of shadows and radar image speckle noise caused by terrain, overlay, foreshortening, etc. The experimental result achieves an overall classification accuracy of 95.8% and a Kappa coefficient value of 0.942, which are both better than traditional threshold segmentation algorithms and support vector machine classification algorithms.","['Engineering', 'Signal, Image and Speech Processing', 'Computer Applications', 'Geography, general', 'Earth System Sciences']"
doi:10.1007/978-981-19-1484-3_11,en,Development of Safety Monitoring for an IOT-Enabled Smart Environment,OriginalPaper,"With the distended utilization of reconnaissance mission cameras, a library is proffered during this paper that is employed to try to image preparing victimization-programming tongues like python. This task uses Open CV Library to show the gap between the article and camera for the Eudaimonia checking of the IOT climate. We have got adscititious 2 modules to the reconnaissance mission camera. First, to understand the articles that area unit falling or unit victimization object space detector once the factor recognized then the client will get the caution message for that we have a tendency to area unit victimization GSM module. Second, there are a unit various prospects to urge upgrade of observation camera thanks to deluge or water; for the popularity of water, we have a tendency to area unit utilizing precipitation board module and management module that includes LM393 comparator on the off likelihood that the appliance knows the precipitation; at that time, it sends the message to the shopper. Results gain through screen catches of the examinations show the suitableness of the planned plot.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-13249-0_10,en,Augmented Intelligence for Architectural Design with Conditional Autoencoders: Semiramis Case Study,OriginalPaper,"We present a design approach that uses machine learning to enhance architect’s design experience. Nowadays, architects and engineers use software for parametric design to generate, simulate, and evaluate multiple design instances. In this paper, we propose a conditional autoencoder that reverses the parametric modelling process and instead allows architects to define the desired properties in their designs and obtain multiple predictions of designs that fulfil them. The results found by the encoder can oftentimes go beyond what the user expected and thus augment human’s understanding of the design task and stimulate design exploration. Our tool also allows the architect to under-define the desired properties to give additional flexibility to finding interesting solutions. We specifically illustrate this tool for architectural design of a multi-storey structure that has been built in 2022 in Zug, Switzerland.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Industrial Design', 'Interaction Design', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/978-981-19-6297-4_7,en,"Comparative Analysis of TANK and SimHyd Rainfall-Runoff Models in the Hemavathi Watershed, Cauvery Basin, India",OriginalPaper,"Water is one of the essential resources of a country, which controls human development activities and very much influences living things. Hydrologists had been interested in the discharge and runoff caused by precipitation. This study compares two daily conceptual models, TANK and SimHyd, using the Rainfall-Runoff Library (RRL) tool and suggests which model is suitable for the Hemavathi watershed. The daily rainfall and evapotranspiration were the inputs in the TANK and SimHyd to determine daily discharge. The daily streamflow was utilized to calibrate models from 1990 to 2006 and then validated for 2007 to 2015. These model parameters are optimized using the genetic algorithm technique. Model prediction effectiveness was assessed using Nash–Sutcliffe efficiency (NSE) and Correlation coefficient (C C ) values. NSE and C C values during calibration was 0.71, 0.85 for TANK model, 0.66, 0.83 for SIMHYD model, and during verification was 0.64, 0.74 for TANK model, 0.62, 0.72 for SIMHYD model. From the obtained results, the TANK model performs satisfactorily in terms of NSE and C C compared to the SimHyd model. This paper evaluates the model abilities in streamflow simulation, which policy makers can use.","['Engineering', 'Fire Science, Hazard Control, Building Safety', 'Natural Hazards', 'Sustainable Development']"
doi:10.1007/978-981-19-2635-8_30,en,Augmented Lagrange Based Particle Swarm Optimization for Missile Interception Guidance,OriginalPaper,"By raising a practical problem in missile interception guidance with detailed system dynamic equations, along with several constraints on initial states and terminal states, the optimal control problem will be converted to a parameter optimization problem by using implicit integration Hermite-Simpson method, which approximates states and control using piecewise continuous polynomials to simplify the involved integration calculations, will be used to discretize the time domain with steps of duration. Furthermore, augmented Lagrangian method (ALM) will be introduced to convert constrained problem (i.e. primal problem) into unconstrained one (i.e. barrier problem) and penalty term will be added to cost function. Meanwhile, particle swarm optimization (PSO) will be applied to optimize the parameter vector, including system states, control inputs and Lagrangian multipliers, evaluating the cost function at each iteration during the simulation until reaching the minimum value of cost function, which also suggests that the particles are converged, and the parameter vector is optimized. The main contribution of the study is to propose a hybrid optimal solver for the constrained optimization problem, an augmented Lagrangian particle swarm optimization (ALPSO), and comparison results between the self-build ALPSO solver, MATLAB built-in fmincon function, and the theoretical ones, verifying the correctness of the self-build solver for constrained optimization problems with accuracy of 15.49 m maximum error during the whole one thousand meters of flight.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-3-031-19958-5_43,en,Deep Learning Approach to 2D Capacitive Resistivity Imaging Inversion,OriginalPaper,"This study developed deep learning models to perform the inversion of capacitive resistivity imaging (CRI) data to acquire 2D subsurface resistivity maps. Inversion of resistivity data using deep learning models had been applied in previous works where it addresses the time-consuming process and inaccurate generation of maps obtained from the conventional iterative process of inversion. The present study contributes to state of the art by training and evaluating the deep learning inversion model dedicated for CRI, which previous works were limited for use with DC ERT surveys. A synthetic dataset of several subsurface resistivity models and expected CRI equipment recordings were generated. Then four deep learning architectures, namely U-Net, SegNet, Wavelet U-Net, and Wavelet SegNet, were trained and evaluated using the synthetic dataset. Results revealed that the Wavelet U-Net performed best among the considered architectures as it had the lowest mean square error, second highest R 2 , and a more accurate resistivity map generated from a sample in the dataset. All deep learning inversion models produced one sample resistivity map, with a survey length of 100 m, within 1 s compared with the Res2dinv software (iterative method), which took 40 s. Overall, the study concludes that the trained deep learning 2D inversion models, especially Wavelet U-Net, can invert CRI data to a high degree of accuracy at a fast computational speed, which becomes essential when requiring real-time and on-site results and dealing with long surveys.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2600-6_23,en,Estimation of Data Parameters Using Cluster Optimization,OriginalPaper,"Machine learning is the kind of process to turns out to be an undividable fraction of any methodical work is mostly because of the simplicity of use and the simplicity of generation of data. The data generation task is pricey or tedious, the data is usually generated in parallel amid various research groups and they are communal by scientists. For this activity is the initial task, is frequently to cluster the data into several categories to set up which data from what source are related to each other. In this paper, optimization is usually is implemented in use for such a clustering activity is proposed. When the data is accomplished then they are entire jumbled jointly. One way to prepare an optimization difficulty is to primary decide on a number of clusters that the data may be separated to. After that, for each cluster in not many parameters are used variables for the optimization task. The parameters have to explain a similarity role for a cluster. The activity can be achieved through an optimization solve the prediction activity can be achieved using an optimization process. This process is used a Semi-Supervised learning approach and Data Mining like the K-Nearest Neighboring process and form the path route cluster. The prediction parameter of SRGM is approximated based upon these data clusters using Least Square Estimation.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-20601-6_30,en,An Enhanced Object Detection Model for Scene Graph Generation,OriginalPaper,"With computer vision improving, a higher level of understanding is needed to solve more complex problems such as semantic image retrieval, image captioning, and scene understanding. Scene understanding has been a long-studied problem due to its complexity and lack of proper data representation. A scene Graph is one of the most powerful data representations that can better understand the scene context. The task of a Scene Graph is to encode the objects presented in the scene, their attributes, as long as the relationships between these objects. With the scene Graph proving its capabilities in complicated tasks, the automation of scene graph generation became a must. Great research has been made to obtain accurate Scene Graphs using different deep learning architectures. The common module among those different architectures is the object detection module, where objects are firstly located in the input image. In this work, we propose using the most recent object detectors from the YOLOv5 family for the scene graph generation task. The proposed YOLOv5x6 achieved a State-Of-The-Art result of 32.7 mean average precision compared to previous works. Furthermore, the paper reviews the different object detectors used in literature for the scene graph generation.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4703-2_3,en,SepMLP: An All-MLP Architecture for Music Source Separation,OriginalPaper,"Most previous deep learning based methods use convolutional neural networks (CNNs) or Recurrent neural networks (RNNs) to model the separation process in music signals. In this paper, we propose a MLP-like encoder-decoder architecture, in which per-location features and spatial information in music signals are exclusively handled by multi-layer perceptrons (MLPs). Additionally, We introduce a novel fully-connected decoder for feature aggregation without using skip-connection. The experimental results on the benchmark dataset MUSDB18 show the proposed model achieves comparable performance to previous CNN and RNN based methods, which demonstrates that MLP serves as a promising backbone for the data-driven methods in music source separation.","['Engineering', 'Signal, Image and Speech Processing', 'Engineering Acoustics', 'Mathematics in Music', 'Music']"
doi:10.1007/978-981-19-2600-6_3,en,"End to End Agile and Automated Machine Learning Framework for Trustworthy, Reliable and Sustainable Artificial Intelligence",OriginalPaper,"Artificial Intelligence is playing pivotal role in automation of processes that were considered hard problems previously, but trustworthiness of these systems is still under question as many of these systems fail to meet expectations. Trustworthiness of artificial intelligence based systems depend on many factors. This paper analyzes human trust lifecycle and proposes an end to end agile and automated machine learning framework for automation of development, deployment, monitoring, and enhancements of AI/ML processes. Further this paper presents results of initial deployments of proposed framework and compares them with benchmark results.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-8140-1_2,en,Basis of Latent Feature Learning,OriginalPaper,"The Internet is speeding up the development of information ear. Online service applications have become very common in our daily life and thousands of services are provided online. Such numerous online services lead to the problem of information overload [1, 2]. Then, an intelligent and efficient system is desired to address such problem [3, 4]. Therefore, as one of the most efficient and effective approaches for addressing information load, the recommender system has attracted much attention.","['Computer Science', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Statistics, general', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-981-19-1844-5_1,en,Implementation of Machine and Deep Learning Algorithms for Intrusion Detection System,OriginalPaper,"The intrusion detection system (IDS) is an important aspect of network security. This research article presents an analysis of machine and deep learning algorithms for intrusion detection systems. The study utilizes the CICIDS2017 dataset that consists of 79 features. Multilayer perceptrons (MLPs) and random forests (RFs) algorithms are implemented. Four features extraction techniques (information gain, extra tree, random forest, and correlation) are considered for experimentation. Two models have been presented, the first one using the machine learning random forest (RF) algorithm and the second using deep learning multilayer perceptron (MLP) algorithm. The increased accuracy has been observed when using the random forest algorithm. The RF algorithm gives the best results for the four feature selection techniques, thus proving that RF is better than MLP. The RF algorithm gives 99.90% accuracy, and 0.068% false positive rate (FPR) with 36 features. Furthermore, the dimensionality of the features has been reduced from 79 to 18 features with an accuracy of 99.70% and FRP of 0.19%.","['Engineering', 'Communications Engineering, Networks', 'Mobile and Network Security', 'Artificial Intelligence', 'Big Data']"
doi:10.1007/978-981-19-3387-5_45,en,A Query Optimization Method for Real-Time Embedded Database Based on Minimum Arborescence,OriginalPaper,"In this paper, a query optimization method based on minimum arborescence for real-time embedded database is proposed to solve the problems of memory limitation and real-time access performance limitation of spaceborne computer system. Based on the characteristics of SQL statements, this method applies the minimum arborescence algorithm to the real-time embedded query processing by relying on the relationship between the attributes of each table during the construction of SQL query. This method can improve the processing speed of multi-table complex query statements, reduce the system memory usage, and optimize query processing in embedded database. The verification on SQLite database shows that this method is an effective query optimization method for embedded database.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Aerospace Technology and Astronautics']"
doi:10.1007/978-981-16-8274-2_17,en,Automatic Generation Control of Multi-Area Power System,OriginalPaper,"In a power system, the load differs continuously. As a result, frequency also differs continually. Automatic generation control (AGC) has a crucial role in the entire power system network, which is used to control the changes in scheduled tie-line powers by maintaining rated frequency within tolerable limits. Basic load frequency control (LFC) with only primary controllers like governors may or may not satisfy the power system constraints due to steady-state errors and sluggish response. Unsatisfactory secondary controllers also may result in power blackouts. To get good transient and steady-state responses, different secondary controllers like proportional, integral, and derivative (PID), active disturbance rejection controller (ADRC), and PI-PD controllers are used for the real three area power system (IEEE paper) with reheat and non-reheat thermal plants. Here the method of particle swarm optimization is used for tuning the PID and PI-PD controllers. The resulting power systems are simulated and analyzed using MATLAB/Simulink software to present a better secondary controller.","['Energy', 'Energy Policy, Economics and Management', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Engineering Fluid Dynamics', 'Environmental Policy', 'Sociology, general']"
doi:10.1007/978-981-19-6737-5_24,en,Generative Adversarial Network-Based Improved Progressive Approach for Image Super-Resolution: ImProSRGAN,OriginalPaper,"Recently, convolutional neural networks (CNNs) have been explored to achieve exceptional performance on super-resolution (SR) in terms of distortion metrics. In these methods, pixel-based loss functions have been employed to optimize their networks leading to overly smooth blurry solutions. In contrast, a generative adversarial network (GAN) has the proficiency to bring out perceptually better SR solutions. In the case of larger upscaling factors, some degradations are still discovered in the SR observations that can be reduced by increasing the number of convolution layers. However, such approach tends to increase the number of trainable parameters and additionally provides a lot of burden on resources which leads it to be unavailable for many real-world tasks. Here, we propose an improved progressive approach for SISR using GAN (i.e. ImProSRGAN). The potency of the proposed model has been seen by conducting different experiments where we observe that the introduced ImProSRGAN model performs better than existing GAN-based SISR approaches even though picking up fewer training parameters.","['Computer Science', 'Computer Communication Networks', 'Computer Applications', 'Computer System Implementation', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/978-3-031-05445-7_15,en,A Physics-Based Reduced Order Model with Machine Learning-Boosted Hyper-Reduction,OriginalPaper,"Physics-Based Reduced Order Models (ROMs) tend to rely on projection-based reduction. This family of approaches utilizes a series of responses of the full-order model to assemble a suitable basis, subsequently employed to formulate a set of equivalent, low-order equations through projection. However, in a nonlinear setting, physics-based ROMs require an additional approximation to circumvent the bottleneck of projecting and evaluating the nonlinear contributions on the reduced space. This scheme is termed hyper-reduction and enables substantial computational time reduction. The aforementioned hyper-reduction scheme implies a trade-off, relying on a necessary sacrifice on the accuracy of the nonlinear terms’ mapping to achieve rapid or even real-time evaluations of the ROM framework. Since time is essential, especially for digital twins representations in structural health monitoring applications, the hyper-reduction approximation serves as both a blessing and a curse. Our work scrutinizes the possibility of exploiting machine learning (ML) tools in place of hyper-reduction to derive more accurate surrogates of the nonlinear mapping. By retaining the POD-based reduction and introducing the machine learning-boosted surrogate(s) directly on the reduced coordinates, we aim to substitute the projection and update process of the nonlinear terms when integrating forward in time on the low-order dimension. Our approach explores a proof-of-concept case study based on a Nonlinear Auto-regressive neural network with eXogenous Inputs (NARX-NN), trying to potentially derive a superior physics-based ROM in terms of efficiency, suitable for (near) real-time evaluations. The proposed ML-boosted ROM (N3-pROM) is validated in a multi-degree of freedom shear frame under ground motion excitation featuring hysteretic nonlinearities.","['Engineering', 'Building Repair and Maintenance', 'Vibration, Dynamical Systems, Control', 'Fourier Analysis', 'Abstract Harmonic Analysis']"
doi:10.1007/978-3-031-18326-3_18,en,Intelligent Robotic Arm Path Planning (IRAP2) Framework to Improve Work Safety in Human-Robot Collaboration (HRC) Workspace Using Deep Deterministic Policy Gradient (DDPG) Algorithm,OriginalPaper,"Industrial robots are widely used in manufacturing systems. The places that humans share with robots are called human-robot collaboration (HRC) workspaces. To ensure the safety in HRC workspaces, a collision-avoidance system is required. In this paper, we regard the collision-avoidance as a problem during the robot action trajectory design and propose an intelligent robotic arm path planning (IRAP 2 ) framework. The IRAP 2 framework is based on the deep deterministic policy gradient (DDPG) algorithm because the path planning is a typical continuous control problem in a dynamic environment, and DDPG is well suited for such problems. To test the IRAP 2 framework, we have studied a HRC workspace in which the robot size is larger than humans. At first, we have applied a physics engine to build a virtual HRC workspace including digital models of a robot and a human. Using this virtual HRC workspace as the environment model, we further trained an agent model using the DDPG algorithm. The trained model can optimize the motion path of the robot to avoid collision with the human.","['Engineering', 'Robotics and Automation', 'Industrial Chemistry/Chemical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-3-031-02097-1_15,en,Intelligent Recognition of Waterline Value Based on Neural Network,OriginalPaper,"The accuracy of waterline recognition will affect the safety and shipping efficiency of the vessel. Due to weather conditions, observation experience, obstacles and other subjective and objective factors, the results of waterline recognition are inaccurate, which will lead to a series of problems. For intelligent recognition of waterline, a concise convolutional neural network with batch normalization transformation is proposed. The method uses drone to obtain image data of ship waterline. According to image features, gray level conversion, image enhancement and shape processing are carried out to reduce the influence of irrelevant background information. The network automatically extracts high-dimensional features with less convolution layer overlays. Parts of the network introduce batch normalization, and the training samples are normalized in small batch and then output through fully connection layer. An evaluation mechanism is introduced in the network to automatically recognize the ship waterline value by summarizing all the detected waterline data in the video. The experimental results show that the method can quickly identify the waterline image. Our proposed method is not affected by subjective factors. It has strong environmental adaptability and high recognition accuracy. The results are more objective.","['Engineering', 'Mechanical Engineering', 'Computational Science and Engineering', 'Theoretical, Mathematical and Computational Physics']"
doi:10.1007/978-981-19-3148-2_60,en,Stock Price Prediction Using Bi-LSTM and GRU-Based Hybrid Deep Learning Approach,OriginalPaper,"Stock market price movement prediction is a critical task for the investors due to its non-stationary and fluctuating nature. So, the automatic price movements forecasting techniques are now the hottest and crucial area for the researcher. Classical statistical models show the poor performance because of the random nature of stock price. In this paper, we proposed a novel hybrid deep learning model employing the bidirectional long short-term memory (Bi-LSTM) and gated recurrent unit (GRU) network. Individually, the long short-term memory (LSTM), Bi-LSTM, GRU, and traditional neural network (NN) modules are implemented to forecast the stock price. Then, the comparison between the individual model’s performances as well as with the proposed hybrid model is done in this work. The proposed stock price prediction model is implemented using the NIFTY-50 stock market data. Our model can predict long time ahead predictions precisely. Experimental results show the proposed hybrid Bi-LSTM-GRU model achieved higher performance than the above-mentioned individual models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-4971-5_14,en,Convolution Neural Network and Continuous Wavelet Transform-Based Islanding Detection of Integrated DG with Phase Angle Between Voltage and Current,OriginalPaper,"The growing electricity market is driving distributed generation forward (DG). Nearly the DG sources of nature are renewable. Islanding is one of the many complications of DG sources. Customers and their appliances can suffer from the insulation. The islanding will be detected in two seconds and the DG will have to be switched off in accordance with IEEE 1547 DG interconnection specifications. In this paper, an integrated island detection method for deeper learning is applied with Continuous Wavelet Transformations (CWT) and Convolution Neural Network (CNN). This technique transforms the details from the sequence of time into scalograms, later the images are used for training and testing of islanding and non-islanding incidents. The results are correlated with the reasoning approaches of the Artificial Neural Networks (ANN). The comparison reveals that the deep study method suggested effectively senses both islanding and non-islanding cases.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-981-19-8202-6_12,en,Urban Surface Solid Waste Detection Based on UAV Images,OriginalPaper,"The monitoring of illegal dumping of urban solid waste requires timeliness. Conventional methods are costly and have difficulty resolving the problem in remote regions timely. In this paper, a method based on semantic segmentation technology, which considers the multiscale characteristics and the disordered distribution of garbage, is proposed for automatic detection of surface solid waste in unmanned aerial vehicle (UAV) images. First, an asymmetrically decomposed atrous convolutional group was embedded to the center of the semantic segmentation network and the residual structure is used to enhance the learning ability of the spatial relationships. Then, a semantic flow alignment module was combined with attention mechanism to solve the disordered edges of information fusion. Experimental results demonstrate that the proposed method achieved better segmentation performance.","['Engineering', 'Signal, Image and Speech Processing', 'Computer Applications', 'Geography, general', 'Earth System Sciences']"
doi:10.1007/978-3-031-18458-1_15,en,Generating Human-Like Motion to Defeat Interaction-Based CAPTCHAs,OriginalPaper,"As more companies implement CAPTCHA systems to try to prevent automated attacks, CAPTCHA creators are increasingly using machine learning to try to filter out unwanted traffic. These systems are increasingly important in the development and maintenance of many web-based applications. As machine learning has evolved, so have the detection methods to block automated web traffic. As a result, some image-based CAPTCHAs are being replaced with systems that analyze mouse movements of the user to identify how likely it is that the user is human. In this research, we develop and evaluate a 2-layer convolutional neural network driven framework that generates human-like motions. These types of movements are tracked by some CAPTCHA systems. We demonstrate that the framework’s automatically generated movement paths can effectively and efficiently trick a classifier trained on features that are extracted from paths generated by humans. Using a 2-feature classifier as a CAPTCHA that was trained to recognize 91% of the human paths as valid human paths from our dataset, we are able to successfully bypass the CAPTCHA 89.25% of the time.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-3-031-16075-2_8,en,Long Short-Term Memory Neural Network for Temperature Prediction in Laser Powder Bed Additive Manufacturing,OriginalPaper,"In context of laser powder bed fusion (L-PBF), it is known that the properties of the final fabricated product highly depend on the temperature distribution and its gradient over the manufacturing plate. In this paper, we propose a novel means to predict the temperature gradient distributions during the printing process by making use of neural networks. This is realized by employing heat maps produced by an optimized printing protocol simulation and used for training a specifically tailored recurrent neural network in terms of a long short-term memory architecture. The aim of this is to avoid extreme and inhomogeneous temperature distribution that may occur across the plate in the course of the printing process. In order to train the neural network, we adopt a well-engineered simulation and unsupervised learning framework. To maintain a minimized average thermal gradient across the plate, a cost function is introduced as the core criteria, which is inspired and optimized by considering the well-known traveling salesman problem (TSP). As time evolves the unsupervised printing process governed by TSP produces a history of temperature heat maps that maintain minimized average thermal gradient. All in one, we propose an intelligent printing tool that provides control over the substantial printing process components for L-PBF, i.e. optimal nozzle trajectory deployment as well as online temperature prediction for controlling printing quality.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3571-8_56,en,Pressure Prediction System in Lung Circuit Using Deep Learning,OriginalPaper,"A massive number of patients infected with SARS-CoV2 and Delta variant of COVID-19 have generated acute respiratory distress syndrome (ARDS) which needs intensive care, which includes mechanical ventilation. But due to the huge no of patients, the workload and stress on healthcare infrastructure and related personnel have grown exponentially. This has resulted in huge demand for innovation in the field of automated health care which can help reduce the stress on the current healthcare infrastructure. This work gives a solution for the issue of pressure prediction in mechanical ventilation. The algorithm suggested by the researchers tries to predict the pressure in the respiratory circuit for various lung conditions. Prediction of pressure in the lungs is a type of sequence prediction problem. Long short-term memory (LSTM) is the most efficient solution to the sequence prediction problem. Due to its ability to selectively remember patterns over the long term, LSTM has an edge over normal RNN. RNN is good for short-term patterns but for sequence prediction problems, LSTM is preferred.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-18326-3_39,en,Hyperspectral Imaging for Non-destructive Testing of Composite Materials and Defect Classification,OriginalPaper,"Carbon fiber composite materials are intensively used in many manufacturing domains such as aerospace, aviation, marine, automation and civil industries due to their excellent strength, corrosion resistance, and lightweight properties. However, their increased use requires a conscious awareness of their entire life cycle and not only of their manufacturing. Therefore, to reduce waste and increase sustainability, reparation, reuse, or recycling are recommended in case of defects and wear. This can be largely improved with reliable and efficient non-destructive defect detection techniques; those are able to identify damages automatically for quality control inspection, supporting the definition of the best circular economy options. Hyperspectral imaging techniques provide unique features for detecting physical and chemical alterations of any material and, in this study, it is proposed to identify the constitutive material and classify local defects of composite specimens. A Middle Wave Infrared Hyperspectral Imaging (MWIR-HSI) system, able to capture spectral signatures of the specimen surfaces in a range of wavelengths between 2.6757 and 5.5056 µm, has been used. The resulting signatures feed a deep neural network with three convolutional layers that filter the input and isolate data-driven features of high significance. A complete experimental case study is presented to validate the methodology, leading to an average classification accuracy of 93.72%. This opens new potential opportunities to enable sustainable life cycle strategies for carbon fiber composite materials.","['Engineering', 'Robotics and Automation', 'Industrial Chemistry/Chemical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/978-3-031-19032-2_32,en,Recurrent and Graph Neural Networks for Particle Tracking at the BM@N Experiment,OriginalPaper,"This work presents a new two-step approach for elementary particle tracking that combines the advantages of both local and global tracking algorithms. On the first stage, where the graph of possible track-candidates is too big to fit into memory, a recurrent neural network model TrackNETv3 is used for building track candidates. On the second stage there is a graph neural network GraphNet needed for clearing the graph from the fake segments. The results of testing the proposed approach on the 3,2 GeV Ar+Pb simulation for the BM@N RUN7 are presented.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-3-031-19032-2_40,en,Multi-input Convolutional Neural Networks in Real-Time Semantic Segmentation Tasks,OriginalPaper,"When processing the information obtained during remote sensing of the Earth’s surface, a number of tasks arise, including the task of semantic segmentation of the corresponding images. An approach to solving this problem directly in the process of obtaining information of remote sensing of the Earth’s surface as the observing object moves is considered. Such an approach is necessary for work in real-time systems. This problem is solved using multi-input convolutional neural networks, nine variants of such networks are considered, including variants with links between encoder and decoder blocks, as well as without such links. When comparing results of segmentation for these variants it is revealed, that the most suitable variant is a network without the mentioned connections between blocks. At increase in quantity of the coders, perceiving and processing sequences of the input patterns, accuracy of an extraction of the prescribed classes of objects raises. The optimum number of inputs (coders) for a given task is determined by the complexity of segmented images and the size of the training set. It is shown that multi-input networks make it possible to solve the task of semantic segmentation of satellite images with an increased accuracy. However, the quality of solving the problem of semantic segmentation significantly depends on the size and informativity of the training set. The minimum allowable size of the training and test sets in relation to the problem to be solved has been determined.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Neurosciences']"
doi:10.1007/978-981-19-4017-0_5,en,Improved Modeling Attack on PUFs based on Tensor Regression Network,OriginalPaper,"In the previous chapter, we have discussed DFNN-based modeling attacks on various APUF compositions. The purpose of this chapter is to understand design and development of an improved machine learning model for launching modeling attacks.","['Engineering', 'Circuits and Systems', 'Artificial Intelligence', 'Mathematics, general', 'Special Purpose and Application-Based Systems', 'Computer Science, general']"
doi:10.1007/978-3-031-15191-0_28,en,Improving the Quality of Service Within Multi-objective Customer-Oriented Dial-A-Ride Problems,OriginalPaper,"Seeking a trade-off between the service offered to customers and the interests of the system provider is a challenging task within Dial a Ride Problems. A continuous reflection is required especially in the resolution of transport problems focusing on the requirements of the customers. In this paper, we address a customer oriented Dial A Ride Problem which minimizes the total transport costs while improving the quality of service provided to the customers. A multi-objective formulation is proposed in order to minimize the total travel costs and the total waiting times. Real life instances of the problem are solved using the branch and bound method embedded in the CPLEX solver for producing exact solutions of the problems and in the worst cases the lower bounds of the search space.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Mobile and Network Security']"
doi:10.1007/978-981-19-3842-9_9,en,Proving Ground Correlation Method Based on Optimized Particle Swarm Optimization Algorithm,OriginalPaper,"In the process of vehicle durability development, it is necessary to accelerate the durability test verification at the proving ground. It is particularly critical to optimize and formulate the proving ground durability procedure scientifically, efficiently and accurately. Based on the Miner linear cumulative damage theory and the road load spectrum data of the proving ground, in this paper, the sub-band damage equivalent proving ground correlation model is constructed, using the optimized particle swarm algorithm, to study the proving ground correlation technology based on frequency domain damage, and finally the rain flow count is adopted to evaluate the accuracy of correlated calculation, with the damage quantification method. The research shows that this method is scientific and reasonable, the correlation calculation efficiency is high, the correlation channel and the verification channel have good correlation in the frequency domain damage and rain flow count, and the frequency domain damage retention ratio is between 50% and 200%. The efficiency and accuracy of the correlated calculation can reach the expected goal of the proving ground correlation, and the solution results can be flexibly constrained by the actual driving route. It can be applied to engineering implementation directly.","['Engineering', 'Mechanical Engineering', 'Automotive Engineering', 'Transportation Technology and Traffic Engineering']"
doi:10.1007/978-3-031-16281-7_51,en,Inter-Operability of Compression Techniques for Efficient Deployment of CNNs on Microcontrollers,OriginalPaper,"Machine Learning (ML) has become state of the art for various tasks, including classification of accelerometer data. In the world of Internet of Things (IoT), the available hardware with low-power consumption is often microcontrollers. However, one of the challenges for embedding machine learning on microcontrollers is that the available memory space is very limited, and this memory is also occupied by the rest of the software elements needed in the IoT device. The problem is then to design ML architectures that have a very low memory footprint, while maintaining a low error rate. In this paper, a methodology is proposed towards the deployment of efficient machine learning on microcontrollers. Then, such methodology is used to investigate the effect of using compression techniques mainly pruning, quantization, and coding on the memory budget. Indeed, we know that these techniques reduce the model size, but not how these techniques interoperate to reach the best accuracy to memory trade-off. A Convolutional Neural Network (CNN) and a Human Activity Recognition (HAR) application has been adopted for the validation of the study .","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-4975-3_37,en,A Particle Swarm Optimization-Based Maximum Power Point Tracking Scheme Employing Dynamic Inertia Weight Strategies,OriginalPaper,"Photovoltaic (PV) systems are one of the most popular forms of Renewable Energy Sources. It is extremely important that these systems be operated at the Maximum Power Point (MPP). Under uniform insolation conditions, we observe a single peak in the P – V characteristics of a PV array. In contrast, under Partial Shading Condition (PSC) the P – V curve is highly non-linear and has multiple peaks that can be classified as Local and Global Peaks. Conventional MPPT Algorithms have failed to deliver satisfactory results under PSC. Hence, nature inspired optimization techniques such as the Particle Swarm Optimization (PSO) algorithm have been applied to MPPT under PSC and have proven to be an effective solution to the problem. In this paper, we employ a set of simple and dynamic Inertia weight strategies which are independent of factors such as maximum number of iterations and can be exploited to increase the speed of tracking of the PSO-based MPPT approach. An inertia weight that is dynamic, simple, and intuitive has also been proposed. The proposed Inertia Weight (IW) is independent of the current iteration number as well as a predetermined value for the maximum number of iterations necessary to converge to the MPP. A significantly lower convergence time and lesser tracking losses are obtained using the proposed IW. The performance of all these techniques has been evaluated using simulations under different shading conditions and validated with hardware implementation.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-3-031-11748-0_11,en,Modeling Individual Humans via a Secondary Task Transfer Learning Method,OriginalPaper,Each chapter should be preceded by an abstract (no more than 200 words) that summarizes the content. The abstract will appear online at www.SpringerLink.com  and be available with unrestricted access. This allows unregistered users to read the abstract as a teaser for the complete chapter. Please use the ’starred’ version of the abstract command for typesetting the text of the online abstracts (cf. source file of this chapter template abstract ) and include them with the source files of your manuscript. Use the plain abstract command if the abstract is also to appear in the printed version of the book.,"['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning']"
doi:10.1007/978-3-031-16159-9_15,en,Condition-Based Monitoring of DC Motors Performed with Autoencoders,OriginalPaper,"This paper describes a condition-based monitoring system estimating DC motor degradation with the use of an autoencoder. Two methods of training the autoencoder are evaluated, namely backpropagation and extreme learning machines. The root mean square (RMS) error in the reconstruction of successive fragments of the measured DC motor angular-frequency signal, which is fed to the input of autoencoder, is used to determine the health indicator (HI). A complete test bench is built using a Raspberry Pi system (i.e., motor driver controlling angular frequency) and Jetson Nano (i.e., embedded compute node to estimate motor degradation) to perform exploratory analysis of autoencoders for condition-based monitoring and comparison of several classical artificial intelligence algorithms. The experiments include detection of degradation of DC motor working in both constant and variable work points. Results indicate that the HI obtained with the autoencoders trained with the use of either training method is suitable for both work points. Next, an experiment with multiple autoencoders trained on each specific work point and running in parallel is reviewed. It is shown that, in this case, the minimum value of RMS error among all autoencoders should be taken as HI. Furthermore, it has been shown that there is a near-linear relationship between HI and the difference between measured and reconstructed angular-frequency waveforms.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-981-19-4960-9_68,en,Deep Learning-Driven Medical Imaging Analysis for COVID-19 Detection,OriginalPaper,"The primary requirement for early detection of COVID and control of the virus’s spread is rapid and precise diagnosis. Computer vision and deep learning-based models can be used to assist this COVID diagnosis process through chest X-ray scans. This study performs a comparative analysis on different deep learning models which can be used to diagnose COVID from chest X-ray scans. For this work, the following deep learning models were selected: VGG16, Xception, ResNet, DenseNet, and MobileNet. This research looks not only at COVID, but also at other SARS-CoV-2-related diseases such as SARS and MERS. The dataset used consists of five categories: normal, COVID, pneumonia, SARS, and MERS. The comparative study showed that both the MobileNet and DenseNet models were able to deliver the best performance, with the highest accuracy and minimal loss.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Professional Computing']"
doi:10.1007/978-981-19-2635-8_8,en,A Study on Path Planning Using Bi-Directional PQ-RRT* Algorithm and Trajectory Tracking Technique Using Incremental Backstepping Control,OriginalPaper,"An autonomous flight system is essential for effective mission performance of UAVs, which are increasingly being applied in civil and military missions. Thus, in this study, an effective approach for implementing guidance and flight control systems is proposed. Based on the rapidly-exploring random tree (RRT) algorithm, the bidirectional potential quick (PQ)-RRT* is proposed to implement the path planner of the guidance system. The proposed bidirectional PQ-RRT* algorithm has a combination of three different types of improvement methods based on RRT*: potential field guided sampling (P-RRT*), modified RRT*(Q-RRT*), and bi-directional searching tree methods (Bi-RRT*). The convergence path was efficiently optimized using the line-of-sight path optimization algorithm. Then, a flyable trajectory was generated by a 7 th order spline generator with waypoints from the path planner. Finally, incremental backstepping control was adopted to ensure trajectory-tracking performance within the overall operational flight envelope. To validate a series of processes, a simulation was performed to examine the practical realization. Based on the results, the bidirectional PQ-RRT* and line-of-sight path optimization algorithms were validated to provide an effective solution. In addition, the proposed flight control system exhibited excellent trajectory-tracking performance.","['Engineering', 'Aerospace Technology and Astronautics', 'Engine Technology', 'Automotive Engineering', 'Mechanical Engineering']"
doi:10.1007/978-981-19-3391-2_37,en,IFF: An Intelligent Fashion Forecasting System,OriginalPaper,"Fashion evolves continuously, and there is a real need for technological solutions to aid the fashion industry in predicting its dynamic nature to reduce waste, enhance quality output, and augment sales. Exploration of this task so far does not account for fine-grained elements and spans shorter time periods, which is not sufficient to analyze a trend’s temporal behavior. In this work, we introduce a framework that can provide a probabilistic popularity score for the apparel in an image. It consists of a neural network that is trained to identify and extract all the fashion attributes from an image and a forecasting model trained over a dataset of attributes and their trendiness over the last century. When provided with an image, the framework extracts a list of all the fashion attributes. The forecasting model then predicts the future trend for each of the attributes in the list using the dataset. Finally, an average score for the dresses’ trendiness is generated. A number of models were tested for the forecasting task, and the best model was selected based on Mean Absolute Error. Models tried ranged from statistical time series forecasting models such as ARIMA and SARIMAX to neural network-based models such LSTMs and Seq2Seq models. The best model was found to be the Seq2Seq model, with an F1 score of 0.813, and hence, it was chosen for building the framework. The framework can be applied to analyze how different attributes of a dress affect the popularity scores of a dress, given a picture.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-3951-8_64,en,Performance Evaluation of Sine Cosine Algorithm-Based Controllers for LFC in an Isolated Hydropower System Integrated with Energy Storage System,OriginalPaper,"In this paper, performance evaluation of I, PI, and PID controllers have been carried out for load frequency control (LFC) in an isolated hydropower generating system (HPGS). The primary objective of LFC is to maintain the electrical power system frequency at a pre-specified level. A population-based bio-inspired optimization algorithm called sine cosine algorithm (SCA) has been utilized for the optimization of I, PI, and PID controllers. The integral of time-squared absolute error (ITSE) has been considered as an objective function in this study for the optimization process. Load disturbance of 20% has been considered in an isolated HPGS for performing various simulation studies. The system under study is also integrated with a superconducting magnetic energy storage system (SMES). The effects of adding SMES in an isolated HPGS have been analyzed. From the results obtained, it has been observed that the SCA optimized PID controllers perform better as compared to SCA optimized I and PI controllers for LFC in the system under consideration. Also, by adding an energy storage system, there is a considerable reduction in the value of ITSE, peak undershoot, peak overshoot, mean value, peak to peak value, and RMS value of the system’s dynamics.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5090-2_22,en,Aerial Image Classification Using Convolution Neural Network,OriginalPaper,"Image classification task has been an important area in the field of computer vision study as it is applied in varied applications. The approaches for image classification is based on feature selection of image class and effectively applied through various low level feature algorithms for matching with a particular class and yielding a classification result identifying one from another. The task for feature selection particularly become extremely challenging to spatial images such as aerial images, as they contain varied complex feature, scale challenges as well as image orientation. A more accepted approach of image classification for aerial image is the use of Convolution Neural Networks (CNN). CNN models are capable of producing higher accuracy compared to contemporary feature based algorithm as they tend to utilize higher local features. Transfer learning approach using readily available and proven CNN for image classification makes the task a step closer to capturing and designing a CNN which adapt to user dataset and classification requirements. There is however the need to create user dataset with sufficient images to retrain the network for fine-tuning and testing its accuracy. This paper presents such experiment using GoogLeNet pre-trained network which is subject to replacing of the fully-Connected layer and output layer as per the classification requirement. The network is further subject to training and testing using test dataset other than that have never been exposed to the network. An accuracy of 98.33% was achieved.","['Engineering', 'Communications Engineering, Networks', 'Signal, Image and Speech Processing', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-981-19-8222-4_8,en,Salient Object Detection with Fusion of RGB Image and Eye Tracking Data,OriginalPaper,"Salient object detection (SOD) is one of the fundamental topics in computer vision, but the current SOD algorithm is difficult to accurately find salient regions in scenarios such as multiple objects or small objects. Considering the above problems, this paper proposes a SOD algorithm with the fusion of RGB image and eye tracking data. The specific methods are as follows: (1) The eye tracking data can well simulate the human visual selection attention mechanism and contains high-level semantic information, so the eye fixation points are integrated into the salient object detection algorithm. (2) Considering the different characteristics of high-level features and low-level features, an improved cascade decoder including channel cascaded decoder (CCD) and spatial cascaded decoder (SCD) is designed. Moreover, the cross-modal fusion module (CMFM) is employed to better fuse the eye tracking data and the RGB image features. (3) The comparative experiments on the two datasets show that the performance of the proposed method exceeds that of the mainstream algorithms and can achieve effective SOD.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-19-4863-3_31,en,Monthly Runoff Prediction by Support Vector Machine Based on Whale Optimisation Algorithm,OriginalPaper,"This study was conducted in catchment area of Baitarani River at Jaraikela, situated in Eastern India. The Baitarani River is one of the most important rivers in the eastern region of peninsular India, which later joins the Bay of Bengal. This region frequently experiences floods due to its erratic rainfall patterns and climatic conditions, which makes runoff prediction important for planning better watershed management techniques and mitigation strategies. To simulate rainfall-runoff process, SVM model integrated with Whale Optimisation Algorithm (WOA) method has been used. WOA enhances the results by reducing the error margin in SVM. For this purpose, 48 years (1981–2020) of statistical data have been used for calibration, validation and testing of the model. The results show that the hybrid SVM-WOA model outperforms the classical SVM model in terms of forecasting accuracy and efficiency based on root mean squared error (RMSE), mean absolute error (MAE), and Nash–Sutcliffe efficiency (NE) performance evaluation measures.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-06780-8_15,en,Towards Learning-Based Control of Connected and Automated Vehicles: Challenges and Perspectives,OriginalPaper,"The exploitation of communication technologies enables connected and automated vehicles (CAVs) to operate more collaboratively, that is, by exchanging or even negotiating future trajectories and control actions. That way, CAVs (or agents) can establish a networked control system such as to safely automate road traffic in a collaborative fashion. A rich body of literature is available, e.g., on intersection automation, automated lane change or lane merging scenarios. These control concepts, though, are most tailored to the particular application and are in general not applicable to multiple scenarios. This chapter conveys the challenges and perspectives of modeling and optimization-based control techniques for the safe coordination of multiple connected agents in road traffic scenarios. Along these lines, the perspective of generalizing controller design to serve multiple use cases simultaneously instead of designing separate controllers for every use case is discussed. Moreover, the opportunities of learning-based control in case of model uncertainties and mixed-traffic scenarios, involving connected and non-connected agents, are outlined.","['Engineering', 'Automotive Engineering', 'Computational Intelligence', 'Transportation Technology and Traffic Engineering', 'Signal, Image and Speech Processing']"
doi:10.1007/978-3-031-13249-0_37,en,Growth-Based Methodology for the Topology Optimisation of Trusses,OriginalPaper,"This paper presents a novel methodology for a growth-based topology optimization of trusses. While most methods of topology optimization are based on voxel grids that result in free-form volumes, the topology optimization of trusses exists as subtractive methods that start with a large number of initial beams. This method instead commences with a minimal amount of beams. The model is iteratively refined by node repositioning and node division according to structural forces to arrive at a complex truss. Case studies of a cantilever and a table show the results and reduction in mass achieved by the algorithm.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Industrial Design', 'Interaction Design', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/978-3-031-14537-7_19,en,Optimisation of Robotic Disassembly Sequence Plans for Sustainability Using the Multi-objective Bees Algorithm,OriginalPaper,"In recent years, remanufacturing Remanufacturing has become critical for environmental protection and natural resource conservation. The purpose of the work reported in this chapter is to find the best plan for product disassembly Disassembly , the first step in the recovery of end-of-life products, balancing the three goals of sustainability Sustainability —economic, energy and environmental. The study proposes three strategies: reuse Reuse , remanufacturing Remanufacturing and recycling Recycling . The Multi-objective Multi-objective Bees Algorithm (MOBA), Non-dominated Non dominated Sorting Sorting Genetic Algorithm II (NSGA II) and Pareto Envelope-based Selection Algorithm II (PESA II) are used to create solutions for two case studies. In this work, MOBA outperforms other algorithms in finding Pareto optimal solutions for robotic disassembly Disassembly sequence planning in all cases.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-981-19-2600-6_29,en,Foreign Object Detection on an Assembly Line,OriginalPaper,"In this paper we present a comparative study of two approaches for the detection of foreign objects in an industrial assembly line setting and proposes a complete solution from the findings. The methodology is vision based and can be used for processing 3D objects conveyed at a constant velocity. Out of the two methods, the CNN based approach is recommended to the company sponsoring this research. The design of the system is accomplished using a fixed camera, a display unit, a conveyor belt and further a raspberry pi or equivalent hardware to run the solution. The novelty of this solution is the possible full automation of the assembly line with low latency and high performance and with a small training dataset.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-1-4842-8945-7_9,en,OCI Command-Line and Application Programming Interfaces,OriginalPaper,"Thus far in this book we have used the OCI web-based console to work with OCI products and services. We saw how to set up our OCI account, create networking services, and create and configure DB Systems. If you are comfortable with the web-based console, you should consider continuing using it as every operation you need to perform is easy to find and most information can be found on a single page or a page with tabs.","['Computer Science', 'Database Management', 'Professional Computing']"
doi:10.1007/978-981-19-6153-3_7,en,"Next Job Application Prediction by Leveraging Textual Information, Metadata, and Personalized-Attention Mechanism",OriginalPaper,"Prediction of the next job application is one of the most important use-cases of job recommender systems. This work proposes to use next-item recommendation methods to model job seekers’ career preferences to more accurately discover the next job postings they may apply for. Our proposed model, Personalized-Attention Next-Application Prediction (PANAP), consists of three modules. The first module learns job posting representations from textual content and metadata attributes in an unsupervised way. The second module learns job seeker representations from job application records and metadata attributes. It includes a personalized-attention mechanism that adapts the importance of each job in the learned career preference representation to the profile of a specific job seeker. The third module then models the Next-Application Prediction task as a top- K search process based on the similarity of representations. Text content plays a major role in describing job postings, and how the job content is encoded affects the model performance. Therefore, we explore the utility of different content encoding methods. In addition, geographic location is another essential factor influencing the preferences of job seekers in the recruitment domain. We also explore the influence of geographic location on the model performance from the perspective of negative sampling strategies.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-981-16-9967-2_49,en,A Hybrid Approach Towards Machine Translation System for English–Hindi and Vice Versa,OriginalPaper,"With the rapid progress in the technology and data in the public domain, the machine translation and data science have made remarkable progress. In this paper, we discuss our specific use case of developing machine translation system for English to Hindi and Hindi to English language translation. For this system, we have used the daily proceedings of the Lok Sabha as data and developed NMT-based machine translation system on the top of already available rule-based machine translation system. Developed system has been evaluated using bilingual evaluation understudy (BLEU) as well as the human evaluation metrics using comprehensibility and fluency. In machine translation (MT), there is the trend of measuring post-editing time, and thus, we have also evaluated our system by measuring post-editing time using open-source tool.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Computational Intelligence', 'Artificial Intelligence', 'Wireless and Mobile Communication', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-3571-8_57,en,Twitter Sentiment Analysis Using Machine Learning and Deep Learning,OriginalPaper,"Twitter is the widely used microblogging site, where millions of people share their feelings, views, or opinion regarding different things be it a product, service, or events. Huge volumes of data are being produced hourly because of the increase in the number of users. These data are unstructured in nature, and thus, it is a difficult task to analyze them and extract the meaning from it. This paper, however, will concentrate on sentiment analysis of Twitter data. We will perform text mining or opinion mining to obtain a better understanding of public sentiment. In this paper, Python is used to acquire, preprocess, and analyze tweets, after those three different machine learning algorithms and one deep learning algorithm are used for sentiment analysis and comparison is done among them to determine which approach or model gives the best accuracy.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-3015-7_16,en,Anomaly Detection Using Feature Selection and Ensemble of Machine Learning Models,OriginalPaper,"Vulnerabilities have increased in cyberspace. This is due to the technology growth and the huge amount of data communicated between various endpoints of a network. Intrusion Detection System (IDS) plays a major role in identifying malicious traffic in the network. Intrusion Detection is much challenging because of the raw network traffic it contains with a large number of attributes that add to the complexity of the model. Several Machine Learning (ML) models have been built to solve these issues. The problem even after the new technologies’ introduction is the lack of datasets, classifiers work best for one problem and serve the least for the other set of problems, decision algorithms are not framed effectively. To overcome these problems, proposed method uses an ensemble approach on classifiers to provide a better solution for feature selection parameters. Our model outperforms the accuracy and detection rates compared to the individual classifiers. Decision Tree (DT), Logistic Regression (LR), and Support Vector Machine (SVM) values are given to the Random Forest (RF) ensemble classifier. Experiments are performed using CICIDS 2017 dataset. The proposed approach has accuracy of 98%, recall 97%, precision 100%, and F-score 98% compared to the individual models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Data Mining and Knowledge Discovery', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-21595-7_3,en,A Novel Diagnostic Model for Early Detection of Alzheimer’s Disease Based on Clinical and Neuroimaging Features,OriginalPaper,"Alzheimer’s Disease (AD) is a dangerous disease that is known for its characteristics of eroding memory and destroying the brain. The classification of Alzheimer's disease is an important topic that has recently been addressed by many studies using Machine Learning (ML) and Deep Learning (DL) methods. Most research papers tackling early diagnosis of AD use these methods as a feature extractor for neuroimaging data. In our research paper, the proposed algorithm is to optimize the performance of the prediction of early diagnosis from the multimodal dataset by a multi-step framework that uses a Deep Neural Network (DNN) as an optimization technique to extract features and train these features by Random Forest (RF) classifier. The results of the proposed algorithm showed that using only demographic and clinical data results in a balanced accuracy of 88% and an area under the curve (AUC) of 94.6. Ultimately, combining clinical and neuroimaging features, prediction results improved further to a balanced accuracy of 92% and an AUC of 97%. This study successfully outperformed other studies for both clinical and the combination of clinical and neuroimaging data, proving that multimodal data is efficient in the early diagnosis of AD.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Computer Communication Networks', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Systems Applications (incl. Internet)', 'Machine Learning']"
doi:10.1007/978-3-031-16281-7_52,en,Energy Consumption Analysis of Pruned Semantic Segmentation Networks on an Embedded GPU,OriginalPaper,"Deep neural networks are the state of the art in many computer vision tasks. Their deployment in the context of autonomous vehicles is of particular interest, since their limitations in terms of energy consumption prohibit the use of very large networks, that typically reach the best performance. A common method to reduce the complexity of these architectures, without sacrificing accuracy, is to rely on pruning, in which the least important portions are eliminated. There is a large literature on the subject, but interestingly few works have measured the actual impact of pruning on energy. In this work, we are interested in measuring it in the specific context of semantic segmentation for autonomous driving, using the Cityscapes dataset. To this end, we analyze the impact of recently proposed structured pruning methods when trained architectures are deployed on a Jetson Xavier embedded GPU.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-981-19-8222-4_5,en,DSNet: EEG-Based Spatial Convolutional Neural Network for Detecting Major Depressive Disorder,OriginalPaper,"Major depressive disorder (MDD) is a mental disease that has a severe negative impact on people’s daily lives, which has become a leading global health burden. Previous neuroscience studies have proved that MDD patients have altered structural and functional connectivity between different brain regions compared to normal individuals. Measuring brain activities via electroencephalography (EEG) is a cost-effective and appropriate method for the detection of mental disorders such as depression. In addition, as deep learning (DL) is gaining attention in various research fields, increasing DL methods have been presented to diagnose depression. Inspired by these angles, this paper proposed an end-to-end spatial convolutional neural network (CNN) called DSNet for depression classification based on the resting-state EEG signals. Evaluated on a public dataset, our model obtained better classification performance with the accuracy of 91.69% via the leave-one-subject-out (LOSO) cross-validation strategy compared to other DL models. The experimental results demonstrate that DSNet can effectively extract information on spatial differences between depressed and normal individuals and could be a potential model for MDD detection.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-19-5403-0_21,en,Using Machine Learning to Predict Business Survival in COVID-19,OriginalPaper,"The global recession due to the pandemic has knocked the business landscape and brought the world to its knees. There were a number of renowned companies that made the headlines for being the top industry hard hits. Nonetheless, there were businesses that survived this pandemic and navigated the COVID complexities so effectively that it tipped the scales in their favor. We attempt to study the factors that helped these businesses masterfully work their way through the conundrums of coronavirus pandemic. We first build a dataset that entailed information pertinent to businesses and relevant COVID-related information that was sourced from Yelp and other platforms. We used a variety of classifiers to make predictions about the survival of these businesses followed by that after assessing their performance through varied methods. The model efficiency was classified based on several rating techniques to evaluate both underperforming and profitable businesses.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-981-19-3951-8_8,en,Study of Decreasing Traffic Routing System Using VANET Hybrid Ant Colony Optimization,OriginalPaper,"Presenting the paper which is green strategy to reduce the network traffic congestion is an important step toward improvement for VANET infrastructure as we consider the dynamically changeable infrastructure of the RSU. With the latest technique of ad hoc network routing in computing generation and communications protocols, we are able to retrieve any form of sensors data and get maintain off in actual-time vehicles traffic congestion at every location of roadside using GPS. This routing technique observes and introduces a latest algorithms that goals to optimize the route selection of Internet site online traffic road congestion in actual-time primarily based mostly on the ant colony optimization algorithm and VANET communication system. The VANET is used as a communication era a good way to exchanging the routing table information among several vehicles and routes. The principle of the ACO is used to compute the shortest routing path that can be observed by way of the using force to handle the congested path. The proposed protocol is based totally on a multi-hop count scenario, in which all region CHs routers work collectively to expose the street and highway traffic congestion and assist the drivers to fast arrive at their destination locations through the use of the tremendous routes with less congestion. Simulation results evaluate that the proposed method can consumes very less time to cover the whole distance traveled from source to destination with optimized path, while compared to the traditional “shortest distance covering technique”.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-981-19-0151-5_33,en,SARM-Net: A Spatial Attention-Based Residual M-Net for Polyp Segmentation,OriginalPaper,"Polyp segmentation is a crucial step for the early diagnosis of colorectal cancer. However, the heterogeneous nature of polyps poses a significant challenge in the segmentation task, and it is still an unsolved problem. So in this study, we have proposed a deep learning network, namely “a spatial attention-based residual M-Net for polyp segmentation” (SARM-Net). The network is inspired by the M-Net architecture, where some additional modules are added to the existing architecture to improve the segmentation performance. We have employed residual connections in the M-Net architecture to preserve gradient information during backpropagation, facilitating optimal gradient flow. Meanwhile, unlike M-Net, where the contextual information is directly fed from the encoder to the decoder by skip connections, a spatial attention block (SAB) is introduced in our proposed network to focus on the relevant significant features and ignore the redundant features in the spatial dimension prior to the concatenation which will facilitate better optimization of the network. The segmentation performance was evaluated on the “Kvasir-SEG” database. The experimental results reflect the segmentation performance improvement compared to the traditional deep learning models and a recent state-of-the-art method.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Biological and Medical Physics, Biophysics', 'Information Storage and Retrieval']"
doi:10.1007/978-3-031-19958-5_45,en,Multi-class Short Text Classification Using Ensemble of Deep Learning Classifier,OriginalPaper,"With the substantial outgrowth of e-commerce, social media and online news portals have witnessed a great wave in expressing views through short text. Most textual contents are unstructured and messy forms, which are impractical and cumbersome to organize or manipulate by human experts. Therefore, developing an automatic short text classification model concerning low-resource languages, including Bengali, is critical. Moreover, the crucial barrier to classifying short text in Bengali is the unavailability of text corpora, scarcity of linguistics tools, a limited number of words in the text, and a lack of dependencies between the words. This paper presents a short text classification model using the ensemble of four base deep learning classifiers (Neural Network (NN), Convolutional Neural Network (CNN), Bidirectional Long Short Term Memory (BiLSTM), and Bidirectional Gated Recurrent Unit (BiGRU)). Additionally, a corpus of around 0.13 million Bengali texts is developed for short text classification into six categories (e.g., international, national, sports, amusement, technology, and politics). The evaluation results on the developed corpus demonstrated that the proposed method outperformed all the baselines machine learning and deep learning models by obtaining the highest weighted f1-score of $$84.4\%$$ 84.4 % .","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3032-4_7,en,Database Design Fundamentals,OriginalPaper,"Database design refers to constructing a suitable database schema for specific application objects according to the characteristics of database system, establishing database and corresponding applications, so that the whole system can effectively collect, store, process and manage data to meet the usage requirements of various users.","['Computer Science', 'Database Management']"
doi:10.1007/978-3-031-18256-3_12,en,Stacked Spatial and Temporal Deep Learning Methods for Identification of Parkinson’s Disease Using Gait Signals,OriginalPaper,"Parkinson’s disease (PD) is a progressive condition that affects dopaminergic neurons, causing motor alterations. Motor disturbances, such as gait impairment, can be used to assess the disease. Unfortunately, gait disturbances, such as decreased walking speed and step variability, can also occur due to aging, affecting the identification of abnormal PD gait. Therefore, developing an adequate tool to evaluate PD patients’ gait is essential. This paper proposes a deep learning algorithm to differentiate between PD gaits and normal walking using vertical ground reaction force (VGRF) signals. CLDNN is a single framework composed of a convolutional neural network, a long-short term memory network, and a deep neural network. To train and validate a CLDNN classifier gait cycles were obtained from VGRF signals. The VGRF signals were from a public database with recordings from 93 PD patients and 73 healthy adult controls. The CLDNN performance was evaluated by five-fold cross-validation. The combined spatial and temporal methods in CLDNN enabled the effective identification of PD gait with less complex architecture. The best weighted accuracy was 98.28 ± 0.38. Thus, our model is compact and efficient for future embedded or portable implementations.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-3-031-08580-2_20,en,Effects Evaluation of Data Augmentation Techniques on Common Seafood Types Classification Tasks,OriginalPaper,"The applications which aim to classify seafood on the ships or markets are essential. However, when fishers have caught a large amount of seafood on board, they have to manually classify each seafood type for a long time, affecting seafood quality after catching. Moreover, When classifying seafood by manual method, fishers who have prolonged exposure to seafood are prone to skin diseases such as skin infections, fungal infections, cracked fingers. In addition, they also often suffer from spinal diseases due to having to stand in one position and move continuously during the classification process. Therefore, this study focuses on developing a method to effectively detect and classify some popular species of saltwater seafood using MobileNetV2 based on selected hyper-parameters via experiments on 9000 images of nine seafood types. Experimental results have reached 0.9956 in accuracy metric on the test set with the selected hyper-parameters combining considered data augmentation techniques. Also, the work evaluates the effects of eliminating each data augmentation technique to compare their influence on image classification tasks improvement. We can consider removing augmentation techniques that have revealed less influence in the image classification performance of seafood types.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6153-3_4,en,Early Mental Health Uncovering with Short Scripted and Unscripted Voice Recordings,OriginalPaper,"Mental illnesses are often undiagnosed, highlighting the need for an effective alternative to traditional screening surveys. We propose our Early Mental Health Uncovering (EMU) framework that conducts rapid mental illness screening with active and passive modalities. We designed, deployed, and evaluated the EMU app to passively collect retrospective digital phenotype data and actively collect short voice recordings. The EMU app also administered a depression screening survey to label the data. We collected data from crowdsourced and student populations, both of whom shared sufficient voice recordings for modeling. We thus assess the classification ability of machine learning and deep learning models trained with scripted and unscripted voice recordings. For the crowdsourced participants, machine learning models screened for depression with an AUC of 0.78 and suicidal ideation with an AUC of 0.73. For the student participants, deep learning models screened for depression with an AUC of 0.70 and suicidal ideation with an AUC of 0.72. Combining datasets did not improve screening capabilities, though the best performing models on the combined dataset notably required voice transcripts. This research facilitates a better understanding of modality selection for mobile screening. We will make the features publicly available to further advance mental illness screening research.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-3-031-11814-2_5,en,Multi-Echelon Inventory Optimization Using Deep Reinforcement Learning,OriginalPaper,"In this chapter, we provide an overview of inventory management within the pharmaceutical industry and how to model and optimize it. Inventory management is a highly relevant topic, as it causes high costs such as holding, shortage, and reordering costs. Especially the event of a stock-out can cause damage that goes beyond monetary damage in the form of lost sales. To minimize those costs is the task of an optimized reorder policy. A reorder policy is optimal when it minimizes the accumulated cost in every situation. However, finding an optimal policy is not trivial. First, the problem is highly stochastic as we need to consider variable demands and lead times. Second, the supply chain consists of several warehouses incl. the factory, global distribution warehouses, and local affiliate warehouses, whereby the reorder policy of each warehouse has an impact on the optimal reorder policy of related warehouses. In this context, we discuss the concept of multi-echelon inventory optimization and a methodology that is capable of capturing both, the stochastic behavior of the environment and how it is impacted by the reorder policy: Markov decision processes (MDPs). On this basis, we introduce the concept, its related benefits and weaknesses of a methodology named Reinforcement Learning (RL). RL is capable of finding (near-) optimal (reorder) policies for MDPs. Furthermore, some simulation-based results and current research directions are presented.","['Economics', 'Health Economics', 'Pharmaceutical Sciences/Technology', 'Economics, general', 'Biomedicine, general', 'Biotechnology', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1007/978-3-031-14771-5_20,en,Predicting Infections in the Covid-19 Pandemic—Lessons Learned,OriginalPaper,"Throughout the Covid-19 pandemic, a significant amount of effort had been put into developing techniques that predict the number of infections under various assumptions about the public policy and non-pharmaceutical interventions. While both the available data and the sophistication of the AI models and available computing power exceed what was available in previous years, the overall success of prediction approaches was very limited. In this paper, we start from prediction algorithms proposed for XPrize Pandemic Response Challenge and consider several directions that might allow their improvement. Then, we investigate their performance over medium-term predictions extending over several months. We find that while augmenting the algorithms with additional information about the culture of the modeled region, incorporating traditional compartmental models and up-to-date deep learning architectures can improve the performance for short term predictions, the accuracy of medium-term predictions is still very low and a significant amount of future research is needed to make such models a reliable component of a public policy toolbox.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-3-031-14771-5_9,en,Phenotyping with Positive Unlabelled Learning for Genome-Wide Association Studies,OriginalPaper,"Identifying phenotypes plays an important role in furthering our understanding of disease biology through practical applications within healthcare and the life sciences. The challenge of dealing with the complexities and noise within electronic health records (EHRs) has motivated applications of machine learning in phenotypic discovery. While recent research has focused on finding predictive subtypes for clinical decision support, here we instead focus on the noise that results in phenotypic misclassification, which can reduce a phenotypes ability to detect associations in genome-wide association studies (GWAS). We show that by combining anchor learning and transformer architectures into our proposed model, AnchorBERT, we are able to detect genomic associations only previously found in large consortium studies with 5 $$\times $$ × more cases. When reducing the number of controls available by 50%, we find our model is able to maintain 40% more significant genomic associations from the GWAS catalog compared to standard phenotype definitions.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-1-0716-2756-3_15,en,Integration of scATAC-Seq with scRNA-Seq Data,OriginalPaper,"Single-cell studies are enabling our understanding of the molecular processes of normal cell development and the onset of several pathologies. For instance, single-cell RNA sequencing (scRNA-Seq) measures the transcriptome-wide gene expression at a single-cell resolution, allowing for studying the heterogeneity among the cells of the same population and revealing complex and rare cell populations. On the other hand, single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-Seq) can be used to define transcriptional and epigenetic changes by analyzing the chromatin accessibility at the single-cell level. However, the integration of multi-omics data still remains one of the most difficult tasks in bioinformatics. In this chapter, we focus on the combination of scRNA-Seq and scATACSeq data to perform an integrative analysis of the single-cell transcriptome and chromatin accessibility of human fetal progenitors.","['Life Sciences', 'Cell Biology', 'Bioinformatics']"
doi:10.1007/12_2021_108,en,Thermal and Thermo-Oxidative Degradation of Rubbers: Some Recent Studies,OriginalPaper,"Elastomers or rubbers are a special kind of material with inherent elasticity, i.e. the ability to retain its structure upon deformation. Due to their unique properties, elastomers are used in different applications starting from automotive tires, seals, gaskets, conveyor belts, V-belts to musical instruments, toys, and even in garments. However, there is a much-awaited need for an elastomer with a higher degree of heat resistance property due to the stringent industrial requirements. The heat resistance of an elastomer is extremely desirable in automotive, aerospace, and off-shore industries where several rubber parts experience much higher temperature. Thus, it is very important to understand the degradation of different elastomers over a range of temperatures to prevent it. This chapter summarizes the high-temperature degradation of various elastomers with the help of the reactive molecular dynamics simulation technique. Different fragmented products were identified with the help of the simulation method and experimentally verified. The simulation technique was also used to calculate the activation energy for the degradation of different elastomers. The final section of this chapter describes the thermo-oxidative degradation of various elastomers and their nanocomposites.","['Materials Science', 'Materials Science, general', 'Structural Materials', 'Mechanical Engineering']"
doi:10.1007/978-3-031-20105-9_5,en,IIR System Identification Using Several Optimization Techniques: A Review Analysis,OriginalPaper,"System identification is a difficult optimization problem, especially those that use the infinite impulse response (IIR) models which are preferred over their equivalent FIR (finite impulse response) models since they represent more accurate real-world applications. Nevertheless, IIR models tend to generate multimodal error surfaces which are significantly difficult to optimize.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-09835-2_10,en,Intelligent and Reliable Cognitive 5G Networks Using Whale Optimization Techniques,OriginalPaper,"In forthcoming networks for high definition radio large bandwidth, low latency and several emergence applications like e-health, Industrial IOT, smart transportation etc. will be conquered by 5G networks. Therefore, more capacity and consequently efficient spectrum sensing will be an awful prerequisite for huge demand for certain applications. In this field of research, attempts have been made to develop new techniques to improve the reliability and channel capacity in 5G Networks using WOA, LDPC and Cognitive Concepts. The results have been presented in the form of various plots and graphs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4182-5_11,en,Chi-Square Top-K Based Incremental Feature Selection Model for BigData Analytics,OriginalPaper,"The exponential rise in advanced software computing, internet technologies, and humongous data has given rise to a new paradigm called BigData, which requires an allied computing environment to ensure 4Vs aspects, often characterized as varieties, volume, velocity, and veracity. In sync with these demands, most of the classical commutating models fail, especially due to large unstructured features of gigantically huge volume. To alleviate this problem, feature selection can be a viable solution; provided it guarantees minimum features with optimal accuracy. In this reference, the proposed work contributed a first of its kind solution which could ensure minimum features while ensuring expected higher accuracy to meet 4V demands. To achieve it, in this paper, a robust Chi-Squared Select-K-Best Incremental Feature Selection (CS-SKB-IFS) model is developed that achieved a minimum set of features yielding the expected accuracy. Subsequently, over the selected features, the CS-SKB-IFS model is used for further classification using the Extra Tree classifier. Thus, the strategic amalgamation of the CS-SKB-IFS model achieved the accuracy of (91.02%), F-Measure (91.20%), and AUC (83.06%) than the other state-of-art methods. In addition to the statistical performance, CS-SKB-IFS exhibited significantly smaller computational time (1.01 s) than the state-of-art method (6.74 s).","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Statistics, general']"
doi:10.1007/978-981-19-3391-2_13,en,Secure Communication and Pothole Detection for UAV Platforms,OriginalPaper,"Everyone around the world relies on roads for transportation on a daily basis. Aging and heavy usage of roads results in deterioration of the surface. Besides potholes, we also see many cracks in roads which may expand and develop into potholes themselves. Potholes can result in vehicle damage and can also cause physical harm to people in their vehicles. Detected potholes, if integrated with information systems, can notify drivers in real time, so they can be aware. Potholes are a problem, especially in developing countries. In this paper, we discuss leveraging and integrating the best modern technologies in order to solve this problem. We have tried to make it a feasible, robust, flexible, and modular system to help solve this problem. Using unmanned aerial vehicles (UAVs) and machine learning/deep learning and networking together, we will be able to spot potholes and cracks on the roads much faster than existing methods. By using the system, the need for manual surveillance is reduced. Using a UAV, you can cover large areas. To detect potholes, a UAV will be used to take videos and take pictures along the roads. These images will be sent to a computer where ML algorithms will detect the pothole. In addition to the CNN, a transfer learning algorithm and YOLO real-time object detection can be used to identify potholes. Video is streamed securely over Wi-Fi by an RTSP server to a local workstation, so the user can view the video from the UAV and may store it for future use.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-16159-9_5,en,COVID-19 Severity Forecast Based on Machine Learning and Complete Blood Count Data,OriginalPaper,"Proper triage of COVID-19 patients is a key factor in effective case management, especially with limited and insufficient resources. In this paper, we propose a machine-aided diagnostic system to predict how badly a patient with COVID-19 will develop disease. The prognosis of this type is based on the parameters of commonly used complete blood count tests, which makes it possible to obtain data from a wide range of patients. We chose the four-tier nursing care category as the outcome variable. In this paper, we compare traditional tree-based machine learning models with approaches based on neural networks. The developed tool achieves a weighted average F1 score of 73% for a three-class COVID-19 severity forecast. We show that the complete blood count test can form the basis of a convenient and easily accessible method of predicting COVID-19 severity. Of course, such a model requires meticulous validation before it is proposed for inclusion in real medical procedures.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-981-16-9348-9_29,en,State-of-the-Art of Artificial Intelligence Methods in Structural Health Monitoring,OriginalPaper,"Development of technology nowadays occur in every aspect of life, including in computational technology. Artificial Intelligence (AI) over the past years became trending topic in field related to computation as it is mimic the way human think. This is led into effective problem-solving execution for a complex problem. Solution based on AI method is proven to be time-efficient compare with traditional ways and require significantly less resource to conduct experiment. AI method is able to perform a decision making based on series of given data with relatively short time, less error and efficient in overall computation. The objective of this paper is to investigate what is the most popular AI method and with the intended use in Structural Health Monitoring (SHM) by referring to reliable publications over the past decade. This paper shows the Neural Network is the most popular AI method in terms of SHM with 38% from total research over the past decade which is used as a performance evaluation.","['Energy', 'Sustainable Architecture/Green Buildings', 'Construction Management', 'Facility Management']"
doi:10.1007/978-3-031-12409-9_10,en,Natural Language Processing,OriginalPaper,"This chapter discusses natural language processing (NLP) which deals with regression modeling of non-tabular or unstructured text data. We explain how words can be embedded into low-dimension spaces that serve as numerical word encodings. These can then be used for text recognition, either using RN networks or attention layers. We give an example where we aim at predicting claim perils from claim descriptions.","['Mathematics', 'Applications of Mathematics', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Machine Learning', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6004-8_36,en,Anomaly Detection in Image Sequences Using Weakly Supervised Learning,OriginalPaper,"This paper proposes a weakly supervised deep learning method to create an anomaly detection system that can identify abnormal behavior in surveillance videos. A video anomaly detection system analyzes the data from various surveillance videos to detect the presence of any abnormal activities. We have used the inflated 3D convolutional neural network (I3D) for feature extraction and created a neural network model which scores the videos for the presence of any abnormal behavior by using the multiple instance learning (MIL) technique. We use the UCF-Crime dataset to train and test this model. The proposed method achieves a better area under curve (AUC) value of 82.03 for the receiver operating characteristic (ROC) plot than the value of 77.92 reported by Sultani et al. (IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp 6479–6488, 2018).","['Engineering', 'Computational Intelligence', 'Cognitive Psychology', 'Personality and Social Psychology', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-3-031-12409-9_6,en,"Bayesian Methods, Regularization and Expectation-Maximization",OriginalPaper,"This chapter summarizes some techniques that use Bayes’ theorem. These are classical Bayesian statistical models using, e.g., the Markov chain Monte Carlo (MCMC) method for model fitting. We discuss regularization of regression models such as ridge and LASSO regularization, which has a Bayesian interpretation, and we consider the Expectation-Maximization (EM) algorithm. The EM algorithm is a general purpose tool that can handle incomplete data settings. We illustrate this for different examples coming from mixture distributions, censored and truncated claims data.","['Mathematics', 'Applications of Mathematics', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Machine Learning', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-8222-4_2,en,Delving into Temporal-Spectral Connections in Spike-LFP Decoding by Transformer Networks,OriginalPaper,"Invasive brain-computer interfaces (iBCIs) have demonstrated great potential in neural function restoration by decoding intention from brain signals for external device control. Spike trains and local field potentials (LFPs) are two typical intracortical neural signals with good complementarity from time and frequency domains. However, existing studies mostly focused on a single type of signal, and the interaction between the two signals has not been well studied. This study proposes a temporal-spectral transformer network (TSNet) to model the temporal (with spikes), spectral (with LFPs), and mutual (with both signals) connections in spike-LFPs towards robust neural decoding. Experiments with clinical neural signals demonstrate that the attention-based connection model enables the dynamic temporal-spectral compensation in spike and LFP signals, which improves the robustness against temporal shifts and noises in neural decoding.","['Computer Science', 'Artificial Intelligence', 'Computer Applications', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Mathematics of Computing']"
doi:10.1007/978-981-19-3391-2_18,en,Indian Classical Dance Forms Classification Using Transfer Learning,OriginalPaper,"Human activity analysis is useful in a variety of domains, including video surveillance, biometrics, and home health monitoring systems. In computer vision field, extraction and recognition of complex human movements from images/videos are a great complex task. In this present work, we propose the Indian classical dances (ICD) classification using the concept of transfer learning. ICD form is a combination of gesticulation of all body parts. It comes in a variety of shapes and sizes, but the most common features include single/double hand mudras, eye movement, legs alignment, hip movements, facial expressions, and legs posture. Each dance has its own gestures and clothes worn by the dancers. India classical dances are categorized into 8 categories. In this work, we used the dataset consists of eight dance classes includes Bharatnatyam, odissi, manipuri, kuchipudi, mohiniyattam, sattriya, kathakali, and kathak. Those images were collected from Internet. While image processing using CNN model training with less data does not give accurate result that leads to over-fitting problem. To overcome this problem, we propose a concept, transfer learning by this use the knowledge that was learned from some problem that can be applied to solve the problem related to the target task. It reduces both time and space complexity. In our proposed work, we use a pre-trained model VGG16. It results high accuracy of 85.4% compared to earlier methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-16281-7_53,en,A Tiny CNN for Embedded Electronic Skin Systems,OriginalPaper,"The quest for efficient Tiny Machine Learning on Microcontroller Units is increasing rapidly due to the vast application spectrum made possible with the advancement of Tiny ML. One application area that could benefit from such advancement is Electronic Skin systems, that are employed in several domains such as: wearable devices, robotics, prosthesis, etc. An e-skin system demands hard constraints including real-time processing, low energy consumption, and low memory footprint. This paper presents a tiny Convolution Neural Network (CNN) architecture suitable for the deployment on an off-the-shelf commercial microcontroller in compliance with the e-skin requirements. The training, optimization, and implementation of the proposed CNN are presented. The CNN implementation is optimized through layer fusion and buffer re-use strategies for efficient inference on edge devices. As a case study, experimental analysis of a touch modality classification task demonstrates that the proposed CNN-based system is capable of processing tactile data in real-time directly near the source while reducing the model size by up to 65% with respect to comparable existing solutions.","['Engineering', 'Cyber-physical systems, IoT', 'Machine Learning', 'Robotics and Automation']"
doi:10.1007/978-3-031-16078-3_33,en,Zero-Shot Visual Emotion Recognition by Exploiting BERT,OriginalPaper,"The explosive growth of multimedia has attracted many people to express their opinions through social media like Flickr and Facebook. As a result, social media has become the rich source of data for analyzing human emotions. Many earlier studies have been conducted to automatically assess human emotions due to their wide range of applications such as education, advertisement, and entertainment. Recently, many researchers have been focusing on visual contents to find out clues for evoking emotions. In literature, this type of study is called visual sentiment analysis. Although a great performance has been achieved by many earlier studies on visual emotion analysis, most of them are limited to classification tasks with pre-determined emotion categories. In this paper, we aim to recognize emotion classes that do not exist in the training set. The proposed model is trained by mapping the visual features to the emotional semantic representation embedded by the BERT language model. By evaluating the model on a cross-domain affective dataset, we achieved 66% accuracy for predicting the unseen emotions not included in the training set.","['Engineering', 'Computational Intelligence', 'Control, Robotics, Mechatronics', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3467-4_14,en,Pitch Controller for Isolated Wind-Diesel System with Super Conducting Magnetic Energy Storage Unit Based on Fractional-Order Fuzzy PID Controller,OriginalPaper,"The fossil fuel store is depleting, and the pace of energy demand is increasing by the day. Renewable energy resources are being exploited to create power to address the growing need for energy while avoiding global warming and pollution. This study proposes a biomass-based diesel-wind system with a superconducting magnetic energy storage (SMES) unit. A fractional-order Fuzzy PID (FOPID) controller is then employed to improve pitch control performance of the wind turbine. As a result, the proposed FOPID controllers will perform better than existing conventional controllers.","['Energy', 'Energy Systems', 'Energy Storage', 'Renewable and Green Energy']"
doi:10.1007/978-3-030-99075-6_21,en,A TFG-CNN Fault Diagnosis Method for Rolling Bearing,OriginalPaper,"It is difficult to obtain enough data to train a robust diagnosis model for different rolling bearing faults, and the existing intelligent bearing fault diagnosis algorithms have insufficient generalization ability. Therefore, a rolling bearing fault detector based on the time–frequency graph and convolution neural network (TFG-CNN) is introduced to improve the generalization performance of the fault diagnosis algorithm as much as possible under the condition of considering the diagnosis accuracy and sample size. The specific implementation method is to use Fast Fourier transform (FFT) to transform the vibration data of rolling bearing into a two-dimensional network graph, and then use CNN to classify them. Finally, the performance of the proposed method is analyzed by using the rolling bearing fault datasets of Case Western Reserve University, and analysis results show that the proposed method can simultaneously diagnose the fault location and severity of rolling bearing, and has good cross-domain diagnosis ability and anti-noise performance.","['Engineering', 'Industrial and Production Engineering', 'Mechanical Engineering', 'Machinery and Machine Elements']"
doi:10.1007/978-981-19-0511-7_39,en,Structural Damage Identification from Acceleration Wavelet Data Using Convolutional Neural Networks,OriginalPaper,"Identification of damage within a structure requires extraction of damage-sensitive features and recognition of patterns in the collected information. In recent years, deep learning-based methods, particularly Convolutional Neural Networks (CNNs) have proved to be remarkable tools in finding the underlying patterns in raw data and establishing accurate mapping between the data and various damage indicators. This study examines the performance of CNNs when the time-frequency information obtained from Continuous Wavelet Transformation (CWT) is employed as input data for prediction of the presence and location of damage in a beam-like structure. The CWT data from the acceleration reflect the variations in the modal properties of the structure and the characteristics of external excitations and measurement noise over time. This can help the network to distinguish different damage cases. For verification of the proposed approach, the acceleration data obtained from finite element dynamic analyses of a simply supported beam under random excitation are utilized where damage is induced in the form of loss in the flexural stiffness of elements. The input to the CNN architecture is a 3D block of CWT data of multiple accelerometers and the network is trained to output the presence and location of damage across the beam. By considering different levels of damage and number of measurement points, it is demonstrated that high detection accuracy can be achieved. Also, robustness of the technique against noisy data is investigated where the acceleration data are intentionally corrupted by added random noise to simulate actual conditions.","['Engineering', 'Building Construction and Design', 'Geoengineering, Foundations, Hydraulics', 'Transportation Technology and Traffic Engineering', 'Environment, general']"
doi:10.1007/978-981-19-3895-5_9,en,Optimization and Analysis of Abrasive Wear of Agro-waste Fiber Reinforced Composites by RSM Design Matrix,OriginalPaper,"Biofibers and Agro-waste in polymer matrix materials are attracting augmented deliberation because of ecological concerns and the acknowledgment that worldwide oil assets are limited. Natural fiber-based hybrid composites were the best choice for automobile industry for interior and exterior parts, bearings, door linings, etc., since they were searching for new ecofriendly material which reduces the cost and weight. The aim of the present work is to conduct an experimental study based on the design of experiments (DOE) of abrasive wear property of Agro-waste areca husk fiber (AHF) reinforced epoxy composites by varying various parameters such as weight % of fiber (0, 7, 14, 21, 28), sliding distance (314.16, 471.24, 628.32 m) and applied load (5, 7.5, 10, 15 N) using the pin-on-disk method. The wear experiments were carried out as per full factorial design of experiment, and response surface methodology (RSM) was adopted to develop an empirical model for the true response surface. Analysis of variance (ANOVA) test is used to check the adequacy and thus estimate the optimum process parameters for the AHF composite for specific wear rate. The optimum value obtained for specific wear rate was 3.87 × 10 –11  m 3 /Nm which when the applied load = 15 N, sliding distance = 628.32 m and the wt% of fiber reinforcement = 15.27%, respectively. FTIR was conducted to analyze the structural modifications caused by NaOH treatment of AHF fibers. Results indicate that incorporation of areca husk fibers considerably enhanced the abrasion properties of AHF composites and can be used for different tribological applications. The major wear mechanisms were revealed by worn surface profile study using SEM (ZEISS EVO-18), and it also shows that chemical surface treatment (5% NaOH) improves the adhesion between fibers and matrix which significantly influences the abrasive wear properties of fabricated composites.","['Materials Science', 'Structural Materials', 'Nanotechnology', 'Materials Science, general']"
doi:10.1007/978-3-031-18050-7_17,en,DR Participants’ Actual Response Prediction Using Artificial Neural Networks,OriginalPaper,"Empowering the consumers will increase the complexity of local communities’ management. Enabling bidirectional communication and appliances to become smarter can be a huge step toward implementing demand response. However, a solution capable of providing the right knowledge and tools must be developed. The authors thereby propose a methodology to manage the active consumers on Demand Response (DR) events optimally, considering the context in which it is triggered. The distribution system operator detects a voltage violation and requests a load reduction to the aggregators. In this study, to test a performance rate designed by the authors to deal with response uncertainty, a comparison between requested and actual reduction is done. The proposed methodology was applied to three scenarios where the goal is predicting the response from the consumers using artificial neural networks, by changing the features used in the input.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-14771-5_21,en,Improving Radiology Report Generation with Adaptive Attention,OriginalPaper,"To avoid the tedious and laborious radiology report writing, automatic radiology reports generation has drawn great attention in recent years. As vision to language task, visual features and language features are equally important for radiology report generation. However, previous methods mainly pay attention to generating fluent reports, which neglects the eminent importance of how to better extract and utilize vision information. Keeping this in mind, we propose a novel architecture with a CLIP-based visual extractor and Multi-Head Adaptive Attention (MHAA) module to address the above two issues: through the vision-language pretrained encoders, more sufficient visual information has been explored, then during report generation, MHAA controls the visual information participating in the generation of each word. Experiments conducted on two public datasets demonstrate that our method outperforms state-of-the-art methods on all the metrics.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-3-031-17554-1_5,en,Short-Term Load Forecasting in Electrical Networks and Systems with Artificial Neural Networks and Taking into Account Additional Factors,OriginalPaper,"The reliability of electrical networks and systems largely depends on the accuracy of load forecasts used to calculate losses, imbalances, and modes of operation of power systems. The use of modern forecasting methods allows us to obtain more accurate results, faster calculations, flexible enough to solve a wide range of problems. Artificial neural networks today are one of the most common tools for building complex mathematical models depending on the tasks. This spread of artificial neural networks is due to the significant development of computer technology. Depending on the characteristics of electrical networks and their loads, the accuracy of different forecasting methods may vary. Additional factors also have a significant impact on forecasting accuracy. Therefore, accurate load forecasts for different load levels require modern and effective methods that could take into account the relationship of additional factors. Among the factors that have a significant impact on changes in the electrical load of the power system are meteorological factors, namely temperature. To determine the exact relationship between load and external factors, the method of decomposition of graphs using the Hilbert-Huang method is considered. This chapter discusses the possibilities and prospects for the application of modern forecasting methods based on artificial neural networks, respectively, for forecasting electrical networks of different hierarchies with the possibility of taking into account temperature.","['Energy', 'Energy Systems', 'Energy Policy, Economics and Management', 'Control and Systems Theory']"
doi:10.1007/978-3-031-16159-9_2,en,Machine-Aided Detection of SARS-CoV-2 from Complete Blood Count,OriginalPaper,"The current gold standard for SARS-CoV-2 detection methods lacks the functionality to perform population screening. Complete blood count (CBC) tests are a cost-effective way to reach a wide range of people – e.g. according to the data of the Central Statistical Office of Poland from 2016, there are 3,000 blood diagnostic laboratories in Poland, and 46% of Polish people have at least one CBC test per year. In our work, we show the possibility of machine detection of SARS-CoV-2 virus on the basis of routine blood tests. The role of the model is to facilitate the screening of SARS-CoV-2 in asymptomatic patients or in the incubation phase. Early research suggests that asymptomatic patients with COVID-19 may develop complications of COVID-19 (e.g., a type of lung injury). The solution we propose has an F1 score of 87.37%. We show the difference in the results obtained on Polish and Italian data sets, challenges in cross-country knowledge transfer and the selection of machine learning algorithms. We also show that CBC-based models can be a convenient, cost-effective and accurate method for the detection of SARS-CoV-2, however, such a model requires validation on an external cohort before being put into clinical practice.","['Engineering', 'Control and Systems Theory', 'Computational Intelligence']"
doi:10.1007/978-981-19-3148-2_71,en,Sentiment Analysis of Reviews Using Bi-LSTM Using a Fine-Grained Approach,OriginalPaper,"In the era of digitization, where everything is available online, and further, information about everything is online, we have transcended into a new phase—finding subjective information about everything in the form of reviews. Performing sentiment analysis on reviews left by customers can prove to be essential for service providers as it can help them engineer better products/services, understand sales dynamics, and target audiences appropriately. The more detailed the analysis, the deeper the insight gained. Due to this, sentiment analysis of online reviews has become a project taken up frequently, prompting data scientists to compile and share numerous data repositories. For this research, we have selected one such dataset consisting of reviews for several products and attempted to build an intelligent model using Bidirectional LSTM that can classify reviews as positive, negative, and neutral, rather than categorizing them only as positive and negative.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory']"
doi:10.1007/978-1-4842-8864-1_19,en,Locking and Blocking,OriginalPaper,"Locking is an essential aspect of any RDBMS, because it allows concurrent users to access the same data, without the risk of their updates conflicting and causing data integrity issues. This chapter discusses how locking, deadlocks, and transactions work in SQL Server; it then moves on to discuss how transactions impact In-Memory transaction functionality and how the DBA can observe lock metadata regarding transactions and contention.","['Computer Science', 'Microsoft and .NET', 'Database Management']"
doi:10.1007/978-3-031-21333-5_73,en,Extended Belief Rule Base Model with Novel Rule Generation for Sensor-Based Human Activity Recognition Under Big Data,OriginalPaper,"As the population ages and health-care costs increase, smart environments can be an effective and economical way to provide care and support for the aged population. Human activity recognition (HAR), a key element of the smart environment research domain, has garnered a lot of attention lately. The present work is to provide a data-driven solution based on the extended belief rule base (EBRB) model for sensor-based HAR in the context of big data. More specifically, in order to increase the efficiency of the EBRB model, this research first offers a new rule generation method based on probability estimation, which forms the link between the extended belief rules and human activities. The number of extended belief rules used to extract knowledge from a sensor-based HAR dataset is exactly equal to the types of human activities, and each rule can be thought of as a collection of class conditional probability distributions. As a result, it is possible to create an EBRB-BD model, an EBRB model for HAR using big data that has a compact but representative rule base. The effectiveness of the EBRB-BD model is further supported by case studies. Experimental findings demonstrate that the modelling time of the EBRB-BD model is one in ten-thousand of the original EBRB model, and the EBRB-BD model also achieves the best area under the curve value (AUC) of 94.95 $$\%$$ % , surpassing the original EBRB model and some other benchmark classifiers.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3015-7_18,en,Using Natural Language Processing to Understand Reasons and Motivators Behind Customer Calls in Financial Domain,OriginalPaper,"In this era of abundant digital information, customer satisfaction has become one of the prominent factors in the success of any business. Customers want a one-click solution for almost everything. They tend to get unsatisfied if they have to call about something which they could have done online. Moreover, incoming calls are a high-cost component for any business. Thus, it is essential to develop a framework capable of mining the reasons and motivators behind customer calls. This paper proposes two models. Firstly, an attention-based stacked bidirectional long short-term memory network is followed by hierarchical clustering for extracting these reasons from transcripts of inbound calls. And, secondly a set of ensemble models to classify the motivators of the inbound calls. It is capable of detecting factors that led to these calls. Extensive evaluation proves the effectiveness of these models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Data Mining and Knowledge Discovery', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-3-031-12409-9_11,en,Selected Topics in Deep Learning,OriginalPaper,"This chapter presents a selection of different topics. We discuss forecasting under model uncertainty, deep quantile regression, deep composite regression and the LocalGLMnet which is an interpretable FN network architecture. Moreover, we provide a bootstrap example to assess prediction uncertainty, we discuss mixture density networks, and we give an outlook to studying variational inference.","['Mathematics', 'Applications of Mathematics', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Machine Learning', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18050-7_40,en,Explainable Artificial Intelligence for the Electric Vehicle Load Demand Forecasting Problem,OriginalPaper,"In this work we will address the short-term electricity consumption forecasting problem related to the electric vehicle load demand. In particular we will focus on the explainability of the model obtained. These are important aspects of this problem, since it would help gaining insight on the most important features involved in the forecasts. For the purpose of forecasting, we will use linear regression and three machine learning methods: random forest, gradient boosting and long short-term memory artificial neural network. Later, We add an explainability layer to the models generated, to get a better understanding of the predictions. As far the predictions are concerned, results obtained by the long short-term memory neural network are more accurate than those obtained by random forest and gradient boost, having used linear regression as baseline. The features that most contribute to the predictions are the 25 closest to the present but also a set of features with 30 to 60 unit lag.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-18256-3_41,en,PET Image Reconstruction Using a GRU-Convolutional Network,OriginalPaper,"Positron emission tomography is widely used for tumor detection and treatment monitoring in oncology. However, the quality of the images depends, among other factors, on the amount of radiopharmaceutical ingested by the patient. In this sense, the quality suffers degradation because there is a limit on the amount of radiation the patient can tolerate. Because of this, image reconstruction algorithms are required to generate images of adequate quality even if the amount of radiopharmaceutical to produce the image is small. In this study, a reconstruction algorithm is proposed based on deep learning using a GRU recurrent network which is expected to model the series of projections produced by the PET scanner as an input sequence to the recurrent network and is capable of reconstructing an image even with low amounts of the radiopharmaceutical. In comparisons using image quality metrics, our proposal achieves a SIMM of 0.95, outperforming other state-of-the-art methods. Additionally, tests were performed for the evaluation of the task of lesion detection; the proposed method obtained a better contrast of the lesion with a value of 0.54 when using the weber contrast metric, very similar to the ground truth contrast of 0.55.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Regenerative Medicine/Tissue Engineering', 'Bioinformatics']"
doi:10.1007/978-3-031-17091-1_59,en,Investigating the Impact of Variable Effects in Virtual Training on the Behavior of a Physical Autonomous Robot,OriginalPaper,"Artificial Intelligence is a current megatrend in computer science and almost every aspect of digitization. A scenario for training robots in a virtual environment to fulfil tasks in the real world is created to offer engineering students relevant insights into the field of neural networks. The proposed system generates training data to train a convolutional neural network (CNN) to autonomously drive a mobile robot using computer vision. The physical aspect of the setup includes the mobile robot and two different race circuits to evaluate the driving characteristics. The digital aspect consists of a 3D environment and a digital representation of the physical robot, both of which are developed using the game engine Unity. Inside the 3D environment, an infinite, procedurally generated road is created for the digital robot to drive on. The generated images from the virtual camera of the virtual robots are the basis to train the CNN to maneuver the physical robot in the real-world experiment.","['Engineering', 'Computational Intelligence', 'Data Engineering', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-19945-5_2,en,A Distributed Task Scheduling Approach for Cloud Computing Based on Ant Colony Optimization and Queue Load Information,OriginalPaper,"Cloud computing is an important computing paradigm based on large scale distributed infrastructures offering resources to consumers in a pay-as-you-go manner. An important aspect of cloud infrastructure management is the task scheduling problem. In this problem, tasks submitted by users and encapsulated in virtual machines are allocated to compute nodes in order to optimize some performance metric. In this paper a distributed task scheduling approach based on swarm intelligence is proposed, where schedulers distributed on different nodes make local task allocation decisions based on principles of ant colony optimization. Ant colony optimization is combined with queue load information for mitigating delayed reward problem that results from high load condition. Experimental evaluation in a simulated environment shows improved results compared to a distributed scheduling approach based on ant colony or queue load information only.","['Engineering', 'Communications Engineering, Networks', 'Computational Intelligence', 'Computer Applications']"
doi:10.1007/978-3-031-14537-7_6,en,Shape Recognition for Industrial Robot Manipulation with the Bees Algorithm,OriginalPaper,"Fitting primitive shapes to point cloud Point Cloud scenes is a challenging but necessary step for many robotic Robotic manipulation operations. State-of-the-art primitive fitting Primitive Fitting methods rely on geometric shape Geometric Shapes estimation or iterative procedures. They are often computationally complex and sensitive to algorithm parameterisation. This study tackles primitive fitting Primitive Fitting as a parameter Parameters optimisation problem, solving it using the Bees Algorithm Bees Algorithm, THE . The performance of the Bees Algorithm Bees Algorithm is evaluated on three sets of artificial scenes of varying degrees of blurriness and benchmarked against an evolutionary algorithm Evolutionary Algorithm . Experimental results proved the precision and consistency of the Bees Algorithm. Primitive fitting times were compatible with real-time Real-Time application.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-3-031-20601-6_18,en,A Comparison of Deep Learning Techniques for Corrosion Detection,OriginalPaper,"Corrosion—degradation in metal structures—is problematic, expensive to rectify, and can be unpredictable in the rate at which it spreads. Traditional preventative maintenance techniques are complemented by human visual inspection, in turn complemented by artificial intelligence vision techniques. The primary objective of this paper was to determine the most accurate deep learning model for use in corrosion detection; to achieve this, we devised an experimental comparison that tested five machine learning algorithms for the detection of corrosion from image data. The deep learning that forms the basis of algorithms used to solve object recognition problems traditionally requires large amounts of training data. As this data requires manual labelling by a person who is expert in the domain of corrosion, it is difficult and expensive to obtain; time and expense that increase considerably as more sophisticated pixel-level annotation is applied. We discovered that high levels of accuracy (98%) can be achieved using deep learning to detect corrosion using samples annotated with simple, image-level labels. We achieved this headline accuracy through the application of transfer learning using models that had been trained on the ImageNet dataset. With many deep learning algorithms to choose from, we systematically determined the most accurate model to use as a basis for further experimentation.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-19620-1_47,en,Optimal Design of Conventional and Freeform Optical Systems with Memetic Mind Evolutionary Computation Algorithm,OriginalPaper,"This paper studies distinct features between optimization of conventional and freeform optical systems. Freeform optical systems include lens or mirror surfaces with shapes that have no axis of revolution to reach better performance in off-axis designs. They have an increased number of design parameters compared to classical rotationally symmetrical optical systems which makes optimization more difficult. The authors applied several well-known optimization techniques both to a classic system and a freeform system, including commercial methods from Zemax OpticStudio software. In addition, a new parallel memetic algorithm was proposed that outperformed many methods under investigation. The algorithm incorporates local search techniques into an asynchronous parallel computing procedure thus helping to speed-up the convergence to a high-quality solution. The description of the proposed algorithm as well as the results a comparative study are presented in this paper. Computational experiments were conducted for two systems – the Cooke triplet and the freeform prism with two optical surfaces. Obtained results were estimated both from the optimization and optical perspectives.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-5845-8_9,en,A Comprehending Deep Learning Approach for Disease Classification,OriginalPaper,"For medical image processing analysis, deep learning is one of the most popular research subjects. It is subset of machine learning comprising of one or more neural network layers to simulate human behavior of learning and predicting. The purpose of this work is to investigate the application of deep learning models in image processing for disease analysis and medical innovations. The work showcased a generic deep learning model based on convolutional neural networks to classify diseases upon image analysis. To demonstrate the extensive medical use case of proposed model, the results demonstrated classifying pneumonia x-ray images alongside normal chest x-ray images, wrist r-ray pictures able to distinguish between normal and fractured wrists, etc.","['Engineering', 'Communications Engineering, Networks', 'Control, Robotics, Mechatronics', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-3-031-14537-7_21,en,Prediction of the Remaining Useful Life of Engines for Remanufacturing Using a Semi-supervised Deep Learning Model Trained by the Bees Algorithm,OriginalPaper,"The estimation of the remaining useful life Remaining Useful Life (RUL) of a component is one of the most important tasks for predictive maintenance systems (PMSs). This research aims to predict the remaining useful life Remaining Useful Life (RUL) of aircraft engines by using the Bees Algorithm Bees Algorithm, THE (BA)-optimised semi-supervised deep learning Deep learning model. To this end, the LSTM Long Short Term Memory architecture has been implemented as a successful deep learning Deep learning architecture to process time-series datasets. The predictions Prediction have been made by using the NASA Turbofan Engine Corruption Simulation (C-MAPSS) dataset. The proposed prediction Prediction model has been formulated as a binary classification Classification task based on the semi-supervised labelling process. Unlike the conventional predictive maintenance models, the data-driven LSTM Long Short Term Memory architecture automatically learns features of the multivariate time series. The model has been verified based on sixteen different time cycles. The proposed semi-supervised deep learning model has been trained using the BA to improve the accuracy value for the safe and unsafe conditions of aircraft engines. The experimental results reached 98% accuracy for the test dataset, and the proposed model performed better in terms of the F1 measure for both the training and test datasets. Experiments proved that the proposed deep learning Deep learning model can be used as a promising RUL Remaining Useful Life prediction Prediction model.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-3-031-05491-4_17,en,Evaluation of Recurrent Neural Network Model Training for Health Care Suggestions,OriginalPaper,"Neural networks have revolutionized the field of data analytics and decision support, where recurrent neural networks are designed for solving time-series data. Total care is a synonym for complete patient care recently to respond patient’s physical, emotional, social, economic, and spiritual needs, and as such an efficient prediction system for total care suggestions could help physicians and other healthcare providers in making clinical judgement. Therefore, this study aims to compare neural network models on predicting patients in need of the total care. The experimental results show that the LSTM (long short-term memory) prediction model outperforms other neural network models on forecasting total care needs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Engineering', 'Big Data', 'Mathematical and Computational Engineering']"
doi:10.1007/978-3-031-11686-5_5,en,Search-Based Variability Model Synthesis from Variant Configurations,OriginalPaper,"The parallel maintenance of independent system variants, developed to supply a wide range of customer-specific demands, is a complex activity. To alleviate this problem and ease the creation of new products, the consolidation of such variants into a Software Product Line (SPL) is an effective solution. For this, a fundamental step is to construct a variability model that represents the combinations of features of all the existing variants. However, the process of extracting an SPL from independent variants, and consequently constructing a variability model, is acknowledged as costly and error-prone. Instead of starting from scratch, many approaches have been proposed for reverse engineering variability models, but they have two limitations. These approaches usually optimize a single objective that does not allow software engineers to consider design trade-offs and do not exploit knowledge from implementation artifacts. This chapter presents our approach to address these limitations. Our approach applies a multi-objective optimization strategy and uses source code dependencies to reverse engineer variability models. The resulting model not only represents the desired feature combinations but are also well-formed regarding source code dependencies, i.e., variability safe. The approach, using two multi-objective evolutionary algorithms, namely NSGA-II and SPEA2, and a single-objective algorithm was evaluated with twelve subject systems. For comparisons, we rely on widely adopted performance indicators. The results indicate that the performance of the multi-objective algorithms is similar in most cases, and that both clearly outperform the single-objective algorithm. Also, the trade-off among different solutions is important to support engineers during decision making.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Software Management', 'Computer System Implementation']"
doi:10.1007/978-3-031-15428-7_27,en,Artificial Intelligence and Machine Learning,OriginalPaper,"With the ever-expanding digitalisation, more information or data is generated and is available within the digital ecosystem. The expansion of available data and the increased competition caused by globalisation contribute to why manufacturers are looking for more advanced methods to optimise their production and products. The general usage of artificial intelligence (AI) within different fields is expanding. As part of Industry 4.0, AI is also gaining interest within the industrial sector, where companies are expanding and trying different usages of AI, both within their production and as a product or service.","['Engineering', 'Industrial and Production Engineering']"
doi:10.1007/978-981-19-4971-5_40,en,Frequency Control of Wind Integrated Isolated Power System with I-PD Controller,OriginalPaper,"To improve the frequency profile in an interconnected power system, the parameter gains of the I-PD controller are achieved with the crow search optimization (CSO) algorithm. Comparative performances of the system in presence of load disturbances, inertia, and droop control of wind are carried out with a PID controller. The improvements in terms of the fitness function, overshoot, and undershoot are observed in isolated power systems integrated with wind power, using the I-PD controller proposed in this paper. Three conventional power generating units-thermal, hydro, and gas are controlled by this I-PD controller when the system is integrated with HVDC tied wind plant. Two control mechanisms are adopted for wind known as inertia and droop control. The optimal settling time values show the superiority of the I-PD controller over the PID controller. Simulation investigations are carried out in MATLAB-SIMULINK.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management', 'Energy Systems']"
doi:10.1007/978-3-031-10780-1_33,en,Applying Behavioral Finance to Influence Consumer Decision-Making and Behavior Via Human-Automation Interaction,OriginalPaper,"This chapter focuses on addressing what drives human decision making Decision making and behavior and how to influence such in the context of real-world human-automation Human automation interaction. Humans do not think or behave like robots Robots /computers, and automation technologies that do not incorporate Human Factors are likely to be sub-optimal at promoting the desired behavioral outcome. The ever-increasing body of knowledge in Behavioral Finance not only provides deep insights into the “Why” behind human decision making Decision making , but also sheds light on “How” to facilitate the desired behavior change and create win–win situations for shareholders all around. In this chapter, we discuss in detail why human decision making Decision making often seems irrational and is subject to the influence of heuristics Heuristics /biases. Then, we provide a viable business framework to help practitioners drive real-world impact through a Behavioral Finance approach. To illustrate how such a framework is deployed, we analyze several applications by organizations across industry sectors. In the end of the chapter, we discuss the ethics standard Standards needed when applying Behavioral Finance and provide a set of practical guidelines for researchers and practitioners to apply Behavioral Finance in an appropriate manner to drive real impact.","['Engineering', 'Computational Intelligence', 'Robotics and Automation', 'Manufacturing, Machines, Tools, Processes', 'Artificial Intelligence']"
doi:10.1007/978-981-19-4739-1_17,en,Analysis of Bearing Capacity and Settlement from Cone Penetration Test Results at an Irrigation Project,OriginalPaper,"A cut-off wall was constructed as part of a multipurpose irrigation project located in East Godavari district of Andhra Pradesh state in India. As the soil strength is a major concern in the design of substructures in soils, it is proposed to study the soil conditions at the site. The cone penetration test results were collected from the water resources department to analyze the bearing capacity and settlement of the soil at different locations. The analysis was carried out using GEO5 software tool considering pile foundations of diameter 1 m and depth of 10 m. From the analyses, it was observed that the bearing capacity of the pile at selected locations was in the range of 2752–4940 kN and the estimated settlements were within the allowable limits.","['Engineering', 'Geoengineering, Foundations, Hydraulics', 'Geotechnical Engineering & Applied Earth Sciences', 'Environment, general']"
doi:10.1007/978-981-19-3575-6_37,en,Soft Computing-Based Approach for Face Recognition on Plastic Surgery and Skin Colour-Based Scenarios Using CNN,OriginalPaper,"In recent years, face recognition has become one of the most common technologies in the identification of humans which is widely used as biometrics. With many of the biometric systems that are already out there, facial recognition has become one of the most important technologies for quickly identifying people without having to ask them. This will not cause any unnecessary delays. However, there are several face-recognition systems that rely on traditional machine learning as well. But they do not work when it comes to things like facial expressions, posture, occlusion, and scale. The face-recognition method described in this article employs a convolutional neural network (CNN) to locate faces within a picture. Viola–Jones face detection is used to locate faces in a picture, and a pre-trained CNN extracts facial traits from the faces automatically. However, a large database of facial images is made in order to have more images for each subject and to include different noise levels for the best training of CNNs. Overall accuracy of the system was increased to 98.81% which will depict the effectiveness of deep face recognition. We attempt to design an algorithm that overcomes the difficulties associated with facial identification owing to skin by feeding faces into a CNN for feature extraction and then passing them to a softmax classifier for classification. Also to develop an algorithm to overcome the challenges that occur in the face recognition due to external and artificial features like plastic surgery.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-5221-0_67,en,A Comparison of Transaction Fees for Various Data Types and Data Sizes of Blockchain Smart Contracts on a Selection of Blockchain Platforms,OriginalPaper,"Blockchain smart contracts promote collaboration and heighten the proliferation of automated production processes in the modern manufacturing supply chain. This is mostly done to enhance traceability of products whilst maintaining data security, integrity, and provenance. Blockchain technology with its obvious advantages is perceived as a possible solution to ensuring data provenance in the modern manufacturing supply chain. Data provenance pertaining to the supply chain process flow from obtaining raw materials, manufacturing, and shipping of the order is incorporated into the blockchain smart contracts. However, there are contributing factors for the selection of a blockchain platforms for incorporating data provenance into smart contracts. These factors include maturity, centralised or decentralised operation, throughput, and transaction fees to name a few. This paper is written based on research done in comparing transaction fees for various data types and data sizes of blockchain smart contracts on a selection of blockchain platforms. With respect to storing data type string of data size 256 bits into Ethereum, blockchain platforms transaction fees are high. With respect to storing data type string of data size 768 bits into Ethereum 2.0, Tron and Polygon blockchain platforms transaction fees are low. With respect to storing data type string of data size 2304 bits Ethereum 2.0, Tron and Polygon blockchain platforms transaction fees are low.","['Engineering', 'Computational Intelligence', 'Communications Engineering, Networks', 'Sociology, general', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-6347-6_10,en,Meta-Heuristic LQI Bio-regulator Benchmark for a Permanent Magnet DC Motor on ARM Platform,OriginalPaper,"There are controllers such as the Linear Quadratic Controller plus Integral part (LQI) that have presented favorable results in critical and complex processes, however, one of the disadvantages of this controller is the parameterization of the Q and R matrices, since they are obtained based on the cost of the controller and a trial and error method. Therefore, the present study aims to optimize these parameters through meta-heuristic algorithms such as: Genetic Algorithms (GA), Bacterial Foraging Optimization (BFO) and Ant Colony Optimization (ACO). In the MATLAB/SIMULINK software, the control loop programming is performed, using the control blocks of the Waijung library and with the STM32F407 card with Advanced Risk Machine (ARM) processor, the capture, reading and processing of data from the plant containing the DC motor for control is obtained. To validate the efficiency of the controller, the Integral of the Absolute Value of the Time Weighted Error (ITAE) is used and together with the Wilcoxon statistical method, it compares the optimization methods or techniques performed in the LQI controller. Interesting and favorable results were obtained for the stability and viability of each bio-controller at the moment of applying them in the speed control of the DC motor.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Statistics, general']"
doi:10.1007/978-981-19-5292-0_10,en,Bounds on Learnability of Neural Networks,OriginalPaper,"Deep learning (DL) got remarkable triumph in wide variety of problems like medical imaging, detection of frauds, e-commerce, health care, etc. It has arguably emerged as a futuristic technique. But designing a neural networks model is still a challenge. Despite the development of many theoretical bounds, there is no clear answer to what and why deep learning models generalize. Many studies have been conducted to develop a better understanding of generalization in deep neural networks. A survey of studies is conducted that aim to better understand the theoretical bounds of neural networks (NN). An empathetic knowledge of the generalization theory will enable researchers/developers to model more cost-effective and powerful networks.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Data Structures and Information Theory', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/978-981-19-4863-3_55,en,Top Five Machine Learning Libraries in Python: A Comparative Analysis,OriginalPaper,"Nowadays machine learning (ML) is used in all sorts of fields like health care, retail, travel, finance, social media, etc. ML system is used to learn from input data to construct a suitable model by continuously estimating, optimizing, and tuning parameters of the model. To attain the stated, Python programming language is one of the most flexible languages, and it does contain special libraries for ML applications, namely SciKit-Learn, TensorFlow, PyTorch, Keras, Theano, etc., which is great for linear algebra and getting to know kernel methods of machine learning. The Python programming language is great to use when working with ML algorithms and has easy syntax relatively. When taking the deep-dive into ML, choosing a framework can be daunting. The most common concern is to understand which of these libraries has the most momentum in ML system modeling and development. The major objective of this paper is to provide extensive knowledge on various Python libraries and different ML algorithms in comparison with meet multiple application requirements. This paper also reviewed various ML algorithms and application domains.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-981-19-5224-1_35,en,Oil Spill Detection in Ocean Using Deep Learning,OriginalPaper,"Oil spill is a type of pollution which affects both directly and indirectly the human beings, economy of the nation, and the marine life. Oil on top of the ocean damages numerous aquatic organisms since it stops sunlight that is sufficient in achieving the surface of the ocean and lowers dissolved oxygen levels. Oil-coated birds and marine mammals can perish hypothermia because crude oil destroys the insulation. In addition, ingested oil is poisonous to affected creatures and habitat. The answer towards the above-mentioned issues is towards building a methodology employing deep learning towards precisely recognizing oil spills and oil-like spill from ocean for taking appropriate action. So, we in this paper have deployed different pre-trained deep learning models towards classification of oil spill in ocean. In addition, different deep learning models performance are compared and validated in terms of accuracy and losses for proposing the best deep learning model for classification of oil spill in ocean.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-3-031-19067-4_9,en,Beyond Distributed Training in the Cloud,OriginalPaper,"Let us summarize the concepts that we learned through this book, and discuss the future of distributed machine learning beyond cloud-based implementations that we studied in this book.","['Mathematics', 'Algorithms', 'Machine Learning', 'Algorithm Analysis and Problem Complexity', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes', 'Computer Science, general']"
doi:10.1007/978-981-19-4193-1_59,en,Route Optimization for Waste Collection,OriginalPaper,"The vehicle routing problem is a synonym used in the enclosure of transport, distribution, and outsourcing to optimize routes. Route planning techniques are one of VRP’s major errands: planning to seek an optimal way on a map from a starting point to a destination. We strive to achieve a GIS-based transport system that provides the easiest, fastest, and shortest route to reach the hub. In this paper, we discuss the description of the different route planning algorithms and then explain their efficiency comparison and analysis when Municipal Corporations implement them throughout the existing road network for use in the waste management framework. Along with Haversine formula, we choose Dijkstra, the most well-known shortest path algorithm and traveling salesman problem.","['Engineering', 'Computational Intelligence', 'Information Storage and Retrieval', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Data Structures and Information Theory', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2535-1_4,en,Brand Logo Detection Using Slim YOLO-V4,OriginalPaper,"Images have been a rich source of information in recent years. Images are available in vast quantities, and most solutions necessitate real-time picture processing. This necessitates the creation of images with human-like capabilities for detecting and locating items in images. Object Detection is a branch of Computer Vision that has applications in a variety of domains, including Face Detection, Video Surveillance, Autonomous Driving Cars, and Medical Image Processing. Object detection should be quick and accurate. For accurate detection, all portions of the image should be searched for objects of all types and sizes. This necessitates a large computation cost as well as a significant quantity of time. It is natural and requires little effort for people, and researchers aim to create models that behave similarly to humans.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-981-19-0095-2_50,en,CoviFacts—A COVID-19 Fake News Detector Using Natural Language Processing,OriginalPaper,"Fake news confronts us on a daily basis in today’s fast-paced social media world. While some instances of fake news might seem innocuous, there are many examples that prove to be menacing. Misinformation or disinformation which takes the form of these weaponized lies which eventually amount to defective information, defamatory allegations, and hoaxes. The only motive behind such a malicious act is to engender emotional instability among the public. One such prevalent example today is COVID-19 which has caused an unprecedented paradigm shift in numerous businesses and quotidian activities across the globe. One of the primary activities is being news reporting. On average, people are spending almost one hour a day reading news via many different sources. The development in technology has obviated the barriers between sharing of information, thereby truly making the industry cosmopolitan. Therefore, it is paramount to curb fake news at source and prevent it from spreading to a larger audience. This paper describes a system, where the user can identify apocryphal news related to COVID-19 so as to ensure its authenticity.","['Engineering', 'Communications Engineering, Networks', 'Science and Technology Studies', 'Systems and Data Security', 'Artificial Intelligence', 'Computational Intelligence']"
doi:10.1007/978-981-19-4863-3_34,en,"Application of a Combined GRNN-FOA Model for Monthly Rainfall Forecasting in Northern Odisha, India",OriginalPaper,"Rainfall forecasting is considered the most complex variable in the hydrological cycle, and often its cause-impact relationship cannot be articulated in complex or simple mathematical terms. Because of climate change, the varying amount of rain can lead to either surplus or dryness in reservoirs. This research introduces a novel hybrid model generalised regression neural network integrated with fruit fly optimisation algorithm (GRNN-FOA), to forecast monthly rainfall. Rainfall data were collected from a local meteorological station from 1971 to 2020 and utilised in this study to assess model performance. Performance of each approach is assessed utilising root mean squared error (RMSE), Nash Sutcliffe efficiency (NSE), and Willmott index (WI). Results specify that the hybrid GRNN-FOA model is consistent and accurate in estimating the risk level of significant rainfall events. Our proposed robust model shows improved performance than conventional techniques, providing a new thought in the area of rainfall prediction. This artificial intelligence-based study would also help quickly and accurately predicting monthly rainfall.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-19958-5_89,en,A Transfer Learning Approach to Detect Face Mask in COVID-19 Pandemic,OriginalPaper,"COVID-19 is tumultuous creating our life so unpredictable. There has no solution of this contagious disease rather than vaccination and prevention. The first and foremost preventative step is using face masks. Face mask can hindrance its droplet from one to another. So this paper has focused the detection of facial mask from image processing using Transfer Learning. For this purpose, total 1376 images have been collected where 690 images of with mask and 686 images of without a mask. Here transfer learning is chosen for the reason of its capability to produce best accurate regardless the limited size of the image dataset. Here, multifarious transfer learning models have been trained to find out the best fitting model. Finally, We have found the VGG16 model with the best accuracy where training accuracy is 98.25% and testing accuracy is 96.38%.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2412-5_8,en,Obstructed Material Classification Using mmWave Radar with Deep Neural Network for Industrial Applications,OriginalPaper,"Radar sensing technology uses radio electromagnetic (EM) waves to provide 3D space localisation and 4D motion sensing. The mmWave radar shows advantages in low cost, low power, environment robustness and capability in material classification. In this paper, the capability of mmWave radar to perform industrial multi-material classification with obstruction is studied by measuring the reflected radar signal. The classified materials are common engineering materials which include metal, polymer, ceramic, composite and natural. The experiment is conducted using the IWR1443BOOST mmWave radar sensor. From a series of experiment results, the received radar signal is the unique material signature of a target object. The relative power measured by IWR1443BOOST is correlated to the target object’s relative permeability and permittivity. This indicated the mmWave radar can easily pick up unique material properties as well as the physical structure of target object with minor assistance from deep neural network model. Three models which are linear classifier, fully connected neural network (FCNN) and convolution neural network (CNN) are trained and inference on the radar signal. CNN shows the most robust performance even under noise, while linear classifier converges fastest. All models achieved satisfactory accuracy with minimum amount of training epochs. This is because the radar signals are having clear discriminative distribution as proven in standard deviation against mean plot. The models also perform under 16 mm thick obstruction and can classify less than 5 mm thin material. From the experiment, the mmWave radar provides highly accurate multi-material classification with deep neural network. Due to its’ capability in wall-penetration and environment robustness characteristics, mmWave radar is a new alternative solution for industrial automation and sensing application.","['Engineering', 'Power Electronics, Electrical Machines and Networks', 'Renewable and Green Energy', 'Energy Storage']"
doi:10.1007/978-981-19-5403-0_16,en,An Axiomatic Analysis for Object Detection and Recognition Using Deep Learning,OriginalPaper,"In the modern era, object detection and analysis is one of the crucial tasks. Numerous challenges have to be faced by the researchers to analyze the objects in images and real-time videos. Such analysis is required in multiple domains such as monitoring health, self-driving including detection of an anomaly, and many more. With the advanced usage of deep learning (DL) models and GPUs, the efficacy of object detection has been improved very well. Therefore, in this paper, we have provided a detailed study of both cases (images and videos) to detect specific objects using DL models. The existing approaches have been compared using on global wheat-head detection dataset to check the performance for object detection and recognition at different stages. From this analysis, we can say that DL is a backbone of computer vision to detect and recognize the various objects in a real-time scenario.","['Engineering', 'Computational Intelligence', 'User Interfaces and Human Computer Interaction', 'Data Structures and Information Theory', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/978-981-19-4703-2_8,en,Learning Optimal Time-Frequency Representations for Heart Sound: A Comparative Study,OriginalPaper,"Computer audition based methods have increasingly attracted efforts among the community of digital health. In particular, heart sound analysis can provide a non-invasive, real-time, and convenient (anywhere and anytime) solution for preliminary diagnosis and/or long-term monitoring of patients who are suffering from cardiovascular diseases. Nevertheless, extracting excellent time-frequency features from the heart sound is not an easy task. On the one hand, heart sound belongs to audio signals, which may be suitable to be analysed by classic audio/speech techniques. On the other hand, this kind of sound generated by our human body should contain some characteristics of physiological signals. To this end, we propose a comprehensive investigation on time-frequency methods for analysing the heart sound, i.e., short-time Fourier transformation, wavelet transformation, Hilbert-Huang transformation, and Log-Mel transformation. The time-frequency representations will be automatically learnt via pre-trained deep convolutional neural networks. Experimental results show that all the investigated methods can reach a mean accuracy higher than 60.0%. Moreover, we find that wavelet transformation can beat other methods by reaching the highest mean accuracy of 75.1% in recognising normal or abnormal heart sounds.","['Engineering', 'Signal, Image and Speech Processing', 'Engineering Acoustics', 'Mathematics in Music', 'Music']"
doi:10.1007/978-981-19-5184-8_9,en,Analyzing the Impact of COVID-19 and Vaccination Using Machine Learning and ANN,OriginalPaper,"The proposed method examines newly developed predicting models in-depth and forecasts the numerous cases of confirmed, recovered, and fatality caused through coronavirus in India. To improve the COVID-19 impact analysis, machine learning techniques such as decision tree, multiple linear model (MLR), random forest, Support Vector Machine algorithm (SVM), and Artificial neural network model were utilized for enhancing precision. With an 0.9992 R 2 score, the projected number of cases matches the actual numbers quite well. A follow-up on the vaccination and its effects is required for research and the development of new ways to protect us from the disease. Also, using XGBoost, the accuracy has been improvised. Importing the matplotlib package is used to visualize the COVID-19 data. Finally, before and after the vaccine, a performance analysis was implemented.","['Engineering', 'Computational Intelligence', 'Statistics, general', 'Artificial Intelligence', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-4863-3_28,en,Improving Streamflow Prediction Using Hybrid BPNN Model Combined with Particle Swarm Optimization,OriginalPaper,"Because of global climate change, sustainable water resource management faces a severe challenge. In this context, streamflow estimation is highly significant for managing different water resources schemes, such as water supply and reservoir scheduling. This work improves stability and accuracy of streamflow estimations using a hybrid model integrating backpropagation neural network with particle swarm optimisation (BPNN-PSO). Proposed models analysed historical monthly streamflow series of Panposh gauging station of Brahmani River, India. Performance of robust BPNN-PSO model is assessed based on Nash–Sutcliffe coefficient (N SE ) and root mean square error (RMSE) measures. Results show that hybrid neural network model significantly enhanced the accurateness of streamflow predictions with NSE-0.9886 and RMSE-0.364 compared to standalone neural network. This work indicates that projected robust model can capture nonlinear characteristics of streamflow process and provide more precise forecasting outcomes.","['Engineering', 'Computational Intelligence', 'Engineering Design', 'Machine Learning', 'Communications Engineering, Networks']"
doi:10.1007/978-3-031-18311-9_7,en,Spatiotemporal Prediction of Nitrogen Dioxide Based on Graph Neural Networks,OriginalPaper,"Air quality prediction, especially spatiotemporal prediction, is still a challenging issue. Considering the impact of numerous factors on air quality causes difficulties in integrating these factors in a spatiotemporal dimension and developing a model to make efficient predictions. At the same time, machine learning and deep learning development bring advanced approaches to addressing these challenges and propose novel solutions. The current work introduces one of the most advanced methods, an attention temporal graph convolutional network, which was implemented on datasets constructed by combining air quality, meteorological and traffic data on a spatiotemporal axis. The datasets were obtained from the city of Madrid for the periods January-June 2019 and January–June 2020. The evaluation metrics, the Root Mean Square Error and the Mean Absolute Error confirmed the proposed model’s advantages compared with long short-term memory (reference model). Particularly, it outperformed the latter method by 14.18% and 3.78%, respectively.","['Business and Management', 'IT in Business', 'Environmental Management', 'Geotechnical Engineering & Applied Earth Sciences', 'Energy Policy, Economics and Management', 'Artificial Intelligence']"
doi:10.1007/978-3-031-05445-7_6,en,On the Use of Cycle-Consistent Generative Adversarial Networks for Nonlinear Modal Analysis,OriginalPaper,"Linear modal analysis has been the major tool for analysis and design of structures. However, the method is restricted to structures with linear behaviour, and application of traditional methods in structures with nonlinearities yields results that do not typically have the desired characteristics of modal analysis. In the current work, a machine learning approach to performing nonlinear modal analysis is proposed. The idea is motivated by the Shaw–Pierre definition of nonlinear normal modes. The machine learning algorithm used is the cycle-consistent generative adversarial network (cycle-GAN). The algorithm provides a forward and inverse mapping between two spaces, which in the current application are the physical coordinate space and the modal space of the studied structures. Together with the cycle-GAN, an assembly of neural networks is used to tune the mappings so that they are angle-preserving (conformal) mappings; in this way, the orthogonality of the mode shapes is imposed during training. A criterion with a view to selecting the best model between the training epochs of the neural networks, based on the decomposition of the modes in the power spectral densities of the modal coordinates, is also introduced. The algorithm is tested on two simulated systems with cubic nonlinearities and different degrees of freedom. Moreover, it is tested on data recorded from an experimental structure, which has a harsh nonlinearity (impact nonlinearity). The results of the applications reveal that the algorithm is able to efficiently provide a decomposition of the modes in terms of the power spectral densities of the modal coordinates, provide an inverse mapping (from the modal space back to the natural coordinates), which is an essential part of modal analysis, and also provide modal coordinates that are statistically largely uncorrelated. The proposed approach seems to outperform previous approaches compared to both the decomposition provided and the definition of the inverse mapping.","['Engineering', 'Building Repair and Maintenance', 'Vibration, Dynamical Systems, Control', 'Fourier Analysis', 'Abstract Harmonic Analysis']"
doi:10.1007/978-981-19-6153-3_6,en,Language Models for Deep Learning Programming: A Case Study with Keras,OriginalPaper,"This chapter explores the application of language models to programming languages and our work in constructing a dataset for the task. More particularly, we focus on the Keras programming language, a popular framework for implementing Deep Learning experiments. Our original model KerasBERT has since been expanded by adding more data and re-training the language model. The original KerasBERT model was trained on two categories of Keras Code Examples and the Keras API reference. This chapter documents adding Keras GitHub Examples, Kaggle Notebook containing Keras Code, Medium articles describing how to use Keras, and StackOverflow questions regarding Keras. With these new data sources, we present new domain generalization analysis, as well as independent and identically distributed (i.i.d.) test set losses. We qualitatively evaluate how well KerasBERT learns the Keras Deep Learning framework through cloze test evaluation. We present miscellaneous properties of these cloze tests such as mask positioning and prompt paraphrasing. KerasBERT is an 80 million parameter RoBERTa model, which we compare to the Zero-Shot learning capability of the 6 billion parameter GPT-Neo model. We present a suite of cloze tests crafted from the Keras documentation to evaluate these models. We find some exciting completions that show KerasBERT is a promising direction for question answering and schema-free database querying. In this chapter, we document the reuse of KerasBERT and integration of additional data sources. We have tripled the size of the original data set and have identified five main sources of Keras information data. With these sources, we present new analyses of how models generalize to novel sources of data, such as a language model trained on Kaggle notebooks and tested on GitHub code. We conclude our work by discussing some future directions for KerasBERT and the development of language models for code documentation support.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Models of Cognitive Processes and Neural Networks']"
doi:10.1007/978-981-19-2535-1_40,en,Prediction of Age-Related Macular Degeneration (ARMD) Using Deep Learning,OriginalPaper,"The further proliferation of age-related eye diseases, mainly age-related macular degeneration (ARMD), is increasing the load on healthcare providers. Although ARMD does not lead to complete blindness, the disease can make it difficult for people to perform daily activities such as driving, reading, writing, cooking, etc. The unavailability of any cure for ARMD, necessitates timely actions of detecting the first symptoms of eye conditions as well as following appropriate treatment options to minimize further damage. Some of the current techniques used to detect and monitor ARMD include the Amsler’s Grid, Near Vision Chart, Optical Coherence Tomography (OCT), etc. which are generally performed on paper in hospitals or clinics. This proposed solution facilitates prediction of age-related macular degeneration in patients using data collected through a Mobile application. The proposed system includes the digitization of paper-based tests as well as a novel approach for prediction of ARMD through Deep Learning. The system eliminates the need to visit a clinic and can be used by citizens from home at their discretion. The high prediction accuracy obtained while real-time testing and prediction of ARMD validates the effectiveness of the proposed approach.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-3-031-22200-9_20,en,An Optimal Cascade Reservoir Operation Based on Multi-objective Water Cycle Algorithm,OriginalPaper,"Optimal cascade reservoir scheduling is a complex problem related to the broad interests of society, economy, and environment. This study proposes a solution for dispatching cascade reservoir operation optimization based on the multi-objective water cycle algorithm (MWCA). Search strategies for confluence, diversion, seepage, evaporation, and rainfall are established in the MWCA by simulating the natural water cycle process. A relative gravity mechanism under multi-objective is constructed to achieve an adequate search for optimal solutions. In the simulation section, the calculation results of the proposed approach are compared with the other methods in the literature, e.g., particle swarm optimization (MOPSO) and the genetic algorithm (NSGA-II). Compared results show that MWCA is superior to other algorithms in calculation diversity and an effective solution to the multi-objective optimal scheduling problem of cascade reservoir groups.","['Engineering', 'Mathematical and Computational Engineering', 'Mechanical Engineering', 'Electrical Engineering']"
doi:10.1007/978-981-19-3575-6_47,en,Analysis of Visual Descriptors for Detecting Image Forgery,OriginalPaper,"Dissemination of digital content over the Internet and social media sites has put forth major concerns about its authenticity and integrity. Specifically, the manipulation of images into their fake version has become common task. Identifying image forgery has become crucial in order to retain the confidence in numerous digital image processing applications. Our proposed method focuses on extracting multiple image attributes solitary and further using a deep ensemble approach to classify images as tampered or original content. In this paper, discriminative multi-queue classifier is fed with four different aspects of images having diversity in color space, edge details, texture, and moments. To achieve this target, different handcrafted image features were fed to the Convolutional Neural Network (CNN) model. Manipulated images are susceptible to duplication of image areas subjected to discrepancies in terms of underlying edge inconsistencies, change in contrast, or anomalies in texture. Despite the capability of CNN as a generic feature extractor to detect spatial patterns, we additionally extracted image characteristics to analyze images and detect various discrepancies. Trained CNN describes image splicing from a unified dataset from different approach and angles. The collaborative results of ensemble network outperformed single networks, thus improving the effectiveness and reliability of our approach with 92.8% accuracy and other performance parameters.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing']"
doi:10.1007/978-981-19-1412-6_1,en,IoT-Assisted Crop Monitoring Using Machine Learning Algorithms for Smart Farming,OriginalPaper,"Agriculture expansion is critical to the economic prosperity of any country. Agriculture employs more than 60% of the Indian population, either directly or indirectly. Nowadays, monitoring the crop is the challenging task in the world. In this article, data has been collected from various sensors to propose an IoT-assisted hybrid machine learning approach for obtaining an effective crop monitoring system. Crop monitoring system here means predicting as well as detecting diseases of crops. This study is about leveraging existing data and applying regression analysis, SVM, and decision tree to predict crop diseases in diverse crops such as rice, ragi, gram, potato, and onion. Among the applied methods, SVM outperforms regression, DT methods. The training and testing accuracy of Gram has 96.29% and 95.67%, respectively.","['Engineering', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security']"
doi:10.1007/978-3-031-14771-5_27,en,A Graph-based Imputation Method for Sparse Medical Records,OriginalPaper,"Electronic Medical Records (EHR) are extremely sparse. Only a small proportion of events (symptoms, diagnoses, and treatments) are observed in the lifetime of an individual. The high degree of missingness of EHR can be attributed to a large number of factors, including device failure, privacy concerns, or other unexpected reasons. Unfortunately, many traditional imputation methods are not well suited for highly sparse data and scale poorly to high dimensional datasets. In this paper, we propose a graph-based imputation method that is both robust to sparsity and to unreliable unmeasured events. Our approach compares favourably to several standard and state-of-the-art imputation methods in terms of performance and runtime. Moreover, results indicate that the model learns to embed different event types in a clinically meaningful way. Our work can facilitate the diagnosis of novel diseases based on the clinical history of past events, with the potential to increase our understanding of the landscape of comorbidities.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3796-5_4,en,Power Allocation Scheme Based on DRL for CF Massive MIMO Network with UAV,OriginalPaper,"Interference is a very critical issue, especially in future communication networks because of their complexity and ultra-dense nature. Also, the interference limits the performance of the communication network in terms of quality of service (QoS) and user net throughput. In order to address this issue, we proposed a cell-free massive MIMO system with an unmanned aerial vehicle (UAV) assist for the power optimisation of the network. A deep reinforcement learning (DRL)-based deep deterministic policy gradient (DDPG) algorithm is used for solving the power optimisation problems in a centralised fashion. Multiple neural networks make the learning approach less sophisticated for solving the power optimisation problem. Numerical results represents that the higher SE in results as compared to existing state-of-the-art approaches achieved with the proposed scheme.","['Engineering', 'Communications Engineering, Networks', 'Systems and Data Security', 'Artificial Intelligence', 'Software Engineering/Programming and Operating Systems', 'Computational Intelligence']"
doi:10.1007/978-981-19-1412-6_17,en,An Intelligent Iris Recognition Technique,OriginalPaper,"Biometrics are vital in security. Facial recognition, fingerprints, and iris recognition are all examples of computer vision biometrics. Unique authentication based on iris structure is one of the finest approaches for iris identification. This research provides an iris-based biometric identification system combining CNN and Softmax classifier. The system consists of picture augmentation by histogram equalization, image reduction by discrete wavelet transformation (DWT), segmentation by circular Hough transform and canny edge detector, and normalizing by Daugman's rubber-sheet model. Each picture is adjusted before being fed into the DenseNet201 model. The Softmax classifier then sorts the 224 IITD iris classes into 249 CASIA-Iris-Interval classes, 241 UBIRIS.v1 iris classes, and 898 CASIA-Iris-Thousand classes. The performance of our suggested system is determined by the setting of its deep networks and optimizers. In terms of accuracy, it exceeds existing approaches by 99%.","['Engineering', 'Cyber-physical systems, IoT', 'Professional Computing', 'Computational Intelligence', 'Artificial Intelligence', 'Systems and Data Security']"
doi:10.1007/978-3-031-12807-3_8,en,Explainable Machine Learning for Autonomous Vehicle Positioning Using SHAP,OriginalPaper,"Despite the recent advancements in Autonomous Vehicle (AV) technology, safety still remains a key challenge for their commercialisation and development. One of the major systems influencing the safety of AVs is its navigation system. Road localisation of autonomous vehicles is reliant on consistent accurate Global Navigation Satellite System (GNSS) positioning information. The GNSS relies on a number of satellites to perform triangulation and may experience signal loss around tall buildings, bridges, tunnels, trees, etc. We previously proposed the Wheel Odometry Neural Network (WhONet) as an approach to provide continuous positioning information in the absence of the GNSS signals. We achieved this by integrating the GNSS output with the wheel encoders’ measurements from the vehicle whilst also learning the uncertainties present in the position estimation. However, the positioning problem is a safety critical one and thus requires a qualitative assessment of the reasons for the predictions of the WhONet model at any point of use. There is therefore the need to provide explanations for the WhONet’s predictions to justify its reliability and thus provide a higher level of transparency and accountability to relevant stakeholders. Explainability in this work is achieved through the use of Shapley Additive exPlanations (SHAP) to examine the decision-making process of the WhONet model on an Inertial and Odometry Vehicle Navigation Benchmark Data subset describing an approximate straight-line trajectory. Our study shows that on an approximate straight-line motion, the two rear wheels are responsible for the most increase in the position uncertainty estimation error compared to the two front wheels.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-981-19-6379-7_15,en,Illustrious Implications of Nature-Inspired Computing Methods in Therapeutics and Computer-Aided Drug Design,OriginalPaper,"The Nature-inspired computing (NIC) techniques have been effectively applied to research pharmaceutical components and compounds. NIC includes problem-solving methods based on abstractions of natural processes and provides new ways to understand, model, and analyse natural complexity. These algorithms mimic biological systems to create new computation paradigms, such as swarm intelligence, neural networks, and evolutionary computing. Nowadays, the NIC algorithms are becoming very popular in solving complex optimisation in most academic and industrial fields, including drug design, development, therapeutics, molecular modelling, and peptide design. These algorithms work on a combinatorial approach for small molecules and compound designs that rely on the pharmacological properties of novel drug candidates. Over the last decade, NIIC techniques have been successfully applied in each drug discovery and development pipeline stage to overrule the obstacle of complex and big data from genomics, proteomics, microarray data, and clinical trials. This chapter summarised the recent applications of NIC methods in therapeutics and computer-aided drug design.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Cancer Research', 'Genetics and Genomics', 'Bioinformatics']"
doi:10.1007/978-3-031-14537-7_8,en,Global Optimisation for Point Cloud Registration with the Bees Algorithm,OriginalPaper,"The problem of 3D 3D registration entails the estimation of spatial transformation which best aligns two point sets. Iterative Closest Point Iterative Closest Point (ICP) is arguably the most popular and one of the most effective algorithms for 3D 3D registration at present. This algorithm uses singular value decomposition Decomposition to obtain a least squares alignment of two point sets. As a greedy alignment procedure, Iterative Closest Point Iterative Closest Point (ICP) is liable to converge to sub-optimal solutions. In this study, the problem of 3D 3D registration is addressed using the popular Bees Algorithm Bees Algorithm metaheuristics Metaheuristics . Thanks to its global search Global Search approach, the Bees Algorithm Bees Algorithm, THE is known to be highly impervious to sub-optimal convergence. To increase the efficiency of the search, singular value decomposition Decomposition is used to exploit the search results of the Bees Bees Algorithm. Experimental evidence showed that the proposed algorithm outperformed Iterative Closest Point Iterative Closest Point (ICP) in terms of consistency and precision and showed high robustness to noise Noise in the point sets.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Industrial and Production Engineering', 'Professional Computing', 'Robotics', 'Machine Learning']"
doi:10.1007/978-981-19-4606-6_68,en,An Adaptive Neuro-fuzzy-Based Methodology for Prediction of Surface Roughness in Wire Arc Additive Manufacturing,OriginalPaper,"Surface roughness is a critical design parameter that has been shown to have a major impact on qualities like wear resistance and fatigue strength. Wire arc additive manufacturing (WAAM) is a metal additive manufacturing technology that is gaining popularity in the aerospace and automotive industries due to its use in the maintenance and repair of key high-cost goods. As a result, in today’s manufacturing industry, surface roughness modeling and prediction of printed components are crucial. The purpose of this research is to develop an optimal neuro-fuzzy system that can represent surface roughness in the WAAM process using a set of input process variables such as welding speed, wire feed speed, and overlap ratio. During hybrid learning of the first-order Sugeno fuzzy system, three distinct membership functions, such as trapezoidal, bell, and Gaussian-shaped, were used to examine the accuracy in prediction of surface roughness by three membership functions. The experimental data were compared to the anticipated surface roughness values produced from the proposed adaptive neuro-fuzzy expert system. According to the results of the comparison research, the suggested technique may provide an ideal database and rule foundation for predicting deposition characteristics in the metal additive manufacturing process.","['Engineering', 'Industrial and Production Engineering', 'Machinery and Machine Elements', 'Materials Engineering']"
doi:10.1007/978-981-19-6347-6_5,en,"Development of an Energy Management System for a Microgrid Using Neural Networks. Case Study: San Cristobal Island, Galapagos Archipelago",OriginalPaper,"In this paper, an energy management system EMS hourly energy management system for a renewable energy system (HRES) is presented. The proposed HRES is composed of hybrid wind turbine (WT), solar photovoltaic (PV) panels, a diesel generator (DG) and a Distributed Collector System (DCS), as primary energy sources. In turn, an energy storage system (ESS), which is a battery sub-system. The wind turbine, PV panels and DCS system are made to work at peak power, while the battery acts as storage. The EMS uses intelligent rule-based controllers and optimizers to meet the energy demanded by the load and maintain the state of charge (SOC) of the battery between certain target margins, while trying to optimize the utilization cost and lifetime of the BESS. Simulation results show that optimization-based control meets the objectives set for the HRES EMS and achieves a total cost savings of 23.5% over other simpler control state-based EMSs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Health Informatics', 'Statistics, general']"
doi:10.1007/978-3-031-15900-8_15,en,Operations,OriginalPaper,"This chapter describes the main tasks, activities, and responsibilities of spacecraft operators during the various phases of a project. Starting with the early design phase of the space and ground segments, the need for a clear specification of requirements impacting operational activities is emphasised. This will ensure that flight operations as defined in the mission operations concept document can actually be performed. A further important contribution in the early design phase is mission analysis, as it helps to find the adequate operational orbit which enables the payload to optimise its output. The orbit geometry also impacts various areas of the satellite design. Examples are the sizing of solar panels and battery capacity which has to consider the length of eclipse periods, the required radiation hardening of electronic components, or the specification of transponder bandwidth and power to downlink the expected data volume with the available TT&C network. Once the space and ground segment have passed their design phases, they go into the development and deployment stage. In the meantime the operator needs to write all the operational products (e.g., the flight and ground operations procedures) and validate them prior to their use in flight. This can only be achieved with a representative ground segement environment that implements a satellite simulator. As long as the satellite is still located at the manufacturer’s premises or a test centre, a remote connection should be established to complement the procedure validation. Furthermore, the importance to form and structure the operations team is discussed which allows to organise the appropriate training in time. Examples for typical roles in a flight control team are given. The bulk of operational activities clearly occurs during flight and the main events between lift-off and initial service provision are described. This comprises the launch and early orbit, the in-orbit test, commissioning, routine, and disposal phases.","['Engineering', 'Aerospace Technology and Astronautics', 'Astronomy, Astrophysics and Cosmology', 'Communications Engineering, Networks', 'Management of Computing and Information Systems', 'Operations Management', 'Security Science and Technology']"
doi:10.1007/978-3-031-07254-3_11,en,Rapid Assessment of Offshore Monopile Fatigue Using Machine Learning,OriginalPaper,"Offshore wind turbine monopiles require structural health monitoring throughout their lifespan, yet direct structural measurements are limited. This paper combines numerical modeling and machine learning to present an approach to obtain rapid estimations of monopile fatigue using hourly metocean conditions. Aero-hydro-servo-elastic numerical simulations for a reference turbine provide the meta-model training dataset that encompasses wind-wave conditions applicable to the North Sea. Analysis reveals conditions whereby higher-order fully non-linear wave kinematics produce larger damage values compared to linear waves. This increase in damage is absent when implementing a simple probabilistic data lumping method. The prototype meta-model is developed based on convolutional neural networks to determine the monopile damage from measured wind-wave conditions at high temporal frequency. The proof-of-concept meta-model provides a step-change that demonstrates a promising approach to estimate monopile fatigue accumulation at high temporal resolution with scope for development to specific real-world offshore wind farms where validation data is available.","['Engineering', 'Building Repair and Maintenance', 'Cyber-physical systems, IoT', 'Industrial and Production Engineering', 'Monitoring/Environmental Analysis', 'Analytical Chemistry']"
doi:10.1007/978-981-19-2126-1_28,en,Video Anomaly Classification Using DenseNet Feature Extractor,OriginalPaper,"The proposed research study emphasises the idea of recognising abnormalities in surveillance videos, which are by far the most widely used source of anomaly data. The suggested methodology includes the development of an algorithm capable of detecting anomalies with better accuracy and precision than previously developed methodologies. The model has gone through stages of development. The development phases comprise extracting features from surveillance videos using pre-trained convolutional neural network (CNN) architecture, i.e. DenseNet121, from the transfer learning technique and training the model with appropriate hyperparameters to categorise the anomalies. We also used labels that are labelled at the frame level to classify the video. The model was trained, and hyperparameters were adjusted until the best results were obtained. The Area Under the ROC Curve (AUC) score obtained on the UCF crime dataset was 82.91%, which is better than many previously published state-of-art results. We also contrasted the AUC score data for several deep learning methodologies proposed by other researchers to demonstrate the competitiveness and efficiency of our methodology.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Machine Learning', 'Big Data', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/978-981-19-3951-8_6,en,A CNN-Based Model to Forecast the South African Unemployment Rate,OriginalPaper,"In the second quarter of 2021, the number of unemployed people in South Africa increased by 54,000 to 14.9 million. A key way in which governments address such a problem is through policies. A key input into these policies would be the unemployment rate forecast. Many researchers have started looking to deep learning to improve the accuracy of the forecast. In South Africa, LSTMs and GRUs have been employed in the same regard. This paper extends previous research conducted using deep learning models to forecast the unemployment rate in South Africa. It investigates whether CNNs offer a comparable performance to top-performing deep learning models. The comparison is limited to the LSTM model as it is consistently the top-performing model. We find that, on average, CNNs are 14.5% less accurate than LSTMs. Given that CNNs are generally used for image classification, this result shows that CNNs should not be considered as an effective way to forecast the South African unemployment rate.","['Engineering', 'Computational Intelligence', 'Signal, Image and Speech Processing', 'Communications Engineering, Networks', 'Artificial Intelligence']"
doi:10.1007/978-3-031-18050-7_56,en,"Feature Ranking for Feature Sorting and Feature Selection, and Feature Sorting: FR4(FSoFS)
              
                
              
              $$\wedge $$
              
                ∧
              
            FSo",OriginalPaper,"This paper introduces the application of feature ranking with a twofold purpose: first it achieves a feature sorting which becomes into a feature selection procedure given that a threshold is defined to only retain features with a positive influence with the class label (feature sorting and feature selection: FSoFS), second the feature subset is optimised using feature ranking to get a promising order of attributes (FSo). The supporting paper introduced a single stage: Feature ranking for feature sorting and feature selection, FR4(FS) $$^2$$ 2 with the shortcoming that for some high-dimensional classification problems the pre-processed data set did not obtain a more accurate classification model than the raw classifier. The results mean that it deserves to consider feature ranking in a couple of sequential stages with different goals: i) feature sorting, accompanied by feature selection, and ii) also alone","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-18050-7_52,en,Neural Networks Techniques for Fault Detection and Offset Prediction on Wind Turbines Sensors,OriginalPaper,"Digital Twins (DT) are one of the basis of Industry 4.0. DTs are used to simulate physical assets and improve the efficiency and decision making of industrial production. DT models are usually fitted with data collected from their physical counterparts through sensor readings. The data quality of the information retrieved by sensors is one main problem when training and retraining DT models. Poor data quality leads to low-accuracy DT predictions. In this study, a methodology is proposed for fault detection and offset error prediction problems related to retraining the DT of two wind turbine systems. Wind turbines are of utmost importance in Industry 4.0, as the use of renewable energy reduces production cost and benefits the environment. Having time series data sets with sensor readings of two real wind turbines, machine learning techniques based on Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) layers were implemented for multiple sensor fault forecasting. High-precision models were obtained and experiments were designed and performed to test the effectiveness of the proposed multi-fault detection system. The strengths and weaknesses of the approach are presented, which shows the relevance of this methodology for the DT retraining process.","['Engineering', 'Computational Intelligence', 'Industrial and Production Engineering']"
doi:10.1007/978-3-031-14771-5_4,en,Clinical Dialogue Transcription Error Correction Using Seq2Seq Models,OriginalPaper,"Good communication is critical to good healthcare. Clinical dialogue is a conversation between health practitioners and their patients, with the explicit goal of obtaining and sharing medical information. This information contributes to medical decision-making regarding the patient and plays a crucial role in their healthcare journey. The reliance on note taking and manual scribing processes are extremely inefficient and leads to manual transcription errors when digitizing notes. Automatic Speech Recognition (ASR) plays a significant role in speech-to-text applications, and can be directly used as a text generator in conversational applications. However, recording clinical dialogue presents a number of general and domain-specific challenges. In this paper, we present a seq2seq learning approach for ASR transcription error correction of clinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD) Dataset which was gathered by healthcare professionals from a NHS Inflammatory Bowel Disease clinic and use this in a comparative study with four commercial ASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on a mask-filling task using a domain-specific PubMed dataset which we have shared publicly for future research. The BART model fine-tuned for mask-filling was able to correct transcription errors and achieve lower word error rates for three out of four commercial ASR outputs.","['Engineering', 'Computational Intelligence', 'Biomedical Engineering and Bioengineering', 'Data Engineering', 'Artificial Intelligence']"
doi:10.1007/978-1-0716-2617-7_9,en,Designing a Model-Driven Approach Towards Rational Experimental Design in Bioprocess Optimization,OriginalPaper,"To enable a more rational optimization approach to drive the transition from lab-scale to large industrial bioprocesses, a systematic framework coupling both experimental design and integrated modeling was established to guide the workflow executed from small flask scale to bioreactor scale. The integrated model relies on the coupling of biotic cell factory kinetics to the abiotic bioreactor hydrodynamics to offer a rational means for an in-depth understanding of two-way spatiotemporal interactions between cell behaviors and environmental variations. This model could serve as a promising tool to inform experimental work with reduced efforts via full-factorial in silico predictions. This chapter thus describes the general workflow involved in designing and applying this modeling approach to drive the experimental design towards rational bioprocess optimization.","['Life Sciences', 'Bioinformatics']"
doi:10.1007/978-981-19-4556-4_14,en,Artificial Neural Network (ANN) for Parametric Appraisal and Milling Efficiency Evaluation of Carbon/Epoxy Nanocomposites,OriginalPaper,"The unique structure formation and features of carbon nano-onions (CNOs) make them quite exceptional from other carbon nanomaterials (CNMs). It is used to expand the electrical, thermal, and mechanical properties required for tribology functions. This article investigates the milling test of zero-dimensional CNO-reinforced epoxy composites using varying parameters, namely weight percentage of CNO (wt%), spindle speed ( N ), feed rate ( F ), and depth of cut ( D ). The Taguchi concept-based orthogonal array (OA) was used for milling experimentation of nanocomposites. The mathematical modeling of process parameters was done by the theory of artificial neural networks (ANNs). The ANOVA statistical observation elaborates that the feed rate is the prominent factor (30.56%) for the surface roughness ( R a ) alteration trailed by the depth of cut ( D ), spindle speed ( N ), CNO weight percentage (wt%). The ANN technique efficiently established the predictive model of the performed experiment. The predicted results of the performance output which are comparable with experimental data show satisfactory outcomes. The predictive modeling is considered to be highly accurate as the neural network structure (4–15–1) shows high-performance with R 2 (%) of 98.47, MSE of 2.1%, and APE of 2.08%. The model can be used for the control of quality and meeting productivity objectives in nanocomposites manufacturing.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Robotics and Automation']"
doi:10.1007/978-3-031-21333-5_108,en,Generation and Classification of Illicit Bitcoin Transactions,OriginalPaper,"Financial fraud is an everyday problem that banking institutions have to face. With the disruption of Bitcoin as a new model which relies on decentralisation and anonymity, attackers have taken advantage of this monetary system. It allows them to obtain funds from illegal activities such as ransomware payments and hide them. At the same time, Law Enforcement Agencies use open-source data to apply network forensics to Blockchain data. The analysis is usually performed by using artificial intelligence. Unfortunately, the current situation shows a scarcity of high-quality data sets to train the detection algorithms. This work tries to overcome this barrier with significant contributions. With nearly 25,000 illicit transactions, we have increased the Elliptic Data Set –the most extensive labelled transaction data publicly available in any cryptocurrency. The former data set only contained 4,545 illicit transactions, resulting in a class imbalance of 9.8:90.2 illicit/licit ratio. Our work has changed that to a 41.2:58.8 illicit/licit ratio. Besides, to show that class imbalance datasets can also be beaten with artificial work, we have studied the use of generative adversarial networks (GAN) for creating synthetic samples. Finally, the last part of this work was dedicated to applying deep learning and, more particularly, long short-term memory networks (LSTM) for the binary classification problem. We show ideal results that can help change the current state-of-the-art trend, mainly focused on machine learning algorithms.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2600-6_35,en,Implementation of Open Domain Question Answering System,OriginalPaper,"The development of an open domain question answering system that can automatically find and give the answer for any question in the given context is always an open challenge for the researcher. This paper discusses a web-based application that accepts a media file from the user and generates a span of context for the same. Then the users can pose any open domain question which gets mapped with the context and an appropriate answer is provided as output. SQuAD 2.0 dataset is used for training the model. The pre-trained models of ALBERT with different parameter sizes are implemented to perform the media file content context matching with the input question and the most optimized one is selected as an output. To measure the performance of the system, the Exact Matching (EM) score and F1 score are calculated.","['Engineering', 'Data Engineering', 'Statistics, general', 'Machine Learning', 'Artificial Intelligence', 'Data Storage Representation', 'Data Structures and Information Theory']"
doi:10.1007/978-3-031-13249-0_11,en,Reducing Bias for Evidence-Based Decision Making in Design,OriginalPaper,"This paper presents a strategy to help designers confronted with a massive flow of new technologies, exhaustive regulations, and design requirements. It summarises the existing proposals for AI-driven design development strategies, and lists frequent pitfalls like the focus on local optima or the lack of backpropagation. It identifies the main source of bias in generative design as a lack of detail and contextual complexity. The paper introduces an augmented AI process for preparing real-world data and its meta information to be used in design processes (BIM, GIS, external statistics including information such as rental price or spatial cognition). This evidence-based approach for deriving verifiable fitness functions presents a way to create holistic designs that reflect the complexity of today’s built environment by using a posteriori and unbiased statistical models to substitute existing speculative categorisations. Hence, it allows avoiding naïve and overfitted solutions, and it inverts the dominant paradigm of automated generation and manual curation.","['Engineering', 'Engineering Design', 'Manufacturing, Machines, Tools, Processes', 'Industrial Design', 'Interaction Design', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/978-981-19-5224-1_75,en,Classification of Pap Smear Image of Cervix Cell Using Machine Learning Techniques and Transfer Learning-Based Convolutional Neural Network Architecture and Scrutinizing Their Performances,OriginalPaper,"Different cervical pap smear cell categorization schemes have recently been presented, the majority of which were binary classifications of normal and abnormal cells. This paper presents the findings of a comprehensive investigation on machine learning and deep learning algorithms for binary and multi-class classification on pap smear images from the Herlev dataset. There are 917 photos in this collection, divided into seven normal and pathological categories. The Google Colab platform was used to generate models utilizing all of the techniques using scikit learn and the keras library from TensorFlow. To begin, several repetitions of processes such as feature importance selection, data normalization, standardization, PCA, T-SNE, and others have been imposed on models such as SVM and XGBoost in this work for machine learning approaches. Second, it was demonstrated in this work that a transfer learning-based CNN model from deep learning can outperform machine learning models in terms of binary and multi-class classifications. Furthermore, it was discovered in this work how computationally time efficient it is to apply a transfer learning model, which required roughly 25 min for 100 epochs. Finally, with several iterations of processes and outcomes, this work demonstrates that given enough data for a multi-class pap smear image classification system, the transfer learning CNN model has a higher potential to get the best results than the machine learning models used.","['Engineering', 'Communications Engineering, Networks', 'Statistics, general', 'Cyber-physical systems, IoT', 'Sociology, general', 'Professional Computing']"
doi:10.1007/978-981-19-2535-1_1,en,Comparative Analysis of Image Segmentation Techniques for Real Field Crop Images,OriginalPaper,"Nowadays various applications are available for plant disease identification using images. Early-stage disease identification can reduce losses and cost in cultivation. Efficient image segmentation is required to improve the performance of plant disease identification. Selecting appropriate segmentation techniques to extract an accurate object of interest while preserving original image properties is a challenging task. This paper presents the principle of image segmentation covering different techniques from traditional thresholding to the latest convolutional neural network-based approach. We reviewed the selected paper based on image segmentation and crop disease identification. Various algorithms are grouped based on the working principles like edge-based, region-based, and combining both properties. Performance evaluation of these algorithms was carried out using factors like time required, accuracy, and similarity to the original image. Holistic image segmentation based on convolutional neural network, K means clustering, etc. algorithms applied to real field crop images. The grab cut algorithm proves very useful for real field crop image segmentation as it preserves original image properties. Combining region and boundary-based techniques and automating segmentation need to be explored in future research work.","['Engineering', 'Communications Engineering, Networks', 'Cyber-physical systems, IoT', 'Professional Computing', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/978-981-19-2468-2_3,en,"Internet of Things and Cognitive Radio Networks: Applications, Challenges and Future",OriginalPaper,"Intelligent technology to cope with bandwidth shortage problems has arisen as cognitive radio (CR). The CR is supposed to utilise the unoccupied spectrum band if the approved person may not use it. Since this technology was introduced in 1999, comprehensive research has taken place, addressing diverse challenges including spectrum sensing, coordination among CR users and the applicability of CR networks. In this article, we introduce new applications of Internet of Things (IoT) CR technology and provide adequate solutions for the real CR technology problems that render IoT more accessible and more useful and also provide an overview of CR-based IoT systems. We illustrate feasible implementations of IoT systems focused on CR.","['Engineering', 'Electronics and Microelectronics, Instrumentation', 'Quantum Physics', 'Measurement Science and Instrumentation']"
doi:10.1007/978-3-031-21333-5_36,en,Domestic Violence Detection Using Smart Microphones,OriginalPaper,"Domestic violence between partners and family members is a worldwide problem increasing every day. As per academic studies and media articles, it escalated during the COVID-19 outbreak. Domestic violence can portray verbally and physically in several ways (for instance, between partners or against children and older people). Deep Learning (DL) combined with the Internet of Things (IoT) technology could support the detection of domestic violence, which is one of many societal issues. This paper describes a system that uses a Deep Learning model and smart microphones to identify domestic violence. The datasets used are from the Google AudioSet (GA) and from the Toronto Emotional Speech Set (TESS). For the training of the dataset, the system used spectrograms and MFCCs (Mel-Frequency Cepstral Coefficients). The system employs two approaches: (i) an ANN (Artificial Neural Network) model, and (ii) a ResNet model. The Resnet model obtained an accuracy of 71%. The ANN model, which brought an accuracy of 83%, was tested and loaded to a Raspberry Pi, i.e., connected to the microphone for audio recording. The recorded audio was fed to the trained model, classifying the audio, and alerting the domestic violence to relatives, friends, or volunteers registered with the system via e-mail. The designed system is compact and can be placed inside the home.","['Engineering', 'Data Engineering', 'Computational Intelligence', 'Artificial Intelligence']"
doi:10.1007/978-981-19-3391-2_27,en,Classification of Brain Tumor of Magnetic Resonance Images Using Convolutional Neural Network Approach,OriginalPaper,"The impact of brain tumors in medical field cannot be ignored and may lead to a short life in their highest grade. Thus, conduction of proper diagnosis that too in its early stage to improve the quality of life of patients is a necessity. Normally, several image processing techniques including computed tomography (CT) and magnetic resonance imaging (MRI) are being utilized to localize and calculate the size and tumor in a brain. But it has limited performance for accurate quantitative measurements that too in a small number of sample images. In this work, a simple yet robust classification using convolutional neural networks (CNN) for brain tumor is proposed. The investigational outcomes with low complication are anticipated and potentially compete the relevant state-of-the-art methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Statistics, general', 'Cyber-physical systems, IoT', 'Professional Computing', 'Communications Engineering, Networks']"
doi:10.1007/s40435-022-01091-4,en,Optimizing an adaptive fuzzy logic controller of a 3-DOF helicopter with a modified PSO algorithm,OriginalPaper,"This paper investigates the controller optimization for a helicopter system with three degrees of freedom (3-DOF). The system is extensively nonlinear and highly sensitive to the controller’s parameters, making it a real challenge to study these parameters’ effects on the controller’s performance. We combined fuzzy logic with adaptive control theory to control the system and used metaheuristic algorithms to determine these parameters. Then, we compare the results with the controller optimized through the standard PSO and PID controller. The results indicate the high ability of MPSO to perform the global search and to find a reasonable search space. The proposed method’s effectiveness and robustness properties are shown through computer simulations, while the system is subject to uncertainties and disturbance. We also prove the efficiency of the MPSO algorithm by comparing it with the standard PSO and six other well-known metaheuristic algorithms and analyzing the results by statistical tests.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1007/s00289-022-04630-3,en,Effectiveness of time and temperature on antioxidant activity and curcumin loss of the prepared nanodispersion using the subcritical water technique,"['OriginalPaper', 'ORIGINAL PAPER']","Curcumin as a bioactive component has antioxidant and antibacterial activity which can be used in various nonpolar food and pharmaceutical formulations. In this study, oil-in-water curcumin nanodispersions were prepared using a subcritical water technique. In order to investigate the antioxidant activity and curcumin loss of the prepared nanodispersions, response surface methodology was applied to assess influence of operating conditions including temperature (100–140 °C), and heating time (60–180 min). The results, however, indicated that nanodispersions with minimum curcumin loss (2.65%), maximum antioxidant activity (57.9%) and conductivity (0.36 ms/cm) were produced at optimal operation conditions of 123.03 °C and 112.12 min. The results also indicated that particle size, PDI and zeta-potential values of the provided nanodispersions at optimum conditions were 17.30 nm, 0.260, and  − 12.6 mV, respectively. In addition, loading ability and efficiency were 0.194 g/L, and 97.35%, respectively. Furthermore, high antibacterial activity against both Gram-positive and Gram-negative bacteria was achieved at optimum conditions.","['Chemistry', 'Polymer Sciences', 'Soft and Granular Matter, Complex Fluids and Microfluidics', 'Characterization and Evaluation of Materials', 'Physical Chemistry', 'Organic Chemistry']"
doi:10.1038/s41598-022-24343-x,en,Enhanced chimp optimization algorithm for high level synthesis of digital filters,"['OriginalPaper', 'Article']","The HLS of digital filters is a complex optimization task in electronic design automation that increases the level of abstraction for designing and scheming digital circuits. The complexity of this issue attracting the interest of the researcher and solution of this issue is a big challenge for the researcher. The scientists are trying to present the various most powerful methods for this issue, but keep in mind these methods could be trapped in the complex space of this problem due to own weaknesses. Due to shortcomings of these methods, we are trying to design a new framework with the mixture of the phases of the powerful approaches for high level synthesis of digital filters in this work. This modification has been done by merging the chimp optimizer with sine cosine functions. The sine cosine phases helped in enhancing the exploitation phase of the chimp optimizer and also ignored the local optima in the search area during the searching of new shortest paths. The algorithms have been applied on 23-standard test suites and 14-digital filters for verifying the performance of the algorithms. Experimental results of single and multi-objective functions have been compared in terms of best score, best maxima, average, standard deviation, execution time, occupied area and speed respectively. Furthermore, by analyzing the effectiveness of the proposed algorithm with the recent algorithms for the HLS digital filters design, this can be concluded that the proposed method dominates the other two methods in HLS digital filters design. Another prominent feature of the proposed system in addition to the stated enhancement, is its rapid runtime, lowest delay, occupied area and lowest power in achieving an appropriate response. This could greatly reduce the cost of systems with broad dimensions while increasing the design speed.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11356-022-24326-5,en,Predicting CO2 trapping in deep saline aquifers using optimized long short-term memory,"['OriginalPaper', 'Research Article']","A sustainable environment by decreasing fossil fuel utilization and anthropogenic greenhouse gases is a globally main goal due to climate change and serious air pollution. Carbon dioxide (CO2) is a heat-trapping (greenhouse) that is released into the earth’s atmosphere from natural processes, such as volcanic respiration and eruptions, as well as human activities, such as burning fossil fuels and deforestation. Due to this fact, underground carbon storage (UCS) is a promising technology to cut carbon emissions. However, there are some barriers to prevent UCS from applying globally. One of them is evaluating the feasibility of storage projects. Thus, the prediction accuracy of CO2 storage efficiencies may promote the attention of the community for UCS. In this study, we utilize the recent advances of swarm intelligence to develop a hybrid algorithm called AOSMA, employed to train the long short-term memory (LSTM). The developed swarm intelligence method (AOSMA) is an enhanced Aquila optimizer (AO) using the search mechanism of the slime mould algorithm (SMA). It is used to boost the prediction capability of the LSTM by optimizing its parameters. We considered two CO2 trapping indices, called residual trapping index (RTI) and solubility trapping index (STI). The evaluation experiments have shown that the AOSMA achieved significant results compared to the original AO and SMA and several swarm intelligence and optimization algorithms. The developed smart tools could use as a game changer to provide fast and accurate storage efficiency for projects that have similar parameters falling within the range of the database.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s13369-022-07466-1,en,A Modified Binary Rat Swarm Optimization Algorithm for Feature Selection in Arabic Sentiment Analysis,"['OriginalPaper', 'Research Article-computer Engineering and Computer Science']","This work focuses on feature selection in Arabic sentiment analysis (ASA) via a swarm-based approach. The importance of swarm intelligence methods comes from their optimization ability and implementation simplicity. The novel algorithm rat swarm optimization (RSO) proves its superiority in both exploitation and exploration capabilities allowing it to achieve good optimization results. A binary version of RSO (BRSO) is applied for the first time in this context. Despite the significant results achieved by BRSO, and like most optimization algorithms, it can entrap in the local optimum and slow convergence problems. A modified binary rat swarm optimization (M-BRSO) algorithm is proposed to overcome these limitations. On the one hand, the M-BRSO algorithm uses the opposite-based learning strategy in the initialization phase to improve the convergence rate of the population. On the other hand, the private thinking mechanism is enhanced by replacing the current rat’s position with its personal best ( P best ) found so far to prevent it from trapping in the local optimum. Four criteria were adopted to measure the performance of the proposed algorithms: fitness value, classification accuracy, percentage of selected features, and elapsed time. The application of the proposed algorithms on different benchmark datasets for ASA gives promising results regarding reducing the feature space with important accuracy compared to the state-of-the-art swarm-based algorithms.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s40123-022-00621-9,en,Efficacy of a Deep Learning System for Screening Myopic Maculopathy Based on Color Fundus Photographs,"['OriginalPaper', 'ORIGINAL RESEARCH']","Introduction The maculopathy in highly myopic eyes is complex. Its clinical diagnosis is a huge workload and subjective. To simply and quickly classify pathologic myopia (PM), a deep learning algorithm was developed and assessed to screen myopic maculopathy lesions based on color fundus photographs. Methods This study included 10,347 ocular fundus photographs from 7606 participants. Of these photographs, 8210 were used for training and validation, and 2137 for external testing. A deep learning algorithm was trained, validated, and externally tested to screen myopic maculopathy which was classified into four categories: normal or mild tessellated fundus, severe tessellated fundus, early-stage PM, and advanced-stage PM. The area under the precision–recall curve, the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, accuracy, and Cohen’s kappa were calculated and compared with those of retina specialists. Results In the validation data set, the model detected normal or mild tessellated fundus, severe tessellated fundus, early-stage PM, and advanced-stage PM with AUCs of 0.98, 0.95, 0.99, and 1.00, respectively; while in the external-testing data set of 2137 photographs, the model had AUCs of 0.99, 0.96, 0.98, and 1.00, respectively. Conclusions We developed a deep learning model for detection and classification of myopic maculopathy based on fundus photographs. Our model achieved high sensitivities, specificities, and reliable Cohen’s kappa, compared with those of attending ophthalmologists.","['Medicine & Public Health', 'Internal Medicine', 'Ophthalmology']"
doi:10.1038/s41598-022-26026-z,en,Developing an integrated approach based on geographic object-based image analysis and convolutional neural network for volcanic and glacial landforms mapping,"['OriginalPaper', 'Article']","Rapid detection and mapping of landforms are crucially important to improve our understanding of past and presently active processes across the earth, especially, in complex and dynamic volcanoes. Traditional landform modeling approaches are labor-intensive and time-consuming. In recent years, landform mapping has increasingly been digitized. This study conducted an in-depth analysis of convolutional neural networks (CNN) in combination with geographic object-based image analysis (GEOBIA), for mapping volcanic and glacial landforms. Sentinel-2 image, as well as predisposing variables (DEM and its derivatives, e.g., slope, aspect, curvature and flow accumulation), were segmented using a multi-resolution segmentation algorithm, and relevant features were selected to define segmentation scales for each landform category. A set of object-based features was developed based on spectral (e.g., brightness), geometrical (e.g., shape index), and textural (grey level co-occurrence matrix) information. The landform modelling networks were then trained and tested based on labelled objects generated using GEOBIA and ground control points. Our results show that an integrated approach of GEOBIA and CNN achieved an ACC of 0.9685, 0.9780, 0.9614, 0.9767, 0.9675, 0.9718, 0.9600, and 0.9778 for dacite lava, caldera, andesite lava, volcanic cone, volcanic tuff, glacial circus, glacial valley, and suspended valley, respectively. The quantitative evaluation shows the highest performance (Accuracy > 0.9600 and cross-validation accuracy > 0.9400) for volcanic and glacial landforms and; therefore, is recommended for regional and large-scale landform mapping. Our results and the provided automatic workflow emphasize the potential of integrated GEOBIA and CNN for fast and efficient landform mapping as a first step in the earth’s surface management.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40747-022-00933-0,en,Two-stage hybrid algorithm for recognition of industrial slab numbers with data quality improvement,"['OriginalPaper', 'Original Article']","As the unique recognition of each slab, the accurate recognition of slab number is especially critical for the hot rolling production process. However, the collected data are often of low quality due to poor production environment conditions, making traditional deep learning algorithms face more significant challenges in slab numbers recognition. In this paper, a two-stage hybrid algorithm based on convolutional neural network and Transformer is proposed to identify industrial slab numbers. In the first stage, an improved CycleGAN (HybridCy) is developed to enhance the quality of real-world unpaired data. In the second stage, a multi-scale hybrid vision transformer model (MSHy-Vit) is proposed to identify slab numbers of the improved data output of stage one. The experimental results on industrial slab data show that HybridCy exhibits stable and efficient performance. Even for low-quality data with severe geometric distortion, HybridCy can accomplish quality improvement, which can help to improve recognition accuracy. In addition, the MSHy-Vit achieves superior accuracy in the recognition of slab numbers in comparison to existing methods in the literature.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s42235-022-00309-7,en,Design of Deep Reinforcement Learning Controller Through Data-assisted Model for Robotic Fish Speed Tracking,"['OriginalPaper', 'Research Article']","It is common for robotic fish to generate thrust using reactive force generated by the tail’s physical motion, which interacts with the surrounding fluid. The coupling effect of the body strongly correlates with this thrust. However, hydrodynamics cannot be wholly modeled in analytical form. Therefore, data-assisted modeling is necessary for robotic fish. This work presents the first method of its kind using Genetic Algorithm (GA)-based optimization methods for data-assistive modeling for robotic fish applications. To begin, experimental data are collected in real time with the robotic fish that has been designed and fabricated using 3D printing. Then, the model’s influential parameters are estimated using an optimization problem. Further, a model-based deep reinforcement learning (DRL) controller is proposed to track the desired speed through extensive simulation work. In addition to a deep deterministic policy gradient (DDPG), a twin delayed DDPG (TD3) is employed in the training of the RL agent. Unfortunately, due to its local optimization problem, the RL-DDPG controller failed to perform well during training. In contrast, the RL-TD3 controller effectively learns the control policies and overcomes the local optima problem. As a final step, controller performance is evaluated under different disturbance conditions. In contrast to DDPG and GA-tuned proportional-integral controllers, the proposed model with RL-TD3 controller significantly improves the performance.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s11263-022-01695-5,en,Automatic Modelling for Interactive Action Assessment,OriginalPaper,"Action assessment, the task of visually assessing the quality of performing an action, has attracted much attention in recent years, with promising applications in areas such as medical treatment and sporting events. However, most existing methods of action assessment mainly target the actions performed by a single person; in particular, they neglect the asymmetric relations among agents (e.g., between persons and objects), limiting their performance in many nonindividual actions. In this work, we formulate a framework for modelling asymmetric interactions among agents for action assessment, considering the subordinations among agents in many interactive actions. Specifically, we propose an asymmetric interaction learner consisting of an automatic assigner and an asymmetric interaction network search module. The automatic assigner is designed to automatically group agents within an action into a primary agent (e.g., human) and secondary agents (e.g., objects); the asymmetric interaction network search module adaptively learns the asymmetric interactions between these agents. We conduct experiments on the JIGSAWS dataset containing surgical actions and additionally collect two new datasets, TASD-2 and PaSk , for action assessment on interactive sporting actions. The experimental results on these three datasets demonstrate the effectiveness of our framework in achieving state-of-the-art performance. The extensive experiments on the AQA-7 dataset also indicate the robustness of our model in conventional action assessment settings.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s10489-022-04327-0,en,Rapid extraction of skin physiological parameters from hyperspectral images using machine learning,OriginalPaper,"Noninvasive assessment of skin structure using hyperspectral images has been intensively studied in recent years. Due to the high computational cost of the classical methods, such as the inverse Monte Carlo (IMC), much research has been done with the aim of using machine learning (ML) methods to reduce the time required for estimating parameters. This study aims to evaluate the accuracy and the estimation speed of the ML methods for this purpose and compare them to the traditionally used inverse adding-doubling (IAD) algorithm. We trained three models – an artificial neural network (ANN), a 1D convolutional neural network (CNN), and a random forests (RF) model – to predict seven skin parameters. The models were trained on simulated data computed using the adding-doubling algorithm. To improve predictive performance, we introduced a stacked dynamic weighting (SDW) model combining the predictions of all three individually trained models. SDW model was trained by using only a handful of real-world spectra on top of the ANN, CNN and RF models that were trained using simulated data. Models were evaluated based on the estimated parameters’ mean absolute error (MAE), considering the surface inclination angle and comparing skin spectra with spectra fitted by the IAD algorithm. On simulated data, the lowest MAE was achieved by the RF model (0.0030), while the SDW model achieved the lowest MAE on in vivo measured spectra (0.0113). The shortest time to estimate parameters for a single spectrum was 93.05 μ s. Results suggest that ML algorithms can produce accurate estimates of human skin optical parameters in near real-time.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-14276-y,en,Texture classification for visual data using transfer learning,OriginalPaper,"The texture is the most fundamental aspect of a picture that contributes to its recognition. Computer vision challenges such as picture identification and segmentation are built on the foundation of texture analysis. Various images of satellite, forestry, medical etc. have been identifiable because of textures. This work aims to offer texture classification models that will outperform previously presented methods. In this work, transfer learning was applied to attain this goal. MobileNetV3 and InceptionV3 are the two pre-trained models employed. Brodatz, Kylberg, and Outex texture datasets were used to evaluate the models. The models achieved excellent results and achieved the objective in most cases. Classification accuracy obtained for the Kylberg dataset were 100% and 99.89%. For the Brodatz dataset, the classification accuracy obtained was 99.83% and 99.94%. For the Outex datasets, the classification accuracy obtained was 99.48% and 99.48%. The model outputs the corresponding label of the texture of the image.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1186/s13677-022-00359-6,en,Efficient 3D object recognition in mobile edge environment,"['OriginalPaper', 'Research']","3D object recognition has great research and application value in the fields of automatic drive, virtual reality, and commercial manufacturing. Although various deep models have been exploited and achieved remarkable results for 3D object recognition, their computational cost is too high for most mobile applications. This paper combines edge computing and 3D object recognition into a powerful and efficient framework. It consists of a cloud-based rendering stage and a terminal-based recognition stage. In the first stage, inspired by the cloud-based rendering technique, we upload the 3D object data from the mobile device to the edge cloud server for multi-view rendering. The rendering stage utilizes the powerful computing resource in the edge cloud server to generate multiple view images of the given 3D object from different views by parallel high-quality rendering. During the terminal-based recognition stage, we integrate a lightweight CNN architecture and a neural network quantization technique into a 3D object recognition model based on the multiple images rendered in the edge cloud server, which can be executed fast in the mobile device. To reduce the cost of network training, we propose a novel semi-supervised 3D deep learning method with fewer labeled samples. Experiments demonstrate that our method achieves competitive performance compared to the state-of-the-art methods with low latency running in the mobile edge environment.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1038/s41598-022-25671-8,en,A novel fast method for identifying the origin of Maojian using NIR spectroscopy with deep learning algorithms,"['OriginalPaper', 'Article']","Maojian is one of China’s traditional famous teas. There are many Maojian-producing areas in China. Because of different producing areas and production processes, different Maojian have different market prices. Many merchants will mix Maojian in different regions for profit, seriously disrupting the healthy tea market. Due to the similar appearance of Maojian produced in different regions, it is impossible to make a quick and objective distinction. It often requires experienced experts to identify them through multiple steps. Therefore, it is of great significance to develop a rapid and accurate method to identify different regions of Maojian to promote the standardization of the Maojian market and the development of detection technology. In this study, we propose a new method based on Near infra-red (NIR) with deep learning algorithms to distinguish different origins of Maojian. In this experiment, the NIR spectral data of Maojian from different origins are combined with the back propagation neural network (BPNN), improved AlexNet, and improved RepSet models for classification. Among them, improved RepSet has the highest accuracy of 99.30%, which is 8.67% and 0.70% higher than BPNN and improved AlexNet, respectively. The overall results show that it is feasible to use NIR and deep learning methods to quickly and accurately identify Maojian from different origins and prove an effective alternative method to discriminate different origins of Maojian.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-08058-8,en,A penalty-based algorithm proposal for engineering optimization problems,"['OriginalPaper', 'Original Article']","This paper presents a population-based evolutionary computation model for solving continuous constrained nonlinear optimization problems. The primary goal is achieving better solutions in a specific problem type, regardless of metaphors and similarities. The proposed algorithm assumes that candidate solutions interact with each other to have better fitness values. The interaction between candidate solutions is limited with the closest neighbors by considering the Euclidean distance. Furthermore, Tabu Search Algorithm and Elitism selection approach inspire the memory usage of the proposed algorithm. Besides, this algorithm is structured on the principle of the multiplicative penalty approach that considers satisfaction rates, the total deviations of constraints, and the objective function value to handle continuous constrained problems very well. The performance of the algorithm is evaluated with real-world engineering design optimization benchmark problems that belong to the most used cases by evolutionary optimization researchers. Experimental results show that the proposed algorithm produces satisfactory results compared to the other algorithms published in the literature. The primary purpose of this study is to provide an algorithm that reaches the best-known solution values rather than duplicating existing algorithms through a new metaphor. We constructed the proposed algorithm with the best combination of features to achieve better solutions. Different from similar algorithms, constrained engineering problems are handled in this study. Thus, it aims to prove that the proposed algorithm gives better results than similar algorithms and other algorithms developed in the literature.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11831-022-09850-4,en,Efficient Initialization Methods for Population-Based Metaheuristic Algorithms: A Comparative Study,"['ReviewPaper', 'Review article']","The size, nature, and diversity of the initial population of population-based metaheuristic algorithms and the number of times the algorithm iterates play a significant role in the performance of the algorithms. In this paper, we presented a comprehensive comparison of the effect of population size, the maximum number of iterations, and eleven (11) different initialization methods on the convergence and accuracy of ten (10) population-based metaheuristic optimizers: Bat algorithm (BA), Grey Wolf optimizer (GWO), Butterfly optimization algorithm (BOA), Whale optimization algorithm (WOA), Moth Flame optimization (MFO), Harris Hawks optimization (HHO), Moth search (MS), Elephant Herding optimization (EHO), linear population size reduction success-history based adaptive DE (LSHADE), and covariance matrix learning with Euclidean neighborhood ensemble sinusoidal LSHADE (LSHADE-cnEpSin). The possible effect of these initialization schemes was tested on ten (10) different classical and ten (10) CEC2020 test functions with different properties and modalities. The simulation results and exhaustive statistical analysis show that for classical functions considered, BA, EHO, WOA, MFO, HHO, and MS are sensitive to the initialization schemes, whereas GWO, BOA, LSHADE, and LSHADE_cnEpSin are not. For CEC2020 test functions, BA and GWO are sensitive to the initialization schemes, whereas BOA, WOA, MFO, HHO, EHO, MS, LSHADE, and LSHADE_cnEpSin are not. The modified BA showed sensitivity for both classical and CEC2020 functions, which confirms that the diversity and nature of the initial population play a role in the algorithm's performance. The sensitivity of the algorithms is also problem-dependent, meaning some functions were insensitive to the initialization schemes. For example, for those algorithms that showed sensitivity to the initialization schemes, only between 70 and 83% of the functions considered are sensitive to those schemes whereas, 37–45% of the functions showed sensitivity for those overall insensitive algorithms to the initialization schemes. The population size and number of iterations also play a role in the performance of the algorithms. We found out that BA performed better with larger population sizes. GWO, WOA, BOA, MS, and LSHADE_cnEpSin performed better when the number of iterations is larger. MFO, LSHADE, EHO, and HHO perform optimally when the population size and the number of iterations are relatively even. This conclusion is heavily dependant on the problem dimension; however, we believe that good population diversity and the number of iterations will most likely lead to optimal solutions.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s11600-022-00988-0,en,Calibration of conceptual rainfall-runoff models by selected differential evolution and particle swarm optimization variants,"['OriginalPaper', 'Research Article - Hydrology']","The performance of conceptual catchment runoff models may highly depend on the specific choice of calibration methods made by the user. Particle Swarm Optimization (PSO) and Differential Evolution (DE) are two well-known families of Evolutionary Algorithms that are widely used for calibration of hydrological and environmental models. In the present paper, five DE and five PSO optimization algorithms are compared regarding calibration of two conceptual models, namely the Swedish HBV model (Hydrologiska Byrans Vattenavdelning model) and the French GR4J model (modèle du Génie Rural à 4 paramètres Journalier) of the Kamienna catchment runoff. This catchment is located in the middle part of Poland. The main goal of the study was to find out whether DE or PSO algorithms would be better suited for calibration of conceptual rainfall-runoff models. In general, four out of five DE algorithms perform better than four out of five PSO methods, at least for the calibration data. However, one DE algorithm constantly performs very poorly, while one PSO algorithm is among the best optimizers. Large differences are observed between results obtained for calibration and validation data sets. Differences between optimization algorithms are lower for the GR4J than for the HBV model, probably because GR4J has fewer parameters to optimize than HBV.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1186/s44147-022-00161-w,en,Support vector regression-bald eagle search optimizer-based hybrid approach for short-term wind power forecasting,"['OriginalPaper', 'Research']","Wind power forecasting deals with the prediction of the expected generation of wind farms in the next few minutes, hours, or days. The application of machine learning techniques in wind power forecasting has become of great interest due to their superior capability to perform regression, classification, and clustering. Support vector regression (SVR) is a powerful and suitable forecasting tool that has been successfully used for wind power forecasting. However, the performance of the SVR model is extremely dependent on the optimal selection of its hyper-parameters. In this paper, a novel forecast model based on hybrid SVR and bald eagle search (BES) is proposed for short-term wind power forecasting. In the proposed model, the BES algorithm, which is characterized by a few adjustable parameters, a simplified search mechanism, and accurate results, is used to enhance the accuracy of the forecasted output by optimizing the hyper-parameters of the SVR model. To evaluate the performance of the developed wind power forecaster, a case study has been conducted on real wind power data from Sotavento Galicia in Spain. The developed model is compared to other forecasting techniques such as decision tree (DT), random forest (RF), traditional SVR, hybrid SVR, and gray wolf optimization algorithm (SVR–GWO) and hybrid SVR and manta ray foraging optimizer (SVR–MRFO). Obtained results uncovered that the proposed hybrid SVR−BES is more accurate than other methods.","['Engineering', 'Engineering, general']"
doi:10.1007/s11831-022-09859-9,en,A Review on Constraint Handling Techniques for Population-based Algorithms: from single-objective to multi-objective optimization,"['ReviewPaper', 'Review Article']","Most real-world problems involve some type of optimization problems that are often constrained. Numerous researchers have investigated several techniques to deal with constrained single-objective and multi-objective evolutionary optimization in many fields, including theory and application. This presented study provides a novel analysis of scholarly literature on constraint-handling techniques for single-objective and multi-objective population-based algorithms according to the most relevant journals and articles. As a contribution to this study, the paper reviews the main ideas of the most state-of-the-art constraint handling techniques in population-based optimization, and then the study addresses the bibliometric analysis, with a focus on multi-objective, in the field. The extracted papers include research articles, reviews, book/book chapters, and conference papers published between 2000 and 2021 for analysis. The results indicate that the constraint-handling techniques for multi-objective optimization have received much less attention compared with single-objective optimization. The most promising algorithms for such optimization were determined to be genetic algorithms, differential evolutionary algorithms, and particle swarm intelligence. Additionally, “Engineering,” “Computer Science,” and “ Mathematics” were identified as the top three research fields in which future research work is anticipated to increase.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s11042-022-14180-5,en,Adaptive Weiner filtering with AR-GWO based optimized fuzzy wavelet neural network for enhanced speech enhancement,OriginalPaper,"Speech signal enhancement is a subject of study in which a large number of researchers are working to improve the quality and perceptibility of speech signals. In the existing Kalman Filter method, the short-time magnitude or power spectrum due to random variations of noise was a serious problem and the signal-to-noise ratio was very low. This issue severely reduced the perceived qualityand intelligibility of enhanced speech. Thus, this paper intent to develop an improved speech enhancement model and it includes“training phase and testing phase”. In the training phase, the input noise corrupted signal is initially fed as input to both STFT-based noise estimation and NMF-based spectrum estimation forestimating the noise spectrum and signal spectrum, respectively. The obtained noise spectrum and the signal spectrum are fed as input to the Wiener filter and these filtered signals are subjected to Empirical Mean Decomposition (EMD).Since, tuning factor η plays a key role in Wiener filter, it has to be determined for each signal and from the denoised signal the bark frequency is evaluated. The computed bark frequency is fed as input to the learning algorithm referred as Fuzzy Wavelet Neural Network (FW-NN)for detecting the suited tuning factor η for the entire input signal in Wiener filter.An Adaptive Randomized Grey Wolf Optimization (AR-GWO) is proposed for proper tuning of the tuning factor η referred as tuned tuning factor ( η tuned ). The proposed AR-GWO is the improved version of standard Grey wolf optimization (GWO). In the testing phase, the training is accomplished initially and from which the tuning factor is gathered for each of the relevant input signal. Then, the properly tuned tuning factor ( η tuned ) from FW-NN is fed as input to EMD via adaptive wiener filter for decomposing the spectral signal and the output of EMD is denoised enhanced speech signal. At last, the performance of the adopted approach is evaluated to the existing approaches in terms of various metrics. In particular, the computation time of the adopted AR-GWO model is 34.07%, 43.57%, 28.86%, 38.88%, and 16.03% better than the existing GA, ABC, PSO, FF, and GWO approaches respectively.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-08121-4,en,Dummy trajectory generation scheme based on generative adversarial networks,"['OriginalPaper', 'Original Article']","Dummy trajectory is widely used to protect the privacy of mobile users’ locations. However, two main challenges remain: (1) Map background information has not been modeled by machine learning methods in existing schemes, and (2) it is difficult to generate a good quality dummy trajectory that is similar to the real one. Focused on these two challenges, in this paper, we propose a dummy trajectory generation scheme with conditional generative adversary network (GAN), where the map features are extracted using convolutional neural network, which is regarded as a prior restriction of conditional GAN. Then, the movement pattern of the real trajectory is deduced by an auto-encoder and is involved in the dummy trajectory generation. Our model is trained and evaluated with two real-world datasets. Experimental results demonstrate that our scheme addresses these challenges well and defends against various attacks effectively.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-08074-8,en,Feasibility of a novel predictive model based on multilayer perceptron optimized with Harris hawk optimization for estimating of the longitudinal dispersion coefficient in rivers,"['OriginalPaper', 'Original Article']","Protecting water resources from pollution is one of the most important challenges facing water management researchers. The governing equation for river pollution is mostly the advection–dispersion equation, with considering the longitudinal dispersion coefficient as its most important effective parameter. The purpose of this paper is to develop a new framework for accurate prediction of the longitudinal dispersion coefficient of rivers based on artificial intelligence (AI) methods. To do this, we used a combination of multilayer perceptron (MLP), one of the most robust neural networks, and a novel metaheuristic algorithm, namely Harris hawk optimization (HHO). Besides, two optimized MLP models with particle swarm optimization (PSO) and imperialist competitive algorithm (ICA) were utilized to demonstrate the accuracy of the proposed model. To evaluate the developed models, 164 series of data collected from previous studies, including hydraulic and geometric parameters of rivers, were used. The indicated results proved the efficiency of the HHO to improve the optimum auto-selection of the AI models. Thus, the recorded results show very high accuracy of the newly developed model, MLP-HHO compared to others. Furthermore, to increase the prediction accuracy, a K-means clustering technique is coupled with MLP-HHO model during dividing the data to train and test categories. The proposed hybrid K-means-MLP-HHO model with coefficient of determination ( R 2 ) and root mean square error (RMSE), of 0.97 and 30.94 m 2 /s, respectively, significantly outperformed all existing and AI-based models. Furthermore, the sensitivity analysis showed that the flow width is the most influential factor in predicting the longitudinal dispersion coefficient.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11042-022-14205-z,en,A hybrid SUGWO optimization for partial face recognition with new similarity index,OriginalPaper,"This paper introduces a Partial Face Recognition (PFR) method with the benefits of optimization logic using an optimized feature matching aspect. Besides, for better recognition, the Sparse Representation Classification (SRC) and Fully Convolutional Network (FCN) have been combined. As a novelty, this work aims to tune the sparse coefficient of Dynamic Feature Matching (DFM) optimally, in which the reconstruction error should be minimal. Also, this work presents the structural similarity index measure to calculate the similarity scores between the gallery sub-feature map and probe feature map. For optimization purposes, this work deploys a proposed Sealion Updated Grey Wolf Optimization (SUGWO) algorithm. Finally, the proposed method is executed over the traditional methods concerning certain measures.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s13349-022-00660-7,en,Intelligent identification and classification of sewer pipeline network defects based on improved RegNetY network,"['OriginalPaper', 'Original Paper']","The operation of a sewer pipeline network affects the urban drainage effect, and identifying the defects of the pipe network is very significant to accurately evaluate the condition of the pipeline network. Closed-circuit television detection has been widely used in urban sewer pipeline network inspection. However, manual discrimination requires professionals to have enough knowledge reserves, which is subjective, laborious, and time-consuming. It is necessary to realize automation and intelligence of defect classification of the sewer pipeline network. An intelligent identification model for sewer pipeline network defects is constructed based on an improved RegNetY network and gradient-weighted class activation mapping (Grad-CAM). The defects are classified and visualized. First, data augmentation is used to deal with data imbalance. Then, LeakyReLU is used as the activation function to improve the RegNetY network. Finally, the training features of the improved RegNetY network are visualized using Grad-CAM. The engineering application results show that the improved RegNetY is more competitive than other types of deep neural networks in defect classification, which can improve the inspection efficiency of engineers. The Grad-CAM can visualize the specific location of defects, which is beneficial to understanding the defect information in images intuitively.","['Engineering', 'Civil Engineering', 'Measurement Science and Instrumentation', 'Vibration, Dynamical Systems, Control']"
doi:10.1038/s41598-022-23546-6,en,Autonomous driving using imitation learning with look ahead point for semi structured environments,"['OriginalPaper', 'Article']","Semi-structured environments are difficult for autonomous driving because there are numerous unknown obstacles in drivable area without lanes, and its width and curvature considerably change. In such environments, searching for a path on a real-time is difficult, and localization data are inaccurate, reducing path tracking accuracy. Instead, alternative methods that reactively avoid obstacles in real-time using candidate paths or an artificial potential field have been studied. However, these require heuristics to identify specific parameters for handling various environments and are vulnerable to inaccurate input data. To address these limitations, this study proposes a method in which a vehicle drives toward drivable area using vision and deep learning. The proposed imitation learning method learns the look-ahead point where the vehicle should reach on a vision-based occupancy grid map to obtain a safe policy with a clear state action pattern relationship. Furthermore, using this point, the data aggregation (DAgger) algorithm with weighted loss function is proposed, which imitates expert behavior more accurately, especially in unsafe or near-collision situations. Experimental results in actual semi-structured environments demonstrated the limitations of general model-based methods and the effectiveness of the proposed imitation learning method. Moreover, simulation experiments showed that DAgger with the weight obtains a safer policy than existing DAgger algorithms.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07716-2,en,A prediction model of stock market trading actions using generative adversarial network and piecewise linear representation approaches,"['OriginalPaper', 'Application of Soft Computing']","Supervised learning applied stock prediction tasks and obtained satisfactory performance. The trading strategies are very complex and diverse but supervised learning is only learned and fitted by gold standard trading strategies. Supervised learning approaches often have over-fitting problems. To learn distribution of gold standard answers, the generative adversarial network (GAN) models can generate extra similar samples to improve performance. Therefore, the paper proposes a generative GAN-based frameworks with the piecewise linear representation (PLR) approach to learn three trading actions, namely buying, selling, and holding. The proposed framework consists of two parts: first, PLR approach uses to detect historical prices to form trading sequences with three actions, PLR can provide a guided trading strategy to discriminator of GAN. Second, the generator of GAN is used to generate/predict daily trading actions, and the discriminator is used to detect the real/fake trading actions from the PLR/generator of GAN. Experimental results indicate that the proposed GAN-based frameworks outperform the long short-term memory network.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10586-022-03814-w,en,Artificial neural networks based predictions towards the auto-tuning and optimization of parallel IO bandwidth in HPC system,OriginalPaper,"Super-computing or HPC clusters are built to provide services to execute computationally complex applications. Generally, these HPC applications involve large scale IO (input/output) processing over the networked parallel file system disks. They are commonly developed on top of the C/C++ based MPI standard library. The HPC clusters MPI–IO performance significantly depends on the particular parameter value configurations, not generally considered when writing the algorithms or programs. Therefore, this leads to poor IO and overall program performance degradation. The IO is mostly left to individual practitioners to be optimised at code level. This usually leads to unexpected consequences due to IO bandwidth degradation which becomes inevitable as the file data scales in size to petabytes. To overcome the poor IO performance, this research paper presents an approach for auto-tuning of the configuration parameters by forecasting the MPI–IO bandwidth via artificial neural networks (ANNs), a machine learning (ML) technique. These parameters are related to MPI–IO library and lustre (parallel) file system. In addition to this, we have identified a number of common configurations out of numerous possibilities, selected in the auto-tuning process of READ/WRITE operations. These configurations caused an overall READ bandwidth improvement of 65.7% with almost 83% test cases improved. In addition, the overall WRITE bandwidth improved by 83% with number of test cases improved by almost 93%. This paper demonstrates that by using auto-tuning parameters via ANNs predictions, this can significantly impact overall IO bandwidth performance.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s10723-022-09630-1,en,DRL-based and Bsld-Aware Job Scheduling for Apache Spark Cluster in Hybrid Cloud Computing Environments,OriginalPaper,"Spark is one of the most important big data computing engines, favored by academia and industry for its low latency and ease of use. The explosive growth in data volumes is causing computing tasks that could otherwise run on local or on-premise resources to become infeasible. The emergence of public clouds has solved the problem of shortage of local or on-premise resources. However, deploying clusters only on public clouds can be costly on the one hand and wasteful of available local resources on the other. Therefore, deploying a Spark cluster on both local and public cloud resources becomes a good solution to save cost and not waste local resources. When Spark is deployed in hybrid cloud environments, its default scheduling policy ignores job and environment characteristics leading to performance degradation and increased cluster usage costs. In this paper, A deep reinforcement learning-based (DRL-based) Spark job scheduler is proposed to improve cluster performance and reduce the total cost of cluster usage in hybrid cloud environments. Specifically, the proposed DRL agent can adaptively learn the characteristics of different types of jobs and hybrid cloud environments to rationally schedule Spark jobs to reduce the total cluster usage cost and the average bounded slowdown of jobs. A simulation environment is built to train the proposed scheduling agent, and the Spark Core module is extended to verify the effectiveness of the proposed scheduling agent. Experimental results show that the DRL-based algorithm improves performance by 5.55% and reduces the total cluster usage cost by 13.9% on average compared to the baseline algorithm in burst arrival mode.","['Computer Science', 'Processor Architectures', 'Management of Computing and Information Systems', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s11042-022-14223-x,en,Extracting Radiomic features from pre-operative and segmented MRI scans improved survival prognosis of glioblastoma Multiforme patients through machine learning: a retrospective study,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","The combination of radiomics and artificial intelligence has emerged as a strong technique for building predictive models in radiology. This study aims to address the clinically important issue of whether a radiomic profile can predict the overall survival (OS) time of glioblastoma multiforme (GBM) patients having gross tumor resection (GTR) through pre-operative structural magnetic resonance imaging (MRI) scans. A retrospective analysis was made using data of glioma patients made publicly available by the University of Pennsylvania. The radiomic characteristics were extracted from pre-operative structural multiparametric MRI (mpMRI) sequences after pre-processing and 3D segmentation using deep learning (DL). After removing irrelevant features, regression models based on machine learning (ML) were developed by considering selected features to predict the OS time of GBM patients within a period of days only. The patients were divided into three survivor groups depending on their projected survival time. To validate the significance of the selected feature set, statistical analysis was performed. As many as 494 patients were considered to improve survival prediction (SP) by using more effective feature extraction and selection techniques. The ridge regressor acquired the highest SpearmanR Rank correlation of 0.635 with an accuracy of 69%, the greatest of all the previous works for categorical predictions of such patients. The researchers in the past who used radiomic characteristics for the OS prognosis of GBM patients could yield limited results only. However, the current research work recorded an enhanced accuracy and SpearmanR rank for the three survivor classes of GBM patients using ML, feature selection, and radiomics. The significance of this work lies in the selection of patients with GTR and the extraction of radiomic characteristics through the use of radiomics and artificial intelligence.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1186/s40850-022-00156-3,en,Complex strategies: an integrative analysis of contests in Siamese fighting fish,"['OriginalPaper', 'Research article']","Background Animals use contests to attain resources and employ strategic decisions to minimise contest costs. These decisions are defined by behavioural response to resource value and competitive ability, but remain poorly understood. This is because the two factors are typically studied separately. Also, their study relies on overgeneralised assumptions that (i) strategies are fixed, (ii) modulated by the motivation or drive to fight and (iii) used to manage costs proportional to the timing of the loser’s retreat. To address these problems, we adopt an integrative sequential analysis that incorporates competitive ability and resource value factors, to characterise territorial contest decisions in male Siamese fighting fish ( Betta splendens ). Results Individuals exhibited a chronological organisation of behaviour, engaging opponents first with frontal display, then switching to lateral display before deciding to attack, and reserved retreats for later stages. Using asymmetries in retreats as a proxy for outcome, the likelihood of winning was found to be mostly dependent on display. However, resource and contest conditions affected initiation latency, display, attack and retreat, suggesting that strategic decisions influence all behaviour. Overall, sequential behaviour varied consistently with individual aggressiveness and resource-value factors, and increasingly with information on competitive ability collected during the contest. This enabled shifts in tactics, such as disadvantaged individuals responding first with aggression and later with submission. Motivation to continue fighting, after interruption by startle, was also adjusted to information gathered during the contest and progressively with energetic state. Two clusters of correlated behaviours were identified, cost-mitigation (display and retreat) and escalation (initiation and attack), but changes in motivation were associated only with cost mitigation. Conclusions Our findings contrast dominant assumptions that strategic decisions are fixed, controlled by motivational state and sufficiently described by outcome-dependent measures. We instead demonstrate that strategic decisions are complex, comprising functional changes in assessment, information use and motivational effects, which are not always inter-dependent.","['Life Sciences', 'Zoology', 'Animal Anatomy / Morphology / Histology', 'Animal Physiology', 'Animal Systematics/Taxonomy/Biogeography', 'Fish & Wildlife Biology & Management', 'Entomology']"
doi:10.1007/s10898-022-01262-9,en,A robust multi-objective Bayesian optimization framework considering input uncertainty,OriginalPaper,"Bayesian optimization is a popular tool for optimizing time-consuming objective functions with a limited number of function evaluations. In real-life applications like engineering design, the designer often wants to take multiple objectives as well as input uncertainty into account to find a set of robust solutions. While this is an active topic in single-objective Bayesian optimization, it is less investigated in the multi-objective case. We introduce a novel Bayesian optimization framework to perform multi-objective optimization considering input uncertainty. We propose a robust Gaussian Process model to infer the Bayes risk criterion to quantify robustness, and we develop a two-stage Bayesian optimization process to search for a robust Pareto frontier, i.e., solutions that have good average performance under input uncertainty. The complete framework supports various distributions of the input uncertainty and takes full advantage of parallel computing. We demonstrate the effectiveness of the framework through numerical benchmarks.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Real Functions', 'Computer Science, general']"
doi:10.1038/s41598-022-25867-y,en,Developing a diagnosis model for dry eye disease in dogs using object detection,"['OriginalPaper', 'Article']","The purpose of this study was to develop an object detection method for the diagnosis of dry eye disease (DED) in dogs. To this end, a methodology was designed to evaluate ocular surface video images using the YOLOv5 model, which is an object detection algorithm that has been widely used because of its simple network structure and fast detection speed. Because the cornea is a transparent organ, an illuminator plate with grid squares was used to provide grid lines, which were analyzed as the reflected straight lines of the light source representing the precorneal tear film (PTF) stability. The original video consisted of the number of 12 normal images(normal, $$n$$ n = 17) and the number of 15 abnormal images(abnormal, $$n$$ n = 17), converted to JPEG images for labeling, learning, and model validation. The labeled image data were divided into a training image data set (normal, $$n$$ n = 15,276; abnormal, $$n$$ n = 26,196) to a validation image data set (normal, $$n$$ n = 6546; abnormal, $$n$$ n = 11,228). As a result of the experiment, the mean average precision ( $$mAP$$ mAP ) achieved 0.995. This study proposes a method to effectively determine ocular surface status in dogs by using YOLOv5 and concludes that an object detection model can be used in the veterinary field.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-08056-w,en,Multi-head attention with CNN and wavelet for classification of hyperspectral image,"['OriginalPaper', 'Original Article']","Hyperspectral Image (HSI) is characterized by large number of bands with a high spectral resolution where continuous spectrum is measured for each pixel. This high volume therefore leads to challenges in processing the dataset. Objective of Dimensionality Reduction (DR) algorithms is to identify and eliminate statistical redundancies of hyperspectral data while keeping as much spectral information as possible. Combining spectral and spatial information offers a more comprehensive classification approach. Convolutional neural network (CNN) has the potential to extract complex spatial and spectral features embedded in Hyperspectral data. Wavelet transform belongs to the family of multi-scale transformation where the input signal is analyzed at different levels of granularity. Attention mechanism is a method in neural networks to guide the algorithm to focus on the important information in the data. In this paper, we use Multi-head Transformer-based Attention (Vaswani et al. in Attention is all you Need, http://arxiv.org/abs/1706.03762 2017) technique for Channel attention which captures the long-range spectral dependencies. The experimental results show that the proposed algorithm MT-CW Band Selection-based multi-head transformer for dimensionality reduction and Wavelet CNN-based algorithm for feature extraction yields impressive results in terms of information conservation and class separability.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11082-022-04294-3,en,Mask-guided infrared small multi-target detection via coarse-to-fine candidate selection,OriginalPaper,"Infrared acquisition technology is often used for target detection in military fields, since it is not easily disturbed by diverse environmental factors. However, the characteristics of low image contrast, coupled with complex imaging backgrounds, long distance and the lack of visual features, making infrared small targets detection a challenging task. The generic deep neural network-based models are struggled to be applied for infrared small targets, since the increased network layers may lead to the gradually loss of target features and positional information. To address above issues, we propose a mask-guided detection model via the coarse-to-fine candidate selection for infrared small multi-target detection. More specifically, to enhance target features and guide the localization process in the neural network, we propose to utilize the foreground mask generated by referring to the non-local self-correlation property of infrared background and the sparse property of target distribution. The obtained mask is treated as the prior to re-weight the convolutional feature maps. Considering the complex background, the multi-target detection is prone to mis-detections. Therefore, we propose a coarse-to-fine candidate selection method on top of the initial detection results. A shallow network is constructed to extract more nuanced visual features from the candidate positions for a binary classification, in which the false positive candidates are ruled out free from the disruptions of other background features. Moreover, given the lack of multi-target infrared datasets, we propose two synthetic datasets based on the public available and own collected infrared data. Extensive experimental results verify the effectiveness and advantages of our model compared to state-of-the-art methods.","['Physics', 'Optics, Lasers, Photonics, Optical Devices', 'Electrical Engineering', 'Characterization and Evaluation of Materials', 'Computer Communication Networks']"
doi:10.1007/s00371-022-02742-5,en,WeedGan: a novel generative adversarial network for cotton weed identification,"['OriginalPaper', 'Original article']","Recently, precision weed management has emerged as a promising solution for reducing the use of herbicides which is hazardous to crops and human health. Thus, accurate identification of weed in the early stage is the urge of current agricultural practices. Despite recent progress, developing an efficient weed identification system for real field scenarios is a serious challenge. To overcome this, a number of deep learning-based methods have been introduced in the literature. However, these methods require large volume of annotated data images which is rarely available. To address this gap, in this paper, a novel WeedGan method has been introduced for generating realistic synthetic images. The proposed WeedGan adopted concept of federated learning to reduce the computational load by introducing two discriminators. Additionally, a new loss function is defined for the efficient training of the generator. Extensive experiments have been performed to validate the performance of the WeedGan. First, the quality of images generated by the WeedGan was validated on cotton weed dataset in terms of FID score, and discriminator accuracy. The results are compared against four other state-of-the-art GAN models namely, DC-GAN, W-GAN, Info-Gan, and VIT-GAN. Further, classifiers performance of the generated dataset was evaluated using seven state-of-the-art transfer learning-based methods on the original, basic augmented, and WeedGan augmented datasets. The experimental results demonstrate that the proposed WeedGan has outperformed all the considered methods by achieving FID score of 282.76. Moreover, the classification performance of WeedGan augmented dataset was recorded highest with 97.82% on testing and 99.87% training accuracy with DenseNet121.","['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1038/s43856-022-00220-6,en,Deep learning-based age estimation from chest X-rays indicates cardiovascular prognosis,"['OriginalPaper', 'Article']","Background In recent years, there has been considerable research on the use of artificial intelligence to estimate age and disease status from medical images. However, age estimation from chest X-ray (CXR) images has not been well studied and the clinical significance of estimated age has not been fully determined. Methods To address this, we trained a deep neural network (DNN) model using more than 100,000 CXRs to estimate the patients’ age solely from CXRs. We applied our DNN to CXRs of 1562 consecutive hospitalized heart failure patients, and 3586 patients admitted to the intensive care unit with cardiovascular disease. Results The DNN’s estimated age (X-ray age) showed a strong significant correlation with chronological age on the hold-out test data and independent test data. Elevated X-ray age is associated with worse clinical outcomes (heart failure readmission and all-cause death) for heart failure. Additionally, elevated X-ray age was associated with a worse prognosis in 3586 patients admitted to the intensive care unit with cardiovascular disease. Conclusions Our results suggest that X-ray age can serve as a useful indicator of cardiovascular abnormalities, which will help clinicians to predict, prevent and manage cardiovascular diseases. Chest X-ray is one of the most widely used medical imaging tests worldwide to diagnose and manage heart and lung diseases. In this study, we developed a computer-based tool to predict patients’ age from chest X-rays. The tool precisely estimated patients’ age from chest X-rays. Furthermore, in patients with heart failure and those admitted to the intensive care unit for cardiovascular disease, elevated X-ray age estimated by our tool was associated with poor clinical outcomes, including readmission for heart failure or death from any cause. With further testing, our tool may help clinicians to predict outcomes in patients with heart disease based on a simple chest X-ray. Ieki et al. train a deep learning model to estimate patients’ age from chest X-ray images. X-ray age is found to be an indicator of poor prognosis in patients with heart failure and patients admitted to the intensive care unit with cardiovascular disease.","['Medicine & Public Health', 'Medicine/Public Health, general']"
doi:10.1007/s11042-022-14271-3,en,Robust UAV detection based on saliency cues and magnified features on thermal images,OriginalPaper,"Recent advances in the development of unmanned aerial vehicles (UAV), also known as drones, raised serious security concerns in critical locations. It is, therefore, necessary to design robust systems that can detect drones in any situation. In this paper, we present a novel approach for robust and accurate drone detection in different lighting conditions from thermal images based on deep learning. The main contributions of this paper consist of: First, the introduction of a thermal saliency map as a new feature to adapt deep learning models to learn features from thermal sources. Second, the introduction of a novel module called Magnifying Small Objects (MSO) as a guide for the deep Neural network to improve the detection and localization of small drones, for robust drone detection. Third, a new data augmentation strategy is proposed to generate novel drone images from different sources. Comprehensive experiments are conducted on University of Southern California (USC) thermal UAV dataset and Drone Detection Dataset. A comparative evaluation of the obtained results with state-of-the-art methods is presented. The experimental results show the robustness and high precision of our approach for drone detection.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40435-022-01086-1,en,Identification of nonlinear bolted lap joint parameters using instantaneous power flow balance-based substructure approach,OriginalPaper,"This paper presents a novel nonlinear lap joint parameter identification method based on the instantaneous power flow balance approach, which includes structural nonlinear parameters for realistic modelling. In the present approach, substructure instantaneous power flow balance, whereby the algebraic sum of input power, dissipated power, transmitted power, and time rate of kinetic and strain energy is equated to zero, is used as an objective criterion to formulate an inverse problem for nonlinear joint parameter identification. The correct values of the nonlinear coefficients are estimated by minimizing the objective function using the particle swarm optimization algorithm. The substructure-based identification strategy reduces the number of sensor requirements and also improves the identification performance than the global identification technique. The method was applied to experiments involving a steel beam assembly connected by a single bolted lap joint with various tightening torques. Furthermore, validation studies were also conducted to predict the effectiveness of the proposed method in nonlinear parameter identification. The experimental structure applications have shown that the proposed method is effective for the nonlinear parameter identification of joints.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1007/s13042-022-01739-9,en,Heterogeneous dual network with feature consistency for domain adaptation person re-identification,"['OriginalPaper', 'Original Article']","To reduce the noisy pseudo-labels generated by clustering for unsupervised domain adaptation (UDA) person re-identification (re-ID), the method of collaborative training between dual networks has been proposed and proved to be effective. However, most of these methods ignore the coupling problem between dual networks with the same architecture, which makes them inevitably share a high similarity and lack heterogeneity and complementarity. In this paper, we propose a heterogeneous dual network (HDNet) framework with two asymmetric networks, one of which applies convolution with limited receptive fields to obtain local information and the other uses Transformer to capture long-range dependency. Additionally, we propose feature consistency loss (FCL) that does not rely on pseudo-labels. FCL focuses more on the consistency of the sample in the feature space rather than the class prediction space, driving the feature learning of UDA re-ID from the task level to the feature level. Furthermore, we propose an adaptive channel mutual-aware (ACMA) module which contains two branches to focus on the global and local information between channels. We evaluate our proposed method on three popular datasets: DukeMTMC-reID, Market-1501 and MSMT17. Extensive experimental results demonstrate that our method achieves a competitive performance.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1038/s41598-022-23738-0,en,Influence of online opinions and interactions on the Covid-19 vaccination in Chile,"['OriginalPaper', 'Article']","We analyze 6 months of Twitter conversations related to the Chilean Covid-19 vaccination process, in order to understand the online forces that argue for or against it and suggest effective digital communication strategies. Using AI, we classify accounts into four categories that emerge from the data as a result of the type of language used. This classification naturally distinguishes pro- and anti-vaccine activists from moderates that promote or inhibit vaccination in discussions, which also play a key role that should be addressed by public policies. We find that all categories display relatively constant opinions, but that the number of tweeting accounts grows in each category during controversial periods. We also find that accounts disfavoring vaccination tend to appear in the periphery of the interaction network, which is consistent with Chile’s high immunization levels. However, these are more active in addressing those favoring vaccination than vice-versa, revealing a potential communication problem even in a society where the antivaccine movement has no central role. Our results highlight the importance of social network analysis to understand public discussions and suggest online interventions that can help achieve successful immunization campaigns.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s13007-022-00968-x,en,Four-dimensional measurement of root system development using time-series three-dimensional volumetric data analysis by backward prediction,"['OriginalPaper', 'Methodology']","Background Root system architecture (RSA) is an essential characteristic for efficient water and nutrient absorption in terrestrial plants; its plasticity enables plants to respond to different soil environments. Better understanding of root plasticity is important in developing stress-tolerant crops. Non-invasive techniques that can measure roots in soils nondestructively, such as X-ray computed tomography (CT), are useful to evaluate RSA plasticity. However, although RSA plasticity can be measured by tracking individual root growth, only a few methods are available for tracking individual roots from time-series three-dimensional (3D) images. Results We developed a semi-automatic workflow that tracks individual root growth by vectorizing RSA from time-series 3D images via two major steps. The first step involves 3D alignment of the time-series RSA images by iterative closest point registration with point clouds generated by high-intensity particles in potted soils. This alignment ensures that the time-series RSA images overlap. The second step consists of backward prediction of vectorization, which is based on the phenomenon that the root length of the RSA vector at the earlier time point is shorter than that at the last time point. In other words, when CT scanning is performed at time point A and again at time point B for the same pot, the CT data and RSA vectors at time points A and B will almost overlap, but not where the roots have grown. We assumed that given a manually created RSA vector at the last time point of the time series, all RSA vectors except those at the last time point could be automatically predicted by referring to the corresponding RSA images. Using 21 time-series CT volumes of a potted plant of upland rice ( Oryza sativa ), this workflow revealed that the root elongation speed increased with age. Compared with a workflow that does not use backward prediction, the workflow with backward prediction reduced the manual labor time by 95%. Conclusions We developed a workflow to efficiently generate time-series RSA vectors from time-series X-ray CT volumes. We named this workflow 'RSAtrace4D' and are confident that it can be applied to the time-series analysis of RSA development and plasticity.","['Life Sciences', 'Plant Sciences', 'Biological Techniques']"
doi:10.1007/s11280-022-01108-0,en,Auto-regressive extractive summarization with replacement,OriginalPaper,"Auto-regressive extractive summarization approaches determine sentence extraction probability conditioning on previous decisions by maintaining a partial summary representation. Despite its popularity, the framework has two main drawbacks: 1) the partial summary representation is irresolutely denoted by a weighted summation of all the processed sentences without any filtering, resulting in a noisy representation and degrading the effectiveness of extracting subsequent sentences; 2) earlier sentences are biased towards a higher extraction probability due to the sequential nature of sequence tagging. To address these two problems, we propose the Auto-regressive Extractive Summarization with Replacement (AES-Rep), a novel auto-regressive extractive summarization model. In particular, the AES-Rep model consists of two main modules: the extraction decision module that determines whether a sentence should be extracted, and the replacement locater module that enables extracted deficient sentences to be replaced with latter sentences by comparing their expressiveness with respect to the main idea of the document. These modules update the partial summary with explicit actions using elaborated multidimensional guidance. We conduct extensive experiments on the benchmark CNN and DailyMail datasets. Experimental results show that AES-Rep can achieve better performance compared with various strong baselines in terms of multiple ROUGE metrics.","['Computer Science', 'Information Systems Applications (incl.Internet)', 'Database Management', 'Operating Systems']"
doi:10.1007/s10640-022-00741-7,en,The environmental effects of the “twin” green and digital transition in European regions,OriginalPaper,"This study explores the nexus between digital and green transformations—the so-called “twin” transition—in European regions in an effort to identify the impact of digital and environmental technologies on the greenhouse gas (GHG) emissions originating from industrial production. We conduct an empirical analysis based on an original dataset that combines information on environmental and digital patent applications with information on GHG emissions from highly polluting plants for the period 2007–2016 at the metropolitan region level in the European Union and the UK. Results show that the local development of environmental technologies reduces GHG emissions, while the local development of digital technologies increases them, albeit in the latter case different technologies seem to have different impacts on the environment, with big data and computing infrastructures being the most detrimental. We also find differential impacts across regions depending on local endowment levels of the respective technologies: the beneficial effect of environmental technologies is stronger in regions with large digital technology endowments and, conversely, the detrimental effect of digital technologies is weaker in regions with large green technology endowments. Policy actions promoting the “twin” transition should take this evidence into account, in light of the potential downside of the digital transformation when not combined with the green transformation.","['Economics', 'Environmental Economics', 'Environmental Law/Policy/Ecojustice', 'Economic Policy', 'Economics, general', 'Environmental Management']"
doi:10.1038/s41598-022-23990-4,en,Physical imaging parameter variation drives domain shift,"['OriginalPaper', 'Article']","Statistical learning algorithms strongly rely on an oversimplified assumption for optimal performance, that is, source (training) and target (testing) data are independent and identically distributed. Variation in human tissue, physician labeling and physical imaging parameters (PIPs) in the generative process, yield medical image datasets with statistics that render this central assumption false. When deploying models, new examples are often out of distribution with respect to training data, thus, training robust dependable and predictive models is still a challenge in medical imaging with significant accuracy drops common for deployed models. This statistical variation between training and testing data is referred to as domain shift (DS).To the best of our knowledge we provide the first empirical evidence that variation in PIPs between test and train medical image datasets is a significant driver of DS and model generalization error is correlated with this variance. We show significant covariate shift occurs due to a selection bias in sampling from a small area of PIP space for both inter and intra-hospital regimes. In order to show this, we control for population shift, prevalence shift, data selection biases and annotation biases to investigate the sole effect of the physical generation process on model generalization for a proxy task of age group estimation on a combined 44 k image mammogram dataset collected from five hospitals.We hypothesize that training data should be sampled evenly from PIP space to produce the most robust models and hope this study provides motivation to retain medical image generation metadata that is almost always discarded or redacted in open source datasets. This metadata measured with standard international units can provide a universal regularizing anchor between distributions generated across the world for all current and future imaging modalities.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07714-4,en,A model fusion method based on multi-source heterogeneous data for stock trading signal prediction,"['OriginalPaper', 'Soft computing in decision making and in modeling in economics']","In the prediction of turning points (TPs) of time series, the improved model of integrating piecewise linear representation and weighted support vector machine (IPLR-WSVM) has achieved good performance. However, due to the single data source and the limitation of algorithm, IPLR-WSVM has encountered challenges in profitability. In this paper, a model fusion method based on multi-source heterogeneous data and different learning algorithms is proposed for the prediction of TPs (MF-MSHD). Multi-source heterogeneous data include weighted unstructured and structured information with different granularities. RF, WSVM, BPNN, GBDT, and LSTM are selected to be the learning algorithms. The differences among meta-models are constructed by different inputs and algorithms as much as possible, and a model fusion rule is designed to determine the final TPs. Moreover, the TPs are generated based on the characteristics of individual stock. For sentiment analysis, a more accurate sentiment dictionary of stock market comments is established. Specifically, the fine-grained data is introduced to jointly determine the accurate trading moment. The prediction level of the proposal improves the accuracy and profitability, and also outperforms the composite indexes. Experimental results show that the profit rate of randomly selected stocks in MF-MSHD reaches 0.5172, while the highest value is 0.2841 in single meta-model and 0.0992 in buy and hold strategy, respectively. The other indicators including the accuracy are also modified. Compared with the increases of 0.1648, 0.4051, and 0.3397 in Shanghai Composite Index, Shenzhen Composite Index, and CSI 300 Index, MF-MSHD shows higher profitability in stock trading signal prediction.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1038/s41467-022-35295-1,en,A Multifaceted benchmarking of synthetic electronic health record generation models,"['OriginalPaper', 'Article']","Synthetic health data have the potential to mitigate privacy concerns in supporting biomedical research and healthcare applications. Modern approaches for data generation continue to evolve and demonstrate remarkable potential. Yet there is a lack of a systematic assessment framework to benchmark methods as they emerge and determine which methods are most appropriate for which use cases. In this work, we introduce a systematic benchmarking framework to appraise key characteristics with respect to utility and privacy metrics. We apply the framework to evaluate synthetic data generation methods for electronic health records data from two large academic medical centers with respect to several use cases. The results illustrate that there is a utility-privacy tradeoff for sharing synthetic health data and further indicate that no method is unequivocally the best on all criteria in each use case, which makes it evident why synthetic data generation methods need to be assessed in context. Synthetic health data have the potential to mitigate privacy concerns when sharing data to support biomedical research and the development of innovative healthcare applications. In this work, the authors introduce a use case oriented benchmarking framework to evaluate data synthesis models through a set of utility and privacy metrics.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11269-022-03395-8,en,Advanced Machine Learning Model for Prediction of Drought Indices using Hybrid SVR-RSM,OriginalPaper,"Drought, as a phenomenon that causes significant damage to agriculture and water resources, has increased across the globe due to climate change. Hence, scientists are attracted to developing drought prediction models for mitigation strategies. Different drought indices (DIs) have been proposed for drought monitoring during the past few decades, most of which are probabilistic, highly stochastic, and non-linear. The present study inspected the capability of various machine learning (ML) models, including artificial neural network (ANN) and support vector regression (SVR) as original predictive models and optimized by two selected algorithms, namely, particle swarm optimization (SVR-PSO) and response surface method (SVR-RSM) to predict the meteorological drought indices of standardized precipitation index (SPI), percentage of normal precipitation (PN), effective drought index (EDI), and modified China-Z index (MCZI) on a monthly time scale. A novel model named SVR-RMS is introduced by using two calibrating processes given from RSM with two inputs and the SVR by predicted data handled with RSM given from the first calibrating procedure. For evaluating the models, different meteorological input variables in the period 1981–2020 were considered from 11 synoptic stations in arid and semi-arid climates of Iran, which frequently experience droughts. The SPI showed the highest and lowest correlation with MCZI (0.71) and EDI (0.34), respectively. The results of testing dataset (2011–2020) indicated that the SVR-RSM produced superior abilities for both accuracy and tendency compared to other models, while the SVR-PSO model is better than the ANN and SVR. The worst results of drought prediction were obtained for EDI. However, all models provided the acceptable EDI prediction in the high-temperature station of Ahvaz in the south of the country. Application of SVR-RSM as a novel hybrid model can be suggested for predicting the DIs on a short time scale in arid and semi-arid areas.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s00521-022-08082-8,en,SAdaBoundNc: an adaptive subgradient online learning algorithm with logarithmic regret bounds,"['OriginalPaper', 'Original Article']","Adaptive (sub)gradient methods have received wide applications such as the training of deep networks. The square-root regret bounds are achieved in convex settings. However, how to exploit strong convexity for improving convergence rate and improve the generalization performance of adaptive (sub)gradient methods remain an open problem. For this reason, we devise an adaptive subgradient online learning algorithm called SAdaBoundNc in strong convexity settings. Moreover, we rigorously prove that the logarithmic regret bound can be achieved by choosing a faster diminishing learning rate. Further, we conduct various experiments to evaluate the performance of SAdaBoundNc on four real-world datasets. The results demonstrate that the training speed of SAdaBoundNc outperforms stochastic gradient descent and several adaptive gradient methods in the initial training process. Moreover, the generalization performance of SAdaBou-ndNc is also better than the current state-of-the-art methods on different datasets.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1186/s12879-022-07921-2,en,Comparing artificial neural network training algorithms to predict length of stay in hospitalized patients with COVID-19,"['OriginalPaper', 'Research']","Background The exponential spread of coronavirus disease 2019 (COVID-19) causes unexpected economic burdens to worldwide health systems with severe shortages in hospital resources (beds, staff, equipment). Managing patients’ length of stay (LOS) to optimize clinical care and utilization of hospital resources is very challenging. Projecting the future demand requires reliable prediction of patients’ LOS, which can be beneficial for taking appropriate actions. Therefore, the purpose of this research is to develop and validate models using a multilayer perceptron-artificial neural network (MLP-ANN) algorithm based on the best training algorithm for predicting COVID-19 patients' hospital LOS. Methods Using a single-center registry, the records of 1225 laboratory-confirmed COVID-19 hospitalized cases from February 9, 2020 to December 20, 2020 were analyzed. In this study, first, the correlation coefficient technique was developed to determine the most significant variables as the input of the ANN models. Only variables with a correlation coefficient at a P-value < 0.2 were used in model construction. Then, the prediction models were developed based on 12 training algorithms according to full and selected feature datasets (90% of the training, with 10% used for model validation). Afterward, the root mean square error (RMSE) was used to assess the models’ performance in order to select the best ANN training algorithm. Finally, a total of 343 patients were used for the external validation of the models. Results After implementing feature selection, a total of 20 variables were determined as the contributing factors to COVID-19 patients’ LOS in order to build the models. The conducted experiments indicated that the best performance belongs to a neural network with 20 and 10 neurons in the hidden layer of the Bayesian regularization (BR) training algorithm for whole and selected features with an RMSE of 1.6213 and 2.2332, respectively. Conclusions MLP-ANN-based models can reliably predict LOS in hospitalized patients with COVID-19 using readily available data at the time of admission. In this regard, the models developed in our study can help health systems to optimally allocate limited hospital resources and make informed evidence-based decisions.","['Medicine & Public Health', 'Infectious Diseases', 'Parasitology', 'Medical Microbiology', 'Tropical Medicine', 'Internal Medicine']"
doi:10.1186/s12859-022-05074-2,en,Automatic identifying and counting blood cells in smear images by using single shot detector and Taguchi method,"['OriginalPaper', 'Research']","Background Researchers have tried to identify and count different blood cells in microscopic smear images by using deep learning methods of artificial intelligence to solve the highly time-consuming problem. Results The three types of blood cells are platelets, red blood cells, and white blood cells. This study used the Resnet50 network as a backbone network of the single shot detector (SSD) for automatically identifying and counting different blood cells and, meanwhile, proposed a systematic method to find a better combination of algorithm hyperparameters of the Resnet50 network for promoting accuracy for identifying and counting blood cells. The Resnet50 backbone network of the SSD with its optimized algorithm hyperparameters, which is called the Resnet50-SSD model, was developed to enhance the feature extraction ability for identifying and counting blood cells. Furthermore, the algorithm hyperparameters of Resnet50 backbone networks of the SSD were optimized by the Taguchi experimental method for promoting detection accuracy of the Resnet50-SSD model. The experimental result shows that the detection accuracy of the Resnet50-SSD model with 512 × 512 × 3 input images was better than that of the Resnet50-SSD model with 300 × 300 × 3 input images on the test set of blood cells images. Additionally, the detection accuracy of the Resnet50-SSD model using the combination of algorithm hyperparameters got by the Taguchi method was better than that of the Resnet50-SSD model using the combination of algorithm hyperparameters given by the Matlab example. Conclusion In blood cell images acquired from the BCCD dataset, the proposed Resnet50-SSD model had higher accuracy in identifying and counting blood cells, especially white blood cells and red blood cells.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s12008-022-01127-1,en,Geometric optimisation of double ended magnetorheological fluid damper,"['OriginalPaper', 'Original Paper']","The main focus of this paper is to optimally design the Magnetorheological Fluid Damper using modern optimization algorithms such as Genetic Algorithm (GA), JAYA and GWO (Grey Wolf Optimizer). The objective function of the problem is to maximize the damping force of the damper. To calculate the damping force for different values of the design parameter Computational Fluid Dynamics (CFD) analysis has been used. The fluid properties to be implemented in the CFD analysis are evaluated through rheological experimentation under different magnetic flux. The design variables in the problem are the length of fluid exposed to the magnetic, Yield Shear stress, and fluid flow gap. The objective function has been formulated by Statistical modeling of the damper wherein a set of Design of Experiments. Obtained set of experiments are statistically (i.e. Analysis of Variance, ANOVA) analyzed to confirm the suitability of the design variable values in the optimization techniques. This equation along with a set of design variables constraints are used in the optimization techniques to find the optimal values of dimensions of the damper to achieve the maximum damping force. From the results it has been observed that the JAYA and GWO technique offered maximum damping force compared to the GA.","['Engineering', 'Engineering, general', 'Engineering Design', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Electronics and Microelectronics, Instrumentation', 'Industrial Design']"
doi:10.1007/s40430-022-03920-1,en,Stochastic efficient global optimization with high noise variance and mixed design variables,"['OriginalPaper', 'Technical Paper']","Engineering design and optimization commonly require the minimization of expected value functions with high noise variance and mixed/discrete design variables. To solve such problems, we extend the stochastic efficient global optimization (SEGO) method of [Carraro et al., Struct Multidiscipl Optim 60(1):245–268 (2019)]. To address high noise variance, we propose two additional stopping criteria for the Monte Carlo integration that is required to approximate the objective function. Moreover, the active learning algorithm within SEGO is adapted to handle discrete design variables. The method is investigated on two numerical examples and the results highlight the efficiency of the proposed method, especially for cases with low computational budget.","['Engineering', 'Mechanical Engineering']"
doi:10.1007/s00521-022-08081-9,en,A three-tiered semi supervised MTL mechanism and its application in dating apps,"['OriginalPaper', 'S.i.: Data Processing Techniques and Applications for Cyber-physical Systems (dpta-2021)']","A thorough understanding of the purpose of dating applications is crucial for service providers in order to optimize the design and user experience of the application. Despite the fact that many APPs prompt users to provide their usage purpose, many do not reveal this attribute. In this study, a three-module framework with semi-supervised and multitask learning mechanisms is proposed (T-SSMTL). Using the T-SSMTL mechanism, the purpose of the dating APP usage can be automatically inferred from the publicly available heterogeneous data of the user. The heterogeneous feature extraction module employs a number of techniques to extract semantic representations, maximizing the use of heterogeneous dating APP data. The multi-task module extracts task-specific knowledge for learning and solves the classification problem involving multiple labels. To alleviate the problem of label insufficiency, the semi-supervised module utilizes a large quantity of unlabeled data generated by users who do not report their usage purpose. A large-scale dataset containing 34,364 active dating APP users with their self-reported usage purpose, portrait image, profile, and posts was collected to evaluate the T-SSMTL framework. In the context of this dataset, simulation experiments have confirmed the efficacy of all three modules of the T-SSMTL framework, demonstrating its substantial theoretical significance as well as its excellent application value.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-08085-5,en,Identification and spatio-temporal analysis of earthquake clusters using SOM–DBSCAN model,"['OriginalPaper', 'Original Article']","Seismic catalogs are vital to understanding and analyzing the progress of active fault systems. The background seismicity rate in a seismic catalog, strongly associated with stressing rate, is the critical parameter in seismic hazard analysis. Estimating background seismicity is a complex task due to the high correlation with aftershock sequences which may dominate the background seismicity rate. In this paper, identification of the significant earthquake aftershocks and independent background events is performed using a two-stage clustering approach. It works in two phases: Self-Organized Map and Density-based Temporal Clustering. The event’s location and depth information in the earthquake catalog is used to identify the major hot spots (SOM prototypes) in the region (Spatial domain). Later, density-based temporal clustering is applied to decipher the neighborhood events of each SOM prototype. The proposed two-level clustering approach performs effective spatio-temporal analysis and identifies the aftershock clusters and background. The experimental study is carried out on the prominent earthquake catalogs of Taiwan, Afghanistan, California, the Himalayas, Indonesia, Chile, and Japan. The statistical parameters: Coefficient of Variation (time-domain) and m -Morisita index (spatial domain) justify and validate the accuracy of the presented approach. The proposed model is compared with benchmark de-clustering algorithms for mainshock and background detection.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1186/s12859-022-05054-6,en,A Poisson reduced-rank regression model for association mapping in sequencing data,"['OriginalPaper', 'Research']","Background Single-cell RNA-sequencing (scRNA-seq) technologies allow for the study of gene expression in individual cells. Often, it is of interest to understand how transcriptional activity is associated with cell-specific covariates, such as cell type, genotype, or measures of cell health. Traditional approaches for this type of association mapping assume independence between the outcome variables (or genes), and perform a separate regression for each. However, these methods are computationally costly and ignore the substantial correlation structure of gene expression. Furthermore, count-based scRNA-seq data pose challenges for traditional models based on Gaussian assumptions. Results We aim to resolve these issues by developing a reduced-rank regression model that identifies low-dimensional linear associations between a large number of cell-specific covariates and high-dimensional gene expression readouts. Our probabilistic model uses a Poisson likelihood in order to account for the unique structure of scRNA-seq counts. We demonstrate the performance of our model using simulations, and we apply our model to a scRNA-seq dataset, a spatial gene expression dataset, and a bulk RNA-seq dataset to show its behavior in three distinct analyses. Conclusion We show that our statistical modeling approach, which is based on reduced-rank regression, captures associations between gene expression and cell- and sample-specific covariates by leveraging low-dimensional representations of transcriptional states.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s12293-022-00385-6,en,Efficient automatically evolving convolutional neural network for image denoising,"['OriginalPaper', 'Regular research paper']","Convolutional neural networks (CNNs) have achieved effective results in image denoising tasks. However, CNN architectures for image denoising tasks are mainly designed manually, which not only relies on CNN-related professional knowledge, but also requires adjustment to different datasets for competitive performance. Algorithms for automatically evolving CNN architectures have been proposed, but most of them are designed for solving image classification tasks and consume considerable computational time and resources. To address these issues, an efficient automatically evolving CNN architecture algorithm for image denoising tasks using genetic algorithm is proposed, which is called fast block-based evolutionary denoising CNN (FBE-DnCNN). In FBE-DnCNN, a genetic encoding strategy based on both deep and wide net blocks is designed to effectively represent the image denoising CNNs for automatic architecture design. With the purpose of solving time-consuming and resource-dependent problems, the partial dataset-based technology is used. A novel refined fitness evaluation method with prior knowledge on parameters of CNNs is designed to improve reliability. For better feature extraction of shallow network layers, convolutional operation, prevention of overfitting, and improvement of the representational capacity, the Feature Block, Transition Block, Dropout Block, and SENet module are introduced in FBE-DnCNN to generate problem-specific search space. With block-specific crossover and mutation, a local search near the good solution is implemented to find better solutions. Experiments show that FBE-DnCNN can evolve distinguished image denoising CNNs with deep and wide architectures in a very short time. FBE-DnCNN achieves competitive performance for the image denoising tasks with different noise levels compared to the traditional approaches, state-of-the-art CNN-based algorithms, and NAS-based methods.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Complex Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1186/s13677-022-00361-y,en,NIDD: an intelligent network intrusion detection model for nursing homes,"['OriginalPaper', 'Research']","In nursing homes using technologies such as IoT, big data, cloud computing, and machine learning, there is a constant risk of attacks such as Brute Force FTP, Brute Force SSH, Web Attack, Infiltration, and Botnet during data communication between individual terminals and the cloud server. Therefore, effectively identifying network communication data is essential to protect data communication security between individual terminals and the cloud server. Aiming at the data mentioned above regarding communication security issues, we propose an intelligent intrusion detection model NIDD (Network Intelligent Data Detection) model that combines deep convolution generation adversarial network (DCGAN) with Light Gradient Boosting Machine (LightGBM) and Shapley Additive exPlanations (SHAP). The NIDD model first generates new attack samples by learning the feature distribution of the existing attack sample data and effectively expands the rare attack samples. Secondly, we use the Light Gradient Boosting Machine (LightGBM) algorithm as the base classifier to train the dataset and start to build the intrusion detection model. Then use Shapley Additive exPlanations (SHAP) to analyze the contribution of the classification results, and adjust the model parameters according to the analysis results. Finally, we obtain the optimal model for the intelligent detection model of network intrusion. This paper conducts experimental tests on the NSL-KDD dataset. The experimental results show that the NIDD model built based on Light Gradient Boosting Machine can detect Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet, PROBE, R2L, and U2R attacks with an accuracy of 99.76%. Finally, we re-verified the NIDD model on the CIC-IDC-2018 dataset. The results once again proved that the NIDD model could solve the data communication security between the nursing robot and the cloud server and the data before the IoT terminal and the cloud server. Communication security provides a sufficient guarantee.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s10895-022-03089-9,en,A Design-assisted Spectrofluorometric Method Utilizing a One-pot Fluorescent Probe for the Quantitation of some Calcium Channel Blockers,"['OriginalPaper', 'Research']","Based on their reaction with highly fluorescent carbon quantum dots (CQDts), a precise and reliable spectrofluorometric approach was developed for the determination of three calcium channel blockers. The studied drugs are: lercanidipine, nimodipine and nifedipine. (CQDts) were produced using a one-step hydrothermal method with ascorbic acid as the carbon source. The produced CQDts were capped by alcohol to create yellow emitters displaying a high fluorescence emission at 524 nm when excited at 325 nm. The fluorescence intensity of CQDts was noticeably quenched by each of the three calcium channel blockers. The relation between their concentrations and fluorescence quenching is linear over the concentration range of 0.5–20 µg/mL for each of the three drugs. A full factorial design was used to optimize the effect of variable factors. Therefore, under optimum experimental design conditions, the detection limits for lercanidipine, nimodipine, and nifedipine were 0.11 ± 1.09, 0.10 ± 0.25 and 0.12 ± 0.71 µg/mL, respectively. The LOQ was 0.33, 0.30, and 0.37 µg/mL respectively. The quenching of fluorescent CQDts occurred through the inner filter effect (IFE) for nimodipine, while it was mixed with dynamic quenching for lercanidipine and nifedipine. The proposed method was effectively used to determine the cited drugs in their pharmaceutical products and had an acceptable level of precision. The selectivity of the CQDts system towards the studied drugs was examined indicating no interference from interfering species.","['Biomedicine', 'Biomedicine, general', 'Biological and Medical Physics, Biophysics', 'Biotechnology', 'Biochemistry, general', 'Analytical Chemistry']"
doi:10.1007/s41365-022-01150-7,en,A novel approach for feature extraction from a gamma-ray energy spectrum based on image descriptor transferring for radionuclide identification,OriginalPaper,"This study proposes a novel feature extraction approach for radionuclide identification to increase the precision of identification of the gamma-ray energy spectrum set. For easier utilization of the information contained in the spectra, the vectors of the gamma-ray energy spectra from Euclidean space, which are fingerprints of the different types of radionuclides, were mapped to matrices in the Banach space. Subsequently, to make the spectra in matrix form easier to apply to image-based deep learning frameworks, the matrices of the gamma-ray energy spectra were mapped to images in the RGB color space. A deep convolutional neural network (DCNN) model was constructed and trained on the ImageNet dataset. The mapped gamma-ray energy spectrum images were applied as inputs to the DCNN model, and the corresponding outputs of the convolution layers and fully connected layers were transferred as descriptors of the images to construct a new classification model for radionuclide identification. The transferred image descriptors consist of global and local features, where the activation vectors of fully connected layers are global features, and activations from convolution layers are local features. A series of comparative experiments between the transferred image descriptors, peak information, features extracted by the histogram of the oriented gradients (HOG), and scale-invariant feature transform (SIFT) using both synthetic and measured data were applied to 11 classical classifiers. The results demonstrate that although the gamma-ray energy spectrum images are completely unfamiliar to the DCNN model and have not been used in the pre-training process, the transferred image descriptors achieved good classification results. The global features have strong semantic information, which achieves an average accuracy of 92.76% and 94.86% on the synthetic dataset and measured dataset, respectively. The results of the statistical comparison of features demonstrate that the proposed approach outperforms the peak-searching-based method, HOG, and SIFT on the synthetic and measured datasets.","['Physics', 'Particle and Nuclear Physics', 'Particle Acceleration and Detection, Beam Physics', 'Nuclear Energy']"
doi:10.1038/s42256-022-00574-5,en,A generalizable deep learning framework for inferring fine-scale germline mutation rate maps,"['OriginalPaper', 'Article']","Germline mutation rates are essential for genetic and evolutionary analyses. Yet, estimating accurate fine-scale mutation rates across the genome is a great challenge, due to relatively few observed mutations and intricate relationships between predictors and mutation rates. Here, we present Mutation Rate Learner (MuRaL), a deep learning framework to predict mutation rates at the nucleotide level using only genomic sequences as input. Harnessing human germline variants for comprehensive assessment, we show that MuRaL achieves better predictive performance than current state-of-the-art methods. Moreover, MuRaL can build models with relatively few training mutations and a moderate number of sequenced individuals, and can leverage transfer learning to further reduce data and time demands. We apply MuRaL to produce genome-wide mutation rate maps for four representative species— Homo sapiens , Macaca mulatta , Drosophila melanogaster and Arabidopsis thaliana —demonstrating its high applicability. As an example, we use improved mutation rate estimates to stratify human genes into distinct groups that are enriched for different functions, and highlight that many developmental genes are subject to high mutational burden. The open-source software and generated mutation rate maps can greatly facilitate related research. Mutation rates are crucial for genetic and evolutionary analyses. Fang et al. present a generalizable deep learning method to build fine-scale mutation rate maps with DNA sequences as input, which can benefit analyses reliant on mutation rates.","['Engineering', 'Engineering, general']"
doi:10.1007/s12596-022-01032-6,en,Application of relative total variation optical decomposition fusion method on medical images,"['OriginalPaper', 'Research Article']","In this paper, we describe a medical image fusion technique based on relative total variation decomposition (RTVD) that can concurrently maintain the texture and contrast information of the input images. The source images are initially separated into structural and texture components based on the relative total variation. The former is mostly composed of the large frame structure and brightness of the source images, while the latter is composed of the texture and noise with low gradient values. Second, distinct fusion weights are created utilizing traits of the structure and texture layers. To maintain the texture information, the weights of texture parts are calculated in accordance with the saliency map, and the weights of structure parts are established in accordance with image energy to maintain the brightness of original images. The fused image could eventually be recreated utilizing sub-images and weights that were previously gathered. We also conduct qualitative and quantitative experiments to verify the effectiveness of the RTVD approach utilizing publically accessible datasets. The findings demonstrate that the RTVD fusion technique performs better than various more sophisticated algorithms in terms of maintaining contrast, preventing edge blurring, and lowering noise. The fusion outcome also more closely matches disease diagnosis.","['Physics', 'Physics, general', 'Optics, Lasers, Photonics, Optical Devices']"
doi:10.1007/s12065-022-00805-z,en,Aero engines remaining useful life prediction based on enhanced adaptive guided differential evolution,"['OriginalPaper', 'Research Paper']","Remaining Useful Life (RUL) prediction is a key process for prognostic health management in almost all engineering real-world applications, especially which are in hazardous and challenging environments where the failures and disastrous faults cannot be avoided such as space vehicles and aircraft. This paper proposes a predictive approach based on our proposed algorithm Enhanced Adaptive Guided Differential Evolution (EAGDE) is used to optimize the parameter selection of Support Vector Machine (SVM) to give high RUL prediction accuracy. The advantages of the proposed approach (EAGDE–SVM) are verified using the popular benchmark C-MAPSS which describes the degradation of the aircraft turbofan engine datasets. The experimental study compares EAGDE–SVM with the basic SVM with randomized parameter selection and with an optimized SVM using three different optimization algorithms. Also, the EAGDE–SVM is evaluated against three popular classifier models that have been used in the comparisons of recent research. Different evaluation criteria of classification, prediction, and optimization aspects have been used, the obtained results show that the EAGDE is capable to achieve the lowest classification error rates and RUL high prediction accuracy through finding the optimum values of the SVM parameters with high stability and fast convergence rate.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1038/s42256-022-00555-8,en,AtomAI framework for deep learning analysis of image and spectroscopy data in electron and scanning probe microscopy,"['OriginalPaper', 'Article']","Over the past several decades, electron and scanning probe microscopes have become critical components of condensed matter physics, materials science and chemistry research. At the same time, the infrastructure for establishing a connection between microscopy observations and materials behaviour over a broader parameter space is lacking. Here we introduce AtomAI, an open-source software package bridging instrument-specific Python libraries, deep learning and simulation tools into a single ecosystem. AtomAI allows direct applications of deep neural networks for atomic and mesoscopic image segmentation converting image and spectroscopy data into class-based local descriptors for downstream tasks such as statistical and graph analysis. For atomically resolved imaging data, the output is types and positions of atomic species, with an option for subsequent refinement. AtomAI further allows the implementation of a broad range of image and spectrum analysis functions, including invariant variational autoencoders for disentangling structural factors of variation and im2spec type of encoder–decoder models for mapping structure–property relationships. Finally, our framework allows seamless connection to the first principles modelling with a Python interface on the inferred atomic positions. In recent years, deep learning techniques have enhanced the possibility to extract useful, high-resolution physical information from electron and scanning probe microscopy images. AtomAI, an open-source software package, can accelerate this process by bringing deep learning and simulation tools into a single framework for a range of instruments.","['Engineering', 'Engineering, general']"
doi:10.1007/s00180-022-01307-3,en,Glomerulosclerosis detection with pre-trained CNNs ensemble,"['OriginalPaper', 'Original paper']","Glomerulosclerosis characterizes many conditions of primary kidney disease in advanced stages. Its accurate diagnosis relies on histological analysis of renal cortex biopsy, and it is paramount to guide the appropriate treatment and minimize the chances of the disease progressing to chronic stages. This article presents an ensemble approach composed of five convolutional neural networks (CNNs) - VGG-19, Inception-V3, ResNet-50, DenseNet-201, and EfficientNet-B2 - to detect glomerulosclerosis in glomerulus images. We fine-tuned the CNNs and evaluated several configurations for the fully connected layers. In total, we analyzed 25 different models. These CNNs, individually, demonstrated effectiveness in the task; however, we verified that the union of these five well-known CNNs improved the detection rate while decreasing the standard deviations of current techniques. The experiments were carried out in a data set comprised of 1,028 images, on which we applied data-augmentation techniques in the training set. The proposed CNNs ensemble achieved a near-perfect accuracy of 99.0% and kappa of 98.0%.","['Statistics', 'Statistics, general', 'Probability and Statistics in Computer Science', 'Probability Theory and Stochastic Processes', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1186/s40486-022-00164-5,en,Simulation of Germanium-on-Nothing cavity’s morphological transformation using deep learning,"['Letter', 'Letter']","Unique self-assembled germanium structures known as Germanium-on-Nothing (GON), which are fabricated via annealing, have buried multiscale cavities with different morphologies. Due to their unique sub-surface morphologies, GON structures are utilized in various applications including optoelectronics, micro-/nanoelectronics, and precision sensors. Each application requires different cavity shapes, and a simulation tool is able to determine the required annealing duration for a given shape. However, a theoretical simulation inevitably requires simplifications which limit its accuracy. Herein, to resolve such dependence on simplification, we introduce a deep learning-based method for simulating the transformation of sub-surface morhpology of GON over annealing. Namely, a deep learning model is trained to predict GON’s morphological transformation from 4 cross-sectional images acquired at different annealing times. Compared to conventional simulation schemes, our proposed deep learning-based simulation method is not only computationally efficient ( $$\sim 10$$ ∼ 10 min) but also physically accurate with its use of empirical data.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Mechanical Engineering', 'Nanotechnology', 'Applied and Technical Physics']"
doi:10.1007/s12559-022-10074-8,en,VSCA: A Sentence Matching Model Incorporating Visual Perception,OriginalPaper,"Stacking multiple layers of attention networks can significantly improve a model’s performance. However, this also increases the model’s time and space complexity, making it difficult for the model to capture detailed information on the underlying features. We propose a novel sentence matching model (VSCA) that uses a new attention mechanism based on variational autoencoders (VAE), which exploits the contextual information in sentences to construct a basic attention feature map and combines it with VAE to generate multiple sets of related attention feature maps for fusion. Furthermore, VSCA introduces a spatial attention mechanism that combines visual perception to capture multilevel semantic information. The experimental results show that our proposed model outperforms pretrained models such as BERT on the LCQMC dataset and performs well on the PAWS-X data. Our work consists of two parts. The first part compares the proposed sentence matching model with state-of-the-art pretrained models such as BERT. The second part conducts innovative research on applying VAE and spatial attention mechanisms in NLP. The experimental results on the related datasets show that the proposed method has satisfactory performance, and VSCA can capture rich attentional information and detailed information with less time and space complexity. This work provides insights into the application of VAE and spatial attention mechanisms in NLP.","['Computer Science', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Artificial Intelligence', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-08112-5,en,Automated steel surface defect detection and classification using a new deep learning-based approach,"['OriginalPaper', 'Original Article']","In this study, a new deep learning-based approach has been developed that detects and classifies surface defects that occur in the steel production process. The proposed methodology was created in four steps. In the first step, a deep learning model is designed that trains the residual and attention structures in parallel, thus increasing the classification performance. In the second step, deep features were extracted from the Parallel Attention Residual-Convolutional Neural Network model. The extracted features in the third step were selected by a new and simple algorithm (NCA-ReliefF Matched Index) based on matching the indexes obtained from the Neighborhood Component Analysis and Relief algorithms. In the last process, classification was done with the support vector machine algorithm. The proposed methodology was used for dual and multi-class classification tasks and evaluated on a dataset in the Kaggle database named Severstal: Steel Defect Detection.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-08096-2,en,CDZoom: a human-like sequential zoom agent for efficient change detection in large scenes,"['OriginalPaper', 'Original Article']","High-resolution (HR) remote sensing images provide rich information for human activities. However, processing entire HR images is time-consuming, and many computations are meaningless for change detection tasks since objects often cluster in local regions. To alleviate the pressure of downstream detectors, previous studies introduce a regional attention process to roughly sample candidate patches, but most solutions are tailored to particular tasks and datasets. Motivated by these, we develop a novel reinforcement learning sampling framework, and train a human-like agent, named CDZoom, to locate regions of interest by simulating human zooming behaviors. To be specific, the proposed network consists of an encoder block, multiple context blocks and a decision block. It speeds up sequential sampling operations by gradually focusing the scope of observed scene and increasing the resolution. To avoid the sparse reward problem when learning complex sampling tasks, we introduce a novel training paradigm based on curriculum learning and policy distillation. The proposed CDZoom can sample multi-size patches from multi-scale scenes, and thus generalizes well to different requirements. Experiments on public change detection datasets demonstrate the effectiveness of our method. CDZoom can reduce the computational cost by over 50%, while maintaining similar detection accuracy to models which use full HR images.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11263-022-01722-5,en,Bipartite Graph Reasoning GANs for Person Pose and Facial Image Synthesis,OriginalPaper,"We present a novel bipartite graph reasoning Generative Adversarial Network (BiGraphGAN) for two challenging tasks: person pose and facial image synthesis. The proposed graph generator consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed bipartite graph reasoning (BGR) block aims to reason the long-range cross relations between the source and target pose in a bipartite graph, which mitigates some of the challenges caused by pose deformation. Moreover, we propose a new interaction-and-aggregation (IA) block to effectively update and enhance the feature representation capability of both a person’s shape and appearance in an interactive way. To further capture the change in pose of each part more precisely, we propose a novel part-aware bipartite graph reasoning (PBGR) block to decompose the task of reasoning the global structure transformation with a bipartite graph into learning different local transformations for different semantic body/face parts. Experiments on two challenging generation tasks with three public datasets demonstrate the effectiveness of the proposed methods in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1186/s13677-022-00372-9,en,Task offloading in hybrid-decision-based multi-cloud computing network: a cooperative multi-agent deep reinforcement learning,"['OriginalPaper', 'Research']","Multi-cloud computing is becoming a promising paradigm to provide abundant computation resources for Internet-of-Things (IoT) devices. For a multi-device multi-cloud network, the real-time computing requirements, frequently varied wireless channel gains and changeable network scale, make the system more dynamic. It is critical to satisfy the dynamic nature of network with different constraints of IoT devices in multi-cloud environment. In this paper, we establish a continuous-discrete hybrid decision offloading model, each device should learn to make coordinated actions, including cloud server selection, offloading ratio and local computation capacity. Therefore, both continuous-discrete hybrid decision and coordination among IoT devices are challenging. To this end, we first develop a probabilistic method to relax the discrete action (e.g. cloud server selection) to a continuous set. Then, by leveraging a centralized training and distributed execution strategy, we design a cooperative multi-agent deep reinforcement learning (CMADRL) based framework to minimize the total system cost in terms of the energy consumption of IoT device and the renting charge of cloud servers. Each IoT device acts as an agent, which not only learns efficient decentralized policies, but also relieves IoT devices’ computing pressure. Experimental results demonstrate that the proposed CMADRL could efficiently learn dynamic offloading polices at each IoT device, and significantly outperform the four state-of-the-art DRL based agents and two heuristic algorithms with lower system cost.","['Computer Science', 'Computer Communication Networks', 'Special Purpose and Application-Based Systems', 'Information Systems Applications (incl.Internet)', 'Computer Systems Organization and Communication Networks', 'Computer System Implementation', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11235-022-00982-3,en,A privacy preserving federated learning scheme using homomorphic encryption and secret sharing,ReviewPaper,"The performance of machine learning models largely depends on the amount of data. However, with the improvement of privacy awareness, data sharing has become more and more difficult. Federated learning provides a solution for joint machine learning, which alleviates this difficulty. Although it works by sharing parameters instead of data, privacy threats like inference attacks still exist owing to the exposed parameters or updates. In this paper, we propose a privacy preserving scheme for federated learning by combining the homomorphism of both secret sharing and encryption. Our scheme ensures the confidentiality of local parameters and tolerates collusion threats under a certain range. Our scheme also tolerates dropping of some clients, performs aggregation without sharing keys and has simple interaction process. Meantime, we use the automatic protocol tool ProVerif to verify its cryptographic functionality, analyze its theoretical complexity and compare them with similar schemes. We verify our scheme by experiment to show that it has less running time compared with some schemes.","['Business and Management', 'IT in Business', 'Computer Communication Networks', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes']"
doi:10.1186/s12859-022-05071-5,en,ReCSAI: recursive compressed sensing artificial intelligence for confocal lifetime localization microscopy,"['OriginalPaper', 'Research']","Background Localization-based super-resolution microscopy resolves macromolecular structures down to a few nanometers by computationally reconstructing fluorescent emitter coordinates from diffraction-limited spots. The most commonly used algorithms are based on fitting parametric models of the point spread function (PSF) to a measured photon distribution. These algorithms make assumptions about the symmetry of the PSF and thus, do not work well with irregular, non-linear PSFs that occur for example in confocal lifetime imaging, where a laser is scanned across the sample. An alternative method for reconstructing sparse emitter sets from noisy, diffraction-limited images is compressed sensing, but due to its high computational cost it has not yet been widely adopted. Deep neural network fitters have recently emerged as a new competitive method for localization microscopy. They can learn to fit arbitrary PSFs, but require extensive simulated training data and do not generalize well. A method to efficiently fit the irregular PSFs from confocal lifetime localization microscopy combining the advantages of deep learning and compressed sensing would greatly improve the acquisition speed and throughput of this method. Results Here we introduce ReCSAI , a compressed sensing neural network to reconstruct localizations for confocal d STORM, together with a simulation tool to generate training data. We implemented and compared different artificial network architectures, aiming to combine the advantages of compressed sensing and deep learning. We found that a U-Net with a recursive structure inspired by iterative compressed sensing showed the best results on realistic simulated datasets with noise, as well as on real experimentally measured confocal lifetime scanning data. Adding a trainable wavelet denoising layer as prior step further improved the reconstruction quality. Conclusions Our deep learning approach can reach a similar reconstruction accuracy for confocal d STORM as frame binning with traditional fitting without requiring the acquisition of multiple frames. In addition, our work offers generic insights on the reconstruction of sparse measurements from noisy experimental data by combining compressed sensing and deep learning. We provide the trained networks, the code for network training and inference as well as the simulation tool as python code and Jupyter notebooks for easy reproducibility.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1186/s12859-022-05075-1,en,Pattern recognition of topologically associating domains using deep learning,"['OriginalPaper', 'Research']","Background Recent increasing evidence indicates that three-dimensional chromosome structure plays an important role in genomic function. Topologically associating domains (TADs) are self-interacting regions that have been shown to be a chromosomal structural unit. During evolution, these are conserved based on checking synteny block cross species. Are there common TAD patterns across species or cell lines? Results To address the above question, we propose a novel task—TAD recognition—as opposed to traditional TAD identification. Specifically, we treat Hi-C maps as images, thus re-casting TAD recognition as image pattern recognition, for which we use a convolutional neural network and a residual neural network. In addition, we propose an elegant way to generate non-TAD data for binary classification. We demonstrate deep learning performance which is quite promising, AUC > 0.80, through cross-species and cell-type validation. Conclusions TADs have been shown to be conserved during evolution. Interestingly, our results confirm that the TAD recognition model is practical across species, which indicates that TADs between human and mouse show common patterns from an image classification point of view. Our approach could be a new way to identify TAD variations or patterns among Hi-C maps. For example, TADs of two Hi-C maps are conserved if the two classification models are exchangeable.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s42835-022-01343-5,en,The Evaluation Distribution of Runoff Value on Hydroelectric Potential Change-Based RCPs Scenarios and Soft-Computing: A Case Study,"['OriginalPaper', 'Original Article']","Severe climate change, caused by the rise of industry and human activities, is one of the world's major issues affecting energy-generating resources. Anticipating hydropower potential is essential for developing, managing, and operating an optimal hydropower plant. The hydropower potential over the next 20 years is estimated in this study based on climate change. In addition, a novel approach for more accurate runoff estimation has been developed in this work, based on the direct influence of runoff on hydropower potential. The Modified Aquila Optimizer (MAO) algorithm was used to optimize this Deep Learning Neural Network (DLNN) model. The runoff is expected to decrease in the following years, according to the improved model's simulation. The rate of change of hydropower potential will fluctuate from a minimum of around − 112.4 MW to a high of about − 171.23 MW, according to predictive potential predictions. Rising temperatures and reduced rainfall in the following years will cause these negative changes in hydropower capacity.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/s13202-022-01593-z,en,Prediction of permeability of highly heterogeneous hydrocarbon reservoir from conventional petrophysical logs using optimized data-driven algorithms,"['OriginalPaper', 'Original Paper - Production Geophysics']","Permeability is an important parameter in the petrophysical study of a reservoir and serves as a key tool in the development of an oilfield. This is while its prediction, especially in carbonate reservoirs with their relatively lower levels of permeability compared to sandstone reservoirs, is a complicated task as it has larger contributions from heterogeneously distributed vugs and fractures. In this respect, the present research uses the data from two wells (well A for modeling and well B for assessing the generalizability of the developed models) drilled into a carbonate reservoir to estimate the permeability using composite formulations based on least square support vector machine (LSSVM) and multilayer extreme learning machine (MELM) coupled with the so-called cuckoo optimization algorithm (COA), particle swarm optimization (PSO), and genetic algorithm (GA). We further used simple forms of convolutional neural network (CNN) and LSSVM for the sake of comparison. To this end, firstly, the Tukey method was applied to identify and remove the outliers from modeling data. In the next step, the second version of the nondominated sorting genetic algorithm (NSGA-II) was applied to the training data (70% of the entire dataset, selected randomly) to select an optimal group of features that most affect the permeability. The results indicated that although including more input parameters in the modeling added to the resultant coefficient of determination ( R 2 ) while reducing the error successively, yet the slope of the latter reduction got much slow as the number of input parameters exceeded 4. In this respect, petrophysical logs of P-wave travel time, bulk density, neutron porosity, and formation resistivity were identified as the most effective parameters for estimating the permeability. Evaluation of the results of permeability modeling based on root-mean-square error (RMSE) and R 2 shed light on the MELM-COA as the best-performing model in the training and testing stages, as indicated by (RMSE = 0.5600 mD, R 2  = 0.9931) and (RMSE = 0.6019 mD, R 2  = 0.9919), respectively. The generalizability assessment conducted on the prediction of permeability in well B confirmed the MELM-COA can provide reliable permeability predictions by achieving an RMSE of 0.9219 mD. Consequently, the mentioned methodology is strongly recommended for predicting the permeability with high accuracy in similar depth intervals at other wells in the same field should the required dataset be available.","['Earth Sciences', 'Geology', 'Industrial and Production Engineering', 'Energy Systems', 'Offshore Engineering', 'Industrial Chemistry/Chemical Engineering', 'Monitoring/Environmental Analysis']"
doi:10.1007/s00521-022-08099-z,en,Stacked ensemble learning based on deep convolutional neural networks for pediatric pneumonia diagnosis using chest X-ray images,"['OriginalPaper', 'Original Article']","Pneumonia is an acute respiratory infection caused by bacteria, viruses, or fungi and has become very common in children ranging from 1 to 5 years of age. Common symptoms of pneumonia include difficulty breathing due to inflamed or pus and fluid-filled alveoli. The United Nations Children’s Fund reports nearly 800,000 deaths in children due to pneumonia. Delayed diagnosis and overpriced tests are the prime reason for the high mortality rate, especially in underdeveloped countries. A time and cost-efficient diagnosis tool: Chest X-rays, was thus accepted as the standard diagnostic test for pediatric pneumonia. However, the lower radiation levels for diagnosis in children make the task much more onerous and time-consuming. The mentioned challenges initiate the need for a computer-aided detection model that is instantaneous and accurate. Our work proposes a stacked ensemble learning of deep learning-based features for pediatric pneumonia classification. The extracted features from the global average pooling layer of the fine-tuned Xception model pretrained on ImageNet weights are sent to the Kernel Principal Component Analysis for dimensionality reduction. The dimensionally reduced features are further trained and validated on the stacking classifier. The stacking classifier consists of two stages; the first stage uses the Random-Forest classifier, K-Nearest Neighbors, Logistic Regression, XGB classifier, Support Vector Classifier (SVC), Nu-SVC, and MLP classifier. The second stage operates on Logistic Regression using the first stage predictions for the final classification with Stratified K-fold cross-validation to prevent overfitting. The model was tested on the publicly available pediatric pneumonia dataset, achieving an accuracy of 98.3%, precision of 99.29%, recall of 98.36%, F1-score of 98.83%, and an AUC score of 98.24%. The performance shows its reliability for real-time deployment in assisting radiologists and physicians.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10586-022-03807-9,en,A hybridized approach for minimizing energy in cloud computing,OriginalPaper,"Traditional scheduling techniques are designed to reduce processing times while disregarding energy costs. One way of lowering energy usage is to implement scheduling strategies that distribute tasks to specified resources, which influence the processing time and power usage. Among the primary objectives to be achieved in cloud computing, power and energy consumption of the cloud environment have become issues due to ecological and economic reasons. Despite the existence of research efforts from the past pertaining to the same topic, an ideal solution to this problem has not yet been found. One of the main drawbacks of utilizing cloud computing is that the cloud data centers hosting cloud computing applications use higher volumes of energy, adding to the increased functioning cost and carbon footprint in the environment, which in turn increases the need for energy-efficient systems. In this research work, a hybrid optimization algorithm is presented, which is intended to minimize energy in the cloud computing environment. Thus, the crow search algorithm (CSA) and the sparrow search algorithm (SSA) are combined to obtain the proposed hybrid model. The hybrid approach achieves the optimal position in the shortest period of time and with the least amount of energy, load, and makespan, hence improving system performance. The experiments were conducted using three setups with different task sizes. The analysis while using the task size = 300 shows that the proposed method improved QoS, Resource Utilization (RU) and decreased makespan, energy usage, and load at a rate of 0.5073, 4.4035, 0.0331, and 0.0014, respectively.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s12065-022-00801-3,en,Qualitative differences between evolutionary strategies and reinforcement learning methods for control of autonomous agents,"['OriginalPaper', 'Research Paper']","In this paper we analyze the qualitative differences between evolutionary strategies and reinforcement learning algorithms by focusing on two popular state-of-the-art algorithms: the OpenAI-ES evolutionary strategy and the Proximal Policy Optimization (PPO) reinforcement learning algorithm – the most similar methods of the two families. We analyze how the methods differ with respect to: (i) general efficacy, (ii) ability to cope with rewards which are sparse in time, (iii) propensity/capacity to discover minimal solutions, (iv) dependency on reward shaping, and (v) ability to cope with variations of the environmental conditions. The analysis of the performance and of the behavioral strategies displayed by the agents trained with the two methods on benchmark problems enable us to demonstrate qualitative differences which were not identified in previous studies, to identify the relative weakness of the two methods, and to propose ways to ameliorate some of those weaknesses. We show that the characteristics of the reward function has a strong impact which vary qualitatively not only for the OpenAI-ES evolutionary algorithm and the PPO reinforcement learning algorithm but also for other reinforcement learning algorithms, thus demonstrating the importance of optimizing the characteristic of the reward function to the algorithm used.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s12652-022-04482-9,en,An integrated framework based deep learning for cancer classification using microarray datasets,"['OriginalPaper', 'Original Research']","Around the world, cancer is one of the leading reasons of mortality. The importance of earlier detection and prognosis of cancer types is highly significant for patients’ health. In recent research, deep neural networks were trained using gene expression microarray, to classify cancer. Biologists are able to monitor thousands of genes in one experiment using microarray technology. Microarray datasets are considered high-dimensional data, as they are cluttered with irrelevant, redundant, and noisy genes that contribute insignificantly to classification. The most informative genes contributing to cancer classification have been identified using computational intelligence algorithms. In this paper, we propose an integrated framework for cancer classification. This framework is divided into three tasks. Firstly, particle swarm optimization with ensemble learning (PSO-ensemble) reduces the microarray dataset's high dimensionality. Secondly, The Adaptive self-training method (ASTM) is used to solve low-size issues. Finally, a Convolutional Neural Network (CNN) was employed for classification. CNN has the ability to discover the complex non-linear relationships between features and select the most informative. Transfer learning was used sequentially with CNN to integrate the classification procedure because it can reduce the training time and computational complexity. Six microarray datasets are used, namely liver, breast, colon, prostate, central nervous system, and lung. The proposed CNN architecture with transfer learning provided 100% classification accuracy for colon, prostate, CNS and lung microarray datasets, and 97.62%, 95.45% accuracy for liver and breast cancer respectively. Experiments show that our proposed method delivers the highest classification accuracy and reduces training time with the smallest gene subset.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s11263-022-01723-4,en,Revisiting Consistency Regularization for Semi-Supervised Learning,OriginalPaper,"Consistency regularization is one of the most widely-used techniques for semi-supervised learning (SSL). Generally, the aim is to train a model that is invariant to various data augmentations. In this paper, we revisit this idea and find that enforcing invariance by decreasing distances between features from differently augmented images leads to improved performance. However, encouraging equivariance instead, by increasing the feature distance, further improves performance. To this end, we propose an improved consistency regularization framework by a simple yet effective technique, FeatDistLoss, that imposes consistency and equivariance on the classifier and the feature level, respectively. Experimental results show that our model defines a new state of the art across a variety of standard semi-supervised learning benchmarks as well as imbalanced semi-supervised learning benchmarks. Particularly, we outperform previous work by a significant margin in low data regimes and at large imbalance ratios. Extensive experiments are conducted to analyze the method, and the code will be published.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s13762-022-04702-x,en,Comparative analysis of deep and machine learning approaches for daily carbon monoxide pollutant concentration estimation,"['OriginalPaper', 'Original Paper']","Air pollution is an increasingly critical global issue around the world largely attributed to rapid industrial development, the rise in the world’s population, and the ever increasing number of cars on the roads. Air pollution is a serious health risk and environmental concerns in many countries. Turkey has seen major industrial development in recent years which has led to increasing levels of air pollution. In this study, the air pollution levels of Sakarya, a major industrial city in Turkey, is analysed by calculating and modelling daily carbon monoxide ( CO) concentrations. The CO values ​​are calculated using deep learning (Long short-term memory (LSTM) and bidirectional long short-term memory (Bi-LSTM)) models. These models are benchmarked against machine learning (Artificial neural networks (ANN), k-nearest neighbours (kNN), and support vector regression (SVR)) methods. The daily antecedent values ​​of observed CO and PM 10 parameters were used as inputs. Considering the correlation between these parameters, 6 different input scenarios with maximum 3-day delays were created and the daily CO values ​​were calculated. The results showed that the LSTM and Bi-LSTM models were the most effective with a correlation coefficient of R  = 0.91, then the kernel-based SVR was ranked with a correlation coefficient of R  = 0.88. Among the 6 scenarios used, the scenario whereby the inputs were 3-lag time with CO values ​​and one-lag delay of PM 10 values was found to be the most effective. The deep learning methods have been observed to give reliable results in air pollution modelling.","['Environment', 'Environment, general', 'Environmental Science and Engineering', 'Environmental Chemistry', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution', 'Soil Science & Conservation', 'Ecotoxicology']"
doi:10.1186/s12911-022-02054-7,en,Adopting transfer learning for neuroimaging: a comparative analysis with a custom 3D convolution neural network model,"['OriginalPaper', 'Research']","Background In recent years, neuroimaging with deep learning (DL) algorithms have made remarkable advances in the diagnosis of neurodegenerative disorders. However, applying DL in different medical domains is usually challenged by lack of labeled data. To address this challenge, transfer learning (TL) has been applied to use state-of-the-art convolution neural networks pre-trained on natural images. Yet, there are differences in characteristics between medical and natural images, also image classification and targeted medical diagnosis tasks. The purpose of this study is to investigate the performance of specialized and TL in the classification of neurodegenerative disorders using 3D volumes of 18F-FDG-PET brain scans. Results Results show that TL models are suboptimal for classification of neurodegenerative disorders, especially when the objective is to separate more than two disorders. Additionally, specialized CNN model provides better interpretations of predicted diagnosis. Conclusions TL can indeed lead to superior performance on binary classification in timely and data efficient manner, yet for detecting more than a single disorder, TL models do not perform well. Additionally, custom 3D model performs comparably to TL models for binary classification, and interestingly perform better for diagnosis of multiple disorders. The results confirm the superiority of the custom 3D-CNN in providing better explainable model compared to TL adopted ones.","['Medicine & Public Health', 'Health Informatics', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1038/s41598-022-25818-7,en,Optimization of accelerated aqueous ethanol extraction to obtain a polyphenol-rich crude extract from rambutan (Nephelium lappaceum L.) peel as natural antioxidant,"['OriginalPaper', 'Article']","An accelerated solvent extraction method was used to recover polyphenol-rich crude extract from rambutan ( Nephelium lappaceum L.) peel, a waste product from the canning industry. The influence of extraction parameters including temperature, extraction time and ethanol concentration on extraction yield, total phenolic content, total anthocyanin content, and ABTS antioxidant activity was investigated. A Box-Behnken design and response surface methodology were used to optimize the extraction process. Optimal conditions were obtained at temperature, extraction time, and ethanol concentration of 60 °C, 34 min, and 54 vol%, respectively. These optimum conditions gave 333.01 ± 5.84 mg gallic acid/g, 318.28 ± 5.56 mg cyanidin-3-O-glucoside/g, and 3.05 ± 0.04 mmol Trolox/mg for total phenolic content, total anthocyanins content, and ABTS activity, respectively with extraction yield of 28.68 ± 1.48 wt%. Important active compounds found in the extract were geraniin, ellagic acid, shikimic acid and corilagin. Crude extract concentrations of 50–500 mg/kg retarded linoleic acid oxidation but efficacy was lower than synthetic antioxidants at 200 mg/kg. The current findings indicated that accelerated aqueous ethanol extraction was an effective method for the recovery of a crude extract rich in polyphenols from rambutan peel with the potential to be used as a natural antioxidant.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00371-022-02731-8,en,Stroke-based semantic segmentation for scene-level free-hand sketches,"['OriginalPaper', 'Original article']","Sketching is a simple and efficient way for humans to express their perceptions of the world. Sketch semantic segmentation plays a key role in sketch understanding and is widely used in sketch recognition, sketch-based image retrieval, or editing. Due to modality difference between images and sketches, existing image segmentation methods may not perform best, which overlook the sparse nature and stroke-based representation in sketches. The existing sketch semantic segmentation methods are mainly designed for single-instance sketches. In this paper, we present a new stroke-based sequential-spatial neural network (S $$^3$$ 3 NN) for scene-level free-hand sketch semantic segmentation, which leverages a bidirectional LSTM and graph convolutional network to capture the sequential and spatial features of sketches. In order to address the data lacking issue, we propose the first scene-level free-hand sketch dataset (SFSD). SFSD is composed of 12K sketch-photo pairs over 40 object categories, where the sketches were completely hand-drawn and each contains 7 objects on average. We conduct comparative and ablative experiments on SFSD to evaluate the effectiveness of our method. The experimental results demonstrate that our method outperforms state-of-the-art methods. The code, models, and dataset will be made public after acceptance.","['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1186/s12903-022-02614-3,en,A deep learning model based on concatenation approach to predict the time to extract a mandibular third molar tooth,"['OriginalPaper', 'Research']","Background Assessing the time required for tooth extraction is the most important factor to consider before surgeries. The purpose of this study was to create a practical predictive model for assessing the time to extract the mandibular third molar tooth using deep learning. The accuracy of the model was evaluated by comparing the extraction time predicted by deep learning with the actual time required for extraction. Methods A total of 724 panoramic X-ray images and clinical data were used for artificial intelligence (AI) prediction of extraction time. Clinical data such as age, sex, maximum mouth opening, body weight, height, the time from the start of incision to the start of suture, and surgeon’s experience were recorded. Data augmentation and weight balancing were used to improve learning abilities of AI models. Extraction time predicted by the concatenated AI model was compared with the actual extraction time. Results The final combined model (CNN + MLP) model achieved an R value of 0.8315, an R-squared value of 0.6839, a p -value of less than 0.0001, and a mean absolute error (MAE) of 2.95 min with the test dataset. Conclusions Our proposed model for predicting time to extract the mandibular third molar tooth performs well with a high accuracy in clinical practice.","['Dentistry', 'Dentistry', 'Oral and Maxillofacial Surgery']"
doi:10.1007/s10115-022-01793-3,en,Exploiting anonymous entity mentions for named entity linking,"['OriginalPaper', 'Regular Paper']","Named entity linking or named entity disambiguation is to link entity mentions to corresponding entities in a knowledge base for resolving the ambiguity of entity mentions. Recently, collective linking methods exploit document-level coherence of the referenced entities by computing a pairwise score between candidates of a pair of named entity mentions (e.g., Raytheon and Boeing ) in a document. However, in a document, named entity mentions are significantly less frequent than anonymous entity mentions (e.g., defense contractor and the company ). In this paper, we propose a method, DOCument-level Anonymous Entity Type words relatedness (DOC-AET), to exploit the document-level coherence between candidate entities and anonymous entity mentions. We use the anonymous entity type (AET) words to extract anonymous entity mentions. We learn embeddings of AET words from their inter-paragraph co-occurrence matrix; thus, the document-level entity-type relatedness is encoded in the AET word embeddings. Then, we compute the coherence scores between candidate entities and anonymous entity mentions using the AET entity embeddings and document context embeddings. By incorporating such coherence scores for candidates ranking, DOC-AET has achieved new state-of-the-art results on two of the five out-domain test sets for named entity linking.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s10994-022-06241-5,en,Aligning model outputs for class imbalanced non-IID federated learning,OriginalPaper,"Federated Learning (FL) aims to generate a global shared model via collaborating with decentralized edge computing devices with privacy considerations. A significant challenge in FL is the non-IID data partitioning across heterogeneous devices, making the local update diverge a lot and difficult to aggregate. This diversity in local models is caused by the different posterior probability of samples when class distribution skews. Meanwhile, FL often faces imbalanced global data in practical scenarios. By analyzing the relationship between the samples’ posterior probability in different data distributions, we propose a statistically principled probability-corrected loss to align the posterior probability when models are trained on heterogeneous clients. Additionally, we share fixed prototypes on each client to constrain the distribution of heterogeneous clients’ features. Our approach can well handle non-IID FL with balanced and imbalanced global data. We combine our approach with existing FL algorithms and investigate it on common FL benchmarks. Abundant experimental results verify the superiorities of our methods.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1038/s41597-022-01576-z,en,Cov-caldas: A new COVID-19 chest X-Ray dataset from state of Caldas-Colombia,"['OriginalPaper', 'Data Descriptor']","The emergence of COVID-19 as a global pandemic forced researchers worldwide in various disciplines to investigate and propose efficient strategies and/or technologies to prevent COVID-19 from further spreading. One of the main challenges to be overcome is the fast and efficient detection of COVID-19 using deep learning approaches and medical images such as Chest Computed Tomography (CT) and Chest X-ray images. In order to contribute to this challenge, a new dataset was collected in collaboration with “S.E.S Hospital Universitario de Caldas” ( https://hospitaldecaldas.com/ ) from Colombia and organized following the Medical Imaging Data Structure (MIDS) format. The dataset contains 7,307 chest X-ray images divided into 3,077 and 4,230 COVID-19 positive and negative images. Images were subjected to a selection and anonymization process to allow the scientific community to use them freely. Finally, different convolutional neural networks were used to perform technical validation. This dataset contributes to the scientific community by tackling significant limitations regarding data quality and availability for the detection of COVID-19. Measurement(s) Radiologic Examination Technology Type(s) Artificial Intelligence Sample Characteristic - Location Colombia","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41467-022-35276-4,en,An in silico method to assess antibody fragment polyreactivity,"['OriginalPaper', 'Article']","Antibodies are essential biological research tools and important therapeutic agents, but some exhibit non-specific binding to off-target proteins and other biomolecules. Such polyreactive antibodies compromise screening pipelines, lead to incorrect and irreproducible experimental results, and are generally intractable for clinical development. Here, we design a set of experiments using a diverse naïve synthetic camelid antibody fragment (nanobody) library to enable machine learning models to accurately assess polyreactivity from protein sequence (AUC > 0.8). Moreover, our models provide quantitative scoring metrics that predict the effect of amino acid substitutions on polyreactivity. We experimentally test our models’ performance on three independent nanobody scaffolds, where over 90% of predicted substitutions successfully reduced polyreactivity. Importantly, the models allow us to diminish the polyreactivity of an angiotensin II type I receptor antagonist nanobody, without compromising its functional properties. We provide a companion web-server that offers a straightforward means of predicting polyreactivity and polyreactivity-reducing mutations for any given nanobody sequence. Off-target binding hinders the development of therapeutic antibodies and reproducibility in basic research settings. Here the authors develop a method to quantify and reduce the polyreactivity of antibody fragments based on protein sequence alone.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-24431-y,en,A novel time-lapse imaging method for studying developing bacterial biofilms,"['OriginalPaper', 'Article']","In nature, bacteria prevailingly reside in the form of biofilms. These elaborately organized surface-bound assemblages of bacterial cells show numerous features of multicellular organization. We recently showed that biofilm growth is a true developmental process, which resembles developmental processes in multicellular eukaryotes. To study the biofilm growth in a fashion of eukaryotic ontogeny, it is essential to define dynamics and critical transitional phases of this process. The first step in this endeavor is to record the gross morphological changes of biofilm ontogeny under standardized conditions. This visual information is instrumental in guiding the sampling strategy for the later omics analyses of biofilm ontogeny. However, none of the currently available visualizations methods is specifically tailored for recording gross morphology across the whole biofilm development. To address this void, here we present an affordable Arduino-based approach for time-lapse visualization of complete biofilm ontogeny using bright field stereomicroscopy with episcopic illumination. The major challenge in recording biofilm development on the air–solid interphase is water condensation, which compromises filming directly through the lid of a Petri dish. To overcome these trade-offs, we developed an Arduino microcontroller setup which synchronizes a robotic arm, responsible for opening and closing the Petri dish lid, with the activity of a stereomicroscope-mounted camera and lighting conditions. We placed this setup into a microbiological incubator that maintains temperature and humidity during the biofilm growth. As a proof-of-principle, we recorded biofilm development of five Bacillus subtilis strains that show different morphological and developmental dynamics.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40747-022-00931-2,en,Neural network fusion with fine-grained adaptation learning for turnover prediction,"['OriginalPaper', 'Original Article']","Turnover prediction has an important impact on alleviating the brain drain, which can help organizations reduce costs and enhance competitiveness. Existing studies on turnover are mainly based on analyzing the turnover correlation, using different models to predict various employee turnover scenarios, and only predicting turnover category, while the class imbalance and turnover possibility have been ignored. To this end, in this paper, we propose a novel fine-grained adaptation-based turnover prediction neural network (FATPNN) model. Specifically, we first employ a GRU to learn profile-aware features representations of the personnel samples. Then, to evaluate the contribution of various turnover factors, we further exploit an attention mechanism to model the profile information. Finally, we creatively design a weighted-based probability loss function suitable for our turnover prediction tasks. Experimental results show the effectiveness and universality of the FATPNN model in terms of turnover prediction.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s12517-022-11035-z,en,Integration of radar and optical sentinel images for land use mapping in a complex landscape (case study: Arasbaran Protected Area),"['OriginalPaper', 'Original Paper']","Considering the importance of accurate and up-to-date land use/cover (LULC) maps and in a situation of fast LULC changes, an accurate mapping of complex landscapes requires real-time high-resolution remote sensed data and powerful classification algorithms. The new ESA Copernicus satellites Sentinel-1 (S-1) and Sentinel-2 (S-2) have contributed to the effective monitoring of the Earth’s surface. This paper aims at assessing the potential of mono-temporal S-1 and S-2 satellite images and three common classification algorithms including maximum likelihood (ML), support vector machine (SVM), and random forest (RF) for LULC classification. The research methodology consists of a sequence of tasks including data collection and preprocessing, the extraction of texture and spectral features, the definition of several feature set configurations, classification, and accuracy assessment. Based on the results, using S-1 data alone leads to quite poor results, even though dual polarimetric C-band and texture features increased the classification accuracy. The S-2 data outperformed the S-1 data in terms of overall and class level accuracies. A combined use of S-1 and S-2 satellite images involving extracted features from both sources led to the best result for identifying all classes. This emphasizes the critical importance of using multi-modal datasets and different features in the LULC classification. Among classification algorithms, the SVM led to the highest accuracies irrespective of the dataset. To sum it up, according to the applied methodology and results, S-1 and S-2 data can provide optimal and up-to-date information for LULC mapping using non-parametric classifiers as SVM or RF.","['Earth Sciences', 'Earth Sciences, general']"
doi:10.1186/s12885-022-10366-0,en,Automatic segmentation of hepatic metastases on DWI images based on a deep learning method: assessment of tumor treatment response according to the RECIST 1.1 criteria,"['OriginalPaper', 'Research']","Background Evaluation of treated tumors according to Response Evaluation Criteria in Solid Tumors (RECIST) criteria is an important but time-consuming task in medical imaging. Deep learning methods are expected to automate the evaluation process and improve the efficiency of imaging interpretation. Objective To develop an automated algorithm for segmentation of liver metastases based on a deep learning method and assess its efficacy for treatment response assessment according to the RECIST 1.1 criteria. Methods One hundred and sixteen treated patients with clinically confirmed liver metastases were enrolled. All patients had baseline and post-treatment MR images. They were divided into an initial ( n  = 86) and validation cohort ( n  = 30) according to the examined time. The metastatic foci on DWI images were annotated by two researchers in consensus. Then the treatment responses were assessed by the two researchers according to RECIST 1.1 criteria. A 3D U-Net algorithm was trained for automated liver metastases segmentation using the initial cohort. Based on the segmentation of liver metastases, the treatment response was assessed automatically with a rule-based program according to the RECIST 1.1 criteria. The segmentation performance was evaluated using the Dice similarity coefficient (DSC), volumetric similarity (VS), and Hausdorff distance (HD). The area under the curve (AUC) and Kappa statistics were used to assess the accuracy and consistency of the treatment response assessment by the deep learning model and compared with two radiologists [attending radiologist (R1) and fellow radiologist (R2)] in the validation cohort. Results In the validation cohort, the mean DSC, VS, and HD were 0.85 ± 0.08, 0.89 ± 0.09, and 25.53 ± 12.11 mm for the liver metastases segmentation. The accuracies of R1, R2 and automated segmentation-based assessment were 0.77, 0.65, and 0.74, respectively, and the AUC values were 0.81, 0.73, and 0.83, respectively. The consistency of treatment response assessment based on automated segmentation and manual annotation was moderate [ K value: 0.60 (0.34–0.84)]. Conclusion The deep learning-based liver metastases segmentation was capable of evaluating treatment response according to RECIST 1.1 criteria, with comparable results to the junior radiologist and superior to that of the fellow radiologist.","['Biomedicine', 'Cancer Research', 'Oncology', 'Surgical Oncology', 'Health Promotion and Disease Prevention', 'Biomedicine, general', 'Medicine/Public Health, general']"
doi:10.1007/s10489-022-04300-x,en,FIRE: knowledge-enhanced recommendation with feature interaction and intent-aware attention networks,OriginalPaper,"To solve the information overload issue and enhance the user experience of various web applications, recommender systems aim to better model user interests and preferences. Knowledge Graphs (KGs), consisting of real-world objective facts and fruitful entities, play a vital role in recommender systems. Recently, a technological trend has been to develop end-to-end Graph Neural Networks (GNNs)-based knowledge-aware recommendation (a.k.a., Knowledge Graph Recommendation, KGR) models. Unfortunately, current GNNs-based KGR approaches focus on how to capture high-order feature information on KGs while neglecting the following two crucial limitations: 1) The explicitly high-order feature interaction and fusion mechanism and 2) The valid user intent modelling mechanism. As such, these issues lead to insufficient user/item representation learning capability and unsatisfactory KGR performance. In this work, we present a novel Knowledge-enhanced Re commendation with F eature I nteraction and Intent-aware Attention Networks (FIRE) to address the latent intent modelling and high-order feature interaction deficiencies ignored by existing KGR methods. Based on the prototype user/item representation learning leveraging the GNNs-based approach, our model offers the following major improvements: One is the innovative use of Convolutional Neural Networks (CNNs) that perform vertical convolutional (a.k.a., bit-level convolutional) and horizontal convolutional (a.k.a., vector-level convolutional) processes to model multi-granular high-order feature interactions to enhance item-side representation learning. Another is to model users’ latent intent factors by utilizing a two-level attention mechanism (i.e., node- and intent-level attention mechanism) to enhance user-side representation learning. Extensive experiments on three KGs domain public datasets demonstrate that our method outperforms the existing state-of-the-art baseline. Last but not least, numerous ablation- and model studies demystify the working mechanism and elucidate the plausibility of the proposed model.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10343-022-00764-6,en,A Pixel-wise Segmentation Model to Identify Bur Chervil (Anthriscus caucalis M. Bieb.) Within Images from a Cereal Cropping Field,"['OriginalPaper', 'Original Article / Originalbeitrag']","Because of insufficient effectiveness after herbicide application in autumn, bur chervil ( Anthriscus caucalis M. Bieb.) is often present in cereal fields in spring. A second reason for spreading is the warm winter in Europe due to climate change. This weed continues to germinate from autumn to spring. To prevent further spreading, a site-specific control in spring is reasonable. Color imagery would offer cheap and complete monitoring of entire fields. In this study, an end-to-end fully convolutional network approach is presented to detect bur chervil within color images. The dataset consisted of images taken at three sampling dates in spring 2018 in winter wheat and at one date in 2019 in winter rye from the same field. Pixels representing bur chervil were manually annotated in all images. After a random image augmentation was done, a Unet-based convolutional neural network model was trained using 560 (80%) of the sub-images from 2018 (training images). The power of the trained model at the three different sampling dates in 2018 was evaluated at 141 (20%) of the manually annotated sub-images from 2018 and all (100%) sub-images from 2019 (test images). Comparing the estimated and the manually annotated weed plants in the test images the Intersection over Union (Jaccard index) showed mean values in the range of 0.9628 to 0.9909 for the three sampling dates in 2018, and a value of 0.9292 for the one date in 2019. The Dice coefficients yielded mean values in the range of 0.9801 to 0.9954 for 2018 and a value of 0.9605 in 2019. Nach einer Herbizidbehandlung im Herbst zur Kontrolle eines breiten Spektrums von Unkrautarten hat sich in den vergangenen Jahren aufgrund deren geringen Wirkung Hunds-Kerbel ( Anthriscus caucalis M. Bieb.) zum Problemunkraut im Frühjahr in Getreidefeldern entwickelt. Durch die Klimaerwärmung kann diese Unkrautart den ganzen Winter über keimen, was zu deren Ausbreitung zusätzlich beiträgt. Eine teilflächenbezogene Herbizidapplikation im Frühjahr wäre aus ökologischen und ökonomischen Gründen sinnvoll. Der Einsatz von Farbbildkameras an landwirtschaftlichen Maschinen oder unbemannten Fluggeräten bietet eine preiswerte Möglichkeit zum lückenlosen Monitoring kompletter Felder. Im vorliegenden Beitrag wurde ein Unet-basiertes künstliches neuronales Netz verwendet, um in Farbbildern das Unkraut zu identifizieren. Im Frühjahr 2018 erfolgte in einem Winterweizenfeld an 3 Terminen eine Farbbildaufnahme an 38 Beprobungspunkten. Im Folgejahr wurden die Bilder an einem Termin an 36 Punkten im gleichen Feld mit Winterroggen generiert. Eine manuelle Markierung (Annotation) von Hunds-Kerbel erfolgte in allen Bildern unter zur Hilfenahme einer Software. Nachdem die Originalbilder in kleinere Bilder geteilt wurden, geschah das Trainieren des künstliches neuronales Netz Modells an 560 (80 %) der Teilbilder von 2018 (Trainingsbilder). Die Klassifizierungsgüte des trainierten Modells wurde für 2018 anhand 141 (20 %) der Teilbilder und für 2019 anhand aller (100 %) Teilbilder (Testbilder) durchgeführt. Der Vergleich der durch das Modell geschätzten mit den annotierten Hunds-Kerbelpflanzen ergaben zu den drei Aufnahmezeitpunkten 2018 Intersection over Union (Jaccard index) Werte von 0.9628 bis 0.9909 sowie einen Wert von 0.9292 für 2019. Der Dice Koeffizient ergab Werte von 0.9801 bis 0.9954 für 2018 und einen Wert von 0.9605 für 2019.","['Life Sciences', 'Plant Pathology', 'Agriculture']"
doi:10.1007/s13198-022-01814-y,en,A novel edge detection method for medicinal plant's leaf features extraction,"['OriginalPaper', 'ORIGINAL ARTICLE']","Morphological features-based leaf identification algorithms provide highly accurate results. But it is required a single-lined edge extraction algorithm for morphological feature generation. Existing edge extraction algorithms have heavy calculations and higher iteration steps to extract edges. The simplicity of the edge detection algorithm helps to reduce the complexity of the image feature extraction process. In this paper, a fast and straightforward novel edge detection algorithm is introduced in the spatial domain. In a single iteration over all the pixels of the image, our algorithm can achieve a better result than existing edge detection techniques. Also, this paper provides a novel algorithm for leaf shape, vein, apex, and base feature extraction techniques using the edge detection algorithm that can be utilized further for the classification and identification of medicinal plant species or any other plant species too. The performance measure of the proposed edge detection algorithm for leaf features is better as compared to the existing edge detection algorithms. This edge detection algorithm achieved 92% of accuracy and a PSNR rate of 10.88 dB with the time complexity of O(n*m), where n is the height and m is the width of the given image. The importance of medicinal plant identification and existing leaf identification techniques are also discussed in this paper.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1038/s41598-022-25693-2,en,Heterogeneous graph construction and HinSAGE learning from electronic medical records,"['OriginalPaper', 'Article']","Graph representation learning is a method for introducing how to effectively construct and learn patient embeddings using electronic medical records. Adapting the integration will support and advance the previous methods to predict the prognosis of patients in network models. This study aims to address the challenge of implementing a complex and highly heterogeneous dataset, including the following: (1) demonstrating how to build a multi-attributed and multi-relational graph model (2) and applying a downstream disease prediction task of a patient’s prognosis using the HinSAGE algorithm. We present a bipartite graph schema and a graph database construction in detail. The first constructed graph database illustrates a query of a predictive network that provides analytical insights using a graph representation of a patient’s journey. Moreover, we demonstrate an alternative bipartite model where we apply the model to the HinSAGE to perform the link prediction task for predicting the event occurrence. Consequently, the performance evaluation indicated that our heterogeneous graph model was successfully predicted as a baseline model. Overall, our graph database successfully demonstrated efficient real-time query performance and showed HinSAGE implementation to predict cardiovascular disease event outcomes on supervised link prediction learning.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-25572-w,en,Capsule robot pose and mechanism state detection in ultrasound using attention-based hierarchical deep learning,"['OriginalPaper', 'Article']","Ingestible robotic capsules with locomotion capabilities and on-board sampling mechanism have great potential for non-invasive diagnostic and interventional use in the gastrointestinal tract. Real-time tracking of capsule location and operational state is necessary for clinical application, yet remains a significant challenge. To this end, we propose an approach that can simultaneously determine the mechanism state and in-plane 2D pose of millimeter capsule robots in an anatomically representative environment using ultrasound imaging. Our work proposes an attention-based hierarchical deep learning approach and adapts the success of transfer learning towards solving the multi-task tracking problem with limited dataset. To train the neural networks, we generate a representative dataset of a robotic capsule within ex-vivo porcine stomachs. Experimental results show that the accuracy of capsule state classification is 97%, and the mean estimation errors for orientation and centroid position are 2.0 degrees and 0.24 mm (1.7% of the capsule’s body length) on the hold-out test set. Accurate detection of the capsule while manipulated by an external magnet in a porcine stomach and colon is also demonstrated. The results suggest our proposed method has the potential for advancing the wireless capsule-based technologies by providing accurate detection of capsule robots in clinical scenarios.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-08079-3,en,Learning task-specific discriminative representations for multiple object tracking,"['OriginalPaper', 'Original Article']","One-shot multiple object tracking (MOT), which learns object detection and identity embedding in a unified network, has attracted increasing attention due to its low complexity and high tracking speed. However, most one-shot trackers ignore that detection and re-identification (ReID) require different representations of features. The inherent difference between these two subtasks leads to optimization contradictions in the training procedure. This issue would result in suboptimal tracking performance. To alleviate this contradiction, we propose a novel dual-path transformation network (DTN) that decouples the shared features into detection-specific and ReID-specific representations. By learning task-specific features, this module satisfies the different requirements of both subtasks. Moreover, we observe that previous trackers generally utilize local information to distinguish targets and ignore global semantic relations, which are crucial for tracking. Therefore, we design a pyramid non-local network (PNN) that allows our network to explore pixel-to-pixel relations with a global receptive field. Meanwhile, PNN considers the scale information to enhance the robustness to scale variations. Extensive experiments conducted on three benchmarks, i.e., MOT16, MOT17, and MOT20, demonstrate the superiority of our tracker, namely DPTrack. The experimental results reveal that DPTrack achieves state-of-the-art performance, e.g., MOTA of 77.1 $$\%$$ % and IDF1 of 74.9 $$\%$$ % on MOT17. Moreover, DPTrack runs at 14.9FPS, and our lightweight version runs at 26.6FPS with only a slight performance decay.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11590-022-01953-y,en,Practical initialization of the Nelder–Mead method for computationally expensive optimization problems,"['OriginalPaper', 'Original Paper']","Black-box optimization (BBO) algorithms are widely employed by practitioners to address computationally expensive real-world problems such as automatic tuning of machine learning models and evacuation route planning. The Nelder–Mead (NM) method is a well-known local search heuristic for BBO that has been applied to solve many real-world problems from way back because of its promising performance. However, this method has a strong dependence on initialization due to its local search tendency. Nevertheless, a discussion on the proper initialization of the NM method is limited to the recent study by Wessing (Optim Lett 13(4):847–856, 2019), which is solely based on an analysis using the simple sphere function. In this study, we take a further step to improve Wessing’s result by massively investigating how the initialization affects the search performance in views of the initial simplex size and shape and a constraint handling method that is employed on 24 BBO benchmarking problems. Based on the numerical results, we present the empirical best practice for the initialization of the NM method for cases involving a limited evaluation budget.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Computational Intelligence', 'Numerical and Computational Physics, Simulation']"
doi:10.1186/s12903-022-02589-1,en,Caries detection with tooth surface segmentation on intraoral photographic images using deep learning,"['OriginalPaper', 'Research']","Background Intraoral photographic images are helpful in the clinical diagnosis of caries. Moreover, the application of artificial intelligence to these images has been attempted consistently. This study aimed to evaluate a deep learning algorithm for caries detection through the segmentation of the tooth surface using these images. Methods In this prospective study, 2348 in-house intraoral photographic images were collected from 445 participants using a professional intraoral camera at a dental clinic in a university medical centre from October 2020 to December 2021. Images were randomly assigned to training (1638), validation (410), and test (300) datasets. For image segmentation of the tooth surface, classification, and localisation of caries, convolutional neural networks (CNN), namely U-Net, ResNet-18, and Faster R-CNN, were applied. Results For the classification algorithm for caries images, the accuracy and area under the receiver operating characteristic curve were improved to 0.813 and 0.837 from 0.758 to 0.731, respectively, through segmentation of the tooth surface using CNN. Localisation algorithm for carious lesions after segmentation of the tooth area also showed improved performance. For example, sensitivity and average precision improved from 0.890 to 0.889 to 0.865 and 0.868, respectively. Conclusion The deep learning model with segmentation of the tooth surface is promising for caries detection on photographic images from an intraoral camera. This may be an aided diagnostic method for caries with the advantages of being time and cost-saving.","['Dentistry', 'Dentistry', 'Oral and Maxillofacial Surgery']"
doi:10.1007/s00034-022-02255-5,en,Conglomeration of Reptile Search Algorithm and Differential Evolution Algorithm for Optimal Designing of FIR Filter,OriginalPaper,"To solve the problems of sluggish convergence at local minima, a combination of reptile search algorithms with differential evolution (CRSADE) has been developed. The conglomerated algorithm also includes a lens opposition-based learning method, which boosts population diversity and speeds up convergence. The differential evolution algorithm used in the developed CRSADE improves the exploration of the reptile search algorithm through its high ability to locate feasible regions with the best solution. This accelerates convergence by enhancing the end product of the algorithm. The proposed CRSADE helps in designing finite impulse response filters in which absolute error difference is utilized as a fitness function which is minimized by the proposed CRSADE to obtain optimal filter coefficients. To demonstrate its superiority and consistency, a comparison has been made between the developed method and other existing optimization algorithms. The developed filter satisfies the intended objective effectively with lower ripples in the pass band and higher attenuation in the stop band.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s42835-022-01323-9,en,"PI, PID and FOPID Controllers Analysis of Grid Connected Doubly Fed Induction Machine Optimized Using Meta Heuristic Algorithms","['OriginalPaper', 'Original Article']","The focus of the world is now on reducing the carbon footprint which has led to a greater effort for generating electrical energy using non-conventional energy sources particularly wind and solar. The wind turbines are invariably using doubly fed asynchronous generator. This paper considers control of a doubly fed induction machine (DFIM). A proportional integral (PI) controller, proportional integral and derivative (PID) and fractional order PID (FOPID) have been proposed for control of DFIM. The controllers have been optimized using bat optimization and antlion optimization (ALO) resulting in optimized control of the system. The controllers enhanced the system response in terms of settling time, rise time and other parameters. However, FOPID controller gave the overall better performance in terms of parameters like rise time, settling time, settling minimum, peak and peak time. The results were obtained using MATLAB. This paper adequately discusses DFIM operation and optimization method.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1186/s13014-022-02172-6,en,Knowledge-based DVH estimation and optimization for breast VMAT plans with and without avoidance sectors,"['OriginalPaper', 'Research']","Background To analyze RapidPlan knowledge-based models for DVH estimation of organs at risk from breast cancer VMAT plans presenting arc sectors en-face to the breast with zero dose rate, feature imposed during the optimization phase (avoidance sectors AS). Methods CT datasets of twenty left breast patients in deep-inspiration breath-hold were selected. Two VMAT plans, PartArc and AvoidArc, were manually generated with double arcs from ~ 300 to ~ 160°, with the second having an AS en-face to the breast to avoid contralateral breast and lung direct irradiation. Two RapidPlan models were generated from the two plan sets. The two models were evaluated in a closed loop to assess the model performance on plans where the AS were selected or not in the optimization. Results The PartArc plans model estimated DVHs comparable with the original plans. The AvoidArc plans model estimated a DVH pattern with two steps for the contralateral structures when the plan does not contain the AS selected in the optimization phase. This feature produced mean doses of the contralateral breast, averaged over all patients, of 0.4 ± 0.1 Gy, 0.6 ± 0.2 Gy, and 1.1 ± 0.2 Gy for the AvoidArc plan, AvoidArc model estimation, RapidPlan generated plan, respectively. The same figures for the contralateral lung were 0.3 ± 0.1 Gy, 1.6 ± 0.6 Gy, and 1.2 ± 0.5 Gy. The reason was found in the possible incorrect information extracted from the model training plans due to the lack of knowledge about the AS. Conversely, in the case of plans with AS set in the optimization generated with the same AvoidArc model, the estimated and resulting DVHs were comparable. Whenever the AvoidArc model was used to generate DVH estimation for a plan with AS, while the optimization was made on the plan without the AS, the optimizer evidentiated the limitation of a minimum dose rate of 0.2 MU/°, resulting in an increased dose to the contralateral structures respect to the estimation. Conclusions The RapidPlan models for breast planning with VMAT can properly estimate organ at risk DVH. Attention has to be paid to the plan selection and usage for model training in the presence of avoidance sectors.","['Biomedicine', 'Cancer Research', 'Oncology', 'Radiotherapy', 'Imaging / Radiology']"
doi:10.1007/s10462-022-10330-1,en,A dive in white and grey shades of ML and non-ML literature: a multivocal analysis of mathematical expressions,OriginalPaper,"With the advent and advancement of machine learning and deep learning techniques, machine-based recognition systems for mathematical text have captivated the attention of the research community for the last four decades. Mathematical Expression Recognition systems have been identified based on terms of their techniques, approach, dataset, and accuracies. This study majorly targets a rigorous review of both the published form of literature and the least attended literature, i.e., grey literature. Apart from the digital libraries, the papers and other instances of information have been gathered from the grey sources like google patents, archives, technical reports, app stores, etc., culminating in 262 instances. After the heedful filtration imposed on both white and grey literature, the final pool of studies has been investigated for eight formulated research questions. The answers extracted have been analyzed, providing both quantitative and qualitative insights. The analysis and surveys have systematically summed up the potentials of both white and grey shades of literature present on MER and brought exciting extractions out of 155 formal white literature and 107 grey sources. The survey extracts and brings out the highlighting observations after analysis, which sublimates the fact that 52% of grey literature is composed of mobile applications and user interfaces, whereas the published 63% of white data is presently concentrated in 39 different conferences, and the prominent conference is ICDAR (#30). A list of challenges and open issues has been extracted for directing future research dimensions.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s00449-022-02818-5,en,Dynamic modeling and parameter estimation of biomethane production from microalgae co-digestion,"['OriginalPaper', 'Research Paper']","This work proposes a dynamic modeling procedure applied to biomethane production from microalgae residual co-digestion. A two-stage anaerobic digestion representation is selected, considering acidogenesis and methanogenesis as main reaction pathways. Based on the experimental database generated in the University of Mons Laboratories, several candidate models, assuming the presence or absence of biomass dynamics, are suggested, and parametric structural and local identifiability studies are performed. An original parameter estimation procedure is applied to a data-set partition used for model direct validation. The remaining experiment data are dedicated to cross-validation. The results point out how these dynamic models may serve as advanced monitoring software tools such as digital twins, even in the presence of incomplete process data.","['Chemistry', 'Biotechnology', 'Industrial and Production Engineering', 'Environmental Engineering/Biotechnology', 'Industrial Chemistry/Chemical Engineering', 'Food Science']"
doi:10.1007/s11042-022-14277-x,en,Detection of five severity levels of diabetic retinopathy using ensemble deep learning model,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","People who have diabetes can develop diabetic retinopathy, a condition that adversely affects the eyes. Detecting diabetic retinopathy early on allows appropriate treatment to be administered based on the severity of the condition. Our main objective is to determine the severity of the disease using fundus photographs taken under a variety of imaging conditions. This is achieved using an ensemble of convolutional models. Initially, convolutional neural networks are trained on the Diabetic Retinopathy dataset, and then they are stacked to form an ensemble, and again trained against the dataset and five labels. The validation accuracy of the ensemble model is 87.31%. The model outputs the corresponding labels (No-DR, Mild, Moderate, Severe, Proliferate-DR) indicating the degree of severity of the disease.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00477-022-02352-6,en,A spatiotemporal graph convolution-based model for daily runoff prediction in a river network with non-Euclidean topological structure,"['OriginalPaper', 'ORIGINAL PAPER']","Deep learning has gained its prevalence in conducting spatiotemporal runoff simulations. For spatial pattern recognition, a majority of previous spatiotemporal models preferred to receive image-like inputs and to learn their hidden features by convolutional neural networks (CNN) in Euclidean space. However, dealing with geospatially non-Euclidean topology-like structures has not been received sufficient attention in hydrological deep learning modeling and needs a further exploration. The purpose of this paper is to propose an innovative spatiotemporal graph convolution-based model for daily runoff prediction in a river network with non-Euclidean topological structure, named hierarchical static-dynamic spatiotemporal prediction model (HSDSTM). The river network is regarded as a graph, and its runoff stations and topological relationships are represented as nodes and edges of the graph. Both static and dynamic graphs are generated to model the spatial confluence process and to capture fixed attributes and varied correlations of topological connectivity, respectively. Instead of focusing on a single station, the proposed model is applied to simultaneously predict the multi-step ahead daily runoff of the multiple stations of a river network in the Mississippi River Basin (MRB). The results show that the HSDSTM significantly outperforms the baseline models with a higher accuracy at a significance level of 1%, which is examined by Diebold-Mariano test. The separate effectiveness of the static and dynamic graphs is compared and it indicates that the dynamic correlations provide more valuable information to enhance the precision than the static one. Not only does the proposed model achieve a lower error, but also it strengthens the physical meaning, since the static graphs reproduce the static confluence properties and the dynamic graph quantitatively describes the varied contributions of the upstream inflow. In conclusion, the research proves the potential of the proposed spatiotemporal model to accuracy improvements in a topological river network with the physical background.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10884-022-10235-1,en,The 3D Nonlinear Schrödinger Equation with a Constant Magnetic Field Revisited,OriginalPaper,"In this paper, we revisit the Cauchy problem for the three dimensional nonlinear Schrödinger equation with a constant magnetic field. We first establish sufficient conditions that ensure the existence of global in time and finite time blow-up solutions. In particular, we derive sharp thresholds for global existence versus blow-up for the equation with mass-critical and mass-supercritical nonlinearities. We next prove the existence and orbital stability of normalized standing waves which extend the previous known results to the mass-critical and mass-supercritical cases. To show the existence of normalized solitary waves, we present a new approach that avoids the celebrated concentration-compactness principle. Finally, we study the existence and strong instability of ground state standing waves which greatly improve the previous literature.","['Mathematics', 'Ordinary Differential Equations', 'Partial Differential Equations', 'Applications of Mathematics']"
doi:10.1007/s43995-022-00008-8,en,Application of internet of things in healthcare domain,"['ReviewPaper', 'Review']","Internet of things (IoT) enables machine-to-machine, human-to-human and machine-to-human interaction. Recent advancement in IoT systems has positively impacted the daily activities of humans, from accessing information to the delivery of service in real-time. This has improved healthcare management and services, especially in medical hospitals, for effective and timely access to diagnostic information and treatment of patients. Several existing research mainly focused on the design of IoT architecture, its challenges, and benefits to human society with minor or without considering applying IoT in the healthcare domain. To bridge this gap, this study investigates the implications of IoT integration in the healthcare management domain. It presents a detailed discussion on IoT utilization to improve the functionalities of hospital management system. It also discusses some potential emerging innovations that aids the development and application of IoT in hospital management systems. Investigations show that healthcare personnel can administer treatments to patients anytime–anywhere. Patients, especially the elderly are administered treatment, as well as monitoring their wellbeing while at home with the support of wearable sensor devices. Also, some of the challenges that impedes the integration of IoT application into healthcare domain includes generation of irrelevant huge amount of data, issues of security and privacy of patient information and high cost of IoT adoption. Furthermore, the future research trends in adoption of IoT to improve healthcare domain includes stroke and epileptic seizure predictions and prosthetic sensors which is used to retrieve relevant data or information to aid the treatment of a patient in real-time.","['Engineering', 'Engineering, general', 'Architecture, general', 'Computer Science, general']"
doi:10.1007/s11704-022-2146-x,en,Domain-specific feature elimination: multi-source domain adaptation for image classification,"['OriginalPaper', 'Research Article']","Multi-source domain adaptation utilizes multiple source domains to learn the knowledge and transfers it to an unlabeled target domain. To address the problem, most of the existing methods aim to minimize the domain shift by auxiliary distribution alignment objectives, which reduces the effect of domain-specific features. However, without explicitly modeling the domain-specific features, it is not easy to guarantee that the domain-invariant representation extracted from input domains contains domain-specific information as few as possible. In this work, we present a different perspective on MSDA, which employs the idea of feature elimination to reduce the influence of domain-specific features. We design two different ways to extract domain-specific features and total features and construct the domain-invariant representations by eliminating the domain-specific features from total features. The experimental results on different domain adaptation datasets demonstrate the effectiveness of our method and the generalization ability of our model.","['Computer Science', 'Computer Science, general']"
doi:10.1007/s13369-022-07461-6,en,Dry Sliding Wear Investigation of Centrifugally Casted AA6061–B4C Functionally Graded Metal Matrix Composite Material by Response Surface Methodology (RSM),"['OriginalPaper', 'Research Article -Mechanical Engineering']","The current study mainly focused on evaluating the dry sliding wear performance of centrifugally casted functionally gradient AA6061–8 vol% B 4 C composite using response surface methodology (RSM). A cylindrical functionally graded material (FGM) component was produced with the combination of stir and horizontal centrifugal casting techniques. Metallographic characterization by scanning electron microscope has confirmed the gradual variation of B 4 C particles from external to internal periphery of FGM. B 4 C concentration and microhardness value were found to be maximum in outer and minimum in inner zone. A mathematical model is established for the outer zone of FGM to estimate response-specific wear rate considering applied load, sliding velocity, and sliding distance as wear parameters. Dry sliding wear tests were performed utilizing a pin-on-disk equipment in accordance with the central composite design of experiments. The high determination coefficient ( R 2  = 0.9981) indicates the derived model's quality of fit. Furthermore, the optimal wear parameter condition was estimated using response optimizer, ensuing a predicted specific wear rate of 1.8051 × 10 –5 mm 3 /N-m. The validation test was conducted under optimum conditions, and the determined specific wear rate value by the experiment was found as 1.8843 × 10 –5 mm 3 /N-m, indicating reasonable accuracy between predicted and experimental specific wear rates. The morphology of worn-out surface for optimized conditions was analyzed with the help of scanning electron microscopy (SEM) images and energy-dispersive spectroscopy (EDS).","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s12559-022-10084-6,en,"A Multilayer Network-Based Approach to Represent, Explore and Handle Convolutional Neural Networks",OriginalPaper,"Deep learning techniques and tools have experienced enormous growth and widespread diffusion in recent years. Among the areas where deep learning has become more widespread there are computational biology and cognitive neuroscience. At the same time, the need for tools able to explore, understand, and possibly manipulate, a deep learning model has strongly emerged. We propose an approach to map a deep learning model into a multilayer network. Our approach is tailored to Convolutional Neural Networks (CNN), but can be easily extended to other architectures. In order to show how our mapping approach enables the exploration and management of deep learning networks, we illustrate a technique for compressing a CNN. It detects whether there are convolutional layers that can be pruned without losing too much information and, in the affirmative case, returns a new CNN obtained from the original one by pruning such layers. We prove the effectiveness of the multilayer mapping approach and the corresponding compression algorithm on the VGG16 network and two benchmark datasets, namely MNIST, and CALTECH-101. In the former case, we obtain a 0.56% increase in accuracy, precision, and recall, and a 21.43% decrease in mean epoch time. In the latter case, we obtain an 11.09% increase in accuracy, 22.27% increase in precision, 38.66% increase in recall, and 47.22% decrease in mean epoch time. Finally, we compare our multilayer mapping approach with a similar one based on single layers and show the effectiveness of the former. We show that a multilayer network-based approach is able to capture and represent the complexity of a CNN. Furthermore, it allows several manipulations on it. An extensive experimental analysis described in the paper demonstrates the suitability of our approach and the goodness of its performance.","['Computer Science', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Artificial Intelligence', 'Computational Biology/Bioinformatics']"
doi:10.1186/s12859-022-04993-4,en,Extract antibody and antigen names from biomedical literature,"['OriginalPaper', 'Research']","Background The roles of antibody and antigen are indispensable in targeted diagnosis, therapy, and biomedical discovery. On top of that, massive numbers of new scientific articles about antibodies and/or antigens are published each year, which is a precious knowledge resource but has yet been exploited to its full potential. We, therefore, aim to develop a biomedical natural language processing tool that can automatically identify antibody and antigen entities from articles. Results We first annotated an antibody-antigen corpus including 3210 relevant PubMed abstracts using a semi-automatic approach. The Inter-Annotator Agreement score of 3 annotators ranges from 91.46 to 94.31%, indicating that the annotations are consistent and the corpus is reliable. We then used the corpus to develop and optimize BiLSTM-CRF-based and BioBERT-based models. The models achieved overall F1 scores of 62.49% and 81.44%, respectively, which showed potential for newly studied entities. The two models served as foundation for development of a named entity recognition (NER) tool that automatically recognizes antibody and antigen names from biomedical literature. Conclusions Our antibody-antigen NER models enable users to automatically extract antibody and antigen names from scientific articles without manually scanning through vast amounts of data and information in the literature. The output of NER can be used to automatically populate antibody-antigen databases, support antibody validation, and facilitate researchers with the most appropriate antibodies of interest. The packaged NER model is available at https://github.com/TrangDinh44/ABAG_BioBERT.git .","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s11042-022-14202-2,en,Lossless segmentation of cardiac medical images by a resolution consistent network with nondamage data preprocessing,OriginalPaper,"Convolutional neural networks originate from image classification tasks. The pooling operation can expand the receptive field and reduce the amount of calculation, but a large amount of pixel information can be lost, which is obviously harmful to pixel-level segmentation accuracy. Dilation convolution expands the receptive field and keeps the resolution unchanged, but it increases the amount of data storage and calculation. Therefore, dilation convolution can only be applied to a limited number of deep layers in a network. It is common that different samples have different sizes in medical image dataset. The resize operation is widely used in the field to obtain uniform sizes. However, the resize operation adds, deletes and modifies a large number of pixels based on the interpolation method. In this way, an image after resizing is damaged to a certain extent at the pixel level, which also significantly affects the performance of the segmentation network. We propose a resolution-consistent network (RCN), which removes all pooling layers and keeps all resolutions consistent to solve the data loss problem caused by downsampling operations. To solve the problem of the increased amount of data storage and calculation caused by dilated convolution and the data damage caused by the resize operation, we propose a nondamage data preprocessing method that includes a coarse segmentation network, cardiac center point positioning algorithm and nondamage cropping to avoid any resize operations. We achieve state-of-the-art performance and reach first place with respect to some indicators on the widely-used automated cardiac diagnosis challenge (ACDC) dataset. Our average dice scores are 0.951 (left ventricle), 0.915 (right ventricle) and 0.910 (myocardium) on the test set.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10994-022-06262-0,en,Online AutoML: an adaptive AutoML framework for online learning,OriginalPaper,"Automated Machine Learning (AutoML) has been used successfully in settings where the learning task is assumed to be static. In many real-world scenarios, however, the data distribution will evolve over time, and it is yet to be shown whether AutoML techniques can effectively design online pipelines in dynamic environments. This study aims to automate pipeline design for online learning while continuously adapting to data drift. For this purpose, we design an adaptive Online Automated Machine Learning (OAML) system, searching the complete pipeline configuration space of online learners, including preprocessing algorithms and ensembling techniques. This system combines the inherent adaptation capabilities of online learners with fast automated pipeline (re)optimization. Focusing on optimization techniques that can adapt to evolving objectives, we evaluate asynchronous genetic programming and asynchronous successive halving to optimize these pipelines continually. We experiment on real and artificial data streams with varying types of concept drift to test the performance and adaptation capabilities of the proposed system. The results confirm the utility of OAML over popular online learning algorithms and underscore the benefits of continuous pipeline redesign in the presence of data drift.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s12046-022-02041-8,en,vCrop: an automated plant disease prediction using deep ensemble framework using real field images,OriginalPaper,"Plant disease monitoring and management are essential for ensuring reliable and lucrative crop production in all kinds of plantations and guaranteeing sustainable agriculture production. Most traditional approaches depend significantly on human effort, which is liable to time delay and is expensive. Moreover, plant pathogens are nearly identical to other non-harmful species in many circumstances. Recently, computer vision based deep learning algorithms have not been deceived by these similar diseases causing false warnings. This paper proposes a novel deep ensemble neural network (D-ENN) framework for automated plant disease detection. The dataset collected in real cultivated fields contains healthy and diseased images with specific class labels. Since there are limited images of a few specific crops, a conditional generative adversarial network (C-GAN) is leveraged to generate the additional synthetic images. Then, the total dataset is split into the training set, validation set, and test set, in the ratio of 70:10:20 used to avoid overfitting problems. The proposed model is trained using real and synthetic images utilizing the transfer learning mechanism. Finally, the experimental outcomes are assessed using standard performance measures evaluating the performance of the proposed method. The proposed vCrop framework attained Precision, Recall, and F1-Measure, and Accuracy of 95.71%, 95.32%, 95.51%, and 96.02% respectively, in classifying the plant diseases in comparison with the other state-of-the-art approaches. The proposed D-ENN model can be a potentially helpful tool for farmers and agronomists in diagnosing and quantifying cotton diseases.","['Engineering', 'Engineering, general']"
doi:10.1038/s41523-022-00496-w,en,Validation and real-world clinical application of an artificial intelligence algorithm for breast cancer detection in biopsies,"['OriginalPaper', 'Article']","Breast cancer is the most common malignant disease worldwide, with over 2.26 million new cases in 2020. Its diagnosis is determined by a histological review of breast biopsy specimens, which can be labor-intensive, subjective, and error-prone. Artificial Intelligence (AI)—based tools can support cancer detection and classification in breast biopsies ensuring rapid, accurate, and objective diagnosis. We present here the development, external clinical validation, and deployment in routine use of an AI-based quality control solution for breast biopsy review. The underlying AI algorithm is trained to identify 51 different types of clinical and morphological features, and it achieves very high accuracy in a large, multi-site validation study. Specifically, the area under the receiver operating characteristic curves (AUC) for the detection of invasive carcinoma and of ductal carcinoma in situ (DCIS) are 0.99 (specificity and sensitivity of 93.57 and 95.51%, respectively) and 0.98 (specificity and sensitivity of 93.79 and 93.20% respectively), respectively. The AI algorithm differentiates well between subtypes of invasive and different grades of in situ carcinomas with an AUC of 0.97 for invasive ductal carcinoma (IDC) vs. invasive lobular carcinoma (ILC) and AUC of 0.92 for DCIS high grade vs. low grade/atypical ductal hyperplasia, respectively, as well as accurately identifies stromal tumor-infiltrating lymphocytes (TILs) with an AUC of 0.965. Deployment of this AI solution as a real-time quality control solution in clinical routine leads to the identification of cancers initially missed by the reviewing pathologist, demonstrating both clinical utility and accuracy in real-world clinical application.","['Biomedicine', 'Biomedicine, general', 'Cancer Research', 'Oncology', 'Human Genetics', 'Cell Biology']"
doi:10.1186/s13244-022-01331-3,en,Improving the diagnosis of acute ischemic stroke on non-contrast CT using deep learning: a multicenter study,"['OriginalPaper', 'Original Article']","Objective This study aimed to develop a deep learning (DL) model to improve the diagnostic performance of EIC and ASPECTS in acute ischemic stroke (AIS). Methods Acute ischemic stroke patients were retrospectively enrolled from 5 hospitals. We proposed a deep learning model to simultaneously segment the infarct and estimate ASPECTS automatically using baseline CT. The model performance of segmentation and ASPECTS scoring was evaluated using dice similarity coefficient (DSC) and ROC, respectively. Four raters participated in the multi-reader and multicenter (MRMC) experiment to fulfill the region-based ASPECTS reading under the assistance of the model or not. At last, sensitivity, specificity, interpretation time and interrater agreement were used to evaluate the raters’ reading performance. Results In total, 1391 patients were enrolled for model development and 85 patients for external validation with onset to CT scanning time of 176.4 ± 93.6 min and NIHSS of 5 (IQR 2–10). The model achieved a DSC of 0.600 and 0.762 and an AUC of 0.876 (CI 0.846–0.907) and 0.729 (CI 0.679–0.779), in the internal and external validation set, respectively. The assistance of the DL model improved the raters’ average sensitivities and specificities from 0.254 (CI 0.22–0.26) and 0.896 (CI 0.884–0.907), to 0.333 (CI 0.301–0.345) and 0.915 (CI 0.904–0.926), respectively. The average interpretation time of the raters was reduced from 219.0 to 175.7 s ( p  = 0.035). Meanwhile, the interrater agreement increased from 0.741 to 0.980. Conclusions With the assistance of our proposed DL model, radiologists got better performance in the detection of AIS lesions on NCCT. The model simultaneously segments infarcts and estimates ASPECTS by using baseline CT. A mirror-assembly module plus dual-path DCNN model improved the segmentation efficiency. We evaluated the model in a multi-reader and multicenter (MRMC) setting.","['Medicine & Public Health', 'Imaging / Radiology', 'Diagnostic Radiology', 'Interventional Radiology', 'Neuroradiology', 'Ultrasound', 'Internal Medicine']"
doi:10.1007/s11269-022-03401-z,en,Short-term Runoff Prediction Optimization Method Based on BGRU-BP and BLSTM-BP Neural Networks,OriginalPaper,"Runoff forecasting is one of the important non-engineering measures for flood prevention and disaster reduction. The accurate and reliable runoff forecasting mainly depends on the development of science and technology, many machine learning models have been proposed for runoff forecasting in recent years. Considering the non-linearity and real-time of hourly rainfall and runoff data. In this study, two runoff forecasting models were proposed, which were the combination of the bidirectional gated recurrent unit and backpropagation (BGRU-BP) neural network and the bidirectional long short-term memory and backpropagation (BLSTM-BP) neural network. The two models were compared with the gated recurrent unit (GRU), long short-term memory (LSTM), bidirectional gated recurrent unit (BGRU), and bidirectional long short-term memory (BLSTM) models. The research methods were applied to simulate runoff in the Yanglou hydrological station, Northern Anhui Province, China. The results show that the bidirectional models were superior to the unidirectional model, and the backpropagation (BP) based bidirectional models were superior to the bidirectional models. The bidirectional propagation was conducive to improving the generalization ability of the model, and BP neural network could better guide the model to find the optimal nonlinear relationship. The results also show that the BGRU-BP model performs equally well as the BLSTM-BP model. The BGRU-BP model has few parameters and a short training time, so it may be the preferred method for short-term runoff forecasting.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s10278-022-00722-8,en,Convolutional Neural Networks for Classifying Cervical Cancer Types Using Histological Images,OriginalPaper,"Cervical cancer is the most common cancer among women worldwide. The diagnosis and classification of cancer are extremely important, as it influences the optimal treatment and length of survival. The objective was to develop and validate a diagnosis system based on convolutional neural networks (CNN) that identifies cervical malignancies and provides diagnostic interpretability. A total of 8496 labeled histology images were extracted from 229 cervical specimens (cervical squamous cell carcinoma, SCC, n  = 37; cervical adenocarcinoma, AC, n  = 8; nonmalignant cervical tissues, n  = 184). AlexNet, VGG-19, Xception, and ResNet-50 with five-fold cross-validation were constructed to distinguish cervical cancer images from nonmalignant images. The performance of CNNs was quantified in terms of accuracy, precision, recall, and the area under the receiver operating curve (AUC). Six pathologists were recruited to make a comparison with the performance of CNNs. Guided Backpropagation and Gradient-weighted Class Activation Mapping (Grad-CAM) were deployed to highlight the area of high malignant probability. The Xception model had excellent performance in identifying cervical SCC and AC in test sets. For cervical SCC, AUC was 0.98 (internal validation) and 0.974 (external validation). For cervical AC, AUC was 0.966 (internal validation) and 0.958 (external validation). The performance of CNNs falls between experienced and inexperienced pathologists. Grad-CAM and Guided Gard-CAM ensured diagnoses interpretability by highlighting morphological features of malignant changes. CNN is efficient for histological image classification tasks of distinguishing cervical malignancies from benign tissues and could highlight the specific areas of concern. All these findings suggest that CNNs could serve as a diagnostic tool to aid pathologic diagnosis.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s10776-022-00586-3,en,Cyber Forensic Investigation in IoT Using Deep Learning Based Feature Fusion in Big Data,OriginalPaper,"The Internet of Things (IoT) device is becoming universal domain and its success cannot be ignored, but its threats on IoT devices increases concurrently. The Cyber-attacks are becoming the component of IoT affecting user’s life. The professionals are forced to sift huge data to unveil and manage litigations. Hence, secure IoT is required for comprehending attacks. A model is presented for discovering cyber attack considering feature fusion. The routing of data towards Base Station (BS) is done with the Fractional gravitational search algorithm (FGSA). At BS, cybercrime detection is done, wherein data is splitted with enhanced Fuzzy c-means clustering (FCM) considering the MapReduce model. In mapper, the feature fusion is done with mutual information and the Deep Quantum Neural Network (DQNN), while reducer performs cybercrime detection. The Fractional Mayfly Shepherd Optimization (FrMSO)-based Deep Belief Network (DBN) is devised for describing the digital examination to notice and trace behaviors of attacks in IoT. Here, the training of DBN is done by the proposed FrMSO algorithm, which is developed by integrating the Fractional Calculus (FC), Mayfly Optimization Algorithm (MA), and the Shuffled shepherd optimization Algorithm (SSOA). The developed model helps to employ the weights of DBN with FrMSO for determining and tracing the abnormal aspects in IoT. The FrMSO-based DBN presented elevated precision of 96.4%, recall of 98.3% and F-measure of 95.4% respectively.","['Engineering', 'Electrical Engineering']"
doi:10.1038/s41598-022-25632-1,en,Social dilemma in the excess use of antimicrobials incurring antimicrobial resistance,"['OriginalPaper', 'Article']","The emergence of antimicrobial resistance (AMR) caused by the excess use of antimicrobials has come to be recognized as a global threat to public health. There is a ‘tragedy of the commons’ type social dilemma behind this excess use of antimicrobials, which should be recognized by all stakeholders. To address this global threat, we thus surveyed eight countries/areas to determine whether people recognize this dilemma and showed that although more than half of the population pays little, if any, attention to it, almost 20% recognize this social dilemma, and 15–30% of those have a positive attitude toward solving that dilemma. We suspect that increasing individual awareness of this social dilemma contributes to decreasing the frequency of AMR emergencies.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-24720-6,en,Machine learning for exploring neurophysiological functionality in multiple sclerosis based on trigeminal and hand blink reflexes,"['OriginalPaper', 'Article']","Brainstem dysfunctions are very common in Multiple Sclerosis (MS) and are a critical predictive factor for future disability. Brainstem functionality can be explored with blink reflexes, subcortical responses consisting in a blink following a peripheral stimulation. Some reflexes are already employed in clinical practice, such as Trigeminal Blink Reflex (TBR). Here we propose for the first time in MS the exploration of Hand Blink Reflex (HBR), which size is modulated by the proximity of the stimulated hand to the face, reflecting the extension of the peripersonal space. The aim of this work is to test whether Machine Learning (ML) techniques could be used in combination with neurophysiological measurements such as TBR and HBR to improve their clinical information and potentially favour the early detection of brainstem dysfunctionality. HBR and TBR were recorded from a group of People with MS (PwMS) with Relapsing-Remitting form and from a healthy control group. Two AdaBoost classifiers were trained with TBR and HBR features each, for a binary classification task between PwMS and Controls. Both classifiers were able to identify PwMS with an accuracy comparable and even higher than clinicians. Our results indicate that ML techniques could represent a tool for clinicians for investigating brainstem functionality in MS. Also, HBR could be promising when applied in clinical practice, providing additional information about the integrity of brainstem circuits potentially favouring early diagnosis.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s12859-022-05058-2,en,RSAP-Net: joint optic disc and cup segmentation with a residual spatial attention path module and MSRCR-PT pre-processing algorithm,"['OriginalPaper', 'Research']","Background Glaucoma can cause irreversible blindness to people’s eyesight. Since there are no symptoms in its early stage, it is particularly important to accurately segment the optic disc (OD) and optic cup (OC) from fundus medical images for the screening and prevention of glaucoma. In recent years, the mainstream method of OD and OC segmentation is convolution neural network (CNN). However, most existing CNN methods segment OD and OC separately and ignore the a priori information that OC is always contained inside the OD region, which makes the segmentation accuracy of most methods not high enough. Methods This paper proposes a new encoder–decoder segmentation structure, called RSAP-Net, for joint segmentation of OD and OC. We first designed an efficient U-shaped segmentation network as the backbone. Considering the spatial overlap relationship between OD and OC, a new Residual spatial attention path is proposed to connect the encoder–decoder to retain more characteristic information. In order to further improve the segmentation performance, a pre-processing method called MSRCR-PT (Multi-Scale Retinex Colour Recovery and Polar Transformation) has been devised. It incorporates a multi-scale Retinex colour recovery algorithm and a polar coordinate transformation, which can help RSAP-Net to produce more refined boundaries of the optic disc and the optic cup. Results The experimental results show that our method achieves excellent segmentation performance on the Drishti-GS1 standard dataset. In the OD and OC segmentation effects, the F1 scores are 0.9752 and 0.9012, respectively. The BLE are 6.33 pixels and 11.97 pixels, respectively. Conclusions This paper presents a new framework for the joint segmentation of optic discs and optic cups, called RSAP-Net. The framework mainly consists of a U-shaped segmentation skeleton and a residual space attention path module. The design of a pre-processing method called MSRCR-PT for the OD/OC segmentation task can improve segmentation performance. The method was evaluated on the publicly available Drishti-GS1 standard dataset and proved to be effective.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s10278-022-00735-3,en,Calibrating the Dice Loss to Handle Neural Network Overconfidence for Biomedical Image Segmentation,OriginalPaper,"The Dice similarity coefficient (DSC) is both a widely used metric and loss function for biomedical image segmentation due to its robustness to class imbalance. However, it is well known that the DSC loss is poorly calibrated, resulting in overconfident predictions that cannot be usefully interpreted in biomedical and clinical practice. Performance is often the only metric used to evaluate segmentations produced by deep neural networks, and calibration is often neglected. However, calibration is important for translation into biomedical and clinical practice, providing crucial contextual information to model predictions for interpretation by scientists and clinicians. In this study, we provide a simple yet effective extension of the DSC loss, named the DSC++ loss, that selectively modulates the penalty associated with overconfident, incorrect predictions. As a standalone loss function, the DSC++ loss achieves significantly improved calibration over the conventional DSC loss across six well-validated open-source biomedical imaging datasets, including both 2D binary and 3D multi-class segmentation tasks. Similarly, we observe significantly improved calibration when integrating the DSC++ loss into four DSC-based loss functions. Finally, we use softmax thresholding to illustrate that well calibrated outputs enable tailoring of recall-precision bias, which is an important post-processing technique to adapt the model predictions to suit the biomedical or clinical task. The DSC++ loss overcomes the major limitation of the DSC loss, providing a suitable loss function for training deep learning segmentation models for use in biomedical and clinical practice. Source code is available at https://github.com/mlyg/DicePlusPlus .","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1140/epjc/s10052-022-11066-6,en,Application of transfer learning to neutrino interaction classification,"['OriginalPaper', 'Regular Article - Experimental Physics ']","Training deep neural networks using simulations typically requires very large numbers of simulated events. This can be a large computational burden and a limitation in the performance of the deep learning algorithm when insufficient numbers of events can be produced. We investigate the use of transfer learning, where a set of simulated images are used to fine tune a model trained on generic image recognition tasks, to the specific use case of neutrino interaction classification in a liquid argon time projection chamber. A ResNet18, pre-trained on photographic images, was fine-tuned using simulated neutrino images and when trained with one hundred thousand training events reached an F1 score of 0.896 $$\,\pm \,$$ ± 0.002 compared to 0.836 $$\,\pm \,$$ ± 0.004 from a randomly-initialised network trained with the same training sample. The transfer-learned networks also demonstrate lower bias as a function of energy and more balanced performance across different interaction types.","['Physics', 'Elementary Particles, Quantum Field Theory', 'Nuclear Physics, Heavy Ions, Hadrons', 'Quantum Field Theories, String Theory', 'Measurement Science and Instrumentation', 'Astronomy, Astrophysics and Cosmology', 'Nuclear Energy']"
doi:10.1007/s10489-022-04294-6,en,"Effective machine learning, Meta-heuristic algorithms and multi-criteria decision making to minimizing human resource turnover",OriginalPaper,"Employee turnover is one of the most important issues in human resource management, which is a combination of soft and hard skills. This makes it difficult for managers to make decisions. In order to make better decisions, this article has been devoted to identifying factors affecting employee turnover using feature selection approaches such as Recursive Feature Elimination algorithm and Mutual Information and Meta-heuristic algorithms such as Gray Wolf Optimizer and Genetic Algorithm. The use of Multi-Criteria Decision-Making techniques is one of the other approaches used to identify the factors affecting the employee turnover in this article. Our expert has used the Best-Worst Method to evaluate each of these variables. In order to check the performance of each of the above methods and to identify the most significant factors on employee turnover, the results are used in some machine learning algorithms to check their accuracy in predicting the employee turnover. These three methods have been implemented on the human resources dataset of a company and the results show that the factors identified by the Mutual Information algorithm can show better results in predicting the employee turnover. Also, the results confirm that managers need a support tool to make decisions because the possibility of making mistakes in their decisions is high. This approach can be used as a decision support tool by managers and help managers and organizations to have a correct insight into the departure of their employees and adopt policies to retain and optimize their employees.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10462-022-10329-8,en,Physical laws meet machine intelligence: current developments and future directions,OriginalPaper,"The advent of technology including big data has allowed machine learning technology to strengthen its place in solving different science and engineering complex problems. Conventional deep machine learning algorithms work as a black box while dealing with various complex physics-driven problems. This problem can be reduced by integrating the physical laws with machine learning algorithms to ensure the developed models are complied with the physics and are potentially more explainable. This physics-informed machine learning (PIML) approach allows the integration of physical laws in the form of PDEs into the loss function of the neural network, hence, constraining the training of the complex problems based on both the physical, experimental, and mathematical boundaries. This, hence, allows the development of a more general predictive model for different science, engineering, and optimization tasks. Considering such advancements in the machine learning domain, this review presents the systematic progress in the development of integrating physics into the neural networks and recent applications in solving various forward and inverse problems in science and engineering. This paper can serve as a reference for the researchers, developers, and users to get all information they need before developing, implementing, and deploying AI models and smart systems that are equipped with the PIML methodology. It highlights the benefits and points out its limitations and recommendations for further development. The review also compares the traditional data-driven machine learning and PIML approach in dealing with the physics of complex problems. In general, the PIML has been found to provide consistent results with the exact solutions and physical nature of the system. However, similar to other AI system development, a more robust and complex AI algorithm requires more computational power which is also the case in PIML development and implementation. It should be noted that different terminologies such as physics-informed neural networks (PINN), science-informed neural networks, physics-inspired neural networks, and physics-constrained neural networks have been used in the literature that describes the very similar concept of integrating physical laws with machine intelligence. For consistency, we use the PIML term throughout this paper which covers all listed terminologies in this regard.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s40747-022-00929-w,en,Surrogate-assisted evolutionary neural architecture search with network embedding,"['OriginalPaper', 'Original Article']","To accelerate the performance estimation in neural architecture search, recently proposed algorithms adopt surrogate models to predict the performance of neural architectures instead of training the network from scratch. However, it is time-consuming to collect sufficient labeled architectures for surrogate model training. To enhance the capability of surrogate models using a small amount of training data, we propose a surrogate-assisted evolutionary algorithm with network embedding for neural architecture search (SAENAS-NE). Here, an unsupervised learning method is used to generate meaningful representation of each architecture and the architectures with more similar structures are closer in the embedding space, which considerably benefits the training of surrogate models. In addition, a new environmental selection based on a reference population is designed to keep diversity of the population in each generation and an infill criterion for handling the trade-off between convergence and model uncertainty is proposed for re-evaluation. Experimental results on three different NASBench and DARTS search space illustrate that network embedding makes the surrogate model achieve comparable or superior performance. The superiority of our proposed method SAENAS-NE over other state-of-the-art neural architecture algorithm has been verified in the experiments.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s12145-022-00896-3,en,Comparison of different optimized machine learning algorithms for daily river flow forecasting,"['OriginalPaper', 'Research']","River flow modeling is essential for critical aspects such as effective water management and structure planning, together with flood and drought analysis. There has been a growing interest in modeling hydrological systems via machine learning (ML) models. Various optimization techniques are utilized to develop the applications of ML-based hydrological models. The ultimate aim of this research is to establish high performance forecasting model. Therefore, this study conducts river flow modeling by using the daily data attained from a gauge station situated in the Euphrates Basin. For this purpose, Artificial Neural Network (ANN) model was hybridized with five different optimization algorithms i.e., Artificial Bee Colony (ABC), Teaching-Learning Based Optimization (TLBO), Ant Colony Optimization (ACO), Ant-Lion Optimization (ALO), and Imperialist Competitive Algorithm (ICA). In determining the inputs used to create the models, the distribution graph and correlation of the data with the previous period data were examined. The results were evaluated with eleven different statistical parameters and an error matrix. Examining the obtained results, the study revealed all models present high performance. When the results were reviewed in general, it was seen that all determination coefficient (R 2 ) and Nash-Sutcliffe coefficient (NSE) values were higher than 0.962, and other parameters were very close to the optimum. By comparing all the developed hybrid models, ANN-ALO model reported the highest performance for river flow forecasting.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s12652-022-04467-8,en,Development of reinforced learning based non-linear controller for unmanned aerial vehicle,"['OriginalPaper', 'Original Research']","Design complexities of trending UAVs and the operational harsh environments necessitates Control Law formulation utilizing intelligent techniques that are both robust, model-free and adaptable. In this research, an intelligent control architecture for an experimental Unmanned Aerial Vehicle (UAV) having an unconventional inverted V-tail design, is presented. Due to unique design of the vehicle strong roll and yaw coupling exists, making the control of vehicle challenging. To handle UAV’s inherent control complexities, while keeping them computationally acceptable, a variant of distinct Deep Reinforcement learning (DRL) algorithm, namely Reformed Deep Deterministic Policy Gradient (R-DDPG) is proposed. Conventional DDPG algorithm after being modified in its learning architecture becomes capable of intelligently handling the continuous state and control space domains besides controlling the platform in its entire flight regime. The paper illustrates the application of modified DDPG algorithm (namely R-DDPG) towards the design, while the performance of the resulting controller is assessed in simulation using dynamic model of the vehicle. Nonlinear simulations were then performed to analyze UAV performance under different environmental and launch conditions. The effectiveness of the proposed strategy is further demonstrated by comparing the results with the linear controller for the same UAV whose feedback loop gains are optimized by employing technique of optimal control theory achieved through application of Linear quadratic regulator (LQR) based control strategy. The efficacy of the results and performance characteristics, demonstrated the ability of the presented algorithm to dynamically adapt to the changing environment, thereby making it suitable for UAV applications.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1038/s42256-022-00568-3,en,Three types of incremental learning,"['OriginalPaper', 'Article']","Incrementally learning new information from a non-stationary stream of data, referred to as ‘continual learning’, is a key feature of natural intelligence, but a challenging problem for deep neural networks. In recent years, numerous deep learning methods for continual learning have been proposed, but comparing their performances is difficult due to the lack of a common framework. To help address this, we describe three fundamental types, or ‘scenarios’, of continual learning: task-incremental, domain-incremental and class-incremental learning. Each of these scenarios has its own set of challenges. To illustrate this, we provide a comprehensive empirical comparison of currently used continual learning strategies, by performing the Split MNIST and Split CIFAR-100 protocols according to each scenario. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of the effectiveness of different strategies. The proposed categorization aims to structure the continual learning field, by forming a key foundation for clearly defining benchmark problems. A challenge for any machine learning system is to continually adapt to new data. While methods to address this issue are developed, their performance is hard to compare. A new framework to facilitate benchmarking divides approaches into three categories, defined by whether models need to adapt to new tasks, domains or classes.","['Engineering', 'Engineering, general']"
doi:10.1007/s40031-022-00838-z,en,Vehicle Type Detection and Classification Using Enhanced ReliefF Algorithm and Long Short-Term Memory Network,"['OriginalPaper', 'ORIGINAL CONTRIBUTION']","In recent times, vehicle type classification (VTC) is considered important for intelligent transport system. The VTC is a challenging research topic, due to the factors like minimum interclass variance and maximum intraclass variance. In this research manuscript, a novel algorithm is implemented for improving VTC. Firstly, the data are acquired from the MIOvision-Traffic Camera Database (MIO-TCD) and a toll booth database. Further, a camera response method is employed for improving the visual capability of the raw acquired data and then, the Kalman filtering technique and Gaussian mixture model are utilized for recognition and tracking of moving vehicles. Next, the feature-extraction techniques: local binary pattern, steerable pyramid transform, and dual tree complex wavelet transform are employed for extracting feature values, and further, the active feature values are selected using the enhanced reliefF algorithm. Finally, the long short-term memory is accomplished for classifying 11 vehicle categories on MIO-TCD, and six vehicle categories on toll booth database. The simulation results stated that the proposed algorithm attained 99.01% and 95.85% of accuracy on MIO-TCD and toll booth databases, respectively.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s00521-022-08080-w,en,"NC
            
              
            
            $$^2$$
            
              
                
                2
              
            
          E: boosting few-shot learning with novel class center estimation","['OriginalPaper', 'Original Article']","Accurate class distribution estimation is expected to solve the problem of the poor generalization ability that exists in few-shot learning models due to data shortages. However, the reliability of class distributions estimates based on limited samples and knowledge is questionable, especially for similar classes. We find that the distribution calibration method is inaccurate in estimating similar classes due to limited knowledge being reused through double-validation experiments. To address this issue, we propose a novel class center estimation (NC $$^2$$ 2 E) method, which consists of a two-stage center estimation (TCE) algorithm and a class centroid estimation (CCE) algorithm. The class centers estimated by TCE in two stages are closer to the truth, and its superiority is demonstrated by error theory. CCE searches for the centroid of the base class iteratively and is used as the basis for the novel class calibration. Sufficient simulation samples are generated based on the estimated class distribution to augment the training data. The experimental results show that, compared with the distribution calibration method, the proposed method achieves an approximately 1% performance improvement on the miniImageNet and CUB datasets; an approximately 1.45% performance improvement for similar class classification; and an approximately 6.06% performance improvement for non-similar class classification.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12599-022-00780-w,en,"Reuse, Reduce, Support: Design Principles for Green Data Mining","['OriginalPaper', 'Research Paper']","This paper reports on a design science research (DSR) study that develops design principles for “green” – more environmentally sustainable – data mining processes. Grounded in the Cross Industry Standard Process for Data Mining (CRISP-DM) and on a review of relevant literature on data mining methods, Green IT, and Green IS, the study identifies eight design principles that fall into the three categories of reuse, reduce, and support. The paper develops an evaluation strategy and provides empirical evidence for the principles’ utility. It suggests that the results can inform the development of a more general approach towards Green Data Science and provide a suitable lens to study sustainable computing.","['Business and Management', 'IT in Business', 'Business and Management, general']"
doi:10.1007/s00163-022-00405-z,en,An augmented formulation for robust design optimization of structures using stochastic simulation method,"['OriginalPaper', 'Original Paper']","Robust Design Optimization (RDO) of structures has emerged as an important methodology that seeks to determine a design that is insensitive to input variations. Computational complexity of determining mean and variance of the structural performance function explicitly as a function of design variables is a prohibitive factor for solving RDO problems through simulation-based techniques. This becomes even more challenging for a complex system. In this paper, for RDO, an ‘augmented optimization problem’ is formulated for mean and variance minimization. In the augmented formulation, the design variables are artificially introduced as uncertain parameters with some prescribed Probability Density Function (PDF) over the design space. This avoids direct evaluation of the mean or variance of structural performance function for specified design variable values. The resulting augmented optimization problem is solved by performing Stochastic Subset Optimization (SSO) to reduce the volume of the design space, followed by direct search optimization via ranking and selection to locate the optimal robust design. The effectiveness of the proposed approach is demonstrated by means of four examples involving linear and nonlinear structural systems.","['Engineering', 'Engineering Design', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s11548-022-02803-z,en,Jigsaw training-based background reverse attention transformer network for guidewire segmentation,"['OriginalPaper', 'Original Article']","Purpose Guidewire segmentation plays a crucial role in percutaneous coronary intervention. However, it is a challenging task due to the low signal-to-noise ratio of X-ray sequences and the great imbalance between the number of foreground and background pixels. Besides, most existing guidewire segmentation methods are designed for single guidewire segmentation. This paper aims to solve the task of single and dual guidewire segmentation in X-ray fluoroscopy sequences. Methods A jigsaw training-based background reverse attention (BRA) transformer network is proposed. A jigsaw training strategy is used to train the guidewire segmentation network. A BRA module is also designed to reduce the influence of background information. First, robust principal component is conducted to generate background maps for guidewire sequences. Then, BRA is computed on the basis of the background features. Results The experimental results on the dataset collected from three hospitals show that the proposed method can achieve single and dual guidewire segmentation in X-ray fluoroscopy sequences. Higher F1 score and precision than state-of-the-art guidewire segmentation methods can be obtained in most cases. Conclusion The jigsaw training strategy helps reduce the need for dual guidewire data and improve the performance of the network. Our BRA module helps reduce the influence of background information and distinguish the guidewire. The proposed methods can obtain higher performance than state-of-the-art guidewire segmentation methods.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s42979-022-01482-6,en,Diabetic Retinopathy Prevention Using EfficientNetB3 Architecture and Fundus Photography,"['OriginalPaper', 'Original Research']","Classification of stages of diabetic retinopathy (DR) is considered a key step in the assessment and management of diabetic retinopathy. Due to damage caused by high blood sugar in the retinal blood vessels, different microscopic structures can be occupied in the retinal area, such as micro-aneurysms, hard exudate, and neovascularization. The convolutional neural network (CNN) based on deep learning has become a promising method for the analysis of biomedical images. In this work, representative images of diabetic retinopathy (DR) are divided into five categories according to the professional knowledge of ophthalmologists. This article focuses on the use of convolutional neural networks to classify background images of DR according to disease severity and on the application of pooling, Softmax activation to achieve greater accuracy. The aptos2019-blindness-detection database makes it possible to verify the performance of the proposed algorithm.","['Computer Science', 'Computer Science, general', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'Data Structures and Information Theory', 'Information Systems and Communication Service', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s41870-022-01134-1,en,Editorial,"['EditorialNotes', 'Editorial']",,"['Computer Science', 'Computer Science, general', 'Artificial Intelligence', 'Machine Learning', 'Image Processing and Computer Vision', 'Software Engineering', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1038/s41467-022-35149-w,en,Precise atom manipulation through deep reinforcement learning,"['OriginalPaper', 'Article']","Atomic-scale manipulation in scanning tunneling microscopy has enabled the creation of quantum states of matter based on artificial structures and extreme miniaturization of computational circuitry based on individual atoms. The ability to autonomously arrange atomic structures with precision will enable the scaling up of nanoscale fabrication and expand the range of artificial structures hosting exotic quantum states. However, the a priori unknown manipulation parameters, the possibility of spontaneous tip apex changes, and the difficulty of modeling tip-atom interactions make it challenging to select manipulation parameters that can achieve atomic precision throughout extended operations. Here we use deep reinforcement learning (DRL) to control the real-world atom manipulation process. Several state-of-the-art reinforcement learning (RL) techniques are used jointly to boost data efficiency. The DRL agent learns to manipulate Ag adatoms on Ag(111) surfaces with optimal precision and is integrated with path planning algorithms to complete an autonomous atomic assembly system. The results demonstrate that state-of-the-art DRL can offer effective solutions to real-world challenges in nanofabrication and powerful approaches to increasingly complex scientific experiments at the atomic scale. Engineering quantum states requires precise manipulations at the atomic level. Here, the authors use deep reinforcement learning to manipulate Ag adatoms on Ag surfaces, which combined with path planning algorithms enables autonomous atomic assembly.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-07910-1,en,Time-encoded multiplication-free spiking neural networks: application to data classification tasks,"['OriginalPaper', 'Original Article']","Spiking neural networks (SNNs) are mimicking computationally powerful biologically inspired models in which neurons communicate through sequences of spikes, regarded here as sparse binary sequences of zeros and ones. In neuroscience it is conjectured that time encoding, where the information is carried by the temporal position of spikes, is playing a crucial role at least in some parts of the brain where estimation of the spiking rate with a large latency cannot take place. Motivated by the efficiency of temporal coding, compared with the widely used rate coding, the goal of this paper is to develop and train an energy-efficient time-coded deep spiking neural network system. To ensure that the similarity among input stimuli is translated into a correlation of the spike sequences, we introduce correlative temporal encoding and extended correlative temporal encoding techniques to map analog input information into input spike patterns . Importantly, we propose an implementation where all multiplications in the system are replaced with at most a few additions. As a more efficient alternative to both rate-coded SNNs and artificial neural networks, such system represents a preferable solution for the implementation of neuromorphic hardware. We consider data classification tasks where input spike patterns are presented to a feed-forward architecture with leaky-integrate-and-fire neurons. The SNN is trained by backpropagation through time with the objective to match sequences of output spikes with those of specifically designed target spike patterns , each corresponding to exactly one class. During inference the target spike pattern with the smallest van Rossum distance from the output spike pattern determines the class. Extensive simulations indicate that the proposed system achieves a classification accuracy at par with that of state-of-the-art machine learning models.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11227-022-04971-w,en,An optimized sensor node localization approach for wireless sensor networks using RSSI,OriginalPaper,"This article offers an efficient isosceles layout model for node deployment, and a parameter-less Jaya algorithm is also proposed as a solution to the sensor node localization issue in wireless sensor networks (WSNs). The proposed method bases its range measuring on the receive signal strength indicator (RSSI) method. The proposed layout ensures that target nodes are always within the anchor nodes’ transmission range, reducing the influence of RSSI interference and improving node localization accuracy. Compared to the earlier proposed square and equilateral layout models, the proposed model shows significant improvement. The performance of the earlier layout models and the proposed isosceles layout model for node localization is analyzed using a Jaya algorithm and compared with particle swarm optimization (PSO) and salp swarm algorithm (SSA)-based node localization algorithm. The proposed algorithm’s performance is evaluated using scalability and localization accuracy assessments. We investigate the influence of the degree of irregularity on localization accuracy for each layout. The proposed isosceles layout performs 14.16% and 8.56% better than the square and equilateral layout models for the proposed Jaya-based node localization algorithm. The proposed algorithm performs 7.16% and 4.24% better than the PSO- and SSA-based node localization algorithms for the isosceles layout model in WSNs for the same parameters. MATLAB simulation evaluates the proposed layout model and node localization algorithm.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s13369-022-07495-w,en,ASL Hand Gesture Classification and Localization Using Deep Ensemble Neural Network,"['OriginalPaper', 'Research Article-electrical Engineering']","The growing number of deaf and hearing-impaired populations and the evolution of vision-based application devices present enormous opportunities for research and modelling in the domain of gesture classification and recognition. Since hand gesture recognition is crucial in sign language interpretation, an efficient gesture recognition system should consider maximum sign characters for recognition. In this work, a robust hand gesture recognition model is proposed by employing deep ensemble neural networks. Initially, a pre-trained network is designed by adopting the VGG16 architecture with a self-attention layer embedded with the VGG16 architecture. This self-attention module enables to learn the potentially distinguishing image features for better differentiability among gesture categories. After that, a weighted ensemble model is introduced, which uses the complementary information contributed by the base model to magnify the overall network performance. A detailed experimental investigation of the individual pre-trained models and the weighted ensemble of these models is conducted to validate the efficacy of the proposed network. Moreover, this work shows that the proposed model can also be employed for weakly supervised object segmentation. Additionally, the performance of the suggested methodology is evaluated on publicly available two complex hand gesture datasets and obtains an accuracy of 99.76% and 95.10%, which relatively outperforms other state-of-the-art approaches.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s00607-022-01138-6,en,Extended efficiency and soft-fairness multiresource allocation in a cloud computing system,"['OriginalPaper', 'Regular Paper']","Efficiency and fairness are two essential objectives for multiresource allocations in shared cloud computing systems. Due to the different demands of different users and the different capacities of each resource, it is impossible for multiresource allocations to achieve absolute fairness and maximum efficiency simultaneously. In this paper, we generalize dominant resource fairness (DRF) and propose a new allocation mechanism, max–min efficiency DRF (MME-DRF), to achieve a tradeoff between fairness and efficiency. MME-DRF first fairly allocates some resources to ensure a lower bound of relative soft fairness among users. Then, MME-DRF allocates the remaining resources with the goal of maximizing the minimum resource utilization. MME-DRF can obtain a max–min resource utilization that directly reflects the overall resource utilization of the system. Rigorous proofs show that MME-DRF satisfies four desirable properties, e.g., the sharing incentive, soft fairness, Pareto efficiency and weighted envy freeness. In addition, we develop an algorithm for MME-DRF and evaluate it via simulations driven by examples and Google cluster traces. The simulation results show that MME-DRF guarantees soft fairness and significantly improves the resource utilization of the system.","['Computer Science', 'Computer Science, general', 'Information Systems Applications (incl.Internet)', 'Computer Communication Networks', 'Software Engineering', 'Artificial Intelligence', 'Computer Appl. in Administrative Data Processing']"
doi:10.1007/s11042-022-14270-4,en,IoT technology enabled stochastic computing paradigm for numerical simulation of heterogeneous mosquito model,OriginalPaper,"In this communication, a fractional order design and numerical form of the solutions are presented for numerical simulations of heterogeneous mosquito model. The use of the fractional order derivatives is exploited to observe more accurate and exhaustive performances of the numerical simulation of the model. The novel design of the fractional order heterogeneous mosquito differential system is analyzed with stochastic solver based on the internet of things (IoT) technologies, represented with four categories i.e., normal individuals, people with reflex behavior, panic behavior and controlled behavior based differential system. The solutions of the novel design of the fractional order system are presented by using the stochastic paradigm of artificial neural network (ANN) procedures along with the Scaled Conjugate Gradient (SCG), i.e., ANN-SCG, for learning of weights. In ANN-SCG implementation, the data statistics are picked as 78% for training, 11% for both authorization and testing samples to approximate the solutions. The accuracy of the ANN-SCG technique is seen by correlation of the determined outcomes and the information base on Adams-Bashforth-Moulton method based standard solutions. To achieve the capacity, legitimacy, consistent quality, fitness, and accuracy of the ANN-SCG strategy, the reproductions-based error histograms (EHs), MSE, regression, and state transitions (STs) are used for extensive experimentations.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s41660-022-00300-z,en,Manufacturing Sustainability Assessment Comprising Physical and Mental Workload: An Integrated Modified SVSM and AHP Approach,"['OriginalPaper', 'Original Research Paper']","Sustainable manufacturing is critical today because it incorporates economic, environmental, and social factors into the manufacturing process. This research proposes a new framework for assessing manufacturing sustainability, including physical and mental workloads. The analytical hierarchy process (AHP) method is proposed to determine the weight of the triple bottom line (TBL) indicator. Each TBL indicator is evaluated for efficiency before being integrated into the sustainable value stream mapping (VSM) and traffic light system (TLS). Time, quality, raw materials, energy, water consumption, mental workload, and physical workload are the seven indicators used in this study. The framework proposed is applied to a case study in the plastics industry. The results show that the proposed framework can solve industrial problems. The manufacturing sustainability score in this industry is 75.06%, indicating that the company has moderate performance and needs improvement. In addition, this study includes implications and recommendations.","['Engineering', 'Industrial and Production Engineering', 'Sustainable Development', 'Industrial Chemistry/Chemical Engineering', 'Energy Policy, Economics and Management', 'Waste Management/Waste Technology']"
doi:10.1007/s13399-022-03616-5,en,Investigations on the combined effects of Thiobacillus Novellus microorganism and process parameters on the bio-machining of NiTi,"['OriginalPaper', 'Original Article']","This paper discusses the importance and effect of novel microorganisms used in the bio-machining process. The NiTi is used as a work specimen, and Thiobacillus Novellus is used as a microorganism to perform machining operations. The microstructure of the fabricated specimen is more suitable for biomedical applications. The experiment is designed based on the design of experiment (DoE); three different bio-machining parameters are taken as input parameters: shaking speed, pH, and temperature. In addition, the surface roughness ( R a ) and specific metal removal rate (SMRR) are taken as performance measures. The cell concentration of the bio-machining process is kept constant for the designed experiment. Finally, for various process conditions, the effectiveness of Thiobacillus Novellus in the machining of NiTi material is presented. The novel Thiobacillus Novellus microorganism is capable removing more material from the specimen compared to Thiooxidans . The experiment results demonstrated that pH and shaking speed both have a role in achieving a higher SMRR and better R a . Scanning electron microscope (SEM) images are used to understand the type of machining mechanism. The Grey Wolf Algorithm (GWA) optimization method is used to determine the importance of process parameters in achieving a greater SMRR and a better R a . It has been observed that 95 as shaking speed, 25℃ as temperature, and 4.4 as pH and 80 as shaking speed, 25℃ as temperature, and 2.5 as pH are the best combinations for getting a greater SMRR and better R a . The developed model can predict the SMRR and R a with a minimum error of 3.59%.","['Energy', 'Renewable and Green Energy', 'Renewable and Green Energy', 'Biotechnology']"
doi:10.1038/s41467-022-35004-y,en,Single-shot self-supervised object detection in microscopy,"['OriginalPaper', 'Article']","Object detection is a fundamental task in digital microscopy, where machine learning has made great strides in overcoming the limitations of classical approaches. The training of state-of-the-art machine-learning methods almost universally relies on vast amounts of labeled experimental data or the ability to numerically simulate realistic datasets. However, experimental data are often challenging to label and cannot be easily reproduced numerically. Here, we propose a deep-learning method, named LodeSTAR (Localization and detection from Symmetries, Translations And Rotations), that learns to detect microscopic objects with sub-pixel accuracy from a single unlabeled experimental image by exploiting the inherent roto-translational symmetries of this task. We demonstrate that LodeSTAR outperforms traditional methods in terms of accuracy, also when analyzing challenging experimental data containing densely packed cells or noisy backgrounds. Furthermore, by exploiting additional symmetries we show that LodeSTAR can measure other properties, e.g., vertical position and polarizability in holographic microscopy. Object detection using machine learning universally requires vast amounts of training datasets. Midtvedt et al. proposes a deep-learning method that enables detecting microscopic objects with sub-pixel accuracy from a single unlabeled image by exploiting the roto-translational symmetries of the problem.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s12559-022-10083-7,en,Sentiment Analysis and Topic Mining Using a Novel Deep Attention-Based Parallel Dual-Channel Model for Online Course Reviews,OriginalPaper,"The sentiment analysis and topic mining of course reviews are helpful for course improvement and development. In order to improve the quality of online teaching and effectively mine the information such as sentiments contained in course reviews, a novel Deep Attention-based Parallel Dual-Channel Model (DAPDM) is proposed by combining deep learning neural network algorithms. Bidirectional Encoder Representation from Transformers (BERT) is used to train word vectors. Convolutional neural network (CNN) and bi-directional long short-term memory (BiLSTM) with attention mechanism are used to form a dual-channel model to extract sentiment features and enrich semantics. Firstly, a total of 48,501 online course reviews are selected for experiment and analysis. BERT is also used for data enhancement to obtain balanced data. And the data are substituted into DAPDM and 8 other comparative models to verify the model performance. Secondly, the student-course-institution tripartite graph relationship network and the different sentiment feature words co-occurrence network are constructed and visualized to further study the internal relationship among students, courses, and institutions. Finally, the latent dirichlet allocation (LDA) model is used to extract concerns of different sentiments. The classification accuracy, the macro-average of F1 and the weighted average of F1 on DAPDM are respectively improved to 89.44%, 0.8195, and 0.8939 compared with the comparison model. And its receiver operating characteristic (ROC) curve results are optimal. The relationship network can uncover the most popular courses and institutions, and discover that courses serve as a bridge between students and institutions. It is also found that learners’ reviews mainly focus on the course content, technical content, difficulty degree, teachers’ teaching level, etc., which are also the main factors affecting the course learners’ satisfaction with the course. The study can provide theoretical and technical support for the specification and development of online courses.","['Computer Science', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Artificial Intelligence', 'Computational Biology/Bioinformatics']"
doi:10.1038/s42256-022-00570-9,en,Evaluating deep learning for predicting epigenomic profiles,"['OriginalPaper', 'Analysis']","Deep learning has been successful at predicting epigenomic profiles from DNA sequences. Most approaches frame this task as a binary classification relying on peak callers to define functional activity. Recently, quantitative models have emerged to directly predict the experimental coverage values as a regression. As new models with different architectures and training configurations continue to emerge, a major bottleneck is forming due to the lack of ability to fairly assess the novelty of proposed models and their utility for downstream biological discovery. Here we introduce a unified evaluation framework and use it to compare various binary and quantitative models trained to predict chromatin accessibility data. We highlight various modelling choices that affect generalization performance, including a downstream application of predicting variant effects. In addition, we introduce a robustness metric that can be used to enhance model selection and improve variant effect predictions. Our empirical study largely supports that quantitative modelling of epigenomic profiles leads to better generalizability and interpretability. Recent developments in deep learning have allowed for a leap in computational analysis of epigenomic data, but a fair comparison of different architectures is challenging. Toneyan et al. use GOPHER, their new framework for model evaluation and comparison, to perform a comprehensive analysis, exploring modelling choices of deep learning for epigenomic profiles.","['Engineering', 'Engineering, general']"
doi:10.1007/s12237-022-01147-w,en,Insights into Salt Marsh Plant Community Distributions Through Computer Vision and Structural Equation Modeling,OriginalPaper,"Community structure and dynamics are influenced by numerous abiotic and biotic factors requiring large datasets to disentangle, which are often difficult to obtain over the spatiotemporal scales necessary for meaningful analysis. The approach outlined here illustrates one potential solution to this problem by leveraging computer vision methods to gain accurate, in-depth community data from ~ 10,000 photographs of salt marsh plants across an elevation gradient at Sapelo Island, GA, USA. A convolutional neural network (ResNext101) trained to detect the 6 dominant plant species achieved high accuracy for all species, allowing mapping of high-marsh plant communities over gradients in elevation and pore-water salinity. To statistically analyze the high-resolution mapping data, we constructed a structural equations model using the generated data as informed by prevailing ecological theory for salt marshes in the Southeastern United States. Model fit to data was strong, with R 2 values for five of six plant species > 0.7. The distribution of the rare understory perennial Limonium carolinianum , however, was not accurately predicted by the model. Modeled effects of abiotic factors elevation and soil salinity were commensurate with the literature. Biotic interactions also largely conformed to ecological understanding of Southeastern marshes, but a potentially novel positive interaction between Borrichia frutescens and Batis maritima was observed. Overall, this approach shows promise as a method of efficiently generating and statistically analyzing community data for sessile species at scales not previously possible. This study contributes to a growing body of work developing integrated computer vision and big data techniques for ecological field work.","['Environment', 'Environment, general', 'Ecology', 'Freshwater & Marine Ecology', 'Environmental Management', 'Coastal Sciences', 'Water and Health']"
doi:10.1007/s00521-022-08076-6,en,Improving node embedding by a compact neighborhood representation,"['OriginalPaper', 'Original Article']","Graph Embedding, a learning paradigm that represents graph vertices, edges, and other semantic information about a graph into low-dimensional vectors, has found wide applications in different machine learning tasks. In the past few years, we have had a plethora of methods centered on graph embedding using different techniques, such as spectral classification, matrix factorization and learning. In this context, choosing the appropriate dimension of the obtained embedding remains a fundamental issue. In this paper, we propose a compact representation of a node’s neighborhood, including attributes and structure, that can be used as an additional dimension to enrich node embedding, to ensure accuracy. This compact representation ensures that both semantic and structural properties of a node’s neighboring-hood are properly captured in a single dimension. Consequently, we improve the learned embedding from state-of-the-art models by introducing the neighborhood compact representation for each node as an additional layer of dimensionality. We leverage on this neighborhood encoding technique and compare with embedding from state-of-the-art models on two learning tasks: node classification and link prediction. The performance evaluation shows that our approach gives a better prediction and classification accuracy in both tasks.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1038/s41467-022-33407-5,en,Federated learning enables big data for rare cancer boundary detection,"['OriginalPaper', 'Article']","Although machine learning (ML) has shown promise across disciplines, out-of-sample generalizability is concerning. This is currently addressed by sharing multi-site data, but such centralization is challenging/infeasible to scale due to various limitations. Federated ML (FL) provides an alternative paradigm for accurate and generalizable ML, by only sharing numerical model updates. Here we present the largest FL study to-date, involving data from 71 sites across 6 continents, to generate an automatic tumor boundary detector for the rare disease of glioblastoma, reporting the largest such dataset in the literature ( n  = 6, 314). We demonstrate a 33% delineation improvement for the surgically targetable tumor, and 23% for the complete tumor extent, over a publicly trained model. We anticipate our study to: 1) enable more healthcare studies informed by large diverse data, ensuring meaningful results for rare diseases and underrepresented populations, 2) facilitate further analyses for glioblastoma by releasing our consensus model, and 3) demonstrate the FL effectiveness at such scale and task-complexity as a paradigm shift for multi-site collaborations, alleviating the need for data-sharing. Federated ML (FL) provides an alternative to train accurate and generalizable ML models, by only sharing numerical model updates. Here, the authors present the largest FL study to-date to generate an automatic tumor boundary detector for glioblastoma.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-25660-x,en,Deep learning approaches to predict 10-2 visual field from wide-field swept-source optical coherence tomography en face images in glaucoma,"['OriginalPaper', 'Article']","Close monitoring of central visual field (VF) defects with 10-2 VF helps prevent blindness in glaucoma. We aimed to develop a deep learning model to predict 10-2 VF from wide-field swept-source optical coherence tomography (SS-OCT) images. Macular ganglion cell/inner plexiform layer thickness maps with either wide-field en face images (en face model) or retinal nerve fiber layer thickness maps (RNFLT model) were extracted, combined, and preprocessed. Inception-ResNet-V2 was trained to predict 10-2 VF from combined images. Estimation performance was evaluated using mean absolute error (MAE) between actual and predicted threshold values, and the two models were compared with different input data. The training dataset comprised paired 10-2 VF and SS-OCT images of 3,025 eyes of 1,612 participants and the test dataset of 337 eyes of 186 participants. Global prediction errors (MAE point-wise ) were 3.10 and 3.17 dB for the en face and RNFLT models, respectively. The en face model performed better than the RNFLT model in superonasal and inferonasal sectors ( P  = 0.011 and P  = 0.030). Prediction errors were smaller in the inferior versus superior hemifields for both models. The deep learning model effectively predicted 10-2 VF from wide-field SS-OCT images and might help clinicians efficiently individualize the frequency of 10-2 VF in clinical practice.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s12598-022-02156-1,en,State of health and remaining useful life prediction for lithium-ion batteries based on differential thermal voltammetry and a long and short memory neural network,"['OriginalPaper', 'Original Article']","Abstract As the lithium-ion battery is widely applied, the reliability of the battery has become a high-profile content in recent years. Accurate estimation and prediction of state of health (SOH) and remaining useful life (RUL) prediction are crucial for battery management systems. In this paper, the core contribution is the construction of a data-driven model with the long short-term memory (LSTM) network applicable to the time-series regression prediction problem with the integration of two methods, data-driven methods and feature signal analysis. The input features of model are extracted from differential thermal voltammetry (DTV) curves, which could characterize the battery degradation characteristics, so that the accurate prediction of battery capacity fade could be accomplished. Firstly, the DTV curve is smoothed by the Savitzky-Golay filter, and six alternate features are selected based on the connection between DTV curves and battery degradation characteristics. Then, a correlation analysis method is used to further filter the input features and three features that are highly associated with capacity fade are selected as input into the data driven model. The LSTM neural network is trained by using the root mean square propagation (RMSprop) technique and the dropout technique. Finally, the data of four batteries with different health levels are deployed for model construction, verification and comparison. The results show that the proposed method has high accuracy in SOH and RUL prediction and the capacity rebound phenomenon can be accurately estimated. This method can greatly reduce the cost and complexity, and increase the practicability, which provides the basis and guidance for battery data collection and the application of cloud technology and digital twin. Graphical abstract  随着锂离子电池的广泛应用, 电池的可靠性成为近年来备受瞩目的内容。准确估计和预测健康状态 (SOH) 和剩余使用寿命 (RUL) 预测对于电池管理系统至关重要。在本文中, 核心贡献是构建了一个适用于时间序列回归预测问题的 LSTM 网络的数据驱动模型, 集成了数据驱动方法和特征信号分析两种方法。从DTV曲线中提取模型的输入特征, 可以表征电池的退化特性, 从而实现对电池容量衰减的准确预测。首先, 通过 SG 滤波器对 DTV 曲线进行平滑处理, 并根据 DTV 曲线与电池劣化特性之间的联系选择 6 个替代特征。然后, 使用相关分析方法对输入特征进行进一步过滤, 选择与容量衰减高度相关的三个特征作为数据驱动模型的输入。 LSTM NN 通过使用均方根传播 (RMSprop) 技术和 dropout 技术进行训练。最后, 部署四种不同健康等级电池的数据, 进行模型构建、验证和比较。结果表明, 该方法在SOH和RUL预测中具有较高的准确率, 能够准确估计容量反弹现象。这种方法可以大大降低成本和复杂性, 增加实用性, 为电池数据采集以及云技术和数字孪生的应用提供依据和指导。","['Materials Science', 'Metallic Materials', 'Materials Engineering', 'Nanoscale Science and Technology', 'Physical Chemistry', 'Energy, general', 'Biomaterials']"
doi:10.1007/s11042-022-14123-0,en,Action fusion recognition model based on GAT-GRU binary classification networks for human-robot collaborative assembly,OriginalPaper,"Action recognition makes the interaction in human-robot collaboration (HRC) more natural and enhances the efficiency of work. The common action recognition models can neither handle the undefined beforehand transitional actions of operators in HRC nor quickly modify the action classes to be recognized according to the change of collaboration process. In this paper, an action recognition model for HRC assembly is proposed by fusing the outputs of multiple binary classification networks. Moreover, in order to meet both the action recognition speed and accuracy requirements in HRC applications, a spatio-temporal feature extraction network based on graph attention-gated recurrent unit network is designed for binary classification. The proposed model can identify the different operational actions from continuous skeletal data of the operator and distinguish between operational actions and transitional actions. Therefore, this model can reduce false action recognition and thus avoid the mistaken controls and the dangerous actions of robot serves HRC application better. Besides, due to the structure of fusion identification, this model is also well scalable and able to quickly adjust the action classes required to be recognized for HRC task with no need of retraining the entire recognition model. The case study of HRC personal computer assembly demonstrates that the proposed action recognition model achieves the accuracy of about 84% and the best effectiveness.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10098-022-02434-7,en,Comprehensive review of solar radiation modeling based on artificial intelligence and optimization techniques: future concerns and considerations,"['ReviewPaper', 'Review ']","An alternative energy source such as solar is one of the most important renewable resources. A reliable solar radiation prediction is essential for various applications in agriculture, industry, transport, and the environment because they reduce greenhouse gases and are environmentally friendly. Solar radiation data series have embedded fluctuations and noise signals due to complexity, stochasticity, non-stationarity, and nonlinearity with uncertain and time-varying nature. Aside from being highly nonlinear, solar radiation is highly influenced by the environment and environmental parameters such as air temperature, cloud cover, surface reflectivity, and aerosols. In addition, the spatial measurements of these variables are not readily available. To tackle these challenges, it is necessary to consider data preprocessing techniques and to develop and test precise solar radiation predicting models at different forecast horizons. There is, however, controversy regarding the performance of such models in various studies. Comparisons are not conducted systematically among the different studies. Using a critical literature review, the authors hope to answer these questions and believe that further investigation of solar radiation can benefit researchers and practitioners alike. This study presents a comprehensive evaluation of solar radiation modeling using artificial intelligence in the last 15 years and provides a novel detailed analysis of the available models. The studies conducted in different climates of the world that were published in distinguished journals were considered (i.e., 90 papers in total) for this purpose. Newly discovered procedures for optimizing forecasts, data cleaning, feature selection, classification methods, and stand-alone or hybrid data-driven models for solar radiation prediction and modeling were evaluated. The results strikingly showed that the most used artificial intelligence methods were artificial neural network, adaptive neuro-fuzzy inference system, and decision tree family of models. In addition, the extreme learning machine, support vector machine, and particle swarm optimization were the most used optimization techniques in solar radiation modeling. In terms of forecast horizons, the most common forecast horizon found in papers was on the daily scale (51% of studies), followed by the hourly scale (26%), and the least common was the monthly scale (18%). Based on the regional studies, the highest number of solar radiation papers originated from Asia, with Europe in second place and African countries in third place. An increasing trend in the number of papers from 2011 to 2015 was noted, and the second peak started from 2018 till the present. Under each section, a summary of findings is provided. The paper concludes with future thoughts and directions on solar radiation modeling. Graphical abstract The process of artificial intelligence models on observational solar radiation data","['Environment', 'Sustainable Development', 'Industrial Chemistry/Chemical Engineering', 'Industrial and Production Engineering', 'Environmental Engineering/Biotechnology', 'Environmental Economics']"
doi:10.1557/s43579-022-00302-5,en,Toward remote and secure authentication: Disambiguation of magnetic microwire signatures using neural networks,"['Letter', 'Research Letter']","Secure and high-throughput authentication systems require materials with uniquely identifiable responses that can be remotely detected and rapidly disambiguated. To this end, complex electromagnetic responses from arrangements of amorphous ferromagnetic microwires were analyzed using machine learning. These novel materials deliver maximal spectral dispersion when the frequency of incident electromagnetic radiation matches the microwire resonance. Utilizing data obtained from 225 unique microwire arrangements, a neural network reproduced the response distribution of unseen data to a confidence level of 90%, with a mean square error less than 0.01. This favorable performance affirms the potential of magnetic microwires for use in tags for secure article surveillance systems. Graphical Abstract ","['Materials Science', 'Materials Science, general', 'Materials Engineering', 'Nanotechnology', 'Characterization and Evaluation of Materials', 'Biomaterials', 'Polymer Sciences']"
doi:10.1007/s00521-022-08097-1,en,"Deep learning-driven automatic detection of mucilage event in the Sea of Marmara, Turkey","['OriginalPaper', 'Original Article']","A slimy and sticky structure is formed in sea surface due to the excessive proliferation of plantlike organisms called phytoplankton, which is formed by the combination of many biological and chemical conditions, the increase in sea temperature and bacterial activities accordingly. The rapid detection of this structure called mucilage is very important in terms of early intervention and cost determination. Remote sensing methods have been used quite frequently in recent years for the automatic classification and localization of such events with the help of satellite images. Deep convolutional neural networks (DCNNs) trained on mucilage images are applied as a very successful method thanks to their ability to automatically extract superior features. The studies carried out for the target point detection obtained as a result of extracting the visual features from natural images with these networks have reached the goal. In this study, transfer learning methods are proposed to improve the detection of mucilage areas from the satellite images. The Sea of Marmara, which has been difficult times due to the mucilage events, was selected as the study area. The dataset was trained to classify mucilage images with the convolutional neural network (CNN) models and then reused to localize mucilage areas. Residual networks (ResNet)-50, visual geometry group (VGG)-16, VGG-19, and Inception-V3 were used for individual CNN models. Gradient-weighted class activation mapping (Grad-CAM) technique was used to visualize the learned behavior. A custom CNN model was created, and comparisons were made with the real mucilage areas with the intersection over union considering the most efficient convolutional layer to better localize the mucilage areas. It was concluded that the custom CNN model has showed superior localization performance compared to other models.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s13201-022-01830-0,en,River flow prediction based on improved machine learning method: Cuckoo Search-Artificial Neural Network,"['OriginalPaper', 'Original Article']","One of the largest hydropower facilities currently in operation in Malaysia is the Terengganu hydroelectric facility. As a result, for hydropower generation to be sustainable, future water availability in hydropower plants must be known. Therefore, it is necessary to precisely estimate how the river flow will alter as a result of changing rainfall patterns. Finding the best value for the hyper-parameters is one of the problems with machine learning algorithms, which have lately been adopted by many academics. In this research, Artificial Neural Network (ANN) is integrated with a nature-inspired optimizer, namely Cuckoo search algorithm (CS-ANN). The performance of the proposed algorithm then will be examined based on statistical indices namely Root-Mean-Square Error (RSME) and Determination Coefficient ( R 2 ). Then, the accuracy of the proposed model will be then examined with the stand-alone Artificial Neural Network (ANN). The statistical indices results indicate that the proposed Hybrid CS-ANN model showed an improvement based on R 2 value as compared to ANN model with R 2 of 0.900 at training stage and R 2 of 0.935 at testing stage. RMSE value, for ANN model, is 127.79 m 3 /s for training stage and 12.7 m 3 /s at testing stage. While for the proposed Hybrid CS-ANN model, RMSE value is equal to 121.7 m 3 /s for training stage and 10.95 m 3 /s for testing stage. The results revealed that the proposed model outperformed the stand-alone model in predicting the river flow with high level of accuracy. Although the proposed model could be applied in different case study, there is a need to tune the model internal parameters when applied in different case study.","['Earth Sciences', 'Hydrogeology', 'Water Industry/Water Technologies', 'Industrial and Production Engineering', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution', 'Nanotechnology', 'Private International Law, International & Foreign Law, Comparative Law']"
doi:10.1007/s11709-022-0882-5,en,Bridging finite element and deep learning: High-resolution stress distribution prediction in structural components,"['OriginalPaper', 'Research Article']","Finite-element analysis (FEA) for structures has been broadly used to conduct stress analysis of various civil and mechanical engineering structures. Conventional methods, such as FEA, provide high fidelity results but require the solution of large linear systems that can be computationally intensive. Instead, Deep Learning (DL) techniques can generate results significantly faster than conventional run-time analysis. This can prove extremely valuable in real-time structural assessment applications. Our proposed method uses deep neural networks in the form of convolutional neural networks (CNN) to bypass the FEA and predict high-resolution stress distributions on loaded steel plates with variable loading and boundary conditions. The CNN was designed and trained to use the geometry, boundary conditions, and load as input to predict the stress contours. The proposed technique’s performance was compared to finite-element simulations using a partial differential equation (PDE) solver. The trained DL model can predict the stress distributions with a mean absolute error of 0.9% and an absolute peak error of 0.46% for the von Mises stress distribution. This study shows the feasibility and potential of using DL techniques to bypass FEA for stress analysis applications.","['Engineering', 'Civil Engineering', 'Cities, Countries, Regions']"
doi:10.1007/s40031-022-00822-7,en,Detection of WBC Cancer Using Image Processing,"['OriginalPaper', 'Original Contribution']","White blood cells (WBC) Cancer detection is a tedious task because cancer diseases worsen rapidly in a short period of time. The manual diagnosing systems used today to detect WBC cancers are lumbar puncture, bone marrow biopsy and lymph node biopsy. These systems are not only time consuming and expensive, but might be inaccurate sometimes which leads to a misdiagnosis in most cases posing a life-threatening situation for patients. To avoid these situations, this paper proposes a method by which an automated system is developed and designed to ease the detection of cancer disease in a short period of time and also which is cost efficient. The aim of the system is to produce a highly accurate and promising results in diagnosing the cancer using digital image processing method of Machine Learning. And an automatic model is developed to work without any requirement of the lab technicians to analyse the result. A robust segmentation, clustering and deep learning techniques like multi-layer perceptron will be used and support vector machine has been used for classification to achieve accurate results on the bone marrow images by training the system. The proposed system will detect the WBC cancer cells with the less amount of dataset by undergoing several processes.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s10462-022-10340-z,en,Hybrid Archimedes optimization algorithm enhanced with mutualism scheme for global optimization problems,OriginalPaper,"Archimedes optimization algorithm (AOA) is a recent metaheuristic method inspired by the Archimedes principle, which is the law of physics. Like other metaheuristic methods, it suffers from the disadvantages of being stuck in local areas, suffering from weak exploitation abilities, and an inability to maintain a balance between exploration and exploitation. To overcome these weaknesses, a new hybrid Mutualism Archimedes Optimization Algorithm (MAOA) method has been proposed by combining the AOA and the mutation phase in the Symbiosis organism search (SOS) method. SOS algorithm is known for its exploitation ability. With the mutation phase, it has been used to improve local search for swarm agents, help prevent premature convergence and increase population diversity. To verify the applicability and performance of the proposed algorithm, extensive analysis of standard benchmark functions, CEC’17 test suites, and engineering design problems were performed. The proposed method is compared with the recently emerged and popular AOA, SOS, Harris Hawks Optimization (HHO), COOT Optimization Algorithm (COOT), Aquila Optimizer (AO), Salp Swarm Algorithm (SSA), and Multi-Verse Optimization (MVO) methods, and statistical analyses were performed. The results obtained from the experiments show that the proposed MAOA method has superior global search performance and faster convergence speed compared to AOA, SOS, and other recently emerged and popular metaheuristic methods. Furthermore, this study compares MAOA to five well-established and recent algorithms constructed using various metaheuristic methodologies utilizing nine benchmark datasets to assess the general competence of MAOA in feature selection. Therefore, the proposed method is considered to be a promising optimization method for real-world engineering design problems, global optimization problems, and feature selection.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s12065-022-00797-w,en,A novel coevolutionary multi-objective particle swarm optimization based on decomposition,"['ReviewPaper', 'Review Article']","To improve the performance of particle swarm optimization (PSO) and balance convergence and diversity, we propose a coevolutionary multi-objective particle swarm optimization based on decomposition (CMOPSO). CMOPSO includes 3 strategies, coevolutionary mechanism, best-effort strategy and weighted maximum approach. Coevolutionary mechanism is used to maintain convergence, while PSO operator focuses on diversity. Best-effort strategy allows operators that perform well enough to execute again, which improves the utilization of computing resources. Weighted maximum approach is an environmental selection strategy based on decomposition, which selects by comparing the maximum of weighted objective values of the subproblem. Each new individual will be compared with the best individual of all subproblems, not only in the sub domain, which helps the PSO operator to maintain the diversity in search process. The CMOPSOD is tested against 6 other algorithms on the ZDT and UF test problems, the results show that the proposed CMOPSOD has significant advantages in terms of convergence and diversity.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s10845-022-02050-8,en,Heterogeneous demand–capacity synchronization for smart assembly cell line based on artificial intelligence-enabled IIoT,OriginalPaper,"An assembly cell line (ACL) is one type of cell production practice, derived from the Toyota Production System in the electronics industry and rapidly spread to other fields. In this mode, the conveyor line is divided into assembly cells (ACs) where various parts and tools are placed closer to the workers, enabling them to perform multiple tasks throughout an entire product assembly from start to finish. In this way, ACL allows manufacturers to rapidly configure an appropriate heterogeneous capacity to match heterogeneous demands with diversified customer orders in the high-mix, low-volume (HMLV) environment, which is the spread of the Just-In-Time (JIT) philosophy from the material level to the organization level. However, due to the lack of real-time information sharing in the ACL workshop, especially the up-to-date individual capacity and asynchronous production processes within and between ACs, it is hard to coordinate the heterogeneous capacities of ACs to meet the HMLV demands in a complex manufacturing environment with uncertainties. In this context, this paper proposes a heterogeneous demand–capacity synchronization (HDCS) for smart ACL by using artificial intelligence-enabled IIoT (AIoT) technologies, in which computer vision (CV) is applied for up-to-date capacity analysis of ACs. Based on these, an AIoT-enabled Graduation Intelligent Manufacturing System (GiMS) with feedback loops is developed to support real-time information sharing for the synchronous coordination of the ACL operation, which also provides the basis for the implementation of the HDCS mechanism through a rolling scheduling approach. Finally, a real-life industrial case is carried out by a proof-of-concept prototype to verify the proposed approach, and the results show that the measures on shipment punctuality and production efficiency are both significantly improved.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00500-022-07621-8,en,A novel elitist fruit fly optimization algorithm,"['OriginalPaper', 'Optimization']","Aiming at the poor population diversity and serious imbalance between global exploration and local exploitation in the original fruit fly optimization algorithm (FOA), a novel elitist fruit fly optimization algorithm (EFOA) with elite guidance and population diversity maintenance is proposed. EFOA consists of two search phases: an osphresis search with elite and random individual guiding and a vision search with elite and boundary guiding in an iteration. The former contains two sub-stages: exploration with random individual guiding and exploitation with elite individual guiding. Randomly selected individual and flight control parameter constructed by the Sigmoid-based function are first introduced into the algorithm to improve the exploration. The elite guiding strategy with two position-update approaches is designed to augment the local ability of the proposed algorithm. With these stages, EFOA can search some areas of the problem space as much as possible. Finally, elite and boundary information is introduced into EFOA to enhance population diversity. The proposed EFOA is compared with other algorithms, including the original FOA, three outstanding FOA variants, and five state-of-the-art meta-heuristic algorithms. The validation tests are conducted based on the classical benchmark functions and CEC2017 benchmark functions. The Wilcoxon signed rank test and Friedman test are utilized to verify the significance of the results from the perspective of non-parametric statistics. The results demonstrate that the elite guiding strategy and the alternating execution of the three search stages can effectively balance the exploration and exploitation capabilities of the EFOA and enhance its convergence speed.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11831-022-09849-x,en,25 Years of Particle Swarm Optimization: Flourishing Voyage of Two Decades,"['ReviewPaper', 'Review article']","From the past few decades many nature inspired algorithms have been developed and gaining more popularity because of their effectiveness in solving problems of distinct application domains. Undoubtedly, Particle swarm optimization (PSO) algorithm is the most successful optimization algorithm among the available nature inspired algorithms such as simulated annealing, genetic algorithm, differential evolution, firefly, cuckoo etc., because of its high efficiency and capability to adjust in different dynamic environments. This year marks its 25th anniversary of PSO, one of the base inspirations for many modern-day metaheuristics development. Because of its simple structure and few number of algorithmic parameters, PSO from its origin has acquired widespread popularity amongst researchers, technocrats and practitioners and has been proven to provide better performance in various functional areas such as networking, robotics, image segmentation, power generation and controlling, fuzzy systems and so on. PSO is a population based global heuristic optimization approach motivated by the social behavior of animals chasing for food such as flock of birds, schools of fish. PSO attempts to stabilize exploration and exploitation by combining local search capabilities with global search capabilities. In this article, an in-depth analysis of PSO with its developments from 1995 to 2020 has been presented. Mainly, the improved variants of PSO along with solvable application areas are discussed in detail to provide a scope for the further development. At the end of the paper, the growth of the PSO in various application areas has been presented with factual representation. The main motive of this survey is to inspire the researchers, practitioners and technocrats to develop improved and innovative solutions for solving complex problems in various domains using PSO.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s12530-022-09474-w,en,Adaptive octopus deep transfer learning based epileptic seizure classification on field programmable gate arrays,"['OriginalPaper', 'Original Paper']","Seizures are a type of neurological illness that can disrupt the processes of the human brain. In most cases, epileptic abnormalities may be detected with direct visual scanning. However, owing to various technical artefacts, this scanning takes more time and is limited. As a result, an effective deep learning-based computer-aided diagnosis system for automatically differentiating seizure signals from non-seizure signals is required. Even if the classification accuracy of deep learning algorithms is sufficient, executing them on field programmable gate arrays (FPGA) is computationally quite expensive. In this paper, a new adaptive octopus deep transfer learning (AODTL) based epileptic seizure classification model is proposed to identify the best trade-off between the classification accuracy and hardware complexity. The proposed model selects the most significant features from the scalogram images using jellyfish search optimizer. Also, it fine-tunes the hyper-parameters automatically using the octopus optimizer. These optimizers are used to reduce the number of parameters required for the proposed AODTL classifier, so that the computational complexity is reduced. The implementation of the proposed work is carried out in Xilinx working platform and validated on the Temple University Hospital Seizure Detection Corpus (TUH EEG) database. Finally, the result of the proposed method showed that the diagnosis and classification of deep transfer learning model with maximum accuracy can be accomplished on FPGA. The maximum performance of 99.48% accuracy, latency of 6.1 ms, slice LUTs of 898 and power of 1.043 µW are achieved when testing on the FPGA board for classifying the epileptic seizures.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s12293-022-00386-5,en,A single-solution–compact hybrid algorithm for continuous optimization,"['OriginalPaper', 'Regular research paper']","This research paper proposes a memetic algorithm based on a hybridization of two metaheuristic approaches, a single-solution method and a compact optimization algorithm. The hybrid algorithm is thus a bi-module framework, where each module encapsulates a different search logic. Both modules use the Non-Uniform Mutation, although with different flavors: the first one acting on a single variable at a time, the second one acting on multiple variables. Hence, the algorithm is dubbed “compact Single/Multi Non-Uniform Mutation” (in short, cSM). It is designed for being suitable for tackling optimization problems on memory-constrained devices, i.e., devices for which the available memory may be not enough to run population-based metaheuristics. The performance of cSM is evaluated by an extensive comparative analysis including 12 state-of-the-art memory-saving (also called “lightweight”) algorithms on three well-known testbeds, namely the BBOB, the CEC-2014, and CEC-2017 benchmarks, as well as seven real-world optimization problems included in the CEC-2011 benchmark. In the case of the CEC benchmarks, our method is also compared against the top (population-based) algorithms that participated in respective competitions. The numerical results indicate that, compared to all the other lightweight algorithms under study, the proposed algorithm is better at handling most functions at different dimensionalities, especially in the case of non-separable problems.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Complex Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s10346-022-01997-2,en,Landslide detection based on shipborne images and deep learning models: a case study in the Three Gorges Reservoir Area in China,"['OriginalPaper', 'Original Paper']","Landslides are one of the main geological disasters in the riparian zone of the Three Gorges Reservoir Area (TGRA) in China, causing massive losses in terms of industrial and agricultural production and people’s lives and property. Due to the difficulty of data collection and lack of facade information, the remote sensing methods currently used for landslide mapping and detection are insufficient. The current landslide information acquisition method is mainly based on visual interpretation and automatic computer classification from aerial orthophotographs, which is subjective and time-consuming and yields low accuracy. New shipborne photogrammetry has unique advantages in small and steep slope landslide detection. In recent years, the deep learning technology represented by convolutional neural networks (CNNs) exhibits more powerful feature learning ability in image recognition than traditional machine learning technology. Meanwhile, transfer learning can make the network suitable for small datasets through fine-tuning, which could transfer the parameters of the pretrained model to the new model in a certain way, thereby increasing the learning efficiency of the model. In this paper, the training of multiple deep learning models is realized through transfer learning, and automatic identification of landslides based on shipborne images is achieved through decision-level fusion. The experimental results show that through the integration of decision-level fusion and transfer learning, it performs well in the classification of landslides based on shipborne images.","['Earth Sciences', 'Natural Hazards', 'Geography, general', 'Agriculture', 'Civil Engineering']"
doi:10.1007/s10489-022-04285-7,en,Stock index prediction based on multi-time scale learning with multi-graph attention networks,OriginalPaper,"We present a stock index prediction model based on a multi-time scale learning (MTSL) and the multi-graph attention network (MGAT) approach. Instead of dealing with individual stock markets, we consider a group of stock markets simultaneously and exploit the effects of their interactions when making predictions. Our model consists of a boosting Hodrick-Prescott (bHP) filter and MGATs, along with MTSL processes. The bHP filter decomposes the stock index series into the slow-varying growth trend and the fast-varying cyclical volatility for training the MGAT to facilitate the learning of multi-time scale data. The MGAT exploits the interplays of stock markets with three different types of graphs: a regionality graph that qualitatively describes the linkages among stock markets within the same region or similar financial systems; a correlativity graph that quantifies the Pearson correlation between stock markets; and a causality graph that measures the convergence cross mapping (CCM) causality between stock markets. Particularly, the last graph is essentially a directed graph, which captures the nonreciprocal relationship between different stock markets. Experimental results on real stock indices reveal the effectiveness and merit of the proposed model in comparison with other models adopted in this paper.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10825-022-01978-7,en,An efficient technique to predict DC characteristics of nano-FinFETs using a deep neural network,OriginalPaper,"This paper presents a deep neural network (DNN) model which predicts DC characteristics of fin field-effect transistors (FinFETs) prior to their fabrication. The model is trained using simulated FinFET characteristics, wherein device physical fabrication parameters including gate length ( L g ), fin height ( H fin ), fin thickness ( T fin ), and fin length ( L fin ) as well as oxide thickness ( T ox ) and doping density ( N d ) are varied to obtain a complete training dataset. The dataset is attained using simulations carried out by COMSOL Multiphysics software. While simulating the device characteristics, by engaging a relatively powerful machine (Intel Xeon Silver 4110 CPU with 8 cores and 16 threads running at 3.0 GHz using 64 GB RAM), it is observed that the time required to attain the device response by varying its physical parameters ranges from 6 min to 41 h. On the other hand, once trained, the proposed DNN model has the ability to predict the device response within a fraction of a second. The average mean square error (MSE) ≤ 2.29 × 10 −3 is achieved by employing the approach presented herein. Thus, the proposed technique can be used by design engineers to efficiently predict FinFET characteristics prior to their fabrication within the regime for which the system was trained.","['Engineering', 'Mathematical and Computational Engineering', 'Electrical Engineering', 'Theoretical, Mathematical and Computational Physics', 'Optical and Electronic Materials', 'Mechanical Engineering']"
doi:10.1007/s11263-022-01721-6,en,Active Perception for Visual-Language Navigation,OriginalPaper,"Visual-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to take navigation actions at every step. In contrast, when humans face such a challenge, we can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make a more confident navigation decision. This work draws inspiration from human navigation behavior and endows an agent with an active perception ability for more intelligent navigation. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides (i) when and where to explore, (ii) what information is worth gathering during exploration, and (iii) how to adjust the navigation decision after the exploration. In this way, the agent is able to turn its past experiences as well as new explored knowledge to contexts for more confident navigation decision making. In addition, an external memory is used to explicitly store the visited visual environments and thus allows the agent to adopt a late action-taking strategy to avoid duplicate exploration and navigation movements. Our experimental results on two standard benchmark datasets show promising exploration strategies emerged from training, which leads to significant boost in navigation performance.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11334-022-00507-w,en,Optimized classification model for plant diseases using generative adversarial networks,"['OriginalPaper', 'S.i. : Intelligence for Systems and Software Engineering']","The agricultural industry, the service sector, and the food processing industry are just a few of the many aspects that affect a country’s economy. One of the most important sectors of the economy in our nation is agriculture. The agriculture industry, however, encounters numerous challenges, such as diverse climatic conditions in various parts of our nation that give rise to various infectious illnesses in various plant sections, leading to a significant decline in crop output and income generation. An essential improvement in the framework for harvest formation is the early and accurate detection of plant diseases. Because it demands knowledge, the traditional approach of eye observation is ineffective for detecting plant diseases. Machine learning (ML) approaches are being used to start the identification and classification of plant illnesses to solve this problem. This project offers an analysis of these various methods. A review of different ML techniques for accurate plant disease identification is done. The major areas of system design, model design, value prediction from observation, and experience from the massive amount of data and diverse gathering are the focus of machine learning (ML), a subset of artificial intelligence techniques. Optimized convolutional neural networks are used in this study to classify various plant leaf diseases. The dataset is enhanced using a generative adversarial network. The model is trained and tested using data from PlantVillage. Images of plant diseases on pepper, tomato, and potato plants are included in the dataset. The classifier is trained and tested using 15 categories of plant diseases. The model’s overall accuracy is 98%.","['Computer Science', 'Software Engineering', 'Artificial Intelligence', 'Computer Applications']"
doi:10.1038/s41598-022-25133-1,en,The formulation of irrigation and nitrogen application strategies under multi-dimensional soil fertility targets based on preference neural network,"['OriginalPaper', 'Article']","With the aim of improving soil fertility, it is of great significance to put forward optimal irrigation and nitrogen fertilizer application strategies for improving land productivity and alleviating non-point source pollution effects. To overcome this task, a 6-hidden layer neural network with a preference mechanism, namely Preference Neural network (PNN), has been developed in this study based on the field data from 2018 to 2020. PNN takes soil total nitrogen, organic matter, total salt, pH, irrigation time and target soil depth as input, and irrigation amount and nitrogen application rate (N rate) as output, and the prior preference matrix was used to adjust the learning of weight matrix of each layer. The outcomes indicated that the predictive accuracy of PNN for irrigation amount were (R 2  = 0.913, MAE = 0.018, RMSE = 0.022), and for N rate were (R 2  = 0.943, MAE = 0.009, RMSE = 0.011). The R 2 predicted by PNN at the irrigation amount and N rate were 40.03% to more than 99% and 40.33% to more than 99% higher than those obtained using support vector regression (SVR), linear regression (LR), logistic regression (LOR) and traditional back propagation neural network (BPNN), respectively. In addition, compared with the neural network (Reverse Multilayer Perceptron, RMLP) with the same structure but no preference structure, the R 2 of the predicted irrigation amount and N rate by PNN increased by 25.81% and 27.99%, respectively. The results showed that, through the irrigation of 93 to 102, 92 to 98 and 92 to 98 mm, along with nitrogen applications of 65 to 71, 64 to 73 and 72 to 81 kg/hm 2 at 17, 59 and 87 days after sowing, respectively, the organic matter, total nitrogen, total salt content and pH of the soil would reach high fertility levels simultaneously.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10514-022-10074-5,en,That was not what I was aiming at! Differentiating human intent and outcome in a physically dynamic throwing task,OriginalPaper,"Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person’s reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1D-CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11042-022-14247-3,en,LWSNet - a novel deep-learning architecture to segregate Covid-19 and pneumonia from x-ray imagery,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Automatic detection of lung diseases using AI-based tools became very much necessary to handle the huge number of cases occurring across the globe and support the doctors. This paper proposed a novel deep learning architecture named LWSNet (Light Weight Stacking Network) to separate Covid-19, cold pneumonia, and normal chest x-ray images. This framework is based on single, double, triple, and quadruple stack mechanisms to address the above-mentioned tri-class problem. In this framework, a truncated version of standard deep learning models and a lightweight CNN model was considered to conviniently deploy in resource-constraint devices. An evaluation was conducted on three publicly available datasets alongwith their combination. We received 97.28%, 96.50%, 97.41%, and 98.54% highest classification accuracies using quadruple stack. On further investigation, we found, using LWSNet, the average accuracy got improved from individual model to quadruple model by 2.31%, 2.55%, 2.88%, and 2.26% on four respective datasets.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s41060-022-00376-3,en,Learning attentive attribute-aware node embeddings in dynamic environments,"['OriginalPaper', 'Regular Paper']","Learning node embeddings is fundamental for numerous applications, such as link prediction and node classification. Node embeddings seek to learn a low-dimensional representation for each node in the graph. Many existing node representation learning methods for dynamic attributed graphs focus on preserving the temporal proximity of the nodes with relatively shallow models. However, real-life graphs are complex and usually exhibit evolutionary patterns of node attributes and graph structure. Therefore, the current state-of-the-art models fail to capture the information in the dynamic attributed graphs and settle for sub-optimal results. In this paper, we propose a novel model for embedding nodes in dynamic attributed graphs that captures the full extent of all relevant node information and the graph interrelations as well as graph evolutionary patterns. During model training, attribute-aware node embedding is learned using both graph and node properties in a dynamic context. Experiments demonstrate that our proposed method is superior to the state-of-the-art models in link prediction tasks. In addition, it introduces a novel way of learning richer representations by fully exploiting node attributes, graph structure, and evolutionary patterns in dynamic attributed graphs.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Database Management', 'Artificial Intelligence', 'Computational Biology/Bioinformatics', 'Business Information Systems']"
doi:10.1038/s41467-022-35233-1,en,Graph-based autoencoder integrates spatial transcriptomics with chromatin images and identifies joint biomarkers for Alzheimer’s disease,"['OriginalPaper', 'Article']","Tissue development and disease lead to changes in cellular organization, nuclear morphology, and gene expression, which can be jointly measured by spatial transcriptomic technologies. However, methods for jointly analyzing the different spatial data modalities in 3D are still lacking. We present a computational framework to integrate Spatial Transcriptomic data using over-parameterized graph-based Autoencoders with Chromatin Imaging data (STACI) to identify molecular and functional alterations in tissues. STACI incorporates multiple modalities in a single representation for downstream tasks, enables the prediction of spatial transcriptomic data from nuclear images in unseen tissue sections, and provides built-in batch correction of gene expression and tissue morphology through over-parameterization. We apply STACI to analyze the spatio-temporal progression of Alzheimer’s disease and identify the associated nuclear morphometric and coupled gene expression features. Collectively, we demonstrate the importance of characterizing disease progression by integrating multiple data modalities and its potential for the discovery of disease biomarkers. Methods for jointly analysing the different spatial data modalities in 3D are lacking. Here the authors report the computational framework STACI (Spatial Transcriptomic data using over-parameterized graph-based Autoencoders with Chromatin Imaging data) which they apply to an Alzheimer’s disease mouse model.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11356-022-23944-3,en,An approach of multi-element fusion method for harmful algal blooms prediction,"['OriginalPaper', 'Research Article']","The harmful algal blooms (HABs) are an issue of concern for water management worldwide. Effective strategies for monitoring and predicting of HAB spatio-temporal variability in waterbodies are more essential. To promote the monitoring and predicting of HABs, we proposed a multi-element fusion prediction (MEFP) method for cyanobacteria bloom. Considering the impact of surrounding factors for HAB occurrence, the proposed MEFP fuses multiple exogenous factors to enhance the prediction accuracy in different environments. Specifically, MEFP adopts a dual-sides network that parallelly captures the potential outbreak patterns on the numerous input features. The restricted Boltzmann machine is utilized to optimize the processing of parameter initialization. Subsequently, the attention mechanism is introduced in the post-network stage to establish the contextual relationship between the current and historical temporal information. The experimental results on the real-world dataset demonstrate the proposed MEFP model outperforms other benchmark methods.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s11760-022-02410-0,en,Edge-guided dynamic feature fusion network for object detection under foggy conditions,"['OriginalPaper', 'Original Paper']","Hazy images are often subject to blurring, low contrast and other visible quality degradation, making it challenging to solve object detection tasks. Most methods solve the domain shift problem by deep domain adaptive technology, ignoring the inaccurate object classification and localization caused by quality degradation. Different from common methods, we present an edge-guided dynamic feature fusion network (EDFFNet), which formulates the edge head as a guide to the localization task. Despite the edge head being straightforward, we demonstrate that it makes the model pay attention to the edge of object instances and improves the generalization and localization ability of the network. Considering the fuzzy details and the multi-scale problem of hazy images, we propose a dynamic fusion feature pyramid network (DF-FPN) to enhance the feature representation ability of the whole model. A unique advantage of DF-FPN is that the contribution to the fused feature map will dynamically adjust with the learning of the network. Extensive experiments verify that EDFFNet achieves 2.4 $$\%$$ % AP and 3.6 $$\%$$ % AP gains over the ATSS baseline on RTTS and Foggy Cityscapes, respectively.","['Computer Science', 'Image Processing and Computer Vision', 'Signal,Image and Speech Processing', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Multimedia Information Systems']"
doi:10.1186/s40537-022-00656-6,en,Arabic aspect sentiment polarity classification using BERT,"['OriginalPaper', 'Research']","Aspect-based sentiment analysis (ABSA) is a textual analysis methodology that defines the polarity of opinions on certain aspects related to specific targets. The majority of research on ABSA is in English, with a small amount of work available in Arabic. Most previous Arabic research has relied on deep learning models that depend primarily on context-independent word embeddings (e.g. word2vec), where each word has a fixed representation independent of its context. This article explores the modeling capabilities of contextual embeddings from pre-trained language models, such as BERT, and making use of sentence pair input on Arabic aspect sentiment polarity classification task. In particular, we develop a simple but effective BERT-based neural baseline to handle this task. Our BERT architecture with a simple linear classification layer surpassed the state-of-the-art works, according to the experimental results on three different Arabic datasets. Achieving an accuracy of 89.51% on the Arabic hotel reviews dataset, 73.23% on the Human annotated book reviews dataset, and 85.73% on the Arabic news dataset.","['Computer Science', 'Database Management', 'Information Storage and Retrieval', 'Data Mining and Knowledge Discovery', 'Computational Science and Engineering', 'Mathematical Applications in Computer Science', 'Communications Engineering, Networks']"
doi:10.1007/s11760-022-02390-1,en,Vehicle detection algorithm based on lightweight YOLOX,"['OriginalPaper', 'Original Paper']","Nowadays, accurate and fast vehicle detection technology is of great significance for constructing intelligent transportation systems in the context of the era of big data. This paper proposes an improved lightweight YOLOX real-time vehicle detection algorithm. Compared with the original network, the detection speed and accuracy of the new algorithm have been improved with fewer parameters. First, referring to the GhostNet, we make a lightweight design of the backbone extraction network, which significantly reduces the network parameters, training cost, and inference time. Furthermore, by introducing the α-CIoU loss function, the regression accuracy of the bounding box (bbox) is improved, while the convergence speed of the model is also accelerated. The experimental results show that the mAP of the improved algorithm on the BIT-Vehicle dataset can reach up to 99.21% with 41.2% fewer network parameters and 12.7% higher FPS than the original network and demonstrate the effectiveness of our proposed method.","['Computer Science', 'Image Processing and Computer Vision', 'Signal,Image and Speech Processing', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Multimedia Information Systems']"
doi:10.1007/s10489-022-04138-3,en,Rumour detection technology based on the BiGRU_capsule network,OriginalPaper,"False information, such as rumours, has seriously affected our daily lives and threatened the political, economic, cultural and other fields. Therefore, rumour detection on social media has become a hot research topic in recent years. The development of deep learning algorithms provides new technology for rumour detection. It has been shown that capsule networks have several advantages, while their validity in the domain of text has been less explored. An effective method named the BiGRU_capsule network model has been recently proposed. This method can fully consider the contextual relationship without the loss of text spatial structure information. The proposed model introduces features of user personas and uses a two-way GRU (BiGRU) network to improve the capsule network, which can improve rumour detection accuracy. Experimental results show that compared with benchmark rumour detection models on the Twitter dataset, the accuracy of the proposed model can be improved by 6.7% under dynamic routing and by 6.1% under static routing. This reflects the effectiveness of the proposed method on rumour detection in social media.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1186/s12938-022-01055-x,en,Assessment of demineralized tooth lesions using optical coherence tomography and other state-of-the-art technologies: a review,"['ReviewPaper', 'Review']","Tooth demineralization is one of the most common intraoral diseases, encompassing (1) caries caused by acid-producing bacteria and (2) erosion induced by acid of non-bacterial origin from intrinsic sources (e.g. stomach acid reflux) and extrinsic sources (e.g. carbonated drinks). Current clinical assessment based on visual-tactile examination and standardized scoring systems is insufficient for early detection. A combination of clinical examination and technology is therefore increasingly adapted. This paper reviews various procedures and technologies that have been invented to diagnose and assess the severity of tooth demineralization, with focus on optical coherence tomography (OCT). As a micron-resolution non-invasive 3D imaging modality, variants of OCT are now available, offering many advantages under different working principles for detailed analytical assessment of tooth demineralization. The roles, capabilities and impact of OCT against other state-of-the-art technologies in both clinical and research settings are described. (139 words).","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Biotechnology', 'Biomedical Engineering/Biotechnology']"
doi:10.1007/s11269-022-03393-w,en,Effect of Gradient Descent Optimizers and Dropout Technique on Deep Learning LSTM Performance in Rainfall-runoff Modeling,OriginalPaper,"Machine learning and deep learning (ML-DL) based models are widely used for rainfall-runoff prediction and they have potential to substitute process-oriented physics based numerical models. However, developing an ML model has also performance uncertainty because of inaccurate choices of hyperparameters and neural networks architectures. Thus, this study aims to search for best optimization algorithms to be used in ML-DL models namely, RMSprop, Adagrad, Adadelta, and Adam optimizers, as well as dropout techniques to be integrated into the Long Short Term Memory (LSTM) model to improve forecasting accuracy of rainfall-runoff modeling. A deep learning LSTMs were developed using 480 model architectures at two hydro-meteorological stations of the Mekong Delta, Vietnam, namely Chau Doc and Can Tho. The model performance is tested with the most ideally suited LSTM optimizers utilizing combinations of four dropout percentages respectively, 0%, 10%, 20%, and 30%. The Adagrad optimizer shows the best model performance in the model testing. Deep learning LSTM models with 10% dropout made the best prediction results while significantly reducing overfitting tendency of the forecasted time series. The findings of this study are valuable for ML-based hydrological models set up by identifying a suitable gradient descent (GD) optimizer and optimal dropout ratio to enhance the performance and forecasting accuracy of the ML model.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s42235-022-00304-y,en,A Boosted Communicational Salp Swarm Algorithm: Performance Optimization and Comprehensive Analysis,"['OriginalPaper', 'Research Article']","The Salp Swarm Algorithm (SSA) is a recently proposed swarm intelligence algorithm inspired by salps, a marine creature similar to jellyfish. Despite its simple structure and solid exploratory ability, SSA suffers from low convergence accuracy and slow convergence speed when dealing with some complex problems. Therefore, this paper proposes an improved algorithm based on SSA and adds three improvements. First, the Real-time Update Mechanism (RUM) underwrites the role of ensuring that excellent individual information will not be lost and information exchange will not lag in the iterative process. Second, the Communication Strategy (CMS), on the other hand, uses the multiplicative relationship of multiple individuals to regulate the exploration and exploitation process dynamically. Third, the Selective Replacement Strategy (SRS) is designed to adaptively adjust the variance ratio of individuals to enhance the accuracy and depth of convergence. The new proposal presented in this study is named RCSSSA. The global optimization capability of the algorithm was tested against various high-performance and novel algorithms at IEEE CEC 2014, and its constrained optimization capability was tested at IEEE CEC 2011. The experimental results demonstrate that the proposed algorithm can converge faster while obtaining better optimization results than traditional swarm intelligence and other improved algorithms. The statistical data in the table support its optimization capabilities, and multiple graphs deepen the understanding and analysis of the proposed algorithm.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s11269-022-03389-6,en,"Data Decomposition, Seasonal Adjustment Method and Machine Learning Combined for Runoff Prediction: A Case Study",OriginalPaper,"Accurate and reliable runoff prediction is essential for water resources management. In this paper, a hybrid model STL-VMD-SFO-ESN which combines seasonal adjustment method (STL), variational mode decomposition (VMD), echo state network (ESN) and sailed fish optimizer (SFO) is proposed for daily runoff prediction. Daily runoff data from three different runoff monitoring stations in China’s Yellow River basin is used to evaluate the performance of proposed model and other newly reported models. The results indicates that: (1) The proposed model performs significantly better than the traditional data-driven models and some newly reported models. (2) STL decomposition can effectively remove the seasonal component of runoff and improve modeling accuracy. (3) ESN has a strong potential in runoff prediction, and its performance can be greatly improved by using bio-optimization algorithms. Thus, this new model has strong potential for runoff prediction for further application.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s10836-022-06033-8,en,Smell Detection Agent Optimization Approach to Path Generation in Automated Software Testing,OriginalPaper,"Software testing is the most crucial stage in the software development process. Structural testing, functional testing and models that even support hybrid testing are different software testing techniques. Basic path testing, the most significant structural testing approach, is focused on evaluating software source code. The method emphasizes developing test data inputs to produce all feasible and efficient test paths that connect to all nodes and edges of the graph. The objective is to define the number of independent paths that can define the number of test cases needed to maximize test coverage. It ensured the execution of every statement and condition at least once. A nature-inspired Smell Detection Agent (SDA) algorithm is proposed in this paper to select all paths and prioritize the feasible solution. This algorithm is an optimization algorithm suitable for identifying optimal paths with priority. The concept is derived from the natural behaviour of canines that identified optimal path from source to the destination. The SDA algorithm is based on the evaporation of smell molecules in the form of gas and the perception capability of a smelling agent. The number of linearly independent paths through a programme module is measured by creating a Control Flow Graph of the code, which measures cyclomatic complexity. SDA algorithm gives significant increases in performance while considering the cyclomatic complexity. Complexity analysis of SDA trends to be in the O(E+V log V), while the competitor algorithms have an exponential growth of O(n $$^2$$ 2 ). Various experiments were also carried out to emphasis the relevance of the proposed method. Ten different benchmarked applications has been taken for experimental analysis and it was observed to have an increased path coverage of 8% when SDA was used over the traditional methods. Also, the time complexity was reduced by 22%, which shows the powerfulness of the proposed SDA algorithm.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s40996-022-01005-6,en,An Estimation Proposal for Engineering Properties of Modified Concrete by using Standalone and Hybrid GRELM,"['OriginalPaper', 'Research Paper']","The presented study pertains to an attempt to propose a novel prediction model to predict the flexural and compressive strengths of concrete modulated using steel fiber (SFb) and silica fume (SF). A completed experimental investigation is adopted for the current study, and a research plan is employed. Three different superplasticizers amount (SP), different SF replacement ratios, and a constant amount of SFb were used by the weight of cement to meet the C25 target strength. A sum of 16 distinct mixtures designed by changing SP and SF ratios were developed. Furthermore, SFb was added at a fixed rate of 65 kg/m 3 to all planned concrete mixes. In addition, SFb was used to create a 16-mix design. Finally, a total of 32 distinct mix designs were created. Produced, hardened specimens were exposed to two different curing conditions. This research uses the mechanical characteristics of concrete treated with SF, SP, and SFb to estimate by conducting standalone and hybridized generalized extreme learning machine (GRELM) algorithms based on available experimental data in terms of the metaheuristic aspects of this work. With continuous input data, four separate data sets were constructed. Compressive strength and flexural strength were estimated separately. With the aid of the Grey Wolf Optimization (GWO) and Particle Swarm Optimization (PSO) algorithms, binary and ternary hybrid approaches were developed and tested on the data. Four distinct estimation models were suggested. Two quality metrics were used to evaluate the estimation performance: Root Mean Square Error (RMSE) and correlation of determination ( R 2 ). The estimation results showed that the hybridized GRELM-PSO-GWO estimation model that was built for prediction was relatively successful in all sets.","['Engineering', 'Civil Engineering']"
doi:10.1007/s10115-022-01787-1,en,"Advanced agronomics model with species classification, minimum support price prediction, and profit suggestion using enhanced deep learning strategy","['OriginalPaper', 'Regular Paper']","Minimum support price (MSP) is an advisory price signal, which is a component of a huge collection of agricultural policies in parts of India. The agricultural commodities evaluate the derivatives by noticing the climate changes regarding external factors like economic and weather conditions. The severe changes in these factors result in significant price changes. Here, it may experience more cost-efficiency in analyzing the agricultural species. The problem arises because the mixed species in the single pixel increases the pixel size. The high-dimensional problem of input and outputs occurs in the hyper-spectral data. Hence, this research implements a new MSP method in Agronomics or Agriculture through deep learning algorithms. In the species prediction phase, the input remote sensing images are gathered and processed in pre-processing phase using median filtering and contrast limited adaptive histogram equalization. Here, the pre-processed images are fed to feature extraction using the gray-level co-occurrence matrix (GLCM) and spatial feature extraction techniques. Further, the deep feature extraction is analyzed using convolutional neural network (CNN) by considering the input as the pre-processed images and extracting GLCM and spatial features. These deep features are forwarded to the enhanced recurrent neural-long short-term memory (ERN-LSTM), where the parameters of RNN and LSTM are tuned by self-adaptive dingo optimizer (SA-DOX). Finally, the species prediction outcomes are attained by enhanced RNN + LSTM. Secondly, in the MSP prediction phase, the major aim is to predict the MSP based on the species detected. Here, the gathered price data, along with the extracted CNN-based deep features, are processed to select the significant optimal features and are carried out by the same improved DOX. The selected features are given to enhanced RNN + LSTM for predicting the MSP price related to the crop type. Thirdly, the predicted prices are split into four shares. Fourthly, profit suggestion is carried out by training the location and regional crop data, and thus, the enhanced RNN + LSTM model gives the best profitable harvests. Through the experimental results, the accuracy of species classification using SA-DOX-based ERN-LSTM was 10.9, 8.3, 6.85, and 4.7%, accordingly advanced than SVM, LSTM, RNN, and RNN-LSTM. From the given findings, the better accuracy rate of the given designed method is 96.75%. Accordingly, the better sensitivity and precision rates are 95.9 and 95.5%. Finally, this study explores competitive performance through the experimental results relative to the traditional approaches.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s11071-022-08120-z,en,Optimal Hilbert transform parameter identification of bistable structures,"['ReviewPaper', 'Original Paper']","Nonlinear bistable structures have received significant attention in the field of energy harvesting and vibration absorption. Obtaining their precise nonlinear restoring force is of significance to predict and enhance the system's performance. However, it is difficult to measure their nonlinear restoring force in experiments due to the distinct characteristic of snap-through. Moreover, the traditional Hilbert transform-based method may have insufficient identification accuracy or even be incapable because numerical integration or differentiation procedure is sensitive to noise disturbance. To address these issues, an optimal Hilbert transform parameter identification is proposed to precisely estimate the parameters in the bistable dynamic equation. The Hilbert transform interval estimation of mass, damping and nonlinear restoring force coefficients are derived for obtaining the reasonable range of identified parameters. Furthermore, an optimization fitness function is established to obtain the optimal value of nonlinear parameters in bistable structures. Numerical simulation of an asymmetric bistable dynamic equation shows that the proposed method exhibits an NMSE value of 2.52% for free vibration and 1.64% for forced periodic oscillation under 20 dB noise level. Besides, the damping effects on identification results are discussed. Experimental measurements of a magnetic coupled bistable cantilever beam under different conditions are performed to identify the nonlinear system parameters. Results indicate that the proposed method can effectively identify the nonlinear bistable structures with an average NMSE value of 8.23% for free vibration and 6.39% for forced periodic responses, respectively.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s00500-022-07688-3,en,Enrichment of voltage stability in power system through novel generalized approximate reasoning based intelligent control with african buffalo optimization approach,"['OriginalPaper', 'Application of Soft Computing']","In recent times, the modern power system has become more complex because of high penetration of generation power, heavy load changes, voltage fluctuation, high reactive power, and environmental and economic problems. These problems can cause voltage disintegration issues in the power system. Thus, the voltage stability in the system should be predicted and required to improve the stable conditions. However, the earlier investigations do not significantly improve the coordination's voltage stability. For this reason, in this research, a novel generalized approximate reasoning based intelligent control based unified voltage collapse proximity indicator is proposed for calculating the system's weak buses voltage. Moreover, the African buffalo optimization is proposed to estimate the finest location of unified power flow controller based flexible alternative current transmission system devices in the scheme for voltage strength enrichment. The implementation of these proposed approaches is done via MATLAB/Simulink. The projected system has been experienced on IEEE 118 and IEEE 30 bus systems. Consequently, the simulation outcome indicates that the proposed methods effectively predict and upgrade the voltage stability index. The results are compared with the conventional models concerning cost, real power loss, and computational time. The proposed system attained real power for IEEE 118 bus is 4.5502 MW and IEEE 30 bus for 4.902 MW, which is very low compared with the existing methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11277-022-10099-0,en,HFTO: Hybrid Firebug Tunicate Optimizer for Fault Tolerance and Dynamic Task Scheduling in Cloud Computing,OriginalPaper,"Task scheduling is an important issue in cloud computing when it comes to achieving multiple goals and satisfying different user needs. The increasing demand and users urge the necessity to minimize the task completion time and enhance the load balancing capacity. To achieve this goal, this article proposes a Hybrid firebug and Tunicate Optimization (HFTO) algorithm. Based on the previous scheduling information, the HFTO classifier classifies the task and creates different variants of Virtual Machine (VM). This step helps to minimize the time taken for VM creation. The proposed HFTO task scheduling framework aims at optimizing different Quality of Service (QoS) parameters such as fault tolerance, response time, efficiency, and makespan. The optimization algorithm helps to expand the search space of the solutions and frames an optimal task scheduling strategy for the virtual machines. The HFTO optimization method has several advantages, including enhanced search capability and faster convergence. The HFTO algorithm improves the fault tolerance capability by allocating the tasks to appropriate resources based on the resource load peak. The lightweight tasks can be allocated to the resources with high CPU utilization and the computation-intensive tasks can be allocated to the resources with low CPU utilization. The response time and execution time are improved by task pre-emption. Hence the time complexity and computational complexity can be improved by the HFTO algorithm even with limited resource capability. The experiments are conducted using the CloudSim experimental platform and the results are compared to the state-of-art techniques. The performance of the proposed methodology is evaluated in terms of different performance metrics namely makespan, load balancing, and average execution time. The results show that, when compared to existing techniques, the proposed methodology provides higher load balancing efficiency and improved cloud task scheduling performance.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s11548-022-02801-1,en,Tissues margin-based analytical anisotropic algorithm boosting method via deep learning attention mechanism with cervical cancer,"['OriginalPaper', 'Original Article']","Purpose Speed and accuracy are two critical factors in dose calculation for radiotherapy. Analytical Anisotropic Algorithm (AAA) is a rapid dose calculation algorithm but has dose errors in tissue margin area. Acuros XB (AXB) has high accuracy but takes long time to calculate. To improve the dose accuracy on the tissue margin area for AAA, we proposed a novel deep learning-based dose accuracy improvement method using Margin-Net combined with Margin-Loss. Methods A novel model ‘Margin-Net’ was designed with a Margin Attention Mechanism to generate special margin-related features. Margin-Loss was introduced to consider the dose errors and dose gradients in tissues margin area. Ninety-five VMAT cervical cancer cases with paired AAA and AXB dose were enrolled in our study: 76 cases for training and 19 cases for testing. Tissues Margin Masks were generated from RT contours with 6 mm extension. Tissues Margin Mask, AAA dose and CTs were input data; AXB dose was used as reference dose for model training and evaluation. Comparison experiments were performed to evaluated effectiveness of Margin-Net and Margin-Loss. Results Compared to AXB dose, the 3D gamma passing rate (1%/1 mm, 10% threshold) for 19 test cases 95.75 ± 1.05% using Margin-Net with Margin-Loss, which was significantly higher than the original AAA dose (73.64 ± 3.46%). The passing rate reduced to 94.07 ± 1.16% without Margin-Loss and 87.3 ± 1.18% if Margin-Net key structure ‘MAM’ was also removed. Conclusion The proposed novel tissues margin-based dose conversion method can significantly improve the dose accuracy of Analytical Anisotropic Algorithm to be comparable to AXB algorithm. It can potentially improve the efficiency of treatment planning process with low demanding of computation resources.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s00332-022-09867-w,en,A Mathematical Framework for Dynamical Social Interactions with Dissimulation,OriginalPaper,"Modeling social interactions is a challenging task that requires flexible frameworks. For instance, dissimulation and externalities are relevant features influencing such systems—elements that are often neglected in popular models. This paper is devoted to investigating general mathematical frameworks for understanding social situations where agents dissimulate and may be sensitive to exogenous objective information. Our model comprises a population where the participants can be honest, persuasive, or conforming. Firstly, we consider a non-cooperative setting, where we establish existence, uniqueness and some properties of the Nash equilibria of the game. Secondly, we analyze a cooperative setting, identifying optimal strategies within the Pareto front. In both cases, we develop numerical algorithms allowing us to computationally assess the behavior of our models under various settings.","['Mathematics', 'Analysis', 'Theoretical, Mathematical and Computational Physics', 'Classical Mechanics', 'Mathematical and Computational Engineering', 'Economic Theory/Quantitative Economics/Mathematical Methods']"
doi:10.1038/s41467-022-35231-3,en,Reference panel guided topological structure annotation of Hi-C data,"['OriginalPaper', 'Article']","Accurately annotating topological structures (e.g., loops and topologically associating domains) from Hi-C data is critical for understanding the role of 3D genome organization in gene regulation. This is a challenging task, especially at high resolution, in part due to the limited sequencing coverage of Hi-C data. Current approaches focus on the analysis of individual Hi-C data sets of interest, without taking advantage of the facts that (i) several hundred Hi-C contact maps are publicly available, and (ii) the vast majority of topological structures are conserved across multiple cell types. Here, we present RefHiC, an attention-based deep learning framework that uses a reference panel of Hi-C datasets to facilitate topological structure annotation from a given study sample. We compare RefHiC against tools that do not use reference samples and find that RefHiC outperforms other programs at both topological associating domain and loop annotation across different cell types, species, and sequencing depths. Predicting topological structures from Hi-C data provides insight into comprehending gene expression and regulation. Here, the authors present RefHiC, an attention-based deep learning framework that leverages a reference panel of Hi-C datasets to assist topological structure annotation from a given study sample.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1140/epjs/s11734-022-00724-1,en,"Dynamics of the COVID-19 pandemic: nonlinear approaches on the modelling,  prediction and control","['EditorialNotes', 'Editorial']","This special issue contains 35 regular articles on the analysis and dynamics of COVID-19 with several applications. Some analyses are on the construction of mathematical models representing the dynamics of COVID-19, and some are on the estimations and predictions of the disease, a few with possible applications. The various contributions report important, timely, and promising results, such as the effects of several waves, deep learning-based COVID-19 classifications, and multivariate time series with applications.","['Physics', 'Condensed Matter Physics', 'Materials Science, general', 'Atomic, Molecular, Optical and Plasma Physics', 'Physics, general', 'Measurement Science and Instrumentation', 'Classical and Continuum Physics']"
doi:10.1007/s12541-022-00716-0,en,Learning-Based Discrete Hysteresis Classifier Using Wire Tension and Compensator for Flexible Endoscopic Surgery Robots,"['OriginalPaper', 'Regular Paper']","The tendon-sheath mechanism can be applied to a flexible endoscopic surgery robot because of its flexibility and power transmission. However, the hysteresis, which is the inherent problem with this mechanism, affects the precision of the control of the surgical robot. Despite several studies that are aimed at tackling hysteresis, only a few literatures consider a practical circumstance such as initial unknown hysteresis, proper surgical procedure, and camera illumination. In this study, we propose a novel framework to reduce the hysteresis of a flexible surgical robot using the learning-based hysteresis classification and a feed-forward compensation based on practical scenarios. We empirically discretize and divide the hysteresis class based on its size and show the correlation between hysteresis and time-series wire tension experimentally to study its potential for use in real surgical robots. The results indicate that the hysteresis can be classified by utilizing the time-series wire tension data. Moreover, the proposed compensator could enhance the performance of a real-size flexible endoscopic surgery robot based on actual surgical environment.","['Engineering', 'Industrial and Production Engineering', 'Materials Science, general']"
doi:10.1038/s41467-022-35007-9,en,Deciphering clinical abbreviations with a privacy protecting machine learning system,"['OriginalPaper', 'Article']","Physicians write clinical notes with abbreviations and shorthand that are difficult to decipher. Abbreviations can be clinical jargon (writing “HIT” for “heparin induced thrombocytopenia”), ambiguous terms that require expertise to disambiguate (using “MS” for “multiple sclerosis” or “mental status”), or domain-specific vernacular (“cb” for “complicated by”). Here we train machine learning models on public web data to decode such text by replacing abbreviations with their meanings. We report a single translation model that simultaneously detects and expands thousands of abbreviations in real clinical notes with accuracies ranging from 92.1%-97.1% on multiple external test datasets. The model equals or exceeds the performance of board-certified physicians (97.6% vs 88.7% total accuracy). Our results demonstrate a general method to contextually decipher abbreviations and shorthand that is built without any privacy-compromising data. Patient notes contain shorthand and abbreviations that may be jargon or clinical vernacular. Here the authors train large machine learning models on public web data to decode such text by replacing abbreviations with their meanings.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07691-8,en,Region-based demand forecasting in bike-sharing systems using a multiple spatiotemporal fusion neural network,"['OriginalPaper', 'Data analytics and machine learning']","Bike-sharing systems (BSSs) have become increasingly popular around the globe and have attracted a wide range of research interests. In this paper, we study the region-based demand forecasting problem in BSSs. State-of-the-art methods usually employ branched residual 2D or 3D convolutional neural networks, in which each branch extracts one spatiotemporal dependence in three fragments: closeness, period, and trend. However, these methods ignore the correlations among the fragments. To address the challenge and extract the correlations, we propose a multiple spatiotemporal fusion network named MSTF-Net. It consists of multiple spatiotemporal layers: shared 3D convolutional network (3D-CNN) layers, eidetic 3D convolutional long short-term memory network (E3D-LSTM) layers, and fully connected (FC) layers. Specifically, the shared 3D-CNN layers highlight extracting low-level and short-term spatiotemporal dependence in each fragment. The E3D-LSTM layers extract long-term spatiotemporal dependence and correlations among the fragments. The FC layers extract nonlinear correlations of external factors such as weather, day-of-week, and time-of-day. MSTF-Net outperforms seven baseline models, including the branched residual 2D and 3D convolutional neural networks on station-free and station-based bike-sharing datasets. Code is available at https://github.com/yanxiao1930/MSTF-Net .","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1038/s41467-022-35115-6,en,Conserved structures of neural activity in sensorimotor cortex of freely moving rats allow cross-subject decoding,"['OriginalPaper', 'Article']","Our knowledge about neuronal activity in the sensorimotor cortex relies primarily on stereotyped movements that are strictly controlled in experimental settings. It remains unclear how results can be carried over to less constrained behavior like that of freely moving subjects. Toward this goal, we developed a self-paced behavioral paradigm that encouraged rats to engage in different movement types. We employed bilateral electrophysiological recordings across the entire sensorimotor cortex and simultaneous paw tracking. These techniques revealed behavioral coupling of neurons with lateralization and an anterior–posterior gradient from the premotor to the primary sensory cortex. The structure of population activity patterns was conserved across animals despite the severe under-sampling of the total number of neurons and variations in electrode positions across individuals. We demonstrated cross-subject and cross-session generalization in a decoding task through alignments of low-dimensional neural manifolds, providing evidence of a conserved neuronal code. Conservation of the neural code across subjects is crucial for training brain-computer interfaces. Through alignment of neural manifolds, the authors show cross-subject generalization in the decoding of unconstrained behavior from sensorimotor cortex.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10898-022-01261-w,en,A linear programming approach to approximating the infinite time reachable set of strictly stable linear control systems,OriginalPaper,"The infinite time reachable set of a strictly stable linear control system is the Hausdorff limit of the finite time reachable set of the origin as time tends to infinity. By definition, it encodes useful information on the long-term behavior of the control system. Its characterization as a limit set gives rise to numerical methods for its computation that are based on forward iteration of approximate finite time reachable sets. These methods tend to be computationally expensive, because they essentially perform a Minkowski sum in every single forward step. We develop a new approach to computing the infinite time reachable set that is based on the invariance properties of the control system and the desired set. These allow us to characterize a polyhedral outer approximation as the unique solution to a linear program with constraints that incorporate the system dynamics. In particular, this approach does not rely on forward iteration of finite time reachable sets.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Real Functions', 'Computer Science, general']"
doi:10.1038/s41598-022-25366-0,en,Metal artifact reduction in kV CT images throughout two-step sequential deep convolutional neural networks by combining multi-modal imaging (MARTIAN),"['OriginalPaper', 'Article']","This work attempted to construct a new metal artifact reduction (MAR) framework in kilo-voltage (kV) computed tomography (CT) images by combining (1) deep learning and (2) multi-modal imaging, defined as MARTIAN (Metal Artifact Reduction throughout Two-step sequentIAl deep convolutional neural Networks). Most CNNs under supervised learning require artifact-free images to artifact-contaminated images for artifact correction. Mega-voltage (MV) CT is insensitive to metal artifacts, unlike kV CT due to different physical characteristics, which can facilitate the generation of artifact-free synthetic kV CT images throughout the first network (Network 1). The pairs of true kV CT and artifact-free kV CT images after post-processing constructed a subsequent network (Network 2) to conduct the actual MAR process. The proposed framework was implemented by GAN from 90 scans for head-and-neck and brain radiotherapy and validated with 10 independent cases against commercial MAR software. The artifact-free kV CT images following Network 1 and post-processing led to structural similarity (SSIM) of 0.997, and mean-absolute-error (MAE) of 10.2 HU, relative to true kV CT. Network 2 in charge of actual MAR successfully suppressed metal artifacts, relative to commercial MAR, while retaining the detailed imaging information, yielding the SSIM of 0.995 against 0.997 from the commercial MAR.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41746-022-00719-1,en,Longitudinal cardio-respiratory fitness prediction through wearables in free-living environments,"['OriginalPaper', 'Article']","Cardiorespiratory fitness is an established predictor of metabolic disease and mortality. Fitness is directly measured as maximal oxygen consumption (VO 2 m a x ), or indirectly assessed using heart rate responses to standard exercise tests. However, such testing is costly and burdensome because it requires specialized equipment such as treadmills and oxygen masks, limiting its utility. Modern wearables capture dynamic real-world data which could improve fitness prediction. In this work, we design algorithms and models that convert raw wearable sensor data into cardiorespiratory fitness estimates. We validate these estimates’ ability to capture fitness profiles in free-living conditions using the Fenland Study (N=11,059), along with its longitudinal cohort ( N  = 2675), and a third external cohort using the UK Biobank Validation Study ( N  = 181) who underwent maximal VO 2 m a x testing, the gold standard measurement of fitness. Our results show that the combination of wearables and other biomarkers as inputs to neural networks yields a strong correlation to ground truth in a holdout sample ( r  = 0.82, 95CI 0.80–0.83), outperforming other approaches and models and detects fitness change over time (e.g., after 7 years). We also show how the model’s latent space can be used for fitness-aware patient subtyping paving the way to scalable interventions and personalized trial recruitment. These results demonstrate the value of wearables for fitness estimation that today can be measured only with laboratory tests.","['Medicine & Public Health', 'Medicine/Public Health, general', 'Biomedicine, general', 'Biotechnology']"
doi:10.1007/s11227-022-04970-x,en,Optimal prediction of cloud spot instance price utilizing deep learning,OriginalPaper,"Cloud platforms often offer a variety of virtual machine (VM) models of various types and capacities, enabling users to choose the instances that best meet their requirements. Cloud providers have devised systems to make the most of their redundant computing resources. The cost fluctuates dynamically based on supply and demand. ""Spot price"" is a common term for this. To be able to use this instance, the user must create a suitable offer above the spot price. Accurate spot price prediction allows users to pre-prepare bid prices and run time to increase the reliability of the method. For this purpose, we consider Amazon EC2 as a testbed and use its spot price history to predict the future price by constructing a proposed modified gated recurrent unit (MGRU) model and providing a proposed dropout method. Compared with other sophisticated methods, test results show that the proposed method works superior and more accurately.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1038/s41598-022-25340-w,en,Composed query image retrieval based on triangle area triple loss function and combining CNN with transformer,"['OriginalPaper', 'Article']","The existing typical combined query image retrieval methods adopt Euclidean distance as sample distance measurement method, and the model trained by triple loss function blindly pursues absolute distance between samples, resulting in unsatisfactory image retrieval performance. Meanwhile, these methods singularly adopt Convolutional Neural Network (CNN) to extract reference image features. However, receptive field of convolution operation has the characteristics of locality, which is easy to cause the loss of edge feature information of reference images. In view of shortcomings of these methods, the following improvements are proposed in this paper: (1) We propose Triangle Area Triple Loss Function (TATLF), which adopts Triangle Area (TA) as measurement of sample distance. TA comprehensively considers the absolute distance and included angle between samples, so that the trained model has better retrieval performance; (2) We combine CNN with Transformer to simultaneously extract local and edge features of reference images, which can effectively reduce the loss of reference images information. Specifically, CNN is adopted to extract local feature information of reference images. Transformer is used to pay attention to the edge feature information of reference images. Extensive experiments on two public datasets, Fashion200k and MIT-States, confirm the excellent performance of our proposed method.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11269-022-03381-0,en,Coupling the Causal Inference and Informer Networks for Short-term Forecasting in Irrigation Water Usage,OriginalPaper,"Precise and reliable irrigation water use (IWU) prediction is beneficial for irrigation district reservoirs interaction and water resources management. However, existing methods face the challenges of high prediction errors at extreme points and accumulative error problem. Meanwhile, ignoring the effects of spurious relationships are among the driving factors on prediction results. This study introduces the Peter and Clark Momentary Conditional Independence (PCMCI) causal inference method to analyze driving factors. The causal inference results of PCMCI are taken as input to the IWU prediction model. This investigation constructs six IWU forecasting models, including the Informer neural network, long short-term memory (LSTM) neural network, attention-based LSTM network, the Prophet model, random forest model, and seasonal autoregressive integrated moving average (SARIMA). The performance of these six models and their variants are evaluated and compared by the long-term month IWU data series of the Zhanghe irrigation area of China. The results show that the Informer model based on self-attention mechanism is more advantageous than others. The PCMCI method can overcome the spurious relationships, and contribute to a clearer understanding of physical mechanisms, as compared to the correlation analysis. Combined with the dynamic time warping barycenter averaging (DBA) data augmentation method, the proposed DBA-PCMCI-Informer method can reduce the prediction errors at extreme points, and improve the IWU forecasting accuracy. This approach alleviates the accumulative error problem, enabling high accuracy even in multistep prediction.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1038/s44172-022-00042-3,en,A digital twin of electrical tomography for quantitative multiphase flow imaging,"['OriginalPaper', 'Article']","Multiphase flow is ubiquitous in nature, industry and research, and accurate flow imaging is critical to understanding this complex phenomenon. Electrical tomography (ET) is a promising technique for multiphase flow visualization and characterization which provides a non-invasive and non-radiative way to unravel the internal physical properties at high temporal resolution. However, existing ET-based multiphase flow imaging methods are inadequate for quantitative imaging of multiphase flows due to inversion errors and limited ground truth data of fluid phases distribution. Here we report a digital twin (DT) framework of ET to address the challenges of real-time quantitative multiphase flow imaging. The proposed DT framework, building upon a synergistic integration of 3D field coupling simulation, model-based deep learning, and edge computing, allows ET to dynamically learn the flow features in the virtual space and implement the model in the physical system, thus providing excellent resolution and accuracy. The proposed DT framework is demonstrated using electrical capacitance tomography (ECT) of a gas-liquid two-phase flow. It can be readily extended to a broader range of tomography modalities, scenarios, and scales in biomedical, energy, and aerospace applications. Shengnan Wang and colleagues report a digital twin framework of electrical tomography for quantitative imaging of a gas-liquid multiphase flow. This framework enables precise flow profile imaging using low-cost and noninvasive tomography techniques and can be extended to biomedical, aerospace and energy applications.","['Engineering', 'Engineering, general']"
doi:10.1007/s11042-022-14274-0,en,Supervision dropout: guidance learning in deep neural network,OriginalPaper,"In deep neural networks, the generalization is a vital evaluation metric. As it contributes to avoid over-fitting, Dropout plays an important role in improving the generalization of deep neural networks. Without fully utilizing the training data and the real-time performance of the networks, traditional Dropout and its variants lack of specificity in the selection of inactivated neurons and the planning of dropout rates, resulting in a weaker performance in enhancing the generalization. Therefore, this paper offers an improved Dropout method. As both the training data and the real-time performance of networks can be quantified by the loss, the method uses the loss of the network prediction to guide the selection of inactivated neurons and the determination of dropout rates. The selection is performed by the genetic algorithm, while the results of the selection are used to plan the dropout rate. In essence, this approach encourages the subset of neurons with the higher loss to be trained so as to increase the robustness of neurons and thus improves the generalization of networks. The experimental results demonstrate that the proposed method achieves better generalization on MiniImageNet and Caltech-256 datasets. Compared with the backbone network, the accuracy improves from 66.56% to 72.95%.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10489-022-04104-z,en,ERINet: efficient and robust identification network for image copy-move forgery detection and localization,OriginalPaper,"Images can be maliciously manipulated to hide content or duplicate certain objects. Detecting an elaborate copy-move forgery is very challenging for both humans and machines, and current methods cannot detect copy-move images with the precision required, especially for pixel-level tampered images, which is a challenge for the current existing methods. In this paper we present our own dataset ( CMF58K ) - the first pixel-level copy-move dataset, which consists of 580,000 images covering copy-move tampered objects in various life scenes with more than 32 object classes. Furthermore, we propose a network for detecting and locating copy-move forgeries: Efficient and Robust Identification Network ( ERINet ). It mainly includes four main modules: the efficient feature pyramid network ( EFPN ), the residual receptive field block ( RRFB ), the hierarchical decoding identification ( HDI ), and the cascaded group-reversal attention ( GRA ) blocks. Considering the inevitable external factors of rotation, scaling, blurring, compression and noise can hide traces of tampering and increase the difficulty of detection, we applied MaxBlurPool to our network and obtained a strong robustness. ERINet outperforms various state-of-the-art manipulation detection baselines on four image manipulation datasets. The inference speed is ∼ $\sim \ $ 49 fps on a single GPU without I/O time on the test dataset.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1038/s41598-022-23406-3,en,Prediction of fluid oil and gas volumes of shales with a deep learning model and its application to the Bakken and Marcellus shales,"['OriginalPaper', 'Article']","The fluid oil and gas volumes (S1) retained within the shales are one of the most important parameter of producible fluid oil and gas saturations of shales together with total organic carbon content. The S1 volumes can directly be obtained by Rock-Eval pyrolysis analysis. However, it is time consuming and not practical to obtain samples from all intervals of all wells in any shale play. S1 volumes prediction with a deep learning (DL) model have increasingly became important with the booming exploration and development of shale oil and gas resources. S1 volumes of shales are controlled by organic matter richness, type and maturity together with reservoir quality and adsorption capacity which are mainly effected by age, depth, organic content, maturity and mineralogy. A dataset consisting of 331 samples from 19 wells of various locations of the world-class organic-rich shales of the Niobrara, Eagle Ford, Barnett, Haynesville, Woodford, Vaca Muerta and Dadaş has been used to determination of a DL model for S1 volumes prediction using Python 3 programing environment with Tensorflow and Keras open-source libraries. The DL model that contains 5 dense layers and, 1024, 512, 256, 128 and 128 neurons has been predicted S1 volumes of shales as high as R 2  = 0.97 from the standard petroleum E&P activities. The DL model has also successfully been applied to S1 volumes prediction of the Bakken and Marcellus shales of the North America. The prediction of the S1 volumes show that the shales have lower to higher reservoir quality and, oil and gas production rate that are well-matches with former studies.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-24936-6,en,Classification and visual explanation for COVID-19 pneumonia from CT images using triple learning,"['OriginalPaper', 'Article']","This study presents a novel framework for classifying and visualizing pneumonia induced by COVID-19 from CT images. Although many image classification methods using deep learning have been proposed, in the case of medical image fields, standard classification methods are unable to be used in some cases because the medical images that belong to the same category vary depending on the progression of the symptoms and the size of the inflamed area. In addition, it is essential that the models used be transparent and explainable, allowing health care providers to trust the models and avoid mistakes. In this study, we propose a classification method using contrastive learning and an attention mechanism. Contrastive learning is able to close the distance for images of the same category and generate a better feature space for classification. An attention mechanism is able to emphasize an important area in the image and visualize the location related to classification. Through experiments conducted on two-types of classification using a three-fold cross validation, we confirmed that the classification accuracy was significantly improved; in addition, a detailed visual explanation was achieved comparison with conventional methods.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10489-022-04335-0,en,Routing hypergraph convolutional recurrent network for network traffic prediction,OriginalPaper,"Effectively predicting network traffic is a fundamental but intractable task in IP network management and operations. Many methods that can capture complex spatiotemporal dependencies from network topology and traffic sequence data have achieved remarkable results and become dominant in this task. However, the previous methods seldom consider the spatial information from the routing scheme, which also determines the flow direction and trend of network traffic. To fill this gap, we regard a routing path as a hyperedge and utilize a hypergraph instead of a simple graph to model network node connections based on the routing relevance. Then, we propose a novel multi-step network traffic prediction model named routing hypergraph convolutional recurrent network (RHCRN), which is built on the seq2seq structure with the hypergraph convolutional recurrent unit (HCRU). The HCRU is composed of the 2-layer hypergraph convolutional network (HGCN) and gated recurrent unit (GRU). The node-edge-node transform process of the HGCN layer is ideal for exploring the complex spatial correlation between the routing paths and network nodes. The GRU is used to extract the temporal correlation from dynamic network traffic data. Extensive experiments on three real-world IP network datasets demonstrate that our model is robust and outperforms other advanced baseline models.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-04307-4,en,"Incorporating semantics, syntax and knowledge for aspect based sentiment analysis",OriginalPaper,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, whose goal is to identify the sentiment polarity of the specific aspect term in a given sentence. Previous work has realized the importance of commonsense knowledge, semantic and syntax information for aspect-based sentiment analysis, while few approaches take them into account simultaneously. To tackle this problem, we propose a novel graph convolutional network to incorporate commonsense knowledge, syntax and semantics information for this task. Specifically, we first construct an aspect-specific dependency tree rooted at aspect by reshaping an ordinary dependency parse tree and then integrate commonsense knowledge into the refined tree. Based on it, a semantic graph convolutional network is utilized to capture semantics information and a syntax-knowledge graph convolutional network with range-aware weight mechanism is adopted to encode important aspect-relevant commonsense knowledge and syntax information. Finally, an information exchange module is applied to interact commonsense knowledge, syntax and semantics information for classification. Experimental results demonstrate that our proposed model outperforms state-of-the-art models.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00521-022-08059-7,en,ArcMask: a robust and fast image-based method for high-speed railway pantograph-catenary arcing instance segmentation,"['OriginalPaper', 'Original Article']","The pantograph-catenary arcing reflects the health of pantograph-catenary and current collection quality of high-speed railway, so the arc detection is of great significance. However, due to the scene complexity, intra-class polymorphism and inter-class similarity of arcing and the fast running speed of high-speed railway, it is still a huge challenge to achieve fine and robust arcing detection. To overcome these issues, a robust and fast image-based instance segmentation method called ArcMask is proposed to detect pantograph-catenary arcing, which designs a new attention-based multi-scale feature fusion module that combines both top-down and down-up modules to realize arcing pixel-level instance segmentation. The effective combination of instance-level information and bottom-level semantic information balances features representation ability of top-level and bottom-level features. Compared with other instance segmentation methods (e.g., BlendMask), it can effectively learn feature representation with tiny, irregular and complex arc features and speeds up the calculation. In addition, both deformable convolution and depth-wise separable convolution are introduced in ArcMask, which aims to improve the segmentation performance of irregular arcing and efficiency. The ArcMask can distinguish different arcing instances at pixel-level with fine granularity and distinguish inter-class and intra-class features of arcing, instead of just focusing on rectangular bounding box. Experiments on self-collected dataset IVAIS-PCA2021 verify the effectiveness and efficiencies of the ArcMask. Its AP, AP 50 and AP 75 are 56.61, 94.14 and 64.56, respectively, and the fastest reasoning speed based on MobileNet is 56 FPS. Compared with other state-of-the-art segmentation methods, the proposed ArcMask has better integrity in arcing edge detection.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1186/s13638-022-02195-3,en,Artificial intelligence for channel estimation in multicarrier systems for B5G/6G communications: a survey,"['ReviewPaper', 'Review']","Multicarrier modulation allows for deploying wideband systems resilient to multipath fading channels, impulsive noise, and intersymbol interference compared to single-carrier systems. Despite this, multicarrier signals suffer from different types of distortion, including channel noise sources and long- and short-term fading. Consequently, the receiver must estimate the channel features and compensate it for data recovery based on channel estimation techniques, such as non-blind, blind, and semi-blind approaches. These techniques are model-based and designed with accurate mathematical channel models encompassing their features. Nevertheless, complex environments challenge accurate mathematical channel estimation modeling, which might neither be accurate nor correspond to reality. This impairment decreases the system performance due to the channel estimation accuracy loss. Fortunately, (AI) algorithms can learn the relationship among different system variables using a model-driven or model-free approach. Thereby, AI algorithms are used for channel estimation by exploiting its complexity without unrealistic assumptions, following a better performance than conventional techniques under the same channel. Hence, this paper comprehensively surveys AI-based channel estimation for multicarrier systems. First, we provide essential background on conventional channel estimation techniques in the context of multicarrier systems. Second, the AI-aided channel estimation strategies are investigated using the following approaches: classical learning, neural networks, and reinforcement learning. Lastly, we discuss current challenges and point out future research directions based on recent findings.","['Engineering', 'Signal,Image and Speech Processing', 'Communications Engineering, Networks', 'Information Systems Applications (incl.Internet)']"
doi:10.1007/s42835-022-01277-y,en,Design of Fractional Order Terminal Sliding Mode Control for Robust Speed Tracking in Sensorless Multiphase Drive Systems,"['OriginalPaper', 'Original Article']","Multiphase induction motor drives are an emerging research are in electric machines. This paper presents a sliding mode control (SMC) and fractional calculus assisted terminal sliding mode control scheme (FOTSMC) for encoder-less operation of 5-phase induction motor speed control. Model predictive control scheme is used to generate switching signals using the reference torque generated through the FOTSMC. The shortcomings of speed sensors are eliminated using a robust sliding mode based speed observer. The proportional integral (PI), SMC and FOTSMC are compared using simulation based analysis carried out in MATLAB/SIMULINK. The PI, SMC and FOTSMC are tested under variable speed reference, speed reversal, and constant load. The PI, SMC and FOTSMC controllers are also evaluated for stator flux trajectories. The results indicates that the FOTSMC has improved performance as compared to SMC in terms of chattering elimination for encoder-less predictive torque control of IM whereas the SMC and FOTSMC are robust as compared to PI controller.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/s12530-022-09425-5,en,Nature-inspired optimization algorithms and their significance in multi-thresholding image segmentation: an inclusive review,"['ReviewPaper', 'Review']","Multilevel Thresholding (MLT) is considered as a significant and imperative research field in image segmentation that can efficiently resolve difficulties aroused while analyzing the segmented regions of multifaceted images with complicated nonlinear conditions. MLT being a simple exponential combinatorial optimization problem is commonly phrased by means of a sophisticated objective function requirement that can only be addressed by nondeterministic approaches. Consequently, researchers are engaging Nature-Inspired Optimization Algorithms (NIOA) as an alternate methodology that can be widely employed for resolving problems related to MLT. This paper delivers an acquainted review related to novel NIOA shaped lately in last three years (2019–2021) highlighting and exploring the major challenges encountered during the development of image multi-thresholding models based on NIOA.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s00500-022-07417-w,en,Application of Coulomb’s and Franklin’s laws algorithm to solve large-scale optimal reactive power dispatch problems,"['OriginalPaper', 'Application of soft computing']","This study focuses on the application of Coulomb’s and Franklin’s laws algorithm (CFA) to solving large-scale optimal reactive power dispatch (LS-ORPD) problems. The CFA optimizer acts on the basis of the charged particles interactions. The ever-increasing effects of ORPD problems for safe and reliable operation of electrical power grids is an important area of study. Such problems are classified as nonlinear optimization problems; the aim of which is to minimize the active power loss through tuning of several control variables. Firstly, the performance of CFA optimizer in solving high-dimensional problems is investigated using standard benchmark problems. Moreover, we apply the CFA optimizer for solving large-scale ORPD problems based on different constraints in three IEEE standard power systems. According to the results, the proposed optimizer offers a more accurate solution when compared with other methods found in the literature. Finally, an early attempt is carried out for improving CFA optimizer, which is tested on benchmark and ORPD problems and yields promising outcome in reaching a more powerful variant of CFA.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10586-022-03675-3,en,A novel link-based Multi-objective Grey Wolf Optimizer for Appliances Energy Scheduling Problem,OriginalPaper,"In this paper, a modified version of the Multi-objective Grey Wolf Optimizer (MGWO), known as linked-based GWO (LMGWO), is proposed for the Appliances Energy Scheduling Problem (AESP). The proposed LMGWO is utilized by combining the MGWO searching mechanism with a novel strategy, called neighbourhood selection strategy, to improve local exploitation capabilities. AESP is a problem that can be tackled by searching for the best appliances schedule according to a set of constraints and a dynamic pricing scheme(s) utilized for optimizing energy consumed at a particular period. Three objectives are considered to handle AESP: improving user comfort while reducing electricity bills and maintaining power systems’ performance. Therefore, AESP is modelled as a multi-objective optimization problem to handle all objectives simultaneously. In the evaluation results, the LMGWO is tested using a new dataset containing 30 power consumption scenarios with up to 36 appliances. For comparative purposes, the same linked-based neighbourhood selection strategy is utilized with other three optimization algorithms, including particle swarm optimization, salp swarm optimization, and wind-driven algorithm. The performance of the modified versions is compared with each other and that of the original versions to show their improvements. Furthermore, the proposed LMGWO is compared with eight state-of-the-art methods using their recommended datasets to show the viability of the proposed LMGWO. Interestingly, the proposed LMGWO is able to outperform the compared methods in almost all produced results.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00366-020-01252-z,en,Improved Salp Swarm Algorithm with mutation schemes for solving global optimization and engineering problems,"['OriginalPaper', 'Original Article']","Salp Swarm Algorithm (SSA) is a recent metaheuristic algorithm developed from the inspiration of salps’ swarming behavior and characterized by a simple search mechanism with few handling parameters. However, in solving complex optimization problems, the SSA may suffer from the slow convergence rate and a trend of falling into sub-optimal solutions. To overcome these shortcomings, in this study, versions of the SSA by employing Gaussian, Cauchy, and levy-flight mutation schemes are proposed. The Gaussian mutation is used to enhance neighborhood-informed ability. The Cauchy mutation is used to generate large steps of mutation to increase the global search ability. The levy-flight mutation is used to increase the randomness of salps during the search. These versions are tested on 23 standard benchmark problems using statistical and convergence curves investigations, and the best-performed optimizer is compared with some other state-of-the-art algorithms. The experiments demonstrate the impact of mutation schemes, especially Gaussian mutation, in boosting the exploitation and exploration abilities.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01442-3,en,Multi-strategy Gaussian Harris hawks optimization for fatigue life of tapered roller bearings,"['OriginalPaper', 'Original Article']","Bearing is one of the most fundamental components of rotary machinery, and its fatigue life is a crucial factor in designing. The design optimization of tapered roller bearing (TRB) is a complex design problem because various arrays of designing parameters and functional requirements should be fulfilled. Since there are many design variables and nonlinear constraints, presenting an optimal design of TRBs poses some challenges for metaheuristic algorithms. The Harris hawks optimization (HHO) algorithm is a robust nature-inspired method with unique exploitation and exploration phases due to its time-varying structure. However, this metaheuristic algorithm may still converge to local optima for more challenging problems such as the design of TRBs. Therefore, this study aims to improve the accuracy and efficiency of the shortcomings of this algorithm. The performance of the proposed algorithm is first evaluated for the TRB optimization problem. The TRB optimization design has nine design variables and 26 constraints because of geometrical dimensions and strength conditions. The productivity of the proposed method is compared with diverse metaheuristic algorithms in the literature. The results demonstrate the significant development of dynamic load capacity in comparison to the standard value. Furthermore, the enhanced version of the HHO algorithm presented in this study is benchmarked with various well-known engineering problems. For supplementary materials regarding algorithms in this research, readers can refer to https://aliasgharheidari.com .","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00521-022-07571-0,en,Dynamic evolutionary data and text document clustering approach using improved Aquila optimizer based arithmetic optimization algorithm and differential evolution,"['OriginalPaper', 'Original Article']","Data and text clustering are popular and frequently used in the data mining domain, mainly to deal with big data analytics. The main problem in these techniques is finding the most coherent clusters allocating similar-related objects into one group. In this paper, an improved clustering analysis approach is proposed using an advanced optimization method called AOAOA. The proposed AOAOA method improved the Aquila optimizer (AO) search performance by the operators of the arithmetic optimization algorithms (AOA) and differential evolution (DE) and using a novel transition mechanism. The primary motivation for this modification is that the original optimizer suffers from local optima stagnation and lacks search balance. Thus, the proposed AOAOA overcame these shortcomings by integrating various powerful search strategies and a new update strategy. Experiments are conducted on two parts; eight standard data clustering datasets and ten text documents benchmark datasets to evaluate the performance of the proposed AOAOA method. The proposed method is compared against several well-known optimization algorithms and advanced state-of-the-art methods published in the literature. The data clustering results also showed promising performance for the proposed AOAOA compared to other comparative data clustering methods. Moreover, the results illustrated that the proposed AOAOA can find new best solutions for several different complicated cases as the text document clustering results. The proposed AOAOA got accurate and robust results compared to several state-of-the-art methods.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00366-020-01234-1,en,SGOA: annealing-behaved grasshopper optimizer for global tasks,"['OriginalPaper', 'Original Article']","An improved grasshopper optimization algorithm (GOA) is proposed in this paper, termed as SGOA, which combines simulated annealing (SA) mechanism with the original GOA that is a natural optimizer widely used in finance, medical and other fields, and receives more promising results based on grasshopper behavior. To compare performance of the SGOA and other algorithms, an investigation to select CEC2017 benchmark function as the test set was carried out. Also, the Friedman assessment was performed to check the significance of the proposed method against other counterparts. In comparison with ten meta-heuristic algorithms such as differential evolution (DE), the proposed SGOA can rank first in the CEC2017, and also ranks first in comparison with ten advanced algorithms. The simulation results reveal that the SA strategy notably improves the exploration and exploitation capacity of GOA. Moreover, the SGOA is also applied to engineering problems and parameter optimization of the kernel extreme learning machine (KELM). After optimizing the parameters of KELM using SGOA, the model was applied to two datasets, Cleveland Heart Dataset and Japanese Bankruptcy Dataset, and they achieved an accuracy of 79.2% and 83.5%, respectively, which were better than the KELM model obtained other algorithms. In these practical applications, it is indicated that the proposed SGOA can provide effective assistance in settling complex optimization problems with impressive results.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11042-021-11268-2,en,Real-time arrhythmia heart disease detection system using CNN architecture based various optimizers-networks,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","The main objective of this paper is to develop an interactive classifier aided deep learning system to assist cardiologists for heart arrhythmia disease classification as it shows a health-threatening condition that can lead to heart-related complications. Therefore, automatic arrhythmia heart disease detection in an early stage is of high interest as it helps to reduce the mortality rate of cardiac disease patients. In this context, a deep learning architecture is propounded for automatic classification of the patient`s electrocardiogram (ECG) signal into a specific class according to the ANSI–AAMI standards. Our proposed methodology is a multistage technique. The first stage combines an R–R peak extraction with a low pass filter applied on the ECG raw data for noise removal. The proposed second stage is a convolutional neural network (CNN) based Fully Connected layers architecture, using different networks optimizer. Different ECG databases have been used for validation purposes. The whole system is implemented on CPU and GPU for complexity analysis. For the predicted improved PTB dataset, the classification accuracy results achieve 99.37%, 99.15%, and 99.31% for training, validation, and testing, respectively. Besides, for the MIT-BIH database, the training, validation, and testing accuracies are 99.5%, 99.06%, and 99.34%, respectively. A top F1-score of 0.99 is obtained. Experimental results show a high achievement compared to the state-of-the-art models. The implementation on GPU confirms the low computational complexity of the system and the possible use in detecting disease events in real-time, which makes it a good candidate for portable healthcare devices.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00366-021-01464-x,en,Elite dominance scheme ingrained adaptive salp swarm algorithm: a comprehensive study,"['OriginalPaper', 'Original Article']","This paper focuses on the performance of an improved algorithm based on the salp swarm algorithm (SSA), called AGSSA. We planned several new ideas to improve the defects of the original optimizer, such as ease to fall into local optimum and low convergence accuracy. To solve these problems, the SSA algorithm is improved in two parts. Salp swarm algorithm (SSA) is a recently proposed optimization algorithm with advantages and disadvantages, simulating a perception of the salp's foraging and navigation behavior in the deep ocean. The first improvement includes the adaptive control parameter introduced into the follower position update stage, which boosts the local exploitative ability of the population. The second improvement includes the elite gray wolf domination strategy introduced in the last stage of the population position update, which helps the population find the globally optimal solution faster. The performance of AGSSA is verified by a series of problems, including the IEEE CEC2014 benchmark functions, engineering design problems, and feature selection tasks. The experimental results of AGSSA are compared with some well-known metaheuristic algorithms. Simulations reveal that the performance of AGSSA is significantly better than lots of competitive metaheuristic algorithms. Moreover, in solving real-world problems, AGSSA also shows high accuracy in comparison with other metaheuristic algorithms. These points prove that the introduction of the two strategies has a positive effect on the original SSA. Promisingly, the proposed AGSSA can be used as a potential optimization tool in many optimization tasks.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01448-x,en,An enhanced Cauchy mutation grasshopper optimization with trigonometric substitution: engineering design and feature selection,"['OriginalPaper', 'Original Article']","Selecting a subset of important features from a high-dimensional dataset is an important prerequisite for data mining. Meta-heuristic algorithms have gained attention in this field in recent years. The grasshopper optimization algorithm (GOA) is a meta-heuristic algorithm recently proposed based on the migration and hunting of grasshoppers in nature. However, the method suffers from a low diversity of the agents, which results in the stagnation problems, or immature convergence. To make GOA more competent in various situations, this paper stabilizes an improved GOA with new exploratory and exploitative features, which we have called it the SCGOA. The mechanism and structure of the proposed SCGOA are mainly divided into two steps: First, to balance the exploration and exploitation stages, trigonometric substitution is utilized for perturbation of the updating (evolution) of the position vectors of the individuals. Secondly, the diversity of the population is boosted using can Cauchy mutation-based strategy, which can help the grasshopper population to avoid the stagnation and lazy convergence. Therefore, Cauchy mutation is introduced to assist in an adequate variety of the position of the grasshopper population. Performance of SCGOA was validated on the latest IEEE CEC2017 benchmark functions in comparison with several well-known meta-heuristic algorithms. Various extensive results reveal that the proposed SCGOA has achieved a significant advantage over the other rivals. Finally, the Cauchy mutation-based SCGOA was also used for tackling four engineering design problems, and the results showed that SCGOA was superior to some state-of-the-art algorithms. We also developed the binary version of Cauchy mutation-based SCGOA in dealing with many feature selection datasets. The results on feature selection reveal that the binary version can outperform original GOA and other optimization algorithms, with higher classification accuracy, smaller error rate, and less number of features. We think the proposed optimizer can be widely tool for solving forms of the optimization problems. The research will be supported by open access materials and web service for any user guide at https://aliasghaheidari.com .","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01479-4,en,Memory-based Harris hawk optimization with learning agents: a feature selection approach,"['OriginalPaper', 'Original Article']","Feature selection is a vital pre-processing phase for most machine learning and data mining courses. This article proposes new variants of the Harris hawk optimization called memory energetic Harris hawk optimization (MEHHO1 and MEHHO2) to select the optimal features for classification purposes. The MEHHO approaches adopt an energetic learning strategy and memory saving and updating mechanism. The former extends the chance of the algorithm escaping the local solutions, while the latter boosts the exploitation behavior. The proposed approaches are applied in the feature selection domain for assessing a subset of high discriminative features. The proposed approaches are evaluated on 13 low-dimensional and eight high-dimensional datasets. Also, the proposed approaches are utilized to solve the feature selection problem for the classification of electromyography signals. Our results prove the capability of the proposed approaches to find the optimal feature subset compared to the other five well-known optimization algorithms. Thus, the proposed MEHHO is expected to be a promising and effective technology to solve the feature selection problem.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s13042-022-01625-4,en,Adaptive hierarchical hyper-gradient descent,"['OriginalPaper', 'Original Article']","Adaptive learning rate strategies can lead to faster convergence and better performance for deep learning models. There are some widely known human-designed adaptive optimizers such as Adam and RMSProp, gradient based adaptive methods such as hyper-descent and practical loss-based stepsize adaptation (L4), and meta learning approaches including learning to learn. However, the existing studies did not take into account the hierarchical structures of deep neural networks in designing the adaptation strategies. Meanwhile, the issue of balancing adaptiveness and convergence is still an open question to be answered. In this study, we investigate novel adaptive learning rate strategies at different levels based on the hyper-gradient descent framework and propose a method that adaptively learns the optimizer parameters by combining adaptive information at different levels. In addition, we show the relationship between regularizing over-parameterized learning rates and building combinations of adaptive learning rates at different levels. Moreover, two heuristics are introduced to guarantee the convergence of the proposed optimizers. The experiments on several network architectures, including feed-forward networks, LeNet-5 and ResNet-18/34, show that the proposed multi-level adaptive approach can significantly outperform many baseline adaptive methods in a variety of circumstances.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s10586-022-03650-y,en,Optimized task scheduling in cloud computing using improved multi-verse optimizer,OriginalPaper,"The multiverse optimizer (MVO) is one of the most trending algorithms used nowadays. The searching space in MVO is restricted by the best solution only, leading to a poor searching domain, therefore, a long searching time. This paper proposes an improved multiobjective multi-verse optimizer (IMOMVO) as a novel population optimization technique to solve task scheduling problems. The IMOMVO is introduced to overcome the drawbacks risen in the original MVO and its latest enhanced version mMVO. The proposed method solves the problem of the average positioning (AP) by dynamically enhancing the equation of updating the AP based on the best and the second-best available solutions. To evaluate The proposed IMOMVO, several datasets scenarios containing various tasks and virtual machines (Vms) were used to test the approach’s capability. Standard evaluation metrics are used to validate the results of the proposed method; task execution time, throughput, and the Vms processing power. The proposed method obtained better results according to the evaluation measures than other state-of-the-art methods. The execution time achieves less time when compared to the mMVO as the proposed method achieved 186.33 s for executing 100 tasks and 934.92 for executing 600 tasks. The throughput results also achieved astonishing results as for 100 tasks, the throughput achieved 0.19, and the Vm processing power for the proposed method was 0.25 Kw for executing 100 tasks.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s10825-022-01987-6,en,Parameter extraction of three-diode solar photovoltaic model using a new metaheuristic resistance–capacitance optimization algorithm and improved Newton–Raphson method,OriginalPaper,"Identifying and estimating uncertain and dynamic photovoltaic characteristics with high accuracy is important when modeling solar photovoltaic (PV) systems. It is critical to have effective and accurate parameters when transforming the complete PV system from solar energy to electrical energy. Therefore, we propose and apply for the first time a unique physics-based metaheuristic algorithm known as the Resistance–Capacitance Optimization Algorithm (RCOA), which is based on the concept of resistance–capacitance circuit response, and it was obviously inspired by the output response of a Resistor–Capacitor (RC) circuit when the input is connected or disconnected suddenly. The RC circuit's total time response can be divided into steady-state and transient-state. Both of these stages of the RC circuit are essential to model the algorithm properly. It is proposed that the RCOA be used as a single-objective algorithm that is simple, reliable, and has zero sensitive parameters, thereby being a parameter-free algorithm. The validity of the proposed RCOA is tested on the solar PV cell/module equivalent circuit parameter estimation of the three-diode PV model. The proposed algorithm combines an improved version of the Newton–Raphson method to find the optimal PV parameters in fewer iterations. The performance of the RCOA is compared with state-of-the-art algorithms, and the obtained results prove the superiority of the RCOA. The proposed methodology was found to be a trustworthy tool and it is proved through a statistical analysis and non-parametric test. Also, the results have revealed that the RCOA has superior reliability and accuracy when estimating the three-diode PV model parameters. It could be used as a viable approach for parameter identification problems in PV systems. With the average Friedman’s ranking test value of 1.6666 and the average runtime of 17.08, the RCOA stands first among all selected algorithms.","['Engineering', 'Mathematical and Computational Engineering', 'Electrical Engineering', 'Theoretical, Mathematical and Computational Physics', 'Optical and Electronic Materials', 'Mechanical Engineering']"
doi:10.1007/s00500-022-07337-9,en,Fault detection based on squirrel search algorithm and support vector data description for industrial processes,"['OriginalPaper', 'Application of soft computing']","This paper proposes a novel fault detection system by the combination of Support Vector Data Description and Squirrel Search Algorithm. This approach is capable to deal with processes or machines where the number of fault observations is small or not even available for training phase. In this work the use of classic Support Vector Data Description as well as its fast version with two kernel functions is proposed. The experimental results showed that the proposed system exhibits suitable capabilities for fault detection in complex industrial processes such as the one presented in this research. Moreover, a nonparametric statistical analysis is also included in order to compare the considered strategies and enhance the efficiency of the presented approach. Finally, a comparison with genetic algorithm approach and the one-class classifier based on support vectors is carried out which shows that the proposed algorithm outperforms traditional techniques.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00521-022-07557-y,en,MOAVOA: a new multi-objective artificial vultures optimization algorithm,"['OriginalPaper', 'Original Article']","This paper presents a multi-objective version of the artificial vultures optimization algorithm (AVOA) for a multi-objective optimization problem called a multi-objective AVOA (MOAVOA). The inspirational concept of the AVOA is based on African vultures' lifestyles. Archive, grid, and leader selection mechanisms are used for developing the MOAVOA. The proposed MOAVOA algorithm is tested oneight real-world engineering design problems and seventeen unconstrained and constrained mathematical optimization problems to investigates its appropriateness in estimating Pareto optimal solutions. Multi-objective particle swarm optimization, multi-objective ant lion optimization, multi-objective multi-verse optimization, multi-objective genetic algorithms, multi-objective salp swarm algorithm, and multi-objective grey wolf optimizer are compared with MOAVOA using generational distance, inverted generational distance, maximum spread, and spacing performance indicators. This paper demonstrates that MOAVOA is capable of outranking the other approaches. It is concluded that the proposed MOAVOA has merits in solving challenging multi-objective problems.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07670-y,en,Optimal operation and planning of hybrid AC/DC power systems using multi-objective grasshopper optimization algorithm,"['OriginalPaper', 'Original Article']","Optimal power flow (OPF) in a hybrid alternating current and multi-terminal high-voltage direct current (AC-MTHVDC) grid is currently one of the most popular optimization problems in modern power systems. The critical necessity of addressing global warming and reducing generation costs is encouraging the integration of eco-friendly renewable energy sources (RESs) into the OPF problem. In this direction, the present research has centred on the formulation and solution of the multi-objective (MO) AC-MTHVDC-OPF problem incorporating RESs such as wind, solar, small-hydro, and tidal power. The available power of RESs is calculated by means of the Weibull, lognormal, and Gumbel probability density functions. The proposed MO-OPF optimizes the double and triple configurations of various objective functions, including total cost, the total cost with the valve-point effect, the total cost with emission and carbon tax, voltage deviation, and power loss. Multi-objective grasshopper optimization algorithm (MOGOA) is applied to find non-dominated Pareto-optimal solutions of the non-convex, nonlinear and high-dimensional MO/AC-MTHVDC-OPF problem. The obtained results are compared with the results of MSSA, MODA, MOALO, and MO_Ring_PSO_SCD algorithms. The comparison of results gives that MOGOA outperforms competitive optimizers with respect to the quality of Pareto-optimal solutions and their distribution.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11227-022-04960-z,en,Accelerating neural network architecture search using multi-GPU high-performance computing,OriginalPaper,"Neural networks stand out from artificial intelligence because they can complete challenging tasks, such as image classification. However, designing a neural network for a particular problem requires experience and tedious trial and error. Automating this process defines a research field usually relying on population-based meta-heuristics. This kind of optimizer generally needs numerous function evaluations, which are computationally demanding in this context as they involve building, training, and evaluating different neural networks. Fortunately, these algorithms are also well suited for parallel computing. This work describes how the teaching–learning-based optimization algorithm has been adapted for designing neural networks exploiting a multi-GPU high-performance computing environment. The optimizer, not applied before for this purpose up to the authors’ knowledge, has been selected because it lacks specific parameters and is compatible with large-scale optimization. Thus, its configuration does not result in another problem and could design architectures with many layers. The parallelization scheme is decoupled from the optimizer. It can be seen as an external evaluation service managing multiple GPUs for promising neural network designs, even at different machines, and multiple CPU’s for low-performing solutions. This strategy has been tested in designing a neural network for image classification based on the CIFAR-10 dataset. The architectures found outperform human designs, and the sequential process is accelerated 4.2 times with 4 GPUs and 96 cores thanks to parallelization, being the ideal speed up 4.39 in this case.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s10462-022-10157-w,en,A new fusion of whale optimizer algorithm with Kapur’s entropy for multi-threshold image segmentation: analysis and validations,OriginalPaper,"The separation of an object from other objects or the background by selecting the optimal threshold values remains a challenge in the field of image segmentation. Threshold segmentation is one of the most popular image segmentation techniques. The traditional methods for finding the optimum threshold are computationally expensive, tedious, and may be inaccurate. Hence, this paper proposes an Improved Whale Optimization Algorithm (IWOA) based on Kapur’s entropy for solving multi-threshold segmentation of the gray level image. Also, IWOA supports its performance using linearly convergence increasing and local minima avoidance technique (LCMA), and ranking-based updating method (RUM). LCMA technique accelerates the convergence speed of the solutions toward the optimal solution and tries to avoid the local minima problem that may fall within the optimization process. To do that, it updates randomly the positions of the worst solutions to be near to the best solution and at the same time randomly within the search space according to a certain probability to avoid stuck into local minima. Because of the randomization process used in LCMA for updating the solutions toward the best solutions, a huge number of the solutions around the best are skipped. Therefore, the RUM is used to replace the unbeneficial solution with a novel updating scheme to cover this problem. We compare IWOA with another seven algorithms using a set of well-known test images. We use several performance measures, such as fitness values, Peak Signal to Noise Ratio, Structured Similarity Index Metric, Standard Deviation, and CPU time.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10489-022-03397-4,en,The water optimization algorithm: a novel metaheuristic for solving optimization problems,OriginalPaper,"Metaheuristic algorithms (MAs) are used to find the answers to NP-Hard problems. NP-Hard problems basically refer to a set of optimization problems that cannot be solved in a polynomial at a time. MAs try to find the optimal or near-definitive answer in the shortest possible time to solve such problems and a set of optimization algorithms with different origins. These algorithms may be inspired by the natural sciences, physics, mathematics, and political science. However, a particular Metaheuristic algorithm may not provide the best answer to all problems. Each MA may have a better response to specific problems than other similar algorithms. Therefore, researchers will try to introduce and discover new algorithms to find optimal answers to a wide range of problems. In this paper, a new Meta-heuristic algorithm called the Water optimization algorithm (WAO) is presented. WAO is inspired by the chemical and physical properties of water molecules. The main idea of the proposed algorithm is to link water molecules together to find the optimal points. Factors such as particle motion, particle evaporation, and particle bonding have created a mechanism based on swarm intelligence and physical intelligence that inspired this algorithm to solve persistent problems. In this algorithm, answers are defined as a water molecule, a set of them is defined as a local answer. Water bonds provide the right move towards the optimal response. In evaluating the performance of the proposed algorithm, the proposed method is applied to some standard functions and some practical problems. The results obtained from the experiments show that the proposed algorithm has provided appropriate and acceptable answers in terms of execution time and accuracy compared to some similar algorithms.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s40747-022-00714-9,en,A competitive swarm optimizer with probabilistic criteria for many-objective optimization problems,"['OriginalPaper', 'Original Article']","Although multiobjective particle swarm optimizers (MOPSOs) have performed well on multiobjective optimization problems (MOPs) in recent years, there are still several noticeable challenges. For example, the traditional particle swarm optimizers are incapable of correctly discriminating between the personal and global best particles in MOPs, possibly leading to the MOPSOs lacking sufficient selection pressure toward the true Pareto front (PF). In addition, some particles will be far from the PF after updating, this may lead to invalid search and weaken the convergence efficiency. To address the abovementioned issues, we propose a competitive swarm optimizer with probabilistic criteria for many-objective optimization problems (MaOPs). First, we exploit a probability estimation method to select the leaders via the probability space, which ensures the search direction to be correct. Second, we design a novel competition mechanism that uses winner pool instead of the global and personal best particles to guide the entire population toward the true PF. Third, we construct an environment selection scheme with the mixed probability criterion to maintain population diversity. Finally, we present a swarm update strategy to ensure that the next generation particles are valid and the invalid search is avoided. We employ various benchmark problems with 3–15 objectives to conduct a comprehensive comparison between the presented method and several state-of-the-art approaches. The comparison results demonstrate that the proposed method performs well in terms of searching efficiency and population diversity, and especially shows promising potential for large-scale multiobjective optimization problems.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00521-022-07512-x,en,Frequency control of hybrid microgrid comprising solid oxide fuel cell using hunger games search,"['OriginalPaper', 'Original Article']","This paper addresses a novel hunger games search (HGS) based on specular reflection-based learning (SRL) and dynamic quasi-opposition-based learning (DQOL), named HGS_RQ, for improving the optimization performance of the classical HGS while dealing with load frequency control task. By these learnings, a fitter solution can be generated whether by SRL or DQOL and therefore, the quality of the best solution can be refined. The effectiveness of the proposed HGS_RQ is demonstrated and validated on two-area interconnected power system with considering nonlinearity effect of governor dead band. Additional supplementary controller is proposed to reinforce frequency regulation through solid oxide fuel cell. The objective function is adapted to minimize the integral time absolute error in frequency deviations and tie line power. The efficacy and superiority are affirmed by the comparisons with some of prominent recent methods. It can be noted that the adequate response is proved, since the maximum frequency deviation is 0.088 Hz, the settling time is about 2 s, and the steady-state frequency change is zero in the two areas. On the other hand, there is a significant reduction in tie line power transient response with maximum deviation of 1.318% for the studied cases. Furthermore, the statistical measures and analysis of variance test are analyzed to exhibit the superior performance of the HGS_RQ in terms of accuracy and reliability.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12652-021-03183-z,en,Performance up-gradation of Symbiotic Organisms Search by Backtracking Search Algorithm,"['OriginalPaper', 'Original Research']","Symbiotic Organisms Search (SOS) algorithm is characterized based on the framework of relationships among the ecosystem species. Nevertheless, it is suffering from wasteful discovery, little productivity, and slack convergence rate. These deficiencies cause stagnation at the local optimum, which is hazardous in deciding the genuine optima of the optimization problem. Backtracking Search Algorithm (BSA) is likewise another streamlining method for comprehending the non-direct complex optimization problem. Consequently, in the current paper, an endeavor has been made toward the expulsion of the downsides from the traditional SOS by proposing a novel ensemble technique called e-SOSBSA to overhaul the degree of intensification and diversification. In e-SOSBSA, firstly, the mutation operator of BSA with the self-adaptive mutation rate is incorporated to produce a mutant of population and leap out from the local optima. Secondly, the crossover operator of BSA with the adaptive component of mixrate is incorporated to leverage the entire active search regions visited previously. The suggested e-SOSBSA has been tested with 20 classical benchmark functions, IEEE CEC2014, CEC2015, CEC2017, and the latest CEC 2020 test functions. Statistical analyses, convergence analysis, and diversity analysis are performed to show the stronger search capabilities of the proposed e-SOSBSA in contrast with the component algorithms and several state-of-the-art algorithms. Moreover, the proposed e-SOSBSA is applied to find the optimum value of the seven problems of engineering optimization. The numerical investigations and examinations show that the proposed e-SOSBSA can be profoundly viable in tackling real-world engineering optimization problems.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s10915-022-02063-8,en,Distributionally Robust Optimization with Moment Ambiguity Sets,OriginalPaper,"This paper studies distributionally robust optimization (DRO) when the ambiguity set is given by moments for the distributions. The objective and constraints are given by polynomials in decision variables. We reformulate the DRO with equivalent moment conic constraints. Under some general assumptions, we prove the DRO is equivalent to a linear optimization problem with moment and psd polynomial cones. A Moment-SOS relaxation method is proposed to solve it. Its asymptotic and finite convergence are shown under certain assumptions. Numerical examples are presented to show how to solve DRO problems.","['Mathematics', 'Algorithms', 'Computational Mathematics and Numerical Analysis', 'Mathematical and Computational Engineering', 'Theoretical, Mathematical and Computational Physics']"
doi:10.1007/s11042-022-12038-4,en,Self-boosted with dynamic semi-supervised clustering method for imbalanced big data classification,OriginalPaper,"Big data plays a major role in the learning, manipulation, and forecasting of information intelligence. Due to the imbalance of data delivery, the learning and retrieval of information from such large datasets can result in limited classification outcomes and wrong decisions. Traditional machine learning classifiers successfully handling the imbalanced datasets still there is inadequacy in overfitting problems, training cost, and sample hardness in classification. To forecast a better classification, the research work proposed the novel “Self-Boosted with Dynamic Semi-Supervised Clustering Method”. The method is initially preprocessed by constructing sample blocks using Hybrid Associated Nearest Neighbor heuristic over-sampling to replicate the minority samples and merge each copy with every sub-set of majority samples to remove the overfitting issue thus slightly reduce noise with the imbalanced data. After preprocessing the data, massive data classification requires big data space which leads to large training costs. Therefore, the approach suggested a Heterogeneous Weight Ensemble classifier which calculates data space in each data sample by its nearest neighbors and adaptive weight adjustment to resolve the training cost with unstable sample problems. Subsequently, the classification acquires poor performance due to sample hardness. Thus the work introduced a Self-Managed Ensample Optimizer which separates the bulk of specimens into the bins according to their rigidity values and provides better classification results. Consequently, the proposed work effectively classifies the imbalanced dataset with high accuracy of 99%, to obtain balanced data with improved classification results.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10489-022-03471-x,en,EDOA: An Elastic Deformation Optimization Algorithm,OriginalPaper,"In recent years, a large number of meta-heuristic algorithms have been proposed to efficiently solve various complex optimization problems in reality. Most of these algorithms are based on the intelligent behavior of swarms in the natural world. In this article, we take Hooke's law of elasticity and Newton's second law of motion as the information interaction tools and innovatively propose a new meta-heuristic algorithm that is based on the laws of physics, called the elastic deformation optimization algorithm (EDOA). A new parameter adaptive adjustment mechanism is designed in the EDOA to better explore and exploit the search space. At the same time, we compare the proposed EDOA with six well-known search algorithms and conduct simulation experiments on 23 classical benchmark functions and IEEE CEC 2020 benchmark functions respectively. We have further analyzed the experimental results, used two nonparametric statistical test methods, and drawn iterative curves of the algorithms to prove the powerful comprehensive performance of the proposed EDOA.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s40030-022-00668-8,en,An Improved Deep Convolutional Neural Network for Image-Based Apple Plant Leaf Disease Detection and Identification,"['OriginalPaper', 'Original Contribution']","Early detection and accurate diagnosis of leaf diseases can be significantly helpful in controlling their spread and improving the yield and quality of production. However, it is challenging to design and develop novel models to analyze complex, noise-contaminated leaf images and infer, preferably in real time, with high accuracy and precision. A Deep Convolutional Neural Network (DCNN) model is proposed, trained and tested from scratch on the subset of the PlantVillage dataset comprising of typical Apple leaf-diseases images. The model uses image data augmentation and image annotation techniques to enhance performance and accuracy. The proposed model was compared with AlexNet, VGG-16, InceptionV3, MobileNetV2, ResNet50, and DenseNet121. It achieved the highest overall accuracy of 99.31% in disease detection with low training time. The low testing time of 5.1 ms per image makes the proposed model suitable for real-time disease detection. Furthermore, the proposed model achieved the maximum precision, recall, F-1 score values and was better than other models on various other performance parameters. The results were validated using a Grad-CAM visualization technique that significantly enhanced the reliability of the suggested model.","['Engineering', 'Civil Engineering']"
doi:10.1007/s00202-022-01636-y,en,An application of Wild Horse Optimizer to multi-objective energy management in a micro-grid,"['OriginalPaper', 'Original Paper']","This paper introduces a novel application of the Wild Horse Optimizer (WHO) to energy management in a typical grid-connected micro-grid (MG) with different types of distributed generation sources, a battery storage device, and loads. The problem is formulated as a nonlinear multi-objective optimization problem with various equality and inequality constraints. The optimal solutions are defined by the following two objective functions: (1) minimization of the total operating costs for the MG, and (2) minimization of the total emission of pollutants in the MG. The multi-objective management problem is transformed into a single-objective problem using the weighted sum method to obtain a Pareto optimum. Simulation results, obtained using the WHO algorithm, are compared with those obtained using other popular optimization algorithms. It is shown that the WHO algorithm provides effective, robust and high-quality solutions.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s42235-022-00307-9,en,IBMSMA: An Indicator-based Multi-swarm Slime Mould Algorithm for Multi-objective Truss Optimization Problems,"['OriginalPaper', 'Research Article']","This work proposes an improved multi-objective slime mould algorithm, called IBMSMA, for solving the multi-objective truss optimization problem. In IBMSMA, the chaotic grouping mechanism and dynamic regrouping strategy are employed to improve population diversity; the shift density estimation is used to assess the superiority of search agents and to provide selection pressure for population evolution; and the Pareto external archive is utilized to maintain the convergence and distribution of the non-dominated solution set. To evaluate the performance of IBMSMA, it is applied to eight multi-objective truss optimization problems. The results obtained by IBMSMA are compared with other 14 well-known optimization algorithms on hypervolume, inverted generational distance and spacing-to-extent indicators. The Wilcoxon statistical test and Friedman ranking are used for statistical analysis. The results of this study reveal that IBMSMA can find the Pareto front with better convergence and diversity in less time than state-of-the-art algorithms, demonstrating its capability in tackling large-scale engineering design problems.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s10825-022-01931-8,en,Parameter estimation of solar PV models with quantum-based avian navigation optimizer and Newton–Raphson method,OriginalPaper,"A mathematical model with precise parameters is required to analyze the performance of a solar photovoltaic generating system. This technical note presents a unique scheme for accurately estimating the parameters of a solar PV system. The proposed method is a combination of quantum-based avian navigation optimizer (QANO) and Newton–Raphson (NR) method. QANO algorithm, a novel metaheuristic algorithm, is employed for identifying a global optimum solution with optimal parameters which suit well the given experimental solar cell/module. The NR method, on the other hand, is used to solve nonlinear equations during the objective function calculation process. Most of the algorithms estimate the parameters based on the conventional objective function, which do not consider the nonlinearities of the I–V characteristics. Such inaccurate models may not be reliable for real-time applications. In this work, an objective function is formulated which offers a more accurate parameters of the equivalent PV models without neglecting nonlinearities. The proposed method is applied to estimate parameters for a single diode model (SDM), a double diode model (DDM), and a PV module. The efficacy of the proposed QANO algorithm is compared to the results of other state-of-the-art algorithms reported in the literature. The proposed algorithm achieves an RMSE of 7.7300630E−04 for SDM and 7.5248E−04 for DDM, which are lower than most of the existing algorithms.","['Engineering', 'Mathematical and Computational Engineering', 'Electrical Engineering', 'Theoretical, Mathematical and Computational Physics', 'Optical and Electronic Materials', 'Mechanical Engineering']"
doi:10.1007/s00500-022-07518-6,en,Boxing Match Algorithm: a new meta-heuristic algorithm,"['OriginalPaper', 'Foundation, algebraic, and analytical methods in soft computing']","This study presents a boxing match algorithm (BMA) as a new efficient meta-heuristic method to solve numerical optimization and NP-hard problems, such as knapsack. The proposed algorithm finds the best position (i.e., best solution) in the ring during the match (i.e., feasible solutions) through a simulation of the boxer’s behavior, who can attract good support from his coach and fans to defeat his rival. The proposed algorithm is seeking good solutions by dividing the solution space into different sections and generating new solutions in each section through a semi-zigzag search. This division causes a proper and targeted search in the solution space to reach an efficient solution. Also, the unique boxer’s movement in the ring (generating new solutions) enables the proposed BMA to give an acceptable level of performance in exploring the problem space. Several unconstrained mathematical, knapsack, and engineering benchmark problems undergo a standard test designed to confirm the powerful performance of the proposed BMA. A wide variety of complex numerical problems are solved and analyzed by a comparison between the results obtained from the test and the other well-known optimization methods. Quantitative data also indicate the proper performance of the proposed algorithm compared to the other ones. The results show that this algorithm can achieve 95% and 100% optimal solutions in 20 mathematical benchmark functions and eight examples of the knapsack problem, respectively.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s13177-022-00316-2,en,Optimal Scheduling of Electric Vehicle Charging at Geographically Dispersed Charging Stations with Multiple Charging Piles,OriginalPaper,"The work presented in this paper deals with developing a charge scheduling strategy for electric vehicles in a predefined geographical region. Charging stations in the geographical region are considered to provide multiple charging levels with separate piles with an individual queue for each charging level. Assigning a charging station to each electric vehicle is considered as an optimization problem to minimize travel time, queue time, recharging time, and cost of energy for battery recharging. The objective function is constrained to the reachability of the electric vehicle with the available state of charge of the battery to the allotted charging station without violating the maximum permissible depth of discharge limit and allowable charging rate. The optimization model empowers the users to prioritize the function variable based on travel requirements and battery specifications in different case studies using the Opposition-Based Marine Predator Algorithm. This proposed algorithm is an improved version of the recently reported Marine Predator Algorithm in which the opposition-based learning mechanism is included to improve the solution accuracy. The solution obtained and the analysis of results show that the proposed strategy significantly reduces travel time, queue time, recharging time, and energy cost while fulfilling the constraints imposed.","['Engineering', 'Electrical Engineering', 'Automotive Engineering', 'Robotics and Automation', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Civil Engineering', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s00521-022-07565-y,en,INNA: An improved neural network algorithm for solving reliability optimization problems,"['OriginalPaper', 'Original Article']","The main objective of this paper is to present an improved neural network algorithm (INNA) for solving the reliability-redundancy allocation problem (RRAP) with nonlinear resource constraints. In this RRAP, both the component reliability and the redundancy allocation are to be considered simultaneously. Neural network algorithm (NNA) is one of the newest and efficient swarm optimization algorithms having a strong global search ability that is very adequate in solving different kinds of complex optimization problems. Despite its efficiency, NNA experiences poor exploitation, which causes slow convergence and also restricts its practical application of solving optimization problems. Considering this deficiency and to obtain a better balance between exploration and exploitation, searching procedure for NNA is reconstructed by implementing a new logarithmic spiral search operator and the searching strategy of the learner phase of teaching–learning-based optimization (TLBO) and an improved NNA has been developed in this paper. To demonstrate the performance of INNA, it is evaluated against seven well-known reliability optimization problems and finally compared with other existing meta-heuristics algorithms. Additionally, the INNA results are statistically investigated with the Wilcoxon sign-rank test and Multiple comparison test to show the significance of the results. Experimental results reveal that the proposed algorithm is highly competitive and performs better than previously developed algorithms in the literature.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s40747-022-00772-z,en,Cyberbullying detection using deep transfer learning,"['OriginalPaper', 'Original Article']","Social networking platforms like Facebook, Twitter, and others have numerous advantages, but they have many dark sides also. One of the issues on these social platforms is cyberbullying. The impact of cyberbullying is immeasurable on the life of victims as it’s very subjective to how the person would tackle this. The message may be a bully for victims, but it may be normal for others. The ambiguities in cyberbullying messages create a big challenge to find the bully content. Some research has been reported to address this issue with textual posts. However, image-based cyberbullying detection is received less attention. This research aims to develop a model that helps to prevent image-based cyberbullying issues on social platforms. The deep learning-based convolutional neural network is initially used for model development. Later, transfer learning models are utilized in this research. The experimental outcomes of various settings of the hyper-parameters confirmed that the transfer learning-based model is the better choice for this problem. The proposed model achieved a satisfactory accuracy of 89% for the best case, indicating that the system detects most cyberbullying posts.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00500-022-07410-3,en,A novel hybrid of chimp with cuckoo search algorithm for the optimal designing of digital infinite impulse response filter using high-level synthesis,"['OriginalPaper', 'Application of soft computing']","High-level synthesis of data paths in digital filters and model identification tasks are a most complicated optimization problem. Generally, the use of infinite impulse response (IIR) models for identification is chosen over their equivalent finite impulse response (FIR) models, because the former yield more perfect models of physical plants for real-world optimization problems. Additionally, infinite impulse response (IIR) model structures tend to make various multimodal error surfaces whose objective cost of the problems is significantly more difficult to minimize. For the solution of these types of issues, we needed more powerful methods that could resolve it perfectly. In this study, a novel hybrid variant of chimp and cuckoo search method has been developed for IIR related large-scale optimization issues, and it is known as chimp–cuckoo search algorithm (ChCS). The strength of the proposed method has been tested on the 23 standard functions and three types of infinite impulse response (IIR) models. Meanwhile, the numerical and statistical tabulated solutions of the ChCS method are also presented, showing the advantages of the proposed method over the competitors.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s40747-022-00791-w,en,A two-stage stacked-based heterogeneous ensemble learning for cancer survival prediction,"['OriginalPaper', 'Original Article']","Cancer survival prediction is one of the three major tasks of cancer prognosis. To improve the accuracy of cancer survival prediction, in this paper, we propose a priori knowledge- and stability-based feature selection (PKSFS) method and develop a novel two-stage heterogeneous stacked ensemble learning model (BQAXR) to predict the survival status of cancer patients. Specifically, PKSFS first obtains the optimal feature subsets from the high-dimensional cancer datasets to guide the subsequent model construction. Then, BQAXR seeks to generate five high-quality heterogeneous learners, among which the shortcomings of the learners are overcome by using improved methods, and integrate them in two stages through the stacked generalization strategy based on optimal feature subsets. To verify the merits of PKSFS and BQAXR, this paper collected the real survival datasets of gastric cancer and skin cancer from the Surveillance, Epidemiology, and End Results (SEER) database of the National Cancer Institute, and conducted extensive numerical experiments from different perspectives based on these two datasets. The accuracy and AUC of the proposed method are 0.8209 and 0.8203 in the gastric cancer dataset, and 0.8336 and 0.8214 in the skin cancer dataset. The results show that PKSFS has marked advantages over popular feature selection methods in processing high-dimensional datasets. By taking full advantage of heterogeneous high-quality learners, BQAXR is not only superior to mainstream machine learning methods, but also outperforms improved machine learning methods, which indicates can effectively improve the accuracy of cancer survival prediction and provide a reference for doctors to make medical decisions.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00521-022-07639-x,en,COVIDOA: a novel evolutionary optimization algorithm based on coronavirus disease replication lifecycle,"['OriginalPaper', 'Original Article']","This paper presents a novel bio-inspired optimization algorithm called Coronavirus Optimization Algorithm (COVIDOA). COVIDOA is an evolutionary search strategy that mimics the mechanism of coronavirus when hijacking human cells. COVIDOA is inspired by the frameshifting technique used by the coronavirus for replication. The proposed algorithm is tested using 20 standard benchmark optimization functions with different parameter values. Besides, we utilized five IEEE Congress of Evolutionary Computation (CEC) benchmark test functions (CECC06, 2019 Competition) and five CEC 2011 real-world problems to prove the proposed algorithm's efficiency. The proposed algorithm is compared to eight of the most popular and recent metaheuristic algorithms from the state-of-the-art in terms of best cost, average cost (AVG), corresponding standard deviation (STD), and convergence speed. The results demonstrate that COVIDOA is superior to most existing metaheuristics.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07574-x,en,An efficient two-stage water cycle algorithm for complex reliability-based design optimization problems,"['OriginalPaper', 'Original Article']","The reliability-based design optimization (RBDO) problem considers the necessary uncertainty of measurements within the scope of planning to minimize the design objective while satisfying probabilistic constraints. Metaheuristic algorithms offer effective tools to address challenges that scientists and practitioners face in RBDO problems, including the use of multimodal objective functions, mixed design variables, and nondifference mathematical models. However, metaheuristic reliability-based design optimization (MRBDO) algorithms require reliability analysis to obtain accurate solutions, which leads to different convergence behaviors than those observed for gradient RBDO algorithms. One of the main drawbacks of such schemes is the high computational cost. In this work, we derive an error propagation rule from the inner reliability analysis to the outer optimization. Then, based on a two-stage water cycle algorithm (TSWCA), an improved MRBDO algorithm called TSWCA-MRBDO is developed to ensure universality and performance. In the proposed algorithm, the water cycle algorithm, with a global capacity, is used to find the best solution. A single-loop strategy is first adopted, in which the MRBDO problem is converted into the deterministic optimization problem to remarkably reduce the computational time of global search. Then, a two-stage algorithm is utilized to perform the local search. Numerical examples demonstrate that the proposed two-stage MRBDO algorithm can converge more quickly and efficiently in the global and local domains than other MRBDO algorithms.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12145-022-00873-w,en,A combined model based on data decomposition and multi-model weighted optimization for precipitable water vapor forecasting,"['OriginalPaper', 'Research']","Water shortage is a major problem facing the world. Artificial precipitation enhancement is an effective way to improve precipitation conversion rate, but the selection of artificial precipitation enhancement operation timing is the main difficulty, and the precipitable water vapor(PWV) is a major index. The variation of PWV is nonlinear and unstable due to complex factors, especially in the Qilian mountains in the northeastern part of the Qinghai-Tibet Plateau, so it is difficult to predict it accurately. Therefore, based on the analysis of the observed data of microwave radiometer in Qilian Mountains, a new combined model is constructed which considers both data decomposition and prediction of several single models in this research. In the data preprocessing stage, the complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) technique is used to decompose and de-noise the PWV sequence. In the prediction stage, four neural network with unique characteristics, back propagation neural network (BPNN), long short term memroy (LSTM), bidirectional gated recurrent unit (BiGRU) and temporal convolutional network (TCN), are selected to predict the decomposed data respectively. A variant of gray Wolf optimization algorithm (IGWO) is used to determine the optimal weight of the model, and finally the comprehensive predicted value is obtained by weighting calculation. Through the analysis of experimental results, in the longest 15-step prediction, compared with CEEMDAN-BP, CEEMDAN-LSTM, CEEMDAN-BiGRU, CEEMDAN-TCN, the prediction accuracy can be improved by 54.17%, 35.05%, 22.38%, 23.86%, respectively. Other step size prediction also achieves the highest prediction accuracy.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s40998-022-00527-z,en,Solving Systems of Nonlinear Equations Using an Innovative Hybrid Algorithm,"['OriginalPaper', 'Research Paper']","In a variety of engineering applications and numerical computation, system of nonlinear equations (SNLEs) is one of the greatest remarkable problems. Among successful metaheuristic algorithms, particle swarm optimization (PSO) and differential evolution (DE) effectively employed in different optimization areas due to their powerful search capacity and simple structure. However, in solving complex optimization problems, still they have some shortcomings such as premature convergence and low search efficiency. An innovative hybrid algorithm of PSO and DE (named i h PSODE) present in this paper, for finding the solution of SNLEs. Besides, a novel inertia weight, acceleration factor and position update structure is adopted in nPSO to increase the population diversity as well as a novel mutation approach and crossover rate is implemented in nDE to help particles escape away from local optima. After population calculation according the fitness function cost recognize the top half member with discard rest half and apply nPSO which help to sustain exploration and exploitation competency of the algorithm. Furthermore, to achieve rapid convergence and fine stability, apply nDE on offspring created by nPSO. The population resultant by nPSO and nDE are combined for repetition. The proficiency of the presented algorithms (nPSO, nDE and i h PSODE) is examined on 23 basic unconstrained benchmark function and 19 scalable high-dimensional continuous functions (200 and 500 dimensions) then solved 7 multifaceted SNLEs. The simulation and relative results have indicated that the presented algorithms offer significant and reasonable performances.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s00366-021-01418-3,en,Performance evaluation of hybrid GA–SVM and GWO–SVM models to predict earthquake-induced liquefaction potential of soil: a multi-dataset investigation,"['OriginalPaper', 'Original Article']","The prediction of the potential of soil liquefaction induced by the earthquake is a vital task in construction engineering and geotechnical engineering. To provide a possible solution to such problems, this paper proposes two support vector machine (SVM) models which are optimized by genetic algorithm (GA) and grey wolf optimizer (GWO) to predict the potential of soil liquefaction. Field observation data based on cone penetration test (CPT), standard penetration test (SPT) and shear wave velocity ( V S ) test (SWVT) are employed to verify the reliability of the GA–SVM model and the GWO–SVM model, the numbers of input variables of these three field testing data sets are 6, 12 and 8, respectively, and the output result is the potential of soil liquefaction. To verify whether the two optimization algorithms GA and GWO have significantly improved the performance of SVM model, an unoptimized SVM model is served as a reference in this study. And five performance metrics, including classification accuracy rate (ACC), precision rate (PRE), recall rate (REC), F1 score (F1) and AUC are used to evaluate the classification performance of the three models. Results of the study confirm that when CPT-based, SPT-based and SWVT-based test sets are input into three classification models, the highest classification accuracy of 0.9825, 0.9032 and 0.9231, respectively, is achieved with GWO–SVM. And based on these three data sets, the values of AUC obtained by GWO–SVM are all higher than those obtained by GA–SVM. Further, by comparing the other metrics of the three classification models, it is found that the classification performance of the two hybrid models is very similar and significantly better than the SVM, which indicates that GWO–SVM, like GA–SVM, can also be used as a reliable model for predicting soil liquefaction potential.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11277-021-08767-8,en,Energy-Effective and Secure Data Transfer Scheme for Mobile Nodes in Smart City Applications,OriginalPaper,"Mobile nodes are deployed at different locations in the smart city to collect meaningful information so captured data can be used as an input in different smart city applications for society benefits. However, data is transferred through a public communication channel and thus, it is important to achieve specific security level to protect from malicious users in the network. There are various data transmission methods for mobile environments, but they are vulnerable against fundamental security attacks and the performance results are not effective for the mobile ad-hoc network. In this paper, we identify some security issues in Islam et al.’s scheme. To address found issues and improve the security and efficiency, we propose an energy-efficient and secure communication scheme for mobile node applications, achieving user identity privacy. We do security evaluations of the proposed protocol to confirm its strengths against various attacks. Further, we discuss performance analysis (for execution cost, energy consumption, communication overhead, and storage cost) for the suggested data transmission method and then, do the comparison with other relevant communication mechanisms.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s00289-022-04626-z,en,A review of recent advances in carbon dioxide absorption–stripping by employing a gas–liquid hollow fiber polymeric membrane contactor,"['ReviewPaper', 'REVIEW PAPER']","Membrane contactors using microporous membranes for acid gas removal have been extensively reviewed and discussed. The microporous membrane acts as a fixed interface between the gas and the liquid phase without dispersing one phase into another that offers a flexible modular and energy-efficient device. The gas absorption process can offer a high selectivity and a high driving force for transport even at low concentrations. Using hollow fiber, gas–liquid membrane contactors are a promising alternative to conventional gas absorption systems for acid gas capture from gas streams. Important aspects of membrane contactor as an efficient energy device for acid gas removal including liquid absorbents, membrane characteristics, combination of membrane and absorbent, mass transfer, membrane modules, model development, advantages and disadvantages were critically discussed. In addition, current status and future potential in research and development of gas–liquid membrane contactors for acid gas removal were also briefly discussed. The most essential factors of membrane contactors for CO 2 absorption/stripping are also discussed, including the hydrophilicity and hydrophobicity of the absorption materials in the membranes, as well as other models published in the literature. The benefits and drawbacks of gas–liquid contactor membranes in CO 2 absorption/stripping are also investigated, and the technique is compared to existing separation methods. The technology’s present condition and potential directions are explored, as well as some recommendations for further study in order to commercialize.","['Chemistry', 'Polymer Sciences', 'Soft and Granular Matter, Complex Fluids and Microfluidics', 'Characterization and Evaluation of Materials', 'Physical Chemistry', 'Organic Chemistry']"
doi:10.1007/s10489-022-03429-z,en,Memetic quantum optimization algorithm with levy flight for high dimension function optimization,OriginalPaper,"Quantum-inspired heuristic search algorithms have received widespread attention in solving the function optimal solution. However, the existing quantum search methods may lose their efficiency for high dimension function problems. To solve these problems, this study develops a novel memetic quantum optimization algorithm with levy flight (called MLQO) by exploring the principle of memetic computing and Levy flight. First, a memetic framework based on quantum mechanism is designed to balance the global search and the local search, which makes all particles gain a better experience from a local search. Second, an efficient local search strategy with levy flight is designed to enhance the searchability due to short walking distance and occasionally long jumps to be made by the particle. Furthermore, the lifetime of each particle is defined to determine whether a particle needs to be redistributed using the levy flight method in the search space. Numerical experiments are performed to evaluate the performance of the proposed algorithm by comparing it to other optimal algorithms. The results demonstrate the superiority of the proposed algorithm on well-known unimodal and multimodal benchmark functions.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10596-022-10174-2,en,Rock thin sections identification under harsh conditions across regions based on online transfer method,"['OriginalPaper', 'Original Paper']","Automatic identification of rock thin sections provides valuable geological information for oil and gas exploration. However, the application of rock thin sections identification in new blocks works poorly. The reason is that the geological data of target blocks are extremely scarce in the early stage of exploration and development. Therefore, using an improved MaSE-ResNetXt network, data enhancement, feature extractors, online transfer learning and other strategies, a stable, efficient and continuously growing end-to-end identification system based on online transfer learning was built. In case one, the use of the basic MaSE-ResNetXt network on regional rock properties and model categories was compared with previous studies and experiments, and performance improvements of 5% and 14.3% were achieved respectively, which verified the high efficiency of the basic MaSE-ResNetXt network for feature extraction in a large-scale dataset. In case two, sensitivity analysis was conducted for similar categories that are easily confused, during which parameters of the network and optimizer were compared. The sensitivity problems of oolitic structure feature maps were discussed and optimized. After that, a weighted F1-score of 0.95 was reached, proving the prediction stability of cross-regional transfer. In case three, real-time online transfer learning for the cold boot was conducted with scarce geological data. Compared with the previous studies, the training speed was improved by 15 times, and the accuracy was improved by 20%. An improved method based on MaSE-ResNeXt with transfer learning using a neural network feature map for rock characteristics analysis was proposed. By proposing inheritance and real-time online transfer learning solutions, the problems of automatic rock thin sections identification in a new area were solved, and the geological data of multiple blocks were fully used. By doing so, this study bridged the gap that experiences from the former regions could not be inherited effectively for cross-regional rock thin sections identification, so that empowered the development of new blocks.","['Earth Sciences', 'Earth Sciences, general', 'Geotechnical Engineering & Applied Earth Sciences', 'Hydrogeology', 'Mathematical Modeling and Industrial Mathematics', 'Soil Science & Conservation']"
doi:10.1007/s13198-022-01780-5,en,Image dehazing using autoencoder convolutional neural network,"['OriginalPaper', 'Original Article']","In hazy weather, the image in the scene suffers from noise which makes them less visible and to detect an object in hazy weather becomes a challenging task in computer vision. To have noise free image, many researchers have devised denoising techniques for enhancing visibility of images. Denoising is to remove the random variation from images and preserve the image features. As hazy images cause lots of visibility issues, this paper proposes removing haze and enhancing visibility of bad weather images with improved efficacy using an unsupervised neural network autoencoder that compress the data using machine learning and learns through Convolutional Neural Network (CNN). It has been observed that to have increased accuracy, the image classification and analysis is most effective using CNN. An end-to-end decoder training model is used to achieve the quality images. Further, various optimizers are compared to have better accuracy. The quality of images identified by estimation of performance such as RMSE and PSNR values are evaluated over single image and images from existing datasets and our own dataset. In the proposed method, RMSE value comes out to be 0.0373 for image from BSD500 dataset for specific image compared with other state of art approaches. The proposed model is intended in addition to other active, or progressive methods and the suggested method exceeds. The performance quality of images is explored applying measurable metrics. The images are taken from the datasets O-Haze, I-Haze, BSDS500, RESIDE, FRIDA and some from google.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s13198-022-01758-3,en,"Novel Western Jackdaw search, antrostomus swarm and Indian ethnic vedic teaching: inspired optimization algorithms for real power loss diminishing and voltage consistency growth","['OriginalPaper', 'Original Article']","This paper proposes Western Jackdaw search optimization (WJSO) algorithm, Antrostomus swarm optimization (ASSO) algorithm and Indian ethnic vedic teaching—inspired optimization (IEVTO) algorithm to solve the loss shrinking problem. Important aims of the paper are Power reliability extension, power oddity minimization and loss lessening. Western Jackdaw search optimization (WJSO) algorithm is designed based on the classiness demeanor of Western Jackdaw. Western Jackdaw search optimization algorithm distributes twofold segments of augmentation and variance. In prime segment, the flying design is crammed to propel up the process’s convergence in the course of the widespread optimal rate. In calculation, enlarging the impediment component in a certain collection will deliberately expand the examination area. ASSO algorithm is enthused by erudition the activities of Antrostomus in the swarm. Activities are communal movements, joint communication, satirist development, flying, and observance deeds are smeared for untangling the glitches. IEVTO algorithm is modeled by the inspiration from Guru–Shishya education system which has been found in ancient India. The new-fangled Gurus will progress their peculiar spiritual ethos by education expeditions. Around there are dual substitutes to complete their education expeditions: the stage selection stratagems is based on the Levy or by normal distribution. Proposed WJSO algorithm, ASSO algorithm and IEVTO algorithms are validated in test systems.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s00202-022-01641-1,en,Classification of distribution power grid structures using inception v3 deep neural network,"['OriginalPaper', 'Original Paper']","To maintain the supply of electrical energy, it is necessary that failures in the distribution grid are identified during inspections of the electrical power system before shutdowns occur. To automate the inspections, artificial intelligence techniques based on computer vision are proposed. Due to the low number of visible faults, it is difficult to train deep learning models based on images of electrical power system inspections. In this paper, it is proposed to use segmentation and edge detection techniques to increase the database, making classification possible using the Inception v3 deep neural network model. From a pre-processing using the Gaussian filter to smooth the image, the techniques of the threshold with binarization, adaptive binarization, and Otsu and riddler-calvard are used for segmentation; and for edge detection, the sobel and canny techniques are used. The Inception v3 had better results than VGG-16 and ResNet50, considering mean squared error, root mean square error, accuracy, precision, recall, F-measure, and speed to convergence in this application.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s40996-022-00906-w,en,Vibration-Based Damage Detection of Arch Dams Using Least-Square Support Vector Machines and Salp Swarm Algorithms,"['OriginalPaper', 'Research Paper']","This paper presents a vibration-based damage-detection approach for arch dams using least-square support vector machines and salp swarm algorithms (SSAs). Least-square support vector regression is used to establish a surrogate model representing the relationship between the dynamic elastic modulus and modal parameters (natural frequency and mode shape). The SSA is applied for dynamic parameter identification by minimizing an objective function composed of vibration data. To verify the performance of the proposed method, we consider a hyperbolic concrete arch dam as a numerical example. Furthermore, the SSA is compared with several other population-based global optimization algorithms. Results show that the proposed approach can significantly improve the damage identification efficiency without compromising accuracy. The SSA is promising for solving dynamic parameter identification problems.","['Engineering', 'Civil Engineering']"
doi:10.1007/s12652-021-03269-8,en,Trigonometric mutation and successful-parent-selection based adaptive asynchronous differential evolution,"['OriginalPaper', 'Original Research']","Asynchronous differential evolution (ADE) supports parallel optimization and effective exploration. The updation in population is done immediately when a vector with better fitness is found in ADE algorithm. The working of ADE and Differential Evolution (DE) is similar except the instant population updation feature and asynchronous nature. In this paper, we have integrated ADE with successful parent-selecting (SPS) framework and trigonometric mutation to enhance the performance. Additionally, the control parameters are updated in an adaptive manner to support better exploration as well as exploitation. The proposed algorithm is named as SPS embedded adaptive ADE with trigonometric mutation (SPS-AADE-TM). The modified mutation operation and adaptive parameters can increase the population diversity and the convergence speed. The parameter adaptation feature can automatically obtain the appropriate values of control parameters to enhance the robustness of SPS-AADE-TM. The proposed algorithm is tested over twenty-five widely used bench-mark functions and four engineering design problems. Two nonparametric statistical tests are also carried out to validate the performance of SPS-AADE-TM. The simulation results show that the proposed work provides promising results and outperforms the competitive algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s12145-022-00880-x,en,Prediction of thermal conductivity of granitic rock: an application of arithmetic and salp swarm algorithms optimized ANN,"['OriginalPaper', 'Research']","Thermal conductivity (TC) is an important rock property as it determines its energy transfer potential. Compared with other rock properties like uniaxial comprehensive strength (UCS), it is rarely investigated. Hence, novel Arithmetic and Salp swarm optimized artificial neural network (ANN) models are used to predict the thermal conductivity of granitic rock based on the results of non-destructive tests. Fifty (50) core samples were obtained from the study location and tested in the laboratory. The results obtained from the laboratory investigations were used to perform the ordinary ANN and the optimized ANN models. The outcomes showed that the performances of the optimized ANN models are better than the ordinary ANN model. The results were also compared with the multiple linear regression model (MLR) although the predictive strength of the MLR model is extremely low. The proposed models were mathematically transformed into simple mathematical models, and a graphic user interface (GUI) prepared with the Visual basic programming language was developed. The proposed models can be practically implemented for TC prediction.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s12065-021-00634-6,en,A comparative analysis of meta-heuristic optimization algorithms for feature selection and feature weighting in neural networks,"['OriginalPaper', 'Research Paper']","Feature selection and feature weighting are frequently used in machine learning for processing high dimensional data. It reduces the number of features in the dataset and makes the classification process easier. Meta-heuristic algorithms are widely adopted for feature selection and feature weighting due to their enhanced searching ability. This paper compares five different meta-heuristic optimization algorithms that are recently introduced for feature selection and feature weighting in artificial neural networks. This includes chimp optimization algorithm, tunicate swarm algorithm, bear smell search algorithm, antlion optimization algorithm and modified antlion optimization algorithm. Experimental evaluations are performed on five different datasets to illustrate the significant improvements observed during classification process of all the algorithms utilised in the comparative analysis. Both tunicate swarm algorithm and chimp optimization algorithm has gained better classification accuracy than other algorithms. However, all these algorithms are found to be more effective for feature selection and feature weighting processes.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s00500-022-06832-3,en,An adaptive neuro-fuzzy inference system to monitor and manage the soil quality to improve sustainable farming in agriculture,"['OriginalPaper', 'Focus']","Neural network is one among the foremost powerful predictive analytics techniques. This paper considers a wide range of neural network topologies, followed by an examination of how to optimize and analyze neural networks using agriculture data. A back-propagation model was proposed here to create a predictive inference model, with five hidden layers and activation function as sigmoid. Four Kaggle datasets are used here to assess the ability of the proposed neural network approach regarding prediction, and three datasets are gathered from Kerala Agriculture University in real time. Finally, as an optimization strategy, adaptive momentum estimation was utilized, and the applied findings demonstrate that Adam optimizer achieves good prediction accuracy, and F1 score with the help of neuro-fuzzy system and therefore better convergence rate. The proposed method enables more precise control of the direction and step size for updating weight vectors, leading to significantly improved generalization performance. Experimental results reveal that the proposed method shows an accuracy of 97% in average. The suggested approach outperforms other benchmark algorithms such as C4.5, ID3, CART, Naïve Bayes, fuzzy, MLP and hence it can be concluded that the proposed hybridization of back-propagation neural networks with adaptive optimization method clearly verifies the efficiency of Adam optimizer with back propagation for text data.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00440-022-01167-4,en,Entropic curvature on graphs along Schrödinger bridges at zero temperature,OriginalPaper,"Lott–Sturm–Villani theory of curvature on geodesic spaces has been extended to discrete graph spaces by C. Léonard by replacing $$W_2$$ W 2 -Wasserstein geodesics by Schrödinger bridges in the definition of entropic curvature (Léonard in Discrete Contin Dyn Syst A 34(4):1533–1574, 2014; Ann Probab 44(3):1864–1915, 2016; in: Gigli N (ed) Measure theory in non-smooth spaces. Sciendo Migration,Warsaw, pp 194–242, 2017). As a remarkable fact, as a temperature parameter goes to zero, these Schrödinger bridges are supported by geodesics of the space. We analyse this property on discrete graphs to reach entropic curvature on discrete spaces. Our approach provides lower bounds for the entropic curvature for several examples of graph spaces: the lattice $$\mathbb {Z}^n$$ Z n endowed with the counting measure, the discrete cube endowed with product probability measures, the circle, the complete graph, the Bernoulli–Laplace model. Our general results also apply to a large class of graphs which are not specifically studied in this paper. As opposed to Erbar–Maas results on graphs (Erbar and Maas in Arch Ration Mech Anal 206(3):997–1038, 2012; Discrete Contin Dyn Syst A 34(4):1355–1374, 2014; Maas in J Funct Anal 261(8):2250–2292, 2011), entropic curvature results of this paper imply new Prékopa–Leindler type of inequalities on discrete spaces, and new transport-entropy inequalities related to refined concentration properties for the graphs mentioned above. For example on the discrete hypercube $$\{0,1\}^n$$ { 0 , 1 } n and for the Bernoulli Laplace model, a new $$W_2-W_1$$ W 2 - W 1 transport-entropy inequality is reached, that can not be derived by usual induction arguments over the dimension n . As a surprising fact, our method also gives improvements of weak transport-entropy inequalities (see Gozlan et al. in J Funct Anal 273(11):3327–3405, 2017) associated to the so-called convex-hull method by Talagrand (Publ Math l’Inst Hautes Etudes Sci 81(1):73–205, 1995).","['Mathematics', 'Probability Theory and Stochastic Processes', 'Theoretical, Mathematical and Computational Physics', 'Quantitative Finance', 'Mathematical and Computational Biology', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Operations Research/Decision Theory']"
doi:10.1007/s10619-021-07360-z,en,Deep learning-based computer aided diagnosis model for skin cancer detection and classification,OriginalPaper,"Skin cancer is a commonly occurring disease, which affects people of all age groups. Automated detection of skin cancer is needed to decrease the death rate by identifying the diseases at the initial stage. The visual inspection during the medical examination of skin lesions is a tedious process as the resemblance among the lesions exists. Recently, imaging-based Computer Aided Diagnosis (CAD) model is widely used to screen and detect the skin cancer. This paper is designed with automated Deep Learning with a class attention layer based CAD model for skin lesion detection and classification known as DLCAL-SLDC. The goal of the DLCAL-SLDC model is to detect and classify the different types of skin cancer using dermoscopic images. During image pre-processing, Dull razor approach-based hair removal and average median filtering-based noise removal processes take place. Tsallis entropy based segmentation technique is applied to detect the affected lesion areas in the dermoscopic images. Also, a DLCAL based feature extractor is used for extracting the features from the segmented lesions using Capsule Network (CapsNet) along with CAL and Adagrad optimizer. The CAL layer incorporated into the CapsNet is intended to capture the discriminative class-specific features to cover the class dependencies and effectively bridge the CapsNet for further process. Finally, the classification is carried out by the Swallow Swarm Optimization (SSO) algorithm based Convolutional Sparse Autoencoder (CSAE) known as SSO-CSAE model. The proposed DLCAL-SLDC technique is validated using a benchmark ISIC dataset. The proposed framework has accomplished promising results with 98.50% accuracy, 94.5% sensitivity and 99.1% specificity over the other methods interms of different measures.","['Computer Science', 'Database Management', 'Data Structures', 'Information Systems Applications (incl.Internet)', 'Operating Systems', 'Memory Structures']"
doi:10.1007/s00354-022-00191-1,en,A Hybrid Deep Learning Model Using Grid Search and Cross-Validation for Effective Classification and Prediction of Suicidal Ideation from Social Network Data,OriginalPaper,"Suicide deaths due to depression and mental stress are growing rapidly at an alarming rate. People freely express their feelings and emotions on social network sites while they feel hesitant to express such feelings during face-to-face interactions with their dear ones. In this study, a dataset comprising 20,000 posts was taken from Reddit and preprocessed into tokens using a variety of effective word2vec techniques. A new hybrid approach is proposed by combining the attention model in a convolutional neural network and long-short-term- memory. The objective of this research is to develop an effective learning model to evaluate the data on social media for the efficient and accurate identification of people with suicidal ideation. The proposed attention convolution long short-term memory (ACL) model uses hyperparameter tuning using a grid search to select optimized hyperparameters. From the experimental evaluation, it is shown that the proposed model, that is, ACL with Glove embedding after hyperparameter tuning gives the highest Accuracy of 88.48%, Precision of 87.36%, F1 score of 90.82% and specificity of 79.23% and ACL with Random embedding gives the highest Recall of 94.94% when compared to the state-of-the-art algorithms.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s10489-022-03269-x,en,Improved Salp swarm algorithm for solving single-objective continuous optimization problems,OriginalPaper,"The Salp Swarm Algorithm (SSA) is an effective single-objective optimization algorithm that was inspired by the navigating and foraging behaviors of salps in their natural habitats. Although SSA was successfully tailored and applied to solve various types of optimization problems, it often suffers from premature convergence and typically does not perform well with high-dimensional optimization problems. This paper introduces an Improved SSA (ISSA) algorithm to enhance the performance of SSA in solving single-objective continuous optimization problems. ISSA has four characteristics. First, it employs Gaussian Perturbation to improve the diversity of initial population. Second, it uses highly disruptive polynomial mutation (HDPM) to update the leader salp in the salp chain. Third, it uses the Laplace crossover operator to improve its exploration ability. Fourth, it uses a new opposition learning method called Mixed Opposition-based Learning (MOBL) to improve its convergence rate and exploration ability. A set of 14 standard benchmark functions was used to evaluate the performance of ISSA and compare it to three variations of SSA (SSA, Hybrid SSA with Particle Swarm Optimization HSSAPSO Singh et al. ( 2020 ) and Enhanced SSA (ESSA) Zhang et al. ( 2020 )). The overall experimental and statistical results indicate that ISSA is a better optimization algorithm than the other SSA variations. Further, the single-objective IEEE CEC 2014 (IEEE Congress on Evolutionary Computation 2014) functions were used to evaluate and compare the performance of ISSA to 18 well-known and state-of-the-art optimization algorithms (Exploratory Cuckoo Search (ECS) Abed-alguni ( 2021 )), Grey Wolf Optimizer (GWO) Mirjalili and Mirjalili ( Advances in Engineering Software , 69 , 46–61, 2014 ), Distributed Grey Wolf Optimize (DGWO) Abed-alguni and Barhoush ( 2018 ), Cuckoo Search (CS) Yang and Deb ( 2009 ), Distributed adaptive differential evolution with linear population size reduction evolution (L-SHADE) Tanabe and Fukunaga ( 2014 ), Memory-based Hybrid Dragonfly Algorithm (MHDA) KS and Murugan ( Expert Syst Appl , 83 , 63–78, 2017 ), Fireworks Algorithm with Differential Mutation (FWA-DM) Yu et al. ( 2014 ), Differential Evolution-based Salp Swarm Algorithm (DESSA) Dhabal et al. ( Soft Comput , 25(3) , 1941–1961, 2021 ), LSHADE with Fitness and Diversity Ranking-Based Mutation Operator (FD-LSHADE) Cheng et al. ( Swarm and Evolutionary Computation , 61 , 100816, 2021 ), Distance based SHADE (Db-SHADE) Viktorin et al. ( Swarm and Evolutionary Computation , 50 , 100462, 2019 ) and Zeng et al. ( Knowl-Based Syst , 226 , 107150, 2021 ), Mean–Variance Mapping Optimization (MVMO) Iacca et al. ( Expert Syst Appl , 165 , 113902, 2021 ), Time-varying strategy-based Differential Evolution (TVDE) Sun et al. ( Soft Comput , 24(4) , 2727–2747, 2020 ), Butterfly Optimization Algorithm with adaptive gbest-guided search strategy and Pinhole-Imaging-based Learning (PIL-BOA)Long et al. ( Appl Soft Comput , 103 , 107146, 2021 ), Memory Guided Sine Cosine Algorithm (MG-SCA) Gupta et al. ( Eng Appl Artif Intell , 93 , 103718, 2020 ), Lévy flight Jaya Algorithm (LJA) Iacca et al. ( 2021 ), Sine Cosine Algorithm (SCA) Dhabala et al. ( 2021 ), Covariance Matrix Adaptation Evolution Strategy (CMA-ES) Hansen et al. ( Evolutionary Computation , 11(1) , 1–18, 2003 ) and Coyote Optimization Algorithm (COA) Pierezan and Coelho ( 2018 )). The results indicate that ISSA performs better than the tested optimization algorithms.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s12293-022-00378-5,en,A diversity-aware memetic algorithm for the linear ordering Problem,"['OriginalPaper', 'Regular research paper']","The Linear Ordering Problem (LOP) is a very popular NP-hard combinatorial optimization problem with many practical applications that may require the use of large instances. The Linear Ordering Library (LOLIB) gathers a set of standard benchmarks that are widely used in validating solvers for the LOP. Among them, xLOLIB2 collects some of the largest and most challenging instances in the current literature. In this work, we present new best-known solutions for each of the 200 complex instances that comprise xLOLIB2 and for the other 93 instances of the benchmarks with smaller sizes. This important advance in the field of LOP has been made possible due to the development of a novel memetic algorithm (MA) that was designed by taking into account some of the weaknesses of state-of-the-art LOP solvers. In particular, one of the keys to success is that the novel proposal allows for a gradual shift from exploration to exploitation. This is done using the novel Best Non-Penalized (BNP) replacement strategy. BNP selects the survivors by taking into account the quality, the Spearman’s footrule distance, the stopping criterion, and the elapsed period of execution simultaneously. The novel diversity-aware proposal is called the memetic algorithm with explicit diversity management (MA-EDM) and extensive comparisons against state-of-the-art techniques provide insights into the reasons for the superiority of MA-EDM.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Complex Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s00440-022-01108-1,en,Maximizing expected powers of the angle between pairs of points in projective space,OriginalPaper,"Among probability measures on d -dimensional real projective space, one which maximizes the expected angle $$\arccos (\frac{x}{|x|}\cdot \frac{y}{|y|})$$ arccos ( x | x | · y | y | ) between independently drawn projective points x and y was conjectured to equidistribute its mass over the standard Euclidean basis $$\{e_0,e_1,\ldots , e_d\}$$ { e 0 , e 1 , … , e d } by Fejes Tóth (Acta Math Acad Sci Hung 10:13–19, 1959. https://doi.org/10.1007/BF02063286 ). If true, this conjecture evidently implies the same measure maximizes the expectation of $$\arccos ^\alpha (\frac{x}{|x|}\cdot \frac{y}{|y|})$$ arccos α ( x | x | · y | y | ) for any exponent $$\alpha > 1$$ α > 1 . The kernel $$\arccos ^\alpha (\frac{x}{|x|}\cdot \frac{y}{|y|})$$ arccos α ( x | x | · y | y | ) represents the objective of an infinite-dimensional quadratic program. We verify discrete and continuous versions of this milder conjecture in a non-empty range $$\alpha > \alpha _{\Delta ^d} \ge 1$$ α > α Δ d ≥ 1 , and establish uniqueness of the resulting maximizer $${\hat{\mu }}$$ μ ^ up to rotation. We show $${\hat{\mu }}$$ μ ^ no longer maximizes when $$\alpha <\alpha _{\Delta ^d}$$ α < α Δ d . At the endpoint $$\alpha =\alpha _{\Delta ^d}$$ α = α Δ d of this range, we show another maximizer $$\mu $$ μ must also exist which is not a rotation of $${\hat{\mu }}$$ μ ^ . For the continuous version of the conjecture, an “Appendix A” provided by Bilyk et al in response to an earlier draft of this work combines with the present improvements to yield $$\alpha _{\Delta ^d}<2$$ α Δ d < 2 . The original conjecture $${\alpha _{\Delta ^d}}=1$$ α Δ d = 1 remains open (unless $$d=1$$ d = 1 ). However, in the maximum possible range $$\alpha >1$$ α > 1 , we show $${\hat{\mu }}$$ μ ^ and its rotations maximize the aforementioned expectation uniquely on a sufficiently small ball in the $$L^\infty $$ L ∞ -Kantorovich–Rubinstein–Wasserstein metric $$d_\infty $$ d ∞ from optimal transportation; the same is true for any measure $$\mu $$ μ which is mutually absolutely continuous with respect to $${\hat{\mu }}$$ μ ^ , but the size of the ball depends on $$\alpha ,d$$ α , d , and $$\Vert \frac{d {\hat{\mu }}}{d\mu }\Vert _{\infty }$$ ‖ d μ ^ d μ ‖ ∞ .","['Mathematics', 'Probability Theory and Stochastic Processes', 'Theoretical, Mathematical and Computational Physics', 'Quantitative Finance', 'Mathematical and Computational Biology', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Operations Research/Decision Theory']"
doi:10.1007/s00366-021-01289-8,en,A water cycle-based error minimization technique in predicting the bearing capacity of shallow foundation,"['OriginalPaper', 'Original Article']","Selecting the appropriate training technique is a significant step in utilizing intelligent approaches. It becomes even more important when it comes to critical problems like analyzing the bearing capacity of foundations. This study investigates the feasibility of a capable metaheuristic algorithm, called the water cycle algorithm (WCA), for training a multi-layer perceptron (MLP). The WCA-MLP is applied to a large finite element dataset to predict the settlement. The results of this model are compared with electromagnetic field optimization (EFO) and shuffled complex evolution (SCE) benchmarks. With reference to the obtained Pearson correlation factors (larger than 0.88 in all stages), all employed models are suitable for the mentioned objective. Moreover, it was observed that the training error of the WCA was 5.84 and 3.89% smaller than the EFO and SCE, respectively. Likewise, the accuracy of the WCA-MLP was 1.85 and 2.04% larger in the testing phase. Also, a predictive equation is finally elicited for practical applications in compatible circumstances.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s12530-022-09426-4,en,Deep learning system applicability for rapid glaucoma prediction from fundus images across various data sets,"['OriginalPaper', 'Original Paper']","Glaucoma damages the optical nerve, which sends visual pictures to the brain, and results in irreversible vision loss. This chronic infection is the second leading cause of permanent blindness across the world and worsens the purpose of life if not cured at an early stage. Traditional ways of diagnosing glaucoma, however, rely on heavy equipment and highly trained personnel, making it impossible to assess huge populations of individuals. This results in high costs and lengthy wait times. As a result, new methods for diagnosing glaucoma that do not exacerbate these problems need to be investigated. Previously, to detect glaucoma through artificial intelligence, features were extracted manually, which not only consumes a lot of time but is also a tedious task to perform and there is a chance of intra-observer variability. Now, deep learning (DL) techniques can be used to extract features automatically, which was not possible in the traditional methods. In view of the multiple associated problems like limited labeled data, difficulty and cost incurred in building glaucoma fundus photographic datasets, and special hardware requirements, this study assessed the performance of a DL model (s) which are trained in detecting glaucoma from fundus pictures and methods. The objective is to present a versatile DL model which should generate auspicious performance across multiple datasets to meet real-life scenarios instead of generating specific dataset performance and, along with it, take care of these coupled problems. Diverse deep learning techniques are investigated in this empirical study to categorise the fundus images into two classes: normal and glaucomatous. On all these models, fine-tuning with transfer learning is also performed. Three different publicly available benchmark datasets (ACRIMA, ORIGA, and HRF) were used for training and validation. The models were tested not only on DRISHTI-GS (a public dataset) and a private dataset but also on twelve combinations of these five datasets. Extensive experiments are conducted to manifest the effectiveness of the proposed approach, and on the basis of Area under Curve values and computed accuracy values, it is concluded that Inception-ResNet-v2 and Xception models outperform other competitive models. The findings show the potential of this technology in the early identification of glaucoma. This automated diagnosis system has great potential to ultimately reduce the human efforts and precious time of ophthalmologists.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s00500-022-07364-6,en,Modeling the leader–follower supply chain network under uncertainty and solving by the HGALO algorithm,"['OriginalPaper', 'Application of soft computing']","The purpose of this article is to develop a competitive supply chain network (SCN) in the face of uncertainty. The objective of the leader chain is to maximize total network profits by strategically locating suppliers, manufacturers, distribution centers, and retailers. Additionally, the follower chain seeks to maximize the network's profit. Both factors, optimal flow allocation to different echelons of the SCN and product pricing, are examined in the leader chain and follower chain. The KKT conditions are used in this article to convert a bi-level model to a one-level model. Additionally, a fuzzy programming technique is used to control the problem's uncertain parameters. According to the results obtained using the fuzzy programming technique, increasing the uncertainty rate increases demand while decreasing the OBFV and average selling price of products. Finally, the problem was untangled using a novel hybrid genetic and ant-lion optimization algorithm (HGALO). The results of problem solving in larger sizes demonstrate HGALO's superior efficiency in comparison with the other algorithm.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11721-022-00219-8,en,Blending multiple algorithmic granular components: a recipe for clustering,OriginalPaper,"Emerging trends in algorithm design have shown that hybrid algorithms, which combine or merge multiple algorithms, can create synergies to overcome the inherent limitations of the underlying individual algorithms. There are two broad types of hybridization: collaborative —individual algorithms tackle an instance of the problem sequentially or in parallel and exchange information accordingly while solving the problem; integrative —individual algorithms are dedicated to tackling different aspect(s) of the problem-solving process. In this research, we propose a schema for an enhanced form of integrative hybridization that blends granular algorithmic components from multiple algorithms to derive a new singular clustering algorithm. As a case study, we examine the ant clustering algorithm (a swarm intelligence algorithm that is based on the natural phenomenon of brood sorting in some species of ants); highlight the strengths and weaknesses of the algorithm; and present a blend of algorithmic components from Tabu search into the algorithm to improve its exploration strategy and solution quality. Empirical results from applying the blended algorithm to clustering benchmark datasets show improved clustering validation measures for the proposed blended hybrid algorithm compared to other hybridization of the same underlying individual algorithms. Besides, the quality of clusters uncovered by this hybrid algorithm competes favorably with those uncovered using popular clustering algorithms such as DBSCAN and mean shift.","['Computer Science', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Mathematical and Computational Engineering', 'Communications Engineering, Networks', 'Computer Communication Networks']"
doi:10.1007/s40305-021-00341-0,en,A Hybrid of Grey Wolf Optimization and Genetic Algorithm for Optimization of Hybrid Wind and Solar Renewable Energy System,OriginalPaper,"In this paper, a hybrid of grey wolf optimization (GWO) and genetic algorithm (GA) has been implemented to minimize the annual cost of hybrid of wind and solar renewable energy system. It was named as hybrid of grey wolf optimization and genetic algorithm (HGWOGA). HGWOGA was applied to this hybrid problem through three procedures. First, the balance between the exploration and the exploitation process was done by grey wolf optimizer algorithm. Then, we divided the population into subpopulation and used the arithmetical crossover operator to utilize the dimension reduction and the population partitioning processes. At last, mutation operator was applied in the whole population in order to refrain from the premature convergence and trapping in local minima. MATLAB code was designed to implement the proposed methodology. The result of this algorithm is compared with the results of iteration method, GWO, GA, artificial bee colony (ABC) and particle swarm optimization (PSO) techniques. The results obtained by this algorithm are better when compared with those mentioned in the text.","['Mathematics', 'Operations Research, Management Science']"
doi:10.1007/s40747-022-00759-w,en,Machine learning-based framework to cover optimal Pareto-front in many-objective optimization,"['OriginalPaper', 'Original Article']","One of the crucial challenges of solving many-objective optimization problems is uniformly well covering of the Pareto-front (PF). However, many the state-of-the-art optimization algorithms are capable of approximating the shape of many-objective PF by generating a limited number of non-dominated solutions. The exponential increase of the population size is an inefficient strategy that increases the computational complexity of the algorithm dramatically—especially when solving many-objective problems. In this paper, we introduce a machine learning-based framework to cover sparse PF surface which is initially generated by many-objective optimization algorithms; either by classical or meta-heuristic methods. The proposed method, called many-objective reverse mapping (MORM), is based on constructing a learning model on the initial PF set as the training data to reversely map the objective values to corresponding decision variables. Using the trained model, a set of candidate solutions can be generated by a variety of inexpensive generative techniques such as Opposition-based Learning and Latin Hypercube Sampling in both objective and decision spaces. Iteratively generated non-dominated candidate solutions cover the initial PF efficiently with no further need to utilize any optimization algorithm. We validate the proposed framework using a set of well-known many-objective optimization benchmarks and two well-known real-world problems. The coverage of PF is illustrated and numerically compared with the state-of-the-art many-objective algorithms. The statistical tests conducted on comparison measures such as HV, IGD, and the contribution ratio on the built PF reveal that the proposed collaborative framework surpasses the competitors on most of the problems. In addition, MORM covers the PF effectively compared to other methods even with the aid of large population size.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00354-022-00190-2,en,Design and Development of Modified Ensemble Learning with Weighted RBM Features for Enhanced Multi-disease Prediction Model,OriginalPaper,"In this computer world, huge data are generated in several fields. Statistics in the healthcare engineering provides data about many diseases and corresponding patient’s information. These data help to evaluate a huge amount of data for identifying the unknown patterns in the diseases and are also utilized for predicting the disease. Hence, this work is to plan and implement a new computer-aided technique named modified Ensemble Learning with Weighted RBM Features (EL-WRBM). Data collection is an initial process, in which the data of various diseases are gathered from UCI repository and Kaggle. Then, the gathered data are pre-processed by missing data filling technique. Then, the pre-processed data are performed by deep belief network (DBN), in which the weighted features are extracted from the RBM regions. Then, the prediction is made by ensemble learning with classifiers, namely, support vector machine (SVM), recurrent neural network (RNN), and deep neural network (DNN), in which hyper-parameters are optimized by the adaptive spreading rate-based coronavirus herd immunity optimizer (ASR-CHIO). At the end, the simulation analysis reveals that the suggested model has implications to support doctor diagnoses.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s12206-022-1141-3,en,Initial fault diagnosis of bearing based on AVMD-SE and multiscale enhanced morphological top-hat filter,"['OriginalPaper', 'Original Article']","Early fault signature detection and background noise removal are essential for bearing fault diagnosis. A novel multiscale enhanced morphological top-hat filter fault diagnosis method, adaptive variational mode decomposition-sample entropy-multiscale enhanced top-hat filter (AVMD-SE-MEMTF), is proposed based on AVMD-SE noise reduction. First, gray wolf optimization algorithm is proposed to optimize the VMD to achieve the optimal decomposition parameters adaptively and combine with SE to eliminate the high noise components and improve the noise reduction effect. Then, based on the pulse extraction property of morphological operations, the concept of MEMTF is proposed. To enhance the multiscale index selection strategy, a synthesis method of eigenfrequency envelope coefficients is constructed to increase the accuracy of the operator during the vibration signal process. Finally, experimental and engineering results show that the proposed method has good diagnostic performance for weak faults in the presence of noise interference.","['Engineering', 'Mechanical Engineering', 'Vibration, Dynamical Systems, Control', 'Industrial and Production Engineering']"
doi:10.1007/s00530-022-00971-1,en,Multi-attribute object detection benchmark for smart city,"['OriginalPaper', 'Regular Paper']","Object detection is an algorithm that recognizes and locates the objects in the image and has a wide range of applications in the visual understanding of complex urban scenes. Existing object detection benchmarks mainly focus on a single specific scenario and their annotation attributes are not rich enough, these make the object detection model not generalized for the smart city scenes. Considering the diversity and complexity of scenes in intelligent city governance, we build a large-scale object detection benchmark for the smart city. Our benchmark contains about 100K images and includes three scenarios: intelligent transportation, intelligent surveillance, and drone. For the complexity of the real scene in the smart city, the diversity of weather, occlusion, and other complex environment diversity attributes of the images in the three scenes are annotated. The characteristics of the benchmark are analyzed and extensive experiments of the current state-of-the-art target detection algorithm are conducted based on our benchmark to show their performance. Our benchmark is available at https://openi.org.cn/projects/Benchmark .","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00366-021-01497-2,en,LSFQPSO: quantum particle swarm optimization with optimal guided Lévy flight and straight flight for solving optimization problems,"['OriginalPaper', 'Original Article']","As a metaheuristic algorithm, particle swarm optimization (PSO) has two main disadvantages. Firstly, it needs to set many parameters, which is not conducive to finding the optimal parameters of the model to be optimized. Secondly, it is easy to fall into the trap of local optimal. Motivated by concepts in quantum mechanics and PSO, quantum-behaved particle swarm optimization (QPSO) was proposed having better global search ability. However, QPSO is deficient in solving high-dimensional problems and performs poorly in adaptability. In this paper, in order to better solve the high-dimensional problems and more applicable to real-world optimization problems, two strategies of Lévy flight (LF) and straight flight (SF) are introduced. An improved quantum particle swarm optimization with Lévy flight and straight flight (LSFQPSO) is proposed. The proposed LSFQPSO algorithm is tested on 22 classic benchmark functions and three engineering optimization problems. The obtained results are compared with seven metaheuristic algorithms and evaluated according to Friedman rank test. The experiments show that LSFQPSO algorithm provides better results with superior performance in most tests compared with seven well-known algorithms, especially in solving high-dimensional problems. What’s more, the proposed LSFQPSO algorithm also shows good performance in solving real-world engineering design optimization problems.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11277-021-08800-w,en,Blockchain Enabled Emperor Penguin Optimizer Based Encryption Technique for Secure Image Management System,OriginalPaper,"In recent years, the electronic sharing of digital images faces a major threat to security, as the existing image transmission infrastructure is mainly based on the trust of third parties. At the same time, the available solutions are placed on the cloud based centralized data center, which is expensive, requires large storage area, and security issues regarding the transmission of data over the network. So, it is needed to develop an image management system which enables sharing and storing of digital images effectively. This paper develops novel multiple share creation schemes with block technology for secure image management (MSCCBT-SIM) systems. The MSCCBT-SIM model allows the user to create consensus with no dependencies on central authorities. It involves an MSC which involves share creation and share encryption using emperor penguin optimizer based ElGamal public key cryptosystem (EPO-EPKC). In addition, the blockchain is used as a distributed data storage mechanism to generate a ledger for permitting access to the user and prevent third party access to the encrypted shares. The application of blockchain technology and MSC techniques helps to achieve decentralization, highly reliable, inexpensive, and secure transmission and storage of digital images. In order to validate the effective performance of the MSCCBT-SIM model, a series of simulations take place and investigated the results interms of different measures. The experimental results ensured the better performance of the MSCCBT-SIM model with the superior PSNR of 57.09 dB whereas the HOCE-ECC, GO-ECC, PSO-ECC, and CS-ECC methods offered a lower PSNR of 53.08 dB, 52.91 dB, 52.65 dB, and 50.51 dB respectively.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s41315-022-00256-w,en,Optimal path planning of multi-robot in dynamic environment using hybridization of meta-heuristic algorithm,"['OriginalPaper', 'Regular Paper']","This paper investigates an innovative strategy for generating a collision and deadlock-free optimal position for the individual robots by satisfying the constraints of a dynamic environment for path planning of multiple mobile robots. The current study emphasized the shortfalls of the previous investigation on multi-robot path planning and offered an energetic methodology through the hybridization of modified Q-learning with the improved version of particle swarm optimization (IPSO) and arithmetic optimization algorithm (AOA). In the current scenario, classical Q-learning is modified through a reward policy and generates the best solution for PSO. The basic PSO is upgraded through the perception of ascendency in human civilization and generates an optimal location in the succeeding iteration using an arithmetic optimization algorithm. The proposed hybrid algorithm primarily highlights evaluating the optimal deadlock and starvation free subsequent positions of every robot from their current position, optimizing the path distance for every robot. The authentication of the projected hybrid procedure has been confirmed through benchmark function, computer real robot through webbots simulator, and simulation. Further, the efficacy of the projected procedure has been confirmed by equating the result achieved from MQL–IPSO–AOA with Q-learning, AOA, and IPSO, and also equating the result of the projected procedure with state-of- arts.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s00521-022-08078-4,en,Harris hawks optimization for COVID-19 diagnosis based on multi-threshold image segmentation,"['OriginalPaper', 'Original Article']","Digital image processing techniques and algorithms have become a great tool to support medical experts in identifying, studying, diagnosing certain diseases. Image segmentation methods are of the most widely used techniques in this area simplifying image representation and analysis. During the last few decades, many approaches have been proposed for image segmentation, among which multilevel thresholding methods have shown better results than most other methods. Traditional statistical approaches such as the Otsu and the Kapur methods are the standard benchmark algorithms for automatic image thresholding. Such algorithms provide optimal results, yet they suffer from high computational costs when multilevel thresholding is required, which is considered as an optimization matter. In this work, the Harris hawks optimization technique is combined with Otsu’s method to effectively reduce the required computational cost while maintaining optimal outcomes. The proposed approach is tested on a publicly available imaging datasets, including chest images with clinical and genomic correlates, and represents a rural COVID-19-positive (COVID-19-AR) population. According to various performance measures, the proposed approach can achieve a substantial decrease in the computational cost and the time to converge while maintaining a level of quality highly competitive with the Otsu method for the same threshold values.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11042-022-13486-8,en,Image enhancement techniques on deep learning approaches for automated diagnosis of COVID-19 features using CXR images,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","The outbreak of novel coronavirus (COVID-19) disease has infected more than 135.6 million people globally. For its early diagnosis, researchers consider chest X-ray examinations as a standard screening technique in addition to RT-PCR test. Majority of research work till date focused only on application of deep learning approaches that is relevant but lacking in better pre-processing of CXR images. Towards this direction, this study aims to explore cumulative effects of image denoising and enhancement approaches on the performance of deep learning approaches. Regarding pre-processing, suitable methods for X-ray images, Histogram equalization, CLAHE and gamma correction have been tested individually and along with adaptive median filter, median filter, total variation filter and gaussian denoising filters. Proposed study compared eleven combinations in exploration of most coherent approach in greedy manner. For more robust analysis, we compared ten CNN architectures for performance evaluation with and without enhancement approaches. These models are InceptionV3, InceptionResNetV2, MobileNet, MobileNetV2, Vgg19, NASNetMobile, ResNet101, DenseNet121, DenseNet169, DenseNet201. These models are trained in 4-way (COVID-19 pneumonia vs Viral vs Bacterial pneumonia vs Normal) and 3-way classification scenario (COVID-19 vs Pneumonia vs Normal) on two benchmark datasets. The proposed methodology determines with TVF + Gamma, models achieve higher classification accuracy and sensitivity. In 4-way classification MobileNet with TVF + Gamma achieves top accuracy of 93.25% with 1.91% improvement in accuracy score, COVID-19 sensitivity of 98.72% and F1-score of 92.14%. In 3-way classification our DenseNet201 with TVF + Gamma gains accuracy of 91.10% with improvement of 1.47%, COVID-19 sensitivity of 100% and F1-score of 91.09%. Proposed study concludes that deep learning modes with gamma correction and TVF + Gamma has superior performance compared to state-of-the-art models. This not only minimizes overlapping between COVID-19 and virus pneumonia but advantageous in time required to converge best possible results.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10653-022-01203-1,en,"Early warning of algal blooms based on the optimization support vector machine regression in a typical tributary bay of the Three Gorges Reservoir, China","['OriginalPaper', 'Original Paper']","Algal blooms caused by climate change and human activities have received considerable attention in recent years. Since chlorophyll a (Chl-a) can be used as an indicator of phytoplankton biomass, it has been selected as a direct indicator for monitoring and early warning of algal blooms. With the development of artificial intelligence, data-driven approaches with small sample data and high accuracy prediction have been gradually applied to water quality prediction. This study aimed at using environment factors (water quality and meteorological data) to assist the prediction of Chl-a concentration based on the optimization support vector machine (SVM) model. The most relevant environment factors were extracted from the commonly used environment factors according to the method of cosine similarity. The traditional particle swarm optimization (PSO) algorithm was adopted to optimize the ANN and SVM models, respectively. Then, the better prediction model PSO-SVM can be obtained according to the results of three scientific evaluation indicators. The latest optimization algorithm of grey wolf optimizer (GWO) was also proposed to optimize the SVM to realize high-accuracy Chl-a concentration predication. The GWO-SVM model achieved higher accuracy than the other models both in training and validation processes. Therefore, the dimension of the input vector could be reduced with using the cosine similarity method, and the prediction of Chl-a concentration in high accuracy and the early warning of algal blooms in the study area of this paper could also achieved.","['Environment', 'Environmental Health', 'Geochemistry', 'Terrestrial Pollution', 'Soil Science & Conservation', 'Environmental Chemistry', 'Public Health']"
doi:10.1007/s00500-022-07436-7,en,A lion optimization algorithm for an integrating maintenance planning and production scheduling problem with a total absolute deviation of completion times objective,"['OriginalPaper', 'Application of soft computing']","In today’s competitive business environment, the need for continuous production, quality improvement, and fast delivery necessitates highly reliable production and delivery processes. A more reliable system can be ensured by performing routine maintenance on the equipment. Maintenance, on the other hand, causes a temporary reduction in production capacity. To ensure a high level of system performance, it is essential to coordinate maintenance and production. The study integrates maintenance and production decisions in order to maximise efficiency by ensuring high-quality output and efficient resource utilisation; however, limited studies have been carried out addressing this type of scheduling problem with the objective function of total absolute deviation of completion times (TADC). Thus, this study aims to investigate the scheduling problem on a parallel machine under periodic maintenance in order to minimise the TADC of the jobs. Due to the complexity of the problem, a metaheuristic method called the lion optimization algorithm (LOA) is presented to solve the problem. This study performs a comprehensive comparative analysis to demonstrate the proposed algorithm’s reliability. The dragonfly algorithm, the grasshopper optimization algorithm, the multi-verse optimization algorithm, the sine cosine algorithm, the salp swarm algorithm, and the whale optimization algorithm are presented and their performance is compared to the LOA in this study The results demonstrate that the LOA consistently outperforms the other presented algorithms across all size ranges. The study’s findings contribute new theoretical and practical insights to the growing body of knowledge about manufacturing environments and have implications for planners and managers, particularly in businesses where unplanned production wastes financial resources.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11227-022-04634-w,en,An efficient DBSCAN optimized by arithmetic optimization algorithm with opposition-based learning,OriginalPaper,"As unsupervised learning algorithm, clustering algorithm is widely used in data processing field. Density-based spatial clustering of applications with noise algorithm (DBSCAN), as a common unsupervised learning algorithm, can achieve clusters via finding high-density areas separated by low-density areas based on cluster density. Different from other clustering methods, DBSCAN can work well for any shape clusters in the spatial database and can effectively cluster exceptional data. However, in the employment of DBSCAN, the parameters, EPS and MinPts, need to be preset for different clustering object, which greatly influences the performance of the DBSCAN. To achieve automatic optimization of parameters and improve the performance of DBSCAN, we proposed an improved DBSCAN optimized by arithmetic optimization algorithm (AOA) with opposition-based learning (OBL) named OBLAOA-DBSCAN. In details, the reverse search capability of OBL is added to AOA for obtaining proper parameters for DBSCAN, to achieve adaptive parameter optimization. In addition, our proposed OBLAOA optimizer is compared with standard AOA and several latest meta heuristic algorithms based on 8 benchmark functions from CEC2021, which validates the exploration improvement of OBL. To validate the clustering performance of the OBLAOA-DBSCAN, 5 classical clustering methods with 10 real datasets are chosen as the compare models according to the computational cost and accuracy. Based on the experimental results, we can obtain two conclusions: (1) the proposed OBLAOA-DBSCAN can provide highly accurately clusters more efficiently; and (2) the OBLAOA can significantly improve the exploration ability, which can provide better optimal parameters.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s11042-020-09988-y,en,Antlion re-sampling based deep neural network model for classification of imbalanced multimodal stroke dataset,OriginalPaper,"Stroke is enlisted as one of the leading causes of death and serious disability affecting millions of human lives across the world with high possibilities of becoming an epidemic in the next few decades. Timely detection and prompt decision making pertinent to this disease, plays a major role which can reduce chances of brain death, paralysis and other resultant outcomes. Machine learning algorithms have been a popular choice for the diagnosis, analysis and predication of this disease but there exists issues related to data quality as they are collected cross-institutional resources. The present study focuses on improving the quality of stroke data implementing a rigorous pre-processing technique. The present study uses a multimodal stroke dataset available in the publicly available Kaggle repository. The missing values in this dataset are replaced with attribute means and LabelEncoder technique is applied to achieve homogeneity. However the dataset considered was observed to be imbalanced which reflect that the results may not represent the actual accuracy and would be biased. In order to overcome this imbalance, resampling technique was used. In case of oversampling, some data points in the minority class are replicated to increase the cardinality value and rebalance the dataset. transformed and oversampled data is further normalized using Standardscalar technique. Antlion optimization (ALO) algorithm is implemented on the deep neural network (DNN) model to select optimal hyperparameters in minimal time consumption. The proposed model consumed only 38.13% of the training time which was also a positive aspect. The experimental results proved the superiority of proposed model.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10462-022-10158-9,en,An intelligent management of power flow in the smart grid system using hybrid NPO-ATLA approach,OriginalPaper,"In this manuscript, an intelligent hybrid approach is proposed to manage the power flow (PF) in the smart grid (SG) system. The proposed approach is the combined execution of Nomadic People Optimizer (NPO) algorithm and artificial transgender longicorn algorithm (ATLA), hence it is named NPO-ATLA approach. The Renewable energy system consists of photovoltaic (PV), wind turbine (WT), battery and grid. The major aim of this work is to control the power flow in the hybrid renewable energy sources (HRES) depending on parameter variation of source and load side and satisfies the load demand of the system. The voltage source inverter (VSI) control signals are generated through the NPO approach based upon the variation of power transfer amid the source and load side. ATLA is utilized to recognize the control signals of the system against the variation of active with reactive power. The proposed approach is carried out in MATLAB, then the performance is compared with various existing approaches.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11042-022-13260-w,en,An improved deep convolutional neural network by using hybrid optimization algorithms to detect and classify brain tumor using augmented MRI images,OriginalPaper,"Automated brain tumor detection is becoming a highly considerable medical diagnosis research. In recent medical diagnoses, detection and classification are highly considered to employ machine learning and deep learning techniques. Nevertheless, the accuracy and performance of current models need to be improved for suitable treatments. In this paper, an improvement in deep convolutional learning is ensured by adopting enhanced optimization algorithms, Thus, Deep Convolutional Neural Network (DCNN) based on improved Harris Hawks Optimization (HHO), called G-HHO has been considered. This hybridization features Grey Wolf Optimization (GWO) and HHO to give better results, limiting the convergence rate and enhancing performance. Moreover, Otsu thresholding is adopted to segment the tumor portion that emphasizes brain tumor detection. Experimental studies are conducted to validate the performance of the suggested method on a total number of 2073 augmented MRI images. The technique’s performance was ensured by comparing it with the nine existing algorithms on huge augmented MRI images in terms of accuracy, precision, recall, f-measure, execution time, and memory usage. The performance comparison shows that the DCNN-G-HHO is much more successful than existing methods, especially on a scoring accuracy of 97%. Additionally, the statistical performance analysis indicates that the suggested approach is faster and utilizes less memory at identifying and categorizing brain tumor cancers on the MR images. The implementation of this validation is conducted on the Python platform. The relevant codes for the proposed approach are available at: https://github.com/bryarahassan/DCNN-G-HHO .","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00366-022-01633-6,en,Analysis of three-dimensional potential problems in non-homogeneous media with physics-informed deep collocation method using material transfer learning and sensitivity analysis,"['OriginalPaper', 'Original Article']","In this work, we present a deep collocation method (DCM) for three-dimensional potential problems in non-homogeneous media. This approach utilizes a physics-informed neural network with material transfer learning reducing the solution of the non-homogeneous partial differential equations to an optimization problem. We tested different configurations of the physics-informed neural network including smooth activation functions, sampling methods for collocation points generation and combined optimizers. A material transfer learning technique is utilized for non-homogeneous media with different material gradations and parameters, which enhance the generality and robustness of the proposed method. In order to identify the most influential parameters of the network configuration, we carried out a global sensitivity analysis. Finally, we provide a convergence proof of our DCM. The approach is validated through several benchmark problems, also testing different material variations.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11517-022-02690-1,en,MsRAN: a multi-scale residual attention network for multi-model image fusion,"['OriginalPaper', 'Original Article']","Fusion is a critical step in image processing tasks. Recently, deep learning networks have been considerably applied in information fusion. But the significant limitation of existing image fusion methods is the inability to highlight typical regions of the source image and retain sufficient useful information. To address the problem, the paper proposes a multi-scale residual attention network (MsRAN) to fully exploit the image feature. Its generator network contains two information refinement networks and one information integration network. The information refinement network extracts feature at different scales using convolution kernels of different sizes. The information integration network, with a merging block and an attention block added, prevents the underutilization of information in the intermediate layers and forces the generator to focus on salient regions in multi-modal source images. Furthermore, in the phase of model training, we add an information loss function and adopt a dual adversarial structure, enabling the model to capture more details. Qualitative and quantitative experiments on publicly available datasets validate that the proposed method provides better visual results than other methods and retains more detail information. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s40031-022-00811-w,en,Assessment of Optimal Size and Location of DG/CB in Distribution Systems using Coulomb–Franklin’s Algorithm,"['OriginalPaper', 'Original Contribution']","In this paper, an efficient Coulomb–Franklin’s algorithm (CFA) has been applied for analyzing the best-suited location and the size of the distributed generator (DG) and capacitor bank (CB) units in the radial distribution system (RDS). CFA is a novel metaheuristic approach based on the mathematical model of Franklin and Coulomb's laws. To minimize real power loss, CFA is applied and tested on four standard RDSs, which are IEEE 15, IEEE 33, IEEE 85, and IEEE 118 bus RDS. Through simulation, the optimal size and location of DG and CB units are determined which shows reduction in real power loss and improvement of the voltage profile in all the cases. The comparison of results with other methods shows the efficacy of CFA over the others.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s10586-022-03622-2,en,Detecting malicious transactions in database using hybrid metaheuristic clustering and frequent sequential pattern mining,OriginalPaper,"Database systems have become imperative for organisations around the world to store and analyse information. However, as one of the ramifications of a massive surge in cloud-based activities and interactions brought forth by the advent of the internet era, the data is exposed to an audience broader than ever and there are a variety of new challenges putting database security in jeopardy. To be able to address the problem of data security, we propose a unique method for Database Intrusion Detection System build on frequent sequential pattern mining and a modified metaheuristic hybrid clustering of Grey Wolf and Whale optimization algorithm to determine malicious transactions in Role Based Access Control and non-RBAC supervised databases. Our proposed approach extracts data dependency rules from the database logs using CM-SPADE mining algorithm to detect outsider threats. It then assigns role profiles to the users based on the previous user activities using the modified metaheuristic clustering to detect insider threats. Thereby, identifying incoming transactions as malicious by matching the role profile of the user and comparing the adherence of the transaction pattern to the generated dependency rules. To evaluate the efficiency of the model we generated a synthetic dataset including malicious and non-malicious transactions adhering to the TPC-C benchmark, and the findings were encouraging, with levels of accuracy of around 97.8 percent.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00366-021-01433-4,en,An intelligent computer method for vibration responses of the spinning multi-layer symmetric nanosystem using multi-physics modeling,"['OriginalPaper', 'Original Article']","This article is the first attempt to employ deep learning to estimate the frequency performance of the rotating multi-layer nanodisks. The optimum values of the parameters involved in the mechanism of the fully connected neural network are determined through the momentum-based optimizer. The strength of the method applied in this survey comes from the high accuracy besides lower epochs needed to train the multi-layered network. It should be mentioned that the current nanostructure is modeled as a nanodisk on the viscoelastic substrate. Due to rotation, the centrifugal and Coriolis effects are considered. Hamilton’s principle and generalized differential quadrature method (GDQM) are presented for obtaining and solving the governing equations of the high-speed rotating nanodisk on a viscoelastic substrate. The outcomes show that the number of layers viscoelastic foundation, angular velocity speed, angle of ply, nonlocal, and length-scale parameters have a considerable impact on the amplitude and vibration behavior of a laminated rotating cantilevered nanodisk. As an applicable result in related industries, in the initial value of radius ratio, damping of the foundation does not have any effect on the dynamics of the system, but when the outer radius is bigger enough, the effect of damping parameter on the frequency of the laminated nanostructure will be bold sharply.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01312-y,en,A comparative study of different dynamic condensation techniques applied to multi-damage identification of FGM and FG-CNTRC plates,"['OriginalPaper', 'Original Article']","The dynamic condensation method has been recognized as an effective alternative for structural damage identification using spatially-incomplete modal measurements. However, comparative studies of different dynamic condensation techniques applied to the subject of structural damage identification have been scarcely found, especially for composite structures. In this regard, we conduct a comparative study of six typical dynamic condensation techniques utilized for addressing damage identification problems of composite plates made of functionally graded materials (FGM) and functionally graded carbon nanotube-reinforced composite (FG-CNTRC) materials. Firstly, the six techniques consisting of Guyan’s method, Kidder’s method, Neumann series expansion-based second-order model reduction (NSEMR-II) method, improved reduced system (IRS) method, iterated IRS (IIRS) method, and iterative order reduction (IOR) method are reviewed. Then, their performance for reduced Eigen and optimization-damage identification problems are evaluated by studying two numerical examples of FGM plate and FG-CNTRC plate. For solving the optimization-damage identification problem of plate structures, the article proposes to use a hybrid global–local algorithm, Manta Ray Foraging Optimization—Sequential Quadratic Programming (MRFO-SQP), where the MRFO algorithm is utilized for global exploration and the SQP algorithm is used for the local searching process. The comparative study indicates that the IOR technique is the best dynamic condensation technique and is effective for addressing the structural damage identification problems when comparing with the other five techniques. It is also found that the damage identification approach based on the hybrid MRFO–SQP algorithm combined with the IOR technique can archive the high accuracy and low computational cost for damage localization and quantification.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11042-022-14098-y,en,AlexSegNet: an accurate nuclei segmentation deep learning model in microscopic images for diagnosis of cancer,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","The nuclei segmentation of microscopic images is a key pre-requisite for cancerous pathological image analysis. However, an accurate nuclei cell segmentation is a long running major challenge due to the enormous color variability of staining, nuclei shapes, sizes, and clustering of overlapping cells. To address this challenges, we proposed a deep learning model, namely, AlexSegNet which is based upon AlexNet model Encoder-Decoder framework. In Encoder part, it stitches feature maps in the channel dimension to achieve feature fusion and uses a skip structure in Decoder part to combine low- and high-level features to ensure the segmentation effect of the nucleus. At final stage, we have also introduced a stacked network where feature maps are stacks on top of each other. We have used a publically available 2018 Data Science Bowl and Triple Negative Breast Cancer (TNBC) datasets of microscopic nuclei images for this study which comprises of several sample types such as small and large fluorescent, pink, purple, and grayscale tissue samples. Experimental results show that our proposed AlexSegNet achieved a segmentation maximum performance of 91.66% for Data Science Bowl dataset and 66.88% for TNBC dataset. The results are competitive compared to the results of other state-of-the-art models. This model is expected to be useful clinically for technician experts to succeed the analysis of cancer diagnosis into the survival chances of patients.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00366-021-01572-8,en,A hybrid TLNNABC algorithm for reliability optimization and engineering design problems,"['OriginalPaper', 'Original Article']","The paper aims to present a new TLNNABC hybrid algorithm to solve reliability and engineering design optimization problems. In this algorithm, the structure of the artificial bee colony (ABC) algorithm has been improved by incorporating the features of the neural network algorithm (NNA) and teaching-learning based optimization (TLBO). In the standard ABC, the onlooker bees apply the same searching method as the employed bees, which causes slow convergence and also restricts its practical application of solving optimization problems. In view of this inadequacy and resulting in a better balance between exploration and exploitation, searching procedures for employed bees and onlooker bees of the conventional ABC are renovated based on NNA and improved TLBO algorithms respectively and a new hybrid algorithm called TLNNABC has been developed in this paper. In TLNNABC, for the employed bee phase, NNA is used to increase the population diversity. However, the improved teaching learning-based optimization is embedded in the onlooker bee phase. In this context, a new search operator is introduced which increases the exploitation capability of the algorithm to operate, and a probabilistic selection strategy, which helps to determine whether to apply the original or the new search operator to construct a new solution. Finally, the performance of the proposed TLNNABC algorithm has been demonstrated by the well-known benchmark problems related to reliability optimization, structural engineering design problems, and 23 unconstrained benchmark functions and finally compared with several existing algorithms. Experimental results show that the proposed algorithm is very effective and achieves superior performance than the other algorithms.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s13198-022-01790-3,en,Remote sensing scene classification using visual geometry group 19 model and multi objective grasshopper optimization algorithm,"['OriginalPaper', 'Original Article']","Recently, the Remote Sensing Scene Classification (RSSC) has played a vital role in several applications: environment monitoring, urban planning, and land management. The deep neural networks are extensively utilized in the RSSC, because of their superior performance. In recent decades, several scene classification models improve classification accuracy by incorporating extra modules, but it increases the computing overhead and parameters of the models at the inference phase. In addition, the complementarity of the features extracted by the deep learning models is exploited to reduce the improvement of classification accuracy. For addressing the aforementioned issues, a new meta-heuristics based Visual Geometry Group-19 (VGG-19) model is implemented in this research manuscript. After acquiring the aerial images from REmote Sensing Image Scene Classification 45 (RESISC45), Aerial Image Dataset (AID) and the University of California Merced (UC Merced) datasets, the VGG-19 network is applied for classifying the scene categories. In the proposed system, a multi-objective Grasshopper Optimization Algorithm (GOA) is implemented for selecting the optimal hyper-parameters of the VGG-19 model, which helps in reducing the computational complexity and training time of the model. The experimental results demonstrated that the meta-heuristics based VGG-19 model achieved 98.67%, 99.57%, and 98.06% of accuracy on the AID, UC Merced, and RESISC45 datasets, which are superior related to the comparative deep learning models.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s11045-022-00846-8,en,Discrete cosine transform interpolation based design of two-dimensional FIR fractional order digital differentiator,OriginalPaper,"In this paper a two-dimensional (2-D) DCT interpolation based method for the designing of a 2-D fractional order digital differentiator (FODD) is presented. The modeling of the FODD is achieved in the form of a finite impulse response (FIR) filter. Here, Grun-wald Letnikov partial fractional derivative of two variable function with discrete cosine transform (DCT) interpolation is used to estimate the impulse response of an ideal 2-D FODD. Here, 2-D DCT-II and DCT-III methods are employed to evaluate the optimal values of coefficients of the 2-D fractional order differentiator. Simulation results demonstrate that the proposed method surpasses the existing method in terms of integral square magnitude error (ISME). The simulated results reflect that the improved response gives a much reduced error of 0.0404 and 0.0165 using 2-D DCT-II and DCT-III methods respectively.The proposed 2-D FODD is applied on an image for edge detection to demonstrate the effectiveness of the method.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Artificial Intelligence']"
doi:10.1007/s00202-022-01599-0,en,Multi-objective chance-constrained transmission congestion management through optimal allocation of energy storage systems and TCSC devices,"['OriginalPaper', 'Original Paper']","In this paper, a multi-objective mixed-integer nonlinear programming model for transmission congestion management through optimal placement and sizing of thyristor-controlled series capacitor (TCSC) devices and electrical energy storages (EESs) is presented. This problem is modeled as a two-objective optimization problem, where objective functions are maximizing social welfare and minimizing flow-gate marginal price index. The proposed model is implemented on 30-bus and 118-bus transmission systems and includes thermal units and wind farms. In order to model the uncertainties of load demand and wind speed, the chance-constrained method has been utilized. Also, in order to solve the proposed model, a modified gray wolf optimizer algorithm has been introduced, where the results demonstrate that the proposed algorithm compared to the other three algorithms not only reduced the computational time, but also achieved more optimal results. In addition, the results illustrate that the optimal placement of TCSCs and EESs has led to a 57.97% reduction in congestion surplus. The results also demonstrate that the implementation of demand response program by increasing the flexibility of the system leads to a smoother local marginal price curve in the network and thus reduces the congestion surplus by 8.71%.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s11063-022-10830-9,en,COLAM: Co-Learning of Deep Neural Networks and Soft Labels via Alternating Minimization,OriginalPaper,"Softening labels of training datasets with respect to data representations has been frequently used to improve the training of deep neural networks. While such a practice has been studied as a way to leverage “privileged information” about the distribution of the data, a well-trained learner with soft classification outputs should be first obtained as a prior to generate such privileged information. To solve such a “chicken-and-egg” problem, we propose COLAM framework that Co-Learns DNNs and soft labels through Alternating Minimization of two objectives—(a) the training loss subject to soft labels and (b) the objective to learn improved soft labels—in one end-to-end training procedure. We performed extensive experiments to compare our proposed method with a series of baselines. The experiment results show that COLAM achieves improved performance on many tasks with better testing classification accuracy. We also provide both qualitative and quantitative analyses that explain why COLAM works well.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s10586-022-03649-5,en,A mixed sine cosine butterfly optimization algorithm for global optimization and its application,OriginalPaper,"This paper proposes a hybrid sine cosine butterfly optimization algorithm (m-SCBOA), in which a modified butterfly optimization algorithm is combined with sine cosine algorithm to achieve superior exploratory and exploitative search capabilities. The newly suggested m-SCBOA algorithm has been tested on 39 benchmark functions and compared with seven state-of-the-art algorithms. Moreover, it is tested with the CEC 2017 test suite, and the results are compared with seven more algorithms. Experimental results have demonstrated superior results of the proposed algorithm in nearly 75% of occasions on average whereas, similar results in nearly 20% of cases. Simulation results and convergence graphs show the supremacy of the m-SCBOA over the compared algorithms. The Friedman rank test validates that the m-SCBOA is statistically the best among the experiments. Additionally, four real-world engineering design problems and a priori multi-objective problem called parameter optimization of Al–4.5% Cu–TiC metal matrix composite are solved and the outcomes are compared to a wide range of algorithms. The results of these case studies establish the merits of m-SCBOA in solving challenging, real-world problems with unknown global optima.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s13369-022-07480-3,en,Integration of ANN and NSGA-II for Optimization of Nusselt Number and Pressure Drop in a Coiled Heat Exchanger via Water-Based Nanofluid Containing Alumina and Ag Nanoparticles,"['OriginalPaper', 'Research Article-chemical Engineering ']","In this research study, minimizing the pressure drop and maximizing the Nusselt number of water-based nanofluid containing alumina and Ag in a coiled heat exchanger have been considered. The artificial neural network has been implemented to predict nanofluid’s pressure drop as well as Nusselt number as a function of nanoparticles volume fraction and Reynolds number. The obtained correlation was found properly accurate as most of the values were accumulated around the line. The value of R, mean error, and STD for the Nusselt number as well as pressure drop were found (0.9996 and 0.9993), (0.007 and − 6.4*10 –5 ), and (0.172 and 0.157), respectively. The obtained correlations using ANN are used as input into the NSGA-II optimization algorithm to obtain the optimum pressure drop as well as Nusselt number as a function of nanoparticles volume fraction and Reynolds number. The optimum values were determined in the range of Nusselt number of 8–24 and pressure drop of 2–16 Pa. The optimal point can be chosen according to the process requirement. For the lowest and highest optimum pressures (8.566 and 23.691 Pa), the optimum Nusselt numbers were 2.502 and 115.35, respectively.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s00366-021-01393-9,en,"Performance evaluation of hybrid WOA-XGBoost, GWO-XGBoost and BO-XGBoost models to predict blast-induced ground vibration","['OriginalPaper', 'Original Article']","Accurate prediction of ground vibration caused by blasting has always been a significant issue in the mining industry. Ground vibration caused by blasting is a harmful phenomenon to nearby buildings and should be prevented. In this regard, a new intelligent method for predicting peak particle velocity (PPV) induced by blasting had been developed. Accordingly, 150 sets of data composed of thirteen uncontrollable and controllable indicators are selected as input dependent variables, and the measured PPV is used as the output target for characterizing blast-induced ground vibration. Also, in order to enhance its predictive accuracy, the gray wolf optimization (GWO), whale optimization algorithm (WOA) and Bayesian optimization algorithm (BO) are applied to fine-tune the hyper-parameters of the extreme gradient boosting (XGBoost) model. According to the root mean squared error (RMSE), determination coefficient ( R 2 ), the variance accounted for (VAF), and mean absolute error (MAE), the hybrid models GWO-XGBoost, WOA-XGBoost, and BO-XGBoost were verified. Additionally, XGBoost, CatBoost (CatB), Random Forest, and gradient boosting regression (GBR) were also considered and used to compare the multiple hybrid-XGBoost models that have been developed. The values of RMSE, R 2 , VAF, and MAE obtained from WOA-XGBoost, GWO-XGBoost, and BO-XGBoost models were equal to (3.0538, 0.9757, 97.68, 2.5032), (3.0954, 0.9751, 97.62, 2.5189), and (3.2409, 0.9727, 97.65, 2.5867), respectively. Findings reveal that compared with other machine learning models, the proposed WOA-XGBoost became the most reliable model. These three optimized hybrid models are superior to the GBR model, CatB model, Random Forest model, and the XGBoost model, confirming the ability of the meta-heuristic algorithm to enhance the performance of the PPV model, which can be helpful for mine planners and engineers using advanced supervised machine learning with metaheuristic algorithms for predicting ground vibration caused by explosions.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11600-022-00875-8,en,Traffic flow prediction in inland waterways of Assam region using uncertain spatiotemporal correlative features,"['OriginalPaper', 'Research Article - Special Issue']","Modern civilization has reported a significant rise in the volume of traffic on inland rivers all over the globe. Traffic flow prediction is essential for a good travel experience, but adequate computer processes for processing unpredictable spatiotemporal data (timestamp, weather, vessel_ID, water level, vessel_position, vessel_speed) in the inland water transportation industry are lacking. Moreover, such type of prediction relies primarily on past traffic patterns and perhaps other pertinent facts. Thus, we propose a deep learning-based computing process, namely Convolution Neural Network-Long Short-Term Memory Network (CNN-LSTM), a progressive predictor of employing uncertain spatiotemporal information to decrease navigation mishaps, traffic and flow prediction failures during transportation. Spatiotemporal correlation of current traffic flow may be processed using a simplified CNN-LSTM model. This hybridized prediction technique decreases update costs and meets the prediction needs with minimal computing overhead. A short case study on the waterways of the Indian state of Assam from Sandiya (27.835090 latitude, 95.658590 longitude) to Dhubri (26.022699 latitude, 89.978401 longitude) is undertaken to assess the model's performance. The evaluation of the suggested method includes a variety of trajectories of water transportation vehicles, including ferries, sailing boats, container ships, etc. The suggested approach outperforms conventional traffic flow predicting methods when it comes to short-term prediction with minimal predictive error (< 2.75) and exhibited a major difference of more than 45% on the comparison of other methods.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s40095-022-00488-3,en,"Near-term, national solar capacity factor forecasts aided by trend attributes and artificial intelligence","['OriginalPaper', 'Original Research']","An attribute technique is applied to forecast countrywide solar capacity. Attributes relate to the prior 12 h of a univariate, hourly time series. The approach avoids uncertainties relating to weather-related variables averaged at the country level. It captures impacts of system curtailments due to abnormal market conditions or grid-offtake limitations. Fifteen attributes relating to each hourly record are input to machine/deep learning (ML/DL) models. 43,824 h of solar capacity factor for Britain from 2015 to 2019 is evaluated. Fifteen ML/DL models are trained with 2015–2018 data with cross-validation. Trained models are then applied to forecast unseen 2019 hourly data. The ML/DL model forecast accuracy is compared with that of ARIMA and regression models. Extreme gradient boosting, random forest and adaptive boosting models outperform ARIMA and regression methods in forecasts for hours t 0 to t  + 12. Those three ML models are more accurate and faster to execute than six DL models evaluated. Suboptimal convergence and/or overfitting hinder the forecasts of DL models with unseen data. A transparent multi-linear regression model is used to identifying attribute influences on the different time period forecasts. The trend attributes are shown to influence the forecasts for different hours ahead in distinct ways.","['Energy', 'Renewable and Green Energy']"
doi:10.1007/s40860-021-00159-w,en,A novel energy-efficient clustering protocol in wireless sensor network: multi-objective analysis based on hybrid meta-heuristic algorithm,"['OriginalPaper', 'Original Article']","Energy efficiency is one of the major challenges in the growing WSNs. Since communication offers a vast place in the consumption of energy, effective routing is the best solution to handle this problem. The lifetime improvement is an important problem since the majority of the WSNs function in an unattended environment, in which monitoring, as well as human access, is not possible in a practical manner. Clustering is one of the powerful approaches, which arranges the system operation for the enhanced lifetime of the network, improves energy efficiency, reduces the consumption of energy, and also attend the scalability of the network. To handle this issue, the present researchers have considered the usage of various clustering algorithms. Yet, the cluster head is burdened by the majority of the suggested algorithms in the process of cluster formation. To handle this problem, this paper plans to develop the energy-efficient clustering for WSN using the improved LEACH protocol. Here, the concept of a hybrid meta-heuristic algorithm is used for the optimal cluster head selection through energy-efficient clustering. The optimal solutions are rated based on the multi-objective function considering the objective constraints like energy, distance, delay, quality of service (QoS), load, and time of death. Communication between the sink node and cluster head uses the distance of separation as a parameter for reducing energy consumption. Two well-performing algorithms, like salp swarm algorithm (SSA) and grasshopper optimization algorithm (GOA) are merged to develop the proposed hybrid algorithm called salp-swarm grasshopper optimization (SS-GO). From the results, for 200 nodes, the normalized energy of SS-GO at 1400th round is 5.41%, 11.43%, 14.71%, and 25.81%, superior to GOA, SSO, O-EHO, and FU-CSA, respectively . Here, the performance of the proposed SS-GO is also higher in the other distance, delay, time of death node, and QOS. The performance of the introduced hybrid algorithm-based LEACH is evaluated in several different scenarios, and it is shown that the proposed protocol improves network lifetime in comparison to a number of the recent similar protocol.","['Computer Science', 'Performance and Reliability', 'Software Engineering/Programming and Operating Systems', 'Artificial Intelligence', 'Simulation and Modeling', 'User Interfaces and Human Computer Interaction', 'Health Informatics']"
doi:10.1007/s00521-022-07605-7,en,A new cloud autonomous system as a service for multi-mobile robots,"['OriginalPaper', 'Original Article']","Today, mobile robot is used in most industrial and commercial fields. It can improve and carry out work complex tasks quickly and efficiently. However, using swarm robots to execute some tasks requires a complex system for assigning robots to these tasks. The main issue in the robot control systems is the limited facilities of robot embedded system components. Although, some researchers used cloud computing to develop robot services. They didn’t use the cloud for solving robot control issues. In this paper, we have used cloud computing for controlling robots to solve the problem of limited robot processing components. The main advantage of using cloud computing is its intensive computing power. This advantage motivates us to propose a new autonomous system for multi-mobile robots as a services-based cloud computing. The proposed system consists of three phases: clustering phase, allocation phase, and path planning phase. It groups all tasks/duties into clusters using the k-means algorithm. After that, it finds the optimal path for each robot to execute its duties in the cluster based on the Nearest neighbor and Harris Hawks Optimizer (HHO). The proposed system is compared with systems that use a genetic algorithm, simulated annealing algorithm, and HHO algorithm. From the finding, we find that the proposed system is more efficient than the other systems in terms of decision time, throughput, and the total distance of each robot.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10825-022-01928-3,en,Kapur’s entropy based honey badger optimization for design of miniaturized frequency selective surface for 5G electromagnetic shielding,OriginalPaper,"Electromagnetic interference (EMI) is the main cause for the malfunctioning of real-time applications such as antenna technology, military application, and mobile technology. The EMIs are caused by microwave radiation, and it is ineluctable to provide shielding against the radiation. Opacity and transparency along with lower weight can be achieved at the chosen frequency for some array of periodic elements. Hence we have proposed complementary frequency selective surfaces with the inclusion of a square loop pattern. The square loop pattern provides better stability and hence we have adopted it for our design. The performances are analyzed with the equivalent circuit (EC) incorporated with the transmission line model. The desired resonance frequency is obtained by using Kapur’s entropy criterion. Then the shielding effectiveness is obtained by optimizing the EC with the honey badger optimization algorithm. This will ensure the desired shielding features. For the normal and oblique incidence angle, our proposed approach produces a stable response along with two controlled passbands. The investigations are made using full-wave simulations.","['Engineering', 'Mathematical and Computational Engineering', 'Electrical Engineering', 'Theoretical, Mathematical and Computational Physics', 'Optical and Electronic Materials', 'Mechanical Engineering']"
doi:10.1007/s10586-022-03633-z,en,An improved bacterial colony optimization using opposition-based learning for data clustering,OriginalPaper,"Data clustering is a technique for dividing data objects into groups based on their similarity. K-means is a simple, effective algorithm for clustering. But, k-means tends to converge to local optima and depends on the cluster’s initial values. To address the shortcomings of the k-means algorithm, many nature-inspired techniques have been used. This paper is offered an improved version of bacterial colony optimization (BCO) based on opposition-based learning (OBL) algorithm called OBL + BCO for data clustering. An OBL is used to increase the speed of the convergence rate and searching ability of BCO by computing the opposite solution to the present solution. The strength of the proposed data clustering technique is evaluated using several well-known UCI benchmark datasets. Different performance measures are considered to analyze the strength of the proposed OBL + BCO such as Rand index, Jaccard index, Beta index, Distance index, Objective values, and computational time. The experimental results demonstrated that the proposed OBL + BCO data clustering technique outperformed other data clustering techniques.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00500-022-07510-0,en,Adaptive niching selection-based differential evolution for global optimization,"['OriginalPaper', 'Optimization']","Niching techniques have been widely incorporated into differential evolution (DE) to improve the diversity of the search. Existing niching-based DE schemes, however, typically employ a certain niching technique during the entire evolution. Since different niching techniques possess different diversity preserving properties, employing a fixed niching technique during DE evolution may have limited performance. In this paper, we propose an adaptive niching selection-based DE for global optimization problems. In the proposed method, instead of employing a certain fixed niching technique, an adaptive niching selection scheme has been devised. In this scheme, multiple niching techniques are employed and adaptively used during the DE evolution, thus properly preserving the population diversity. Further, to appropriately facilitate the adaptive niching selection, both the fitness improvement and entropy of population resulting from niching techniques have been considered to measure their effectiveness. The performance of the proposed method has been evaluated on multi-modal and hybrid composition test functions and compared with related methods. The results show that our proposed method can deliver a satisfying performance and is competitive with state-of-the-art algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s13198-022-01802-2,en,A novel brainstorm based optimization method for optimum planning of reactive power with FACTS devices,"['OriginalPaper', 'Original Article']","This paper presents a solution for planning of reactive power using a novel brainstorm optimization method with flexible alternating current transmission system (FACTS) devices. For shunt compensation static VAR compensator (SVC) and for series compensation thyristor-controlled series compensator (TCSC) are being used in this proposed work which is adapted into IEEE-14, IEEE-30 and IEEE-57 test bus system. The location for TCSC and SVC has been chosen by performing power flow analysis. In this paper brainstorm optimization algorithm (BSOA) is proposed to evaluate the optimal value of reactive VAR generation of generators, tap setting of transformers, and size of SVC and TCSC to reduce the active power loss as well as operating cost in the transmission network. The outcomes proposed by BSOA approach is obtained and compared with some other techniques like krill heard algorithm (KHA), symbiotic organisms search (SOS), particle swarm optimization (PSO), biogeography-based optimization (BBO) methods.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1038/s41598-022-25208-z,en,Hybrid the long short-term memory with whale optimization algorithm and variational mode decomposition for monthly evapotranspiration estimation,"['OriginalPaper', 'Article']","The sustainability of artificial sand-binding vegetation is determined by the water balance between evapotranspiration (ET) and precipitation in desert regions. Consequently, accurately estimating ET is a critical prerequisite for determing the types and spatial distribution of artificial vegetation in different sandy areas. For this purpose, a novel hybrid estimation model was proposed to estimate monthly ET by coupling the deep learning long short term memory (LSTM) with variational mode decomposition (VMD) and whale optimization algorithm (WOA) (i.e., VMD-WOA-LSTM) to estimate the monthly ET in the southeast margins of Tengger Desert. The superiority of LSTM was selected due to its capability of automatically extracting the nonlinear and nonstationary features from sequential data, WOA was employed to optimize the hyperparameters of LSTM, and VMD was used to extract the intrinsic traits of ET time series. The estimating results of VMD-WOA-LSTM has been compared with actual ET and estimation of other hybrid models in terms of standard performance metrics. The results reveale that VMD-WOA-LSTM provide more accurate and reliable estimating results than that of LSTM, the support vector machine (SVM), and the variants of those models. Therefore, VMD-WOA-LSTM could be recommended as an essential auxiliary method to estimate ET in desert regions.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40031-022-00806-7,en,"Water Quality Prediction System Based on Adam Optimised LSTM Neural Network for Aquaculture: A Case Study in Kerala, India","['OriginalPaper', 'Original Contribution']","Accurate water quality prediction (WQP) and assessment have a significant role in making aquaculture production profitable and sustainable. The water quality (WQ) parameters in aquaculture undergo dynamic changes, generally nonlinear and complex. The conventional prediction mechanisms show insufficient and poor accuracy with high computation time. This research work proposes Adam optimized long short-term memory (LSTM) deep learning neural network-based WQP system for aquaculture. The WQ data collected from the aqua-ponds located in Kerala, India, from January 2016 to January 2019 are utilized for training and testing the proposed LSTM-based prediction model. The proposed LSTM model results show that predicted and actual values accurately match and outperform the autoregressive integrated moving average model in terms of prediction accuracy. The results show the viability and effectiveness of utilizing LSTM to accurately predict the aquaculture WQ parameters.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s40996-022-00893-y,en,Application of Random Forest and Multi-layer Perceptron ANNS in Estimating the Axial Compression Capacity of Concrete-Filled Steel Tubes,"['OriginalPaper', 'Research Paper']","The aim of the research is to develop a deep learning-based artificial neural network model in order to predict the axial compression capacity of concrete-filled steel tubular (CFST) column. Examining the behaviour of CFST columns, especially the axial compression (ACC) is crucial, where core concrete and steel tubular relationship is restricted and complex in CFST. Further, the association of material properties with geometric and axial compression is estimated as highly nonlinear. Henceforth, to ensure safety in operations of structures in engineering, limitations/restrictions via soft-computing had been adopted by the researchers to predict the ‘Nu’ (i.e. ultimate-bearing capacity). Three thousand one hundred and three datasets have been taken from thorough examination of 173 literature as sources, and the data have been used for training, testing, evaluating and validating the proposed ANN with random forest and linear regression-based hybrid model. To ascertain the best model of ANN, trial and error techniques were utilized with the lowest mean-square error value (MSE) and the highest correlation coefficient value ( R ). The comparative outcomes of the research revealed that MLP regressor with ANN model is more accurate and stable than that of other ANN models. By utilizing the acquired datasets, estimation of ACC upon circular and rectangular steel tubular columns filled with concrete for circular-concrete-filled steel tube (C-CFST), rectangular-concrete-filled steel tube (R-CFST), circular-concrete-filled steel tube eccentrically loaded (C-CFST-BC) and rectangular-concrete-filled steel tube eccentrically loaded (R-CFST-BC) has been carried-out. Through the relative findings, it could be interpreted that the proposed experimental model achieved higher success than existing models, where MLP regression (MLPR) score was found to be best than linear and random forest regression, with R2 scores of 98.9% in C-CFST, 99% in CFST-BC, 97% in R-CFST and 98.9% in R-CFST-BC. Similarly, MLPR was found effective with best RMSE scores for C-CFST as 452, CFST-BC as 87 and R-CFST as 313, whereas linear regression was best for R-CFST-BC with 394. Thus, research concludes that overall, the MLP regression is best and effective in predicting ACC in concrete-filled steel tube columns.","['Engineering', 'Civil Engineering']"
doi:10.1007/s12065-021-00626-6,en,Multi-view fusion for recommendation with attentive deep neural network,"['OriginalPaper', 'Special Issue']","Recommendation systems have been widely developed and introduced in numerous applications. However, due to the lack of sufficient user feedback data, the recommendation performance of such systems is often affected by data sparsity. To address this problem, a multi-view fusion recommendation algorithm with an attentive deep neural network is proposed. A two-stage joint learning solution is designed in the proposed model, which combines user attributes, item attributes, and user-item interaction information into a unified framework. The convolutional neural network and attention mechanism are applied to improve effect of extracting features from user and item attributes. The extended deep neural network model based on matrix factorization and multiple-layer perception is used to enhance the feature extraction of user and item interaction information. Experimental results on the MovieLens-1 M and Book-Crossing real datasets show that the proposed algorithm can achieve the best recommendation accuracy compared with other classical recommendation algorithms, even with extremely sparse data.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s12293-022-00377-6,en,A bi-level transformation based evolutionary algorithm framework for equality constrained optimization,"['OriginalPaper', 'Regular research paper']","Evolutionary algorithms (EAs) have been widely used by researchers and practitioners to solve optimization problems with constraints. However, equality constrained optimization problems (ECOPs) have posed a great challenge to traditional EA methods due to the dramatically narrowed search space caused by the equality constraints. In this paper, a bi-level transformation based evolutionary algorithm (BiTEA) framework is proposed to transform the ECOP into a bi-level optimization problem. In the BiTEA framework, the original ECOP is solved by an EA as the upper level problem, and the equality constraints are handled by another EA as the lower level problem. To facilitate performance comparison, a set of scalable ECOP test instances with various composable complexities is constructed for experimental studies. The performance of an implementation of the proposed BiTEA on these constructed instances is verified by comparing its performance to that of three state-of-the-art constraints handling EA methods.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Complex Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s00366-021-01459-8,en,Estimating heavy metals absorption efficiency in an aqueous solution using nanotube-type halloysite from weathered pegmatites and a novel Harris hawks optimization-based multiple layers perceptron neural network,"['OriginalPaper', 'Original Article']","In this study, nanotube-type halloysites from weathered pegmatites were investigated to absorb Pb 2+ in an aqueous solution. Also, a novel hybrid intelligent model based on the multiple layers perceptron (MLP) neural network and the Harris hawks optimization (HHO) algorithm (i.e., HHO-MLP neural network) was proposed for estimating the absorption of Pb 2+ from an aqueous solution using this novel material. XRD, SEM–EDS, and TEM analysis revealed the existence of overlapping tubular halloysites in the studied sample, similar to the results of previous studies. Various conditions of contact time, solution pH, the adsorbent weight, and Pb 2+ initial concentration were considered and evaluated using batch adsorption experiments with a total of 53 cases. Subsequently, an HHO-MLP neural network was developed and applied to predict Pb 2+ absorption efficiency in water by the nanotube-type halloysite from weathered pegmatites. A traditional MLP neural network model (without optimized by the HHO algorithm) was also investigated to predict and compare with that of the proposed HHO-MLP neural network model. The experimental results indicated that the nanotube-type halloysite from weathered pegmatites is a potential material used in processing water and removing heavy metals, i.e., Pb 2+ , with a promising development. Furthermore, the obtained results of the proposed HHO-MLP neural network model showed that this model is a robust intelligent model for estimating the efficiency of the Pb 2+ absorption in water using nanotube-type halloysite from weathered pegmatites (i.e., MSE = 1.647; RMSE = 1.283; R 2  = 0.931). It can be applied to increase the Pb 2+ absorption efficiency to eliminate Pb 2+ in an aqueous solution.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s12555-021-0343-2,en,High-dimensional Multiple Fractional Order Controller for Automatic Generation Control and Automatic Voltage Regulation,"['OriginalPaper', 'Regular Papers']","A fractional-order proportional-integral-derivative (FO-PID) with two more fractional-order operations can achieve higher control performance than a proportional-integral-derivative (PID). Besides, an active disturbance rejection controller (ADRC) and a fractional-order active disturbance rejection controller (FO-ADRC) with more input information can obtain higher control performance than a PID, an FO-PID. To obtain higher control performance with more information and more fractional-order operations, this paper proposes a high-dimensional multiple fractional-order controller (HDMFOC). The HDMFOC cascades multiple high-dimensional controllers with fractional-order for a controller. The numerical simulations of an automatic generation control (AGC) system and an automatic voltage regulation (AVR) system compare the proposed approach with four other algorithms, i.e., the PID, FO-PID, ADRC, and FO-ADRC. Then, the HDMFOC achieves the highest control performances for AGC and AVR of power systems through multiple dimensional information and multiple fractional-order operations.","['Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/s12063-022-00293-5,en,Financial risk assessment to improve the accuracy of financial prediction in the internet financial industry using data analytics models,OriginalPaper,"A sound credit assessment mechanism has been explored for many years and is the key to internet finance development, and scholars divide credit assessment mechanisms into linear assessment and nonlinear assessment. The purpose is to explore the role of two important data analytics models including machine learning and deep learning in internet credit risk assessment and improve the accuracy of financial prediction. First, the problems in the current internet financial risk assessment are understood, and data of MSE (Micro small Enterprises) are chosen for analysis. Then, a feature extraction method based on machine learning is proposed to solve data redundancy and interference in enterprise credit risk assessment. Finally, to solve the data imbalance problem in the credit risk assessment system, a credit risk assessment system based on the deep learning DL algorithm is introduced, and the proposed credit risk assessment system is verified through a fusion algorithm in different models with specific enterprise data. The results show that the credit risk assessment model based on the machine learning algorithm optimizes the standard algorithm through the global optimal solution. The credit risk assessment model based on deep learning can effectively solve imbalanced data. The algorithm generalization is improved through layer-by-layer learning. Comparison analysis shows that the accuracy of the proposed fusion algorithm is 25% higher than that of the latest CNN (Convolutional Neural Network) algorithm. The results can provide a new research idea for the assessment of internet financial risk, which has important reference value for preventing financial systemic risk.","['Business and Management', 'Operations Management', 'Engineering Economics, Organization, Logistics, Marketing', 'Industrial and Production Engineering', 'Operations Research/Decision Theory', 'Management', 'Innovation/Technology Management']"
doi:10.1007/s12145-022-00878-5,en,Integrating the Particle Swarm Optimization (PSO) with machine learning methods for improving the accuracy of the landslide susceptibility model,"['OriginalPaper', 'Research Article']","Landslide is one of the serious concerns due to which, the safety and sustainability of hilly areas across the globe, become vulnerable. Therefore, preparing of landslide susceptibility maps (LSMs) with the sound methods is a preliminary task for the safe and sustainable land use planning and design. In this research, Particle Swarm Optimization (PSO) was integrated with the pre-existing machine learning techniques such as Artificial Neural Network (ANN), Radial Basis Functional Neural Network (RBF-net), Reduced Error Pruning Tree (REPTree), Random Tree, Multivariate Adaptive Regression Splines (MARS) and M5tree to increase their efficiency and accuracy of prediction of landslide susceptibility in upper catchment of Meenachil river of Kerala. For the advancement of the ongoing research, a total of 189 landslide locations were analysed and datasets were obtained. To prepare LSMs, 70% of the total datasets was utilized for training and the rest 30% was used for the validation purposes. In this research, methods like: ROC, Precision, Proportion Incorrectly Classify, Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Taylor diagram were applied for the validation of the models. Based on the prior pieces of literature, a total of twelve landslide conditioning factors (LCFs) were chosen. However, none of them found to be possessing multi-collinearity. It is challenging to select features from a dataset through optimization. In this regards PSO is effective because, it is straightforward with efficient universal optimization techniques. The PSO algorithm has updated and optimized the weights of models, and as results, the efficiency of the used models in predicting landslide susceptibility has increased. Nearly, 13% of the study area is very highly susceptible to landslide. The area under the curve (AUC) value of the Random Tree-PSO integrated model is the highest, 86.38% for the training dataset and 88.05% for the validation dataset. According to the sensitivity analysis elevation is most sensitive factor (0.285) and curvature is very less sensitive factor (0.115). As a result, it can be concluded that, of all the models evaluated, it is the most suitable for predicting a landslide tragedy.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s00366-021-01477-6,en,A novel enhanced exploration firefly algorithm for global continuous optimization problems,"['OriginalPaper', 'Original Article']","In the global optimization process of the firefly algorithm (FA), there is a need to provide a fast convergence rate and to explore the search space more effectively. Therefore, we conduct modular analysis of the FA and propose a novel enhanced exploration firefly algorithm (EE-FA), which includes an enhanced attractiveness term module and an enhanced random term module. The attractiveness term module can improve the exploration efficiency and accelerate the convergence rate by enhancing the attraction between fireflies. The random term module improves the exploration efficiency by introducing a damped vibration distribution factor. The EE-FA uses multiple parameters to balance its exploration efficiency and convergence rate. The parameters have a great influence on the performance of the EE-FA. In order to achieve the best performance of the EE-FA, each parameter of the EE-FA needs to be simulated to determine its optimal value. Compared to multiple variants of the FA, the EE-FA has better exploration efficiency and a faster convergence speed. Experimental results reveal that the EE-FA recreated consistently vanquishes the front for 24 benchmark functions and 4 real design case studies in terms of both convergence rate and exploration efficiency.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01586-2,en,Stochastic deep collocation method based on neural architecture search and transfer learning for heterogeneous porous media,"['OriginalPaper', 'Original Article']","We present a stochastic deep collocation method (DCM) based on neural architecture search (NAS) and transfer learning for heterogeneous porous media. We first carry out a sensitivity analysis to determine the key hyper-parameters of the network to reduce the search space and subsequently employ hyper-parameter optimization to finally obtain the parameter values. The presented NAS based DCM also saves the weights and biases of the most favorable architectures, which is then used in the fine-tuning process. We also employ transfer learning techniques to drastically reduce the computational cost. The presented DCM is then applied to the stochastic analysis of heterogeneous porous material. Therefore, a three dimensional stochastic flow model is built providing a benchmark to the simulation of groundwater flow in highly heterogeneous aquifers. The performance of the presented NAS based DCM is verified in different dimensions using the method of manufactured solutions. We show that it significantly outperforms finite difference methods in both accuracy and computational cost.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11760-022-02393-y,en,Deep Learning for vision systems in Construction 4.0: a systematic review,"['OriginalPaper', 'Original Paper']","Recently, the construction industry has been digitizing its production processes, the so-called Construction 4.0, in allusion to the paradigm of the fourth industrial revolution. The application of Deep Learning in computer vision systems has been highlighted in Construction 4.0. Thus, the main contribution of this work is to present a systematic review of Deep Learning for vision systems under Construction 4.0, considering the most cited and most recent journal articles between 2017 and 2021 from Scopus database. For this, a research method selected and analyzed 76 published papers. Six main points were evaluated in the proposed methodology: study area, computer vision applications, Deep Learning methods, hyperparameter tuning, data augmentation, and future work. The following topics stand out as relevant perspectives and directions for continued advancement in this field of research: improving Deep Learning models, increasing the quality of databases, investigating the generality techniques and optimizing processing capacity.","['Computer Science', 'Image Processing and Computer Vision', 'Signal,Image and Speech Processing', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Multimedia Information Systems']"
doi:10.1007/s11269-022-03339-2,en,Optimized Model Inputs Selections for Enhancing River Streamflow Forecasting Accuracy Using Different Artificial Intelligence Techniques,OriginalPaper,"The development of a river inflow prediction is a prerequisite for dam reservoir management. Precise forecasting leads to better irrigation water management, reservoir operation refinement, enhanced hydropower output and mitigation of risk of natural hazards such as flooding. Dam created reservoirs prove to be an essential source of water in arid and semi-arid regions. Over the years, Artificial Intelligence (AI) has been used for development of models for prediction of various natural variables in different engineering fields. Also, several AI models have been proved to be beneficial over the conventional models in efficient prediction of various natural variables. In this study, four AI models, namely, Artificial Neural Network (ANN), Support Vector Machine (SVM), Random Forest (RF) and Boosted Tree Regression (BTR) were developed and trained over 130-years of monthly historical rainfall data to forecast streamflow at Aswan High Dam, Egypt. The input parameters were selected according to the Autocorrelation Function (ACF) plot. The findings revealed that RF model outperformed other techniques and could provide precise monthly streamflow prediction with the lowest RMSE (2.2395) and maximum WI (0.998462), R 2 (0.9012). The input combination for the optimum RF model was Q t-1 , Q t-11 , and Q t-12 (i.e., one-, eleven- and twelve-months delay inputs). The optimum RF model provides a reliable source of data for inflow predictions, which allows improved utilization of water resources and long-term water resource planning and management.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s10601-022-09337-w,en,Short- and medium-term optimization of underground mine planning using constraint programming,OriginalPaper,"In the past few years, the mining industry has seen a lot of operational changes. Digitalization and automation of many processes have paved the way for an increase in its general productivity. In keeping with this trend, this article presents a novel approach for optimizing underground mine scheduling for the short- and medium-term. This problem is similar to the Resource-Constrained Project Scheduling Problem, with the difference that all task completions are optional. The model uses Constraint Programming principles to maximize the Net Present Value of a mining project. It plans work shifts for up to a year in advance, considering specialized equipment, rock support and operational constraints. This is the first published paper using optional variables to model optional tasks in a real-life application. Results from its applications to datasets based on a Canadian gold mine demonstrate its ability to find optimal solutions in a reasonable time. A comparison with an equivalent Mixed Integer Programing model proves that the Constraint Programming approach offers clear gains in terms of computability and readability of the constraints.","['Computer Science', 'Artificial Intelligence', 'Optimization', 'Operations Research/Decision Theory']"
doi:10.1007/s00521-022-07635-1,en,Multi-modal medical image fusion based on densely-connected high-resolution CNN and hybrid transformer,"['OriginalPaper', 'Original Article']","Multi-modal medical image fusion (MMIF) has found wide application in the field of disease diagnosis and surgical guidance. Despite the popularity of deep learning (DL)-based fusion methods, these DL algorithms cannot provide satisfactory fusion performance due to the difficulty in capturing the local information and the long-range dependencies effectively. To address these issues, this paper has presented an unsupervised MMIF method by combining a densely-connected high-resolution network (DHRNet) with a hybrid transformer. In this method, the local features are firstly extracted from the source image using the DHRNet. Then these features are input into the fine-grained attention module in the hybrid transformer to produce the global features by exploring their long-range dependencies. The local and global features are fused by the projection attention module in the hybrid transformer. Finally, based on the fused features, the fused result is reconstructed by the decoder network. The presented network is trained using an unsupervised loss function including edge preservation value, structural similarity, sum of the correlations of differences and structural tensor. Experiments on various multi-modal medical images show that, compared with several traditional and DL-based fusion methods, the presented method can generate visually better fused results and provide better quantitative metrics values.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00202-022-01621-5,en,Dynamic environmental economic dispatch with an enhanced-accuracy probabilistic wind cost model,"['OriginalPaper', 'Original Paper']","This paper proposes a dynamic wind-thermal generation scheduling (DWTGS) framework to minimize system generation costs and pollutant emissions. To deal with wind power uncertainty, a new probabilistic wind cost function is introduced, which decomposes wind power uncertainty into several components with varying frequencies. This model not only allows for the use of a more accurate wind power prediction technique in system scheduling, but it also enables the system operator to schedule different thermal units with distinct characteristics to compensate for specific components of uncertainty. This paper also proposes a straightforward procedure for dynamic load dispatch based on power flow, taking into account system constraints and requirements, thereby obtaining more realistic results compared to a large number of previous works in this area. To make the optimization algorithm able to deal with such a complex and high-dimensional optimization problem, a novel heuristic technique is suggested, which is also capable to be readily adapted to other population-based optimization algorithms to improve accuracy. The proposed DWTGS framework is implemented on the IEEE-30 bus test system for performance evaluation. Beneficial information is extracted from the obtained results, and the efficacy of each of the proposed techniques in the optimization procedure is investigated meticulously.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s11633-022-1354-z,en,Glaucoma Detection with Retinal Fundus Images Using Segmentation and Classification,"['OriginalPaper', 'Research Article']","Glaucoma is a prevalent cause of blindness worldwide. If not treated promptly, it can cause vision and quality of life to deteriorate. According to statistics, glaucoma affects approximately 65 million individuals globally. Fundus image segmentation depends on the optic disc (OD) and optic cup (OC). This paper proposes a computational model to segment and classify retinal fundus images for glaucoma detection. Different data augmentation techniques were applied to prevent overfitting while employing several data pre-processing approaches to improve the image quality and achieve high accuracy. The segmentation models are based on an attention U-Net with three separate convolutional neural networks (CNNs) backbones: Inception-v3, visual geometry group 19 (VGG19), and residual neural network 50 (ResNet50). The classification models also employ a modified version of the above three CNN architectures. Using the RIM-ONE dataset, the attention U-Net with the ResNet50 model as the encoder backbone, achieved the best accuracy of 99.58% in segmenting OD. The Inception-v3 model had the highest accuracy of 98.79% for glaucoma classification among the evaluated segmentation, followed by the modified classification architectures.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s12530-021-09409-x,en,A new optimal prediction technique for energy demand based on CNN and improved water strider algorithm: a study on socio-economic-climatic parameters,"['OriginalPaper', 'Original Paper']","Rising demand and the potential decline in renewables resources have made energy production one of the most important challenges of the future. In hydropower plants, various factors can be the reasons to limit resources and energy supply. In this study, population and GDP, and climatic parameters (temperature and precipitation) are analyzed for the prediction of the energy demand. The innovation of this research is the Introduction of a developed algorithm called the Improved Water Strider Algorithm (IWSA). The results displayed that the improved algorithm has the faster convergence and the lowest error with the value of 1.55, 1.48, 1.44, and 0.78 in population, and GDF, temperature, and precipitation, respectively. This model has the highest correlation coefficient with the value of 0.77, 0.79, 0.8, and 80 in population, GDF, temperature, and precipitation, respectively. Also, this model with the highest correlation of 0.91, and the lowest error of 0.47 can have the best performance. Therefore, after confirming this proposed method, energy demand is predicted. The results showed that among the four input parameters of the CNN-IWSA model the precipitation parameter can have a significant effect on limiting resources and increasing demand because it can directly affect the amount and timing of energy production distribution. The results also show an increasing trend in energy demand for the next 20 years. These results can be of great help to hydrologists and energy managers in controlling and supplying energy.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s11356-022-21850-2,en,Deep neural network prediction of modified stepped double-slope solar still with a cotton wick and cobalt oxide nanofluid,"['OriginalPaper', 'Research Article']","This research work intends to enhance the stepped double-slope solar still performance through an experimental assessment of combining linen wicks and cobalt oxide nanoparticles to the stepped double-slope solar still to improve the water evaporation and water production. The results illustrated that the cotton wicks and cobalt oxide (Co 3 O 4 ) nanofluid with 1wt% increased the hourly freshwater output (HP) and instantaneous thermal efficiency (ITE). On the other hand, this study compares four machine learning methods to create a prediction model of tubular solar still performance. The methods developed and compared are support vector regressor (SVR), decision tree regressor, neural network, and deep neural network based on experimental data. This problem is a multi-output prediction problem which is HP and ITE. The prediction performance for the SVR was the lowest, with 70 (ml/m 2 h) mean absolute error (MAE) for HP and 4.5% for ITE. Decision tree regressor has a better prediction for HP with 33 (ml/m 2 h) MAE and almost the same MAE for ITE. Neural network has a better prediction for HP with 28 (ml/m 2 h) MAE and a bit worse prediction for ITE with 5.7%. The best model used the deep neural network with 1.94 (ml/m 2 h) MAE for HP and 0.67% MAE for ITE.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s00521-022-08062-y,en,Insight into breast cancer detection: new hybrid feature selection method,"['OriginalPaper', 'Original Article']","Breast cancer, which is also the leading cause of death among women, is one of the most common forms of the disease that affects females all over the world. The discovery of breast cancer at an early stage is extremely important because it allows selecting appropriate treatment protocol and thus, stops the development of cancer cells. In this paper, a new patients detection strategy has been presented to identify patients with the disease earlier. The proposed strategy composes of two parts which are data preprocessing phase and patient detection phase (PDP). The purpose of this study is to introduce a feature selection methodology for determining the most efficient and significant features for identifying breast cancer patients. This method is known as new hybrid feature selection method (NHFSM). NHFSM is made up of two modules which are quick selection module that uses information gain, and feature selection module that uses hybrid bat algorithm and particle swarm optimization. Consequently, NHFSM is a hybrid method that combines the advantages of bat algorithm and particle swarm optimization based on filter method to eliminate many drawbacks such as being stuck in a local optimal solution and having unbalanced exploitation. The preprocessed data are then used during PDP in order to enable a quick and accurate detection of patients. Based on experimental results, the proposed NHFSM improves the efficiency of patients’ classification in comparison with state-of-the-art feature selection approaches by roughly 0.97, 0.76, 0.75, and 0.716 in terms of accuracy, precision, sensitivity/recall, and F -measure. In contrast, it has the lowest error rate value of 0.03.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s13198-022-01718-x,en,Cloud music resources-oriented secure data storage and defense using edge computing,"['OriginalPaper', 'Original Article']","Research motivation: The aim is to study the secure storage of cloud music resources, especially the Secure Data Storage and Defense (SDSD) in the Edge Computing Ecosystem (ECE). The main problems and solutions: the present work builds an anti-malware propagation mechanism and proposes an Edge Devices (EDs)-oriented ant-malware propagation SDSD algorithm for user cloud music resources. The proposed ED-oriented SDSD model and algorithm are based on the Mean Field Game (MFD), realizing the combination of SDSD strategies in ECE with game theory. Research results: In the actual ECE, the anti-malware SDSD model can reduce the data loss by adjusting the network and ED parameters. Specifically, within 0 ~ 20s, the number of infected devices decreases gradually at a rate of about 4% over time. Users can minimize their data loss while minimizing their Computing Resource (CORE) consumption. The data loss decreases monotonically with time. When t > 20s, the Average Data Loss (ADL) is stable at 0. Research conclusion: the proposed SDSD model for users’ cloud music resources is efficient and practical in securely storing data in the ECE.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s10288-021-00496-9,en,A study on sequential minimal optimization methods for standard quadratic problems,"['OriginalPaper', 'Research Paper']","In this work, we consider the relevant class of Standard Quadratic Programming problems and we propose a simple and quick decomposition algorithm, which sequentially updates, at each iteration, two variables chosen by a suitable selection rule. The main features of the algorithm are the following: (1) the two variables are updated by solving a subproblem that, although nonconvex, can be analytically solved; (2) the adopted selection rule guarantees convergence towards stationary points of the problem. Then, the proposed Sequential Minimal Optimization algorithm, which optimizes the smallest possible sub-problem at each step, can be used as efficient local solver within a global optimization strategy. We performed extensive computational experiments and the obtained results show that the proposed decomposition algorithm, equipped with a simple multi-start strategy, is a valuable alternative to the state-of-the-art algorithms for Standard Quadratic Optimization Problems.","['Business and Management', 'Operations Research/Decision Theory', 'Optimization', 'Industrial and Production Engineering']"
doi:10.1007/s00366-020-01200-x,en,Frequency simulation of viscoelastic multi-phase reinforced fully symmetric systems,"['OriginalPaper', 'Original Article']","Honeycomb structures have the geometry of the lattice network to allow the minimization of the amount of used material to reach minimal material cost and minimal weight. In this regard, this article deals with the frequency analysis of imperfect honeycomb core sandwich disk with multiscale hybrid nanocomposite (MHC) face sheets rested on an elastic foundation. The honeycomb core is made of aluminum due to its low weight and high stiffness. The rule of the mixture and modified Halpin–Tsai model are engaged to provide the effective material constant of the composite layers. By employing Hamilton’s principle, the governing equations of the structure are derived and solved with the aid of the generalized differential quadrature method (GDQM). Afterward, a parametric study is done to present the effects of the orientation of fibers ( $$\theta_{{\text{f}}} /\pi$$ θ f / π ) in the epoxy matrix, Winkler–Pasternak constants ( $$K_{{\text{w}}}$$ K w and $$K_{{\text{p}}}$$ K p ), thickness to length ratio of the honeycomb network ( $$t_{{\text{h}}} /l_{{\text{h}}}$$ t h / l h ), the weight fraction of CNTs, value fraction of carbon fibers, angle of honeycomb networks, and inner to outer radius ratio on the frequency of the sandwich disk. The results show that it is true that the roles of $$K_{{\text{w}}}$$ K w and $$K_{{\text{p}}}$$ K p are the same as an enhancement, but the impact of $$K_{{\text{w}}}$$ K w could be much more considerable than the effect of $$K_{{\text{p}}}$$ K p on the stability of the structure. Additionally, when the angle of the fibers is close to the horizon, the frequency of the system improves.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s40998-022-00529-x,en,Optimal Scheduling of Grid Connected Solar Photovoltaic and Battery Storage System Considering Degradation Cost of Battery,"['OriginalPaper', 'Research Paper']","The major objectives of this paper are to optimize the scheduling of solar photovoltaic (SPV) and battery energy storage systems (BESS) with the grid in order to reduce power loss and improve reliability. An unbalanced 8-bus rural distribution network in the village of Jalalabad, in the district of Ghaziabad, Uttar Pradesh, India, is under consideration. The main issue in village-based rural communities is excessive power outages and restricted grid power supplies. A modified artificial bee colony optimization technique has been used to identify optimum sizing, location, and timing in order to minimize the system's total cost and losses in order to overcome the aforementioned challenge. The management resource and demand response strategy are used to manage the load demand profile. The Coulomb Counting method is used to improve the estimation accuracy of the battery. The various results demonstrate the efficacy of the suggested method for determining appropriate PV, BESS, and grid size, location, and timing. In this work, only summer season is considered for SPV generation. In addition, the degradation cost of the battery and the excess power production have also been analyzed in this paper. It is evident that with the increase in the non-essential load shifting fraction β NELS from 0 to 25%, the fraction of excess power production decreases from 9.15 to 6.21%. The results demonstrate that combining solar PV with a rural network reduces carbon dioxide (CO2) emissions while also providing power 24 h a day, seven days a week.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s00500-021-06184-4,en,Multi-swarm optimization model for multi-cloud scheduling for enhanced quality of services,"['OriginalPaper', 'Focus']","Cloud services gain more attention due to its accessibility, performance, and cost factors. Cloud offers a wide range of services and completes the task without any delay due to its scheduling policies. Task scheduling is an important factor in cloud computing applications. The performance of applications increases due to an effective scheduling strategy. The cloud resources are allocated to the tasks through task scheduling. Factors like customer satisfaction, resource utilization, better performance make task scheduling crucial for service providers. Depending on the scheduling schemes support in clouds, scheduling is categorized into single cloud or multi-cloud scheduling. Multi-cloud environment provides diverse resources and significantly reduces the cost and commercial limitations. However, reducing the cost functions and makespan are the major factors considered to avoid customer dissatisfaction. But it is essential to concentrate on other factors, such as throughput, delay, Makespan, waiting time, response time, utilization, and efficiency to improve the quality of services. This research work presents a Multi-Swarm Optimization model for Multi-Cloud Scheduling for Enhanced Quality of Services for a multi-cloud environment. Experimental results demonstrate that the proposed approach performs better in all aspects compared to existing techniques, such as Adaptive energy-efficient scheduling, single objective particle swarm optimization scheduling, and improves the quality of services.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10825-022-01921-w,en,Design and synthesis of circular antenna array using artificial hummingbird optimization algorithm,OriginalPaper,"Circular antenna arrays are extensively utilized in next-generation communication applications like IoT, 5G, and beamforming, although maintaining the subsidiary lobes along with directivity remains a barrier. Many conventional methods might be employed to estimate the array parameters in real-time, but they would lag in maintaining high directivity and a low side lobe level. An optimization problem is used in this study to get the requisite primary lobe orientation, inhibit the subsidiary lobe and optimize directivity. To estimate the regulating parameters in a timely way, an artificial hummingbird method is employed for the circular antenna array issue. Simulations are run, and the results are compared to those of other standard techniques. The results reveal that the artificial hummingbird method achieves great side lobe reduction while maintaining acceptable directivity. According to the dimension study, high directivity values with low side lobe levels may be attained with fewer antenna parts as well.","['Engineering', 'Mathematical and Computational Engineering', 'Electrical Engineering', 'Theoretical, Mathematical and Computational Physics', 'Optical and Electronic Materials', 'Mechanical Engineering']"
doi:10.1007/s40996-022-00908-8,en,Comparison of Four Chaotic Meta-Heuristic Algorithms for Optimal Design of Large-Scale Truss Structures,"['OriginalPaper', 'Research Paper']","The combination of meta-heuristic algorithms with chaos map creates a significant improvement in the optimal design of truss structures. The main reason for this success is to balance the key stages of exploration and exploitation. Some meta-heuristic algorithms need to be improved for the exploration phase and others need to be improved for the exploitation phase. Also, in a limited number of algorithms, both steps need to be improved. Truss shape and cross-sectional optimization involves nonlinear and non-convex modes, often with several local optima. Chaos maps play a major role in escaping these local optima and achieving global optimization. Accordingly, chaotic maps prevent premature convergence and accelerate access to global optimizations by expanding the search points in scattered parts of the decision space and carefully examining the neighborhood of these points. In this study, logistic and Gauss chaos maps are incorporated in four meta-heuristic algorithms providing suitable conditions to improve the optimization results. These chaotic algorithms include Chaotic Water Evaporation Optimization (CWEO), Chaotic Artificial Bee Colony (CABC), Chaotic Imperialist Competitive Algorithm (CICA) and Chaotic Shuffled Frog-Leaping Algorithm (CSFLA).","['Engineering', 'Civil Engineering']"
doi:10.1007/s12532-022-00223-3,en,A graph-based modeling abstraction for optimization: concepts and implementation in Plasmo.jl,"['OriginalPaper', 'Full Length Paper ']","We present a general graph-based modeling abstraction for optimization that we call an OptiGraph . Under this abstraction, any optimization problem is treated as a hierarchical hypergraph in which nodes represent optimization subproblems and edges represent connectivity between such subproblems. The abstraction enables the modular construction of complex models in an intuitive manner, facilitates the use of graph analysis tools (to perform partitioning, aggregation, and visualization tasks), and facilitates communication of structures to decomposition algorithms. We provide an open-source implementation of the abstraction in the Julia -based package Plasmo.jl . We provide tutorial examples and large application case studies to illustrate the capabilities.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Theory of Computation', 'Mathematics of Computing']"
doi:10.1007/s12206-022-1139-x,en,Grey wolf optimization based support vector machine model for tool wear recognition in fir-tree slot broaching of aircraft turbine discs,"['OriginalPaper', 'Original Article']","Broaching tool condition monitoring is the basis of intelligent manufacturing of high-end broaching equipment. There are still technical bottlenecks in tool wear recognition accuracy and response speed. Aiming at the characteristics of complex cutter tooth shape and variable spatial distribution of turbine disc fir-tree slot broaching tool, a method of wear state recognition for broaching tool based on maximum relevance and minimum redundancy and gray wolf optimization algorithm is proposed. In the process of broaching, the broach vibration signals are collected in real time. The signal characteristics in time domain, frequency domain and time-frequency domain are extracted by signal processing technology, and the support vector machine (SVM) recognition model of broach wear state is established. The maximum relevance and minimum redundancy (mRMR) method is used to reduce the dimension of data, grey wolf optimization algorithm (GWO) is used to optimize parameters to improve the recognition accuracy of SVM. The experimental results show that the model can accurately recognize the wear state of fir-tree slot broach at different stages. In addition, grey wolf optimization-support vector machine (GWO-SVM) model shows higher accu-racy in classification than particle swarm optimization based support vector machine (PSO-SVM) and genetic algorithm based support vector machine (GA-SVM) models. Compared with PSO-SVM and GA-SVM models, the computational time of GWO-SVM is reduced by 54.2 % and 60.5 % respectively.","['Engineering', 'Mechanical Engineering', 'Vibration, Dynamical Systems, Control', 'Industrial and Production Engineering']"
doi:10.1007/s13748-022-00289-z,en,A novel chaotic flower pollination algorithm for modelling an optimized low-complexity neural network-based NAV predictor model,"['OriginalPaper', 'Regular Paper']","Investment instruments for structured investments include mutual funds, and the net asset value (NAV) is used to calculate their value. Due to uncertainty and influences from economic and political factors, it is challenging to predict such complex financial series. The study developed a model to predict NAV by using a low-complexity neural network, the Legendre polynomial neural network (LPNN). Moreover, a new chaotic flower pollination algorithm (NCHFPA) was developed to adjust the unknown parameters of the network through the learning process. NCHFPA is a fusion of chaos-based meta-heuristics with differentiated evolution (DE) algorithm in the local pollination phase of flower pollination algorithm (FPA). In order to determine the best variant of NCHFPA, five different chaotic functions have been investigated in three control parameters. The model was enhanced by integrating the natural evolution features from DE and the pollination process from FPA along with chaos theory. Three real-time mutual fund data sets of reputed Indian financial firms Aditya Birla (AB), SBI and ICICI were used to test this proposed LPNN-NCHFPA model. In order to verify and validate the predictor model further, a comparative analysis is performed with other optimization algorithms such as FPA, Chaotic FPA, particle swarm optimization (PSO) and DE. The proposed framework exhibits an improved performance of 36.65%, 28.22%, 20.10% and 17.18% in RMSE over LPNN-PSO, LPNN-DE, LPNN-FPA and LPNN-CHFPA, respectively, for AB mutual fund. For SBI, an improvement of 46.88%, 32.31%, 18.77% and 6.05% in RMSE and for ICICI, an improvement of 28.87%, 24.58%, 15.63% and 10.05% in RMSE are reported over LPNN-PSO, LPNN-DE, LPNN-FPA and LPNN-CHFPA, respectively, which clearly reveal the competency of the proposed framework over other experimented models.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Natural Language Processing (NLP)', 'Computational Intelligence', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10586-022-03715-y,en,A new approach for mechanical parameter inversion analysis of roller compacted concrete dams using modified PSO and RBFNN,OriginalPaper,"The mechanical parameter inversion model is an essential part of ensuring dam health; it provides a parametric basis for assessing the safe operational behavior of dams using numerical simulation techniques. Due to the complicated nonlinear mapping relationship between the roller compacted concrete (RCC) dam's mechanical parameters and various environmental quantities, as well as conventional statistical models, machine learning methods, and neural networks fail to consider the inputs of fuzzy uncertainty factors. Therefore, the accuracy, efficiency, and stability of inversion models are usually affected by their modeling methods. In this paper, a novel hybrid model for mechanical parameter inversion of an RCC dam is proposed, which uses a radial basis function neural network (RBFNN) to establish the nonlinear mapping relationship between the dam mechanical parameters and the environmental quantities, and the modified particle swarm optimization (PSO) algorithm is used to find the optimal parameters of the model. The modified PSO algorithm makes the inertia weight ω dynamically adjust with the number of iterations to improve the randomness and diversity of the particle population, and population crossover and mutation are introduced to improve the global search ability and convergence speed of the algorithm. The proposed hybrid model is verified and comparatively analyzed by four typical mathematical test functions, and the results show that the proposed model exhibits good performance in parameter inversion accuracy, convergence speed, stability and robustness. Finally, the model is applied to the mechanical parameter inversion analysis of an RCC gravity dam in Henan Province in China. The results show that the proposed model is feasible and reasonable for practical engineering applications, and the relative error between the results obtained by inputting the inverted parameters into the numerical model and monitoring data was within 10%. The methodology derived from this study can provide technical support and a reference for the mechanical parameter inversion analysis of similar dam projects.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00354-022-00182-2,en,KEAHT: A Knowledge-Enriched Attention-Based Hybrid Transformer Model for Social Sentiment Analysis,OriginalPaper,"Social media materialized as an influential platform that allows people to share their views on global and local issues. Sentiment analysis can handle these massive amounts of unstructured reviews and convert them into meaningful opinions. Undoubtedly, COVID-19 originated as the enormous challenge across the world that physically and financially bruted humankind. Meanwhile, farmers' protests shook up the world against three pieces of legislation passed by the Indian government. Hence, an artificial intelligence-based sentiment model is needed for suggesting the right direction toward outbreaks. Although Deep Neural Network (DNN) gained popularity in sentiment analysis applications, these still have a limitation of sequential training, high-dimension feature space, and equal feature importance distribution. In addition, inaccurate polarity scoring and utility-based topic modeling are other challenging aspects of sentiment analysis. It motivates us to propose a Knowledge-Enriched Attention-based Hybrid Transformer (KEAHT) model by enriching the explicit knowledge of Latent Dirichlet Allocation (LDA) topic modeling and lexicalized domain ontology. A pre-trained Bidirectional Encoder Representation from Transformer (BERT) is employed to train within a minimum training corpus. It provides the facility of attention mechanism and can solve complex text problems accurately. A comparative study with existing baselines and recent hybrid models affirms the credibility of the proposed KEAHT in the field of Natural Language Processing (NLP). This model emphasizes artificial intelligence's role in handling the situation of the global pandemic and democratic dispute in a country. Furthermore, two benchmark datasets, namely “COVID-19-Vaccine-Labelled-Tweets"" and ""Indian-Farmer-Protest-Labelled-Tweets”, are also constructed to accommodate future researchers for outlining the essential facts associated with the outbreaks.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s10586-022-03630-2,en,A novel deep reinforcement learning scheme for task scheduling in cloud computing,OriginalPaper,"Recently, the demand of cloud computing systems has increased drastically due to their significant use in various real-time online and offline applications. Moreover, it is widely being adopted from research, academia and industrial field as a main solution for computation and storage platform. Due to increased workload and big-data, the cloud servers receive huge amount of data storage and computation request which need to be processed through cloud modules by mapping the tasks to available virtual machines. The cloud computing models consume huge amount of energy and resources to complete these tasks. Thus, the energy aware and efficient task scheduling approach need to be developed to mitigate these issues. Several techniques have been introduced for task scheduling, where most of the techniques are based on the heuristic algorithms, where the scheduling problem is considered as NP-hard problem and obtain near optimal solution. But handling the different size of tasks and achieving near optimal solution for varied number of VMs according to the task configuration remains a challenging task. To overcome these issues, we present a machine learning based technique and adopted deep reinforcement learning approach. In the proposed approach, we present a novel policy to maximize the reward for task scheduling actions. An extensive comparative analysis is also presented, which shows that the proposed approach achieves better performance, when compared with existing techniques in terms of makespan, throughput, resource utilization and energy consumption.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00366-022-01637-2,en,On the virtual element method for topology optimization of non-Newtonian fluid-flow problems,"['OriginalPaper', 'Original Article']","This paper presents some virtual element method (VEM) applications for topology optimization of non-Newtonian fluid-flow problems in arbitrary two-dimensional domains. The objective is to design an optimal layout for the incompressible non-Newtonian fluid flow, governed by the Navier–Stokes–Brinkman equations, to minimize the viscous drag. The porosity approach is used in the topology optimization formulation. The VEM is used to solve the governing boundary value problem. The key feature distinguishing the VEM from the classical finite element method is that the local basis functions in the VEM are only implicitly known. Instead, the VEM uses local projection operators to describe each element’s rigid body motion and constant strain components. Therefore, the VEM can handle meshes with arbitrarily shaped elements. Several numerical examples are provided to demonstrate the efficacy and efficiency of the VEM for the topology optimization of fluid-flow problems. A MATLAB code for reproducing the results provided in this paper is freely available at https://github.com/mampueros/VEM_TopOpt_FluidFlow .","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00477-022-02263-6,en,"Landslide susceptibility mapping using deep learning models in Ardabil province, Iran","['OriginalPaper', 'Original Paper']","Landslides are one of the most destructive natural phenomena in the world, which occur mostly in mountainous areas and cause damage to the economic sectors, agricultural lands, residential areas and infrastructures of any country, and also threaten the lives and property of human beings. Therefore, landslide susceptibility mapping (LSM) can play a critical role in identifying prone areas and reducing the damage caused by landslides in each area. In the present study, deep learning algorithms including convolutional neural network (CNN) and long short-term memory (LSTM) were used to identify landslide prone areas in Ardabil province, Iran. Then 312 landslide locations were identified and randomly divided into train and test datasets, and according to previous studies and environmental conditions in the study area, twelve factors affecting the occurrence of landslides were selected. The ratio of the importance of each influential factor in landslide occurrence was obtained through information gain ranking filter method and it was found that land-use and profile curvature had the highest and lowest impacts, respectively. Afterwards, LSMs were generated using CNN and LSTM algorithms. In the next step, the performance of the models was evaluated based on the area under curve (AUC) value of receiver operating characteristics curve and the root mean square error (RMSE) method. The AUC values for CNN and LSTM models were 0.821 and 0.832, respectively. Furthermore, the RMSE values in the CNN model for each of the training and testing dataset were 0.121 and 0.132, respectively. The RMSE values in the LSTM model for each of the training and testing dataset were 0.185 and 0.188, respectively. Therefore, it can be concluded that LSTM performance is slightly better than CNN; but in general, both models have close performance and the accuracy of both models is acceptable.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10586-022-03631-1,en,Lightweight image super-resolution with feature cheap convolution and attention mechanism,OriginalPaper,"Since deep learning is introduced into the field of super-resolution (SR), many deep learning-based SR methods have been proposed and achieved good results. At present, most neural networks use ordinary convolution and deeper neural layer in image super-resolution reconstruction in order to achieve better results. Therefore, most existing models have a massive amount of parameters and calculation, which limits the practical application. To solve this problem, in this paper, we propose an efficient feature cheap convolution SR (FCCSR) model, which consists of several multi-level information fusion Blocks (MIFB). In the MIFB, the features are extracted by combining normal convolution and cheap convolution, and the features obtained from each layer of convolution are fused. Under limited parameters and reconstruction speed, it enables the whole model to obtain the features from shallow to deep layers in the low-resolution (LR) images. Secondly, a novel lightweight channel attention module is designed to obtain fewer parameters and better performance. Finally, we add a gradient loss function to the original L1 loss function, making the model pay more attention to the high-frequency part of the LR image and making the reconstructed image’s texture details more straightforward. The experimental results show that the proposed model has reached the most advanced level regarding image quality, memory consumption, parameter number, and calculation amount. The operation speed of FCCSR is roughly the same as that of the current STOA models, and the number of parameters is only 0.63 M. On some data sets, the best performance of FCCSR can be improved by 7%.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s11069-022-05520-7,en,"Modeling landslide susceptibility using data mining techniques of kernel logistic regression, fuzzy unordered rule induction algorithm, SysFor and random forest","['OriginalPaper', 'Original Paper']","This paper introduces four advanced intelligent algorithms, namely kernel logistic regression, fuzzy unordered rule induction algorithm, systematically developed forest of multiple decision trees and random forest (RF), to perform the landslide susceptibility mapping in Jian’ge County, China, as well as well study of the connection between landslide occurrence and regional geo-environment characteristics. To start with, 262 landslide events were determined, and the proportion of randomly generated training data is 70%, while the proportion of randomly generated validation data is 30%, respectively. Then, through the comprehensive consideration of local geo-environment characteristics and relevant studies, fifteen conditioning factors were prepared, such as slope angle, slope aspect, altitude, profile curvature, plan curvature, sediment transport index, topographic wetness index, stream power index, distance to rivers, distance to roads, distance to lineaments, soil, land use, lithology and NDVI. Next, frequency ratio model was utilized to identify the corresponding relations for conditioning factors and landslides distribution. In addition, four data mining techniques were conducted to implement the landslide susceptibility research and generated landslide susceptibility maps. In order to examine and compare model performance, receiver operating characteristic curve was brought for judging accuracy of those four models. Finally, the results indicated that a traditional model, namely RF model, acquired the highest AUC value (0.859). Last but gained a lot of attention, the results can provide references for land use management and landslide prevention.","['Earth Sciences', 'Natural Hazards', 'Hydrogeology', 'Geophysics/Geodesy', 'Geotechnical Engineering & Applied Earth Sciences', 'Civil Engineering', 'Environmental Management']"
doi:10.1007/s40747-022-00844-0,en,Dynamic scheduling for semiconductor manufacturing systems with uncertainties using convolutional neural networks and reinforcement learning,"['OriginalPaper', 'Original Article']","The dynamic scheduling problem of semiconductor manufacturing systems (SMSs) is becoming more complicated and challenging due to internal uncertainties and external demand changes. To this end, this paper addresses integrated release control and production scheduling problems with uncertain processing times and urgent orders and proposes a convolutional neural network and asynchronous advanced actor critic-based method (CNN-A3C) that involves a training phase and a deployment phase. In the training phase, actor–critic networks are trained to predict the evaluation of scheduling decisions and to output the optimal scheduling decision. In the deployment phase, the most appropriate release control and scheduling decisions are periodically generated according to the current production status based on the networks. Furthermore, we improve the four key points in the deep reinforcement learning (DRL) algorithm, state space, action space, reward function, and network structure and design four mechanisms: a slide-window-based two-dimensional state perception mechanism, an adaptive reward function that considers multiple objectives and automatically adjusts to dynamic events, a continuous action space based on composite dispatching rules (CDR) and release strategies, and actor–critic networks based on convolutional neural networks (CNNs). To verify the feasibility and effectiveness of the proposed dynamic scheduling method, it is implemented on a simplified SMS. The simulation experimental results show that the proposed method outperforms the unimproved A3C-based method and the common dispatching rules under the new uncertain scenarios.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00500-022-07445-6,en,Software fault prediction using evolving populations with mathematical diversification,"['OriginalPaper', 'Application of soft computing']","Software fault prediction (SFP) plays a vital role into fostering high quality throughout the software development process. It allows to identify the fault-prone modules in early development phases and facilitates the focused and effective testing over the fault-prone modules. Machine learning (ML)-based classifiers are prominently being used for fault prediction in the software industry. The accuracy of the ML models depends upon the training data and its quality. The curse of high dimensionality adversely impacts the classification power of a ML model. The presence of inter-correlated, insignificant and/or redundant features (or attributes) in the training data hinders the performance of ML classifiers. Feature preprocessing (or feature selection (FS)) is the solution to this issue. Meta-heuristics is the key method to find out the most significant feature subset. In this paper, a novel feature selection method is devised using mathematical diversification for genetic evolution. It avoids the local optimums by utilizing arithmetic diversification among the candidate solutions (or populations). The survival of fittest is the working principle of evolving populations with crossover and mutation operations. The selected feature subset is fed to five classification algorithms, namely artificial neural network, support vector machine, decision tree, k-nearest neighbor and naïve Bayes. The proposed model is trained and tested over five datasets from NASA corpus, namely CM1, JM1, KC1, KC2 and PC1. In total, 100 SFP models are implemented (4 feature selection methods $$\times $$ × 5 datasets $$\times $$ × 5 classification algorithms). From the experiments, it is observed that the SFP models with proposed feature selection technique of evolving populations with mathematical diversification (FS-EPwMD) are better than other models. It can be concluded that the proposed SFP model built using proposed FS-EPwMD with artificial neural networks performs statistically best among all the competing 100 SFP models irrespective of the datasets used.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10586-022-03694-0,en,Efficient greedy heuristic approach for fault-tolerant distributed controller placement in scalable SDN architecture,OriginalPaper,"Software-Defined Network (SDN) enables a centralized networking architecture that employs controllers to administer a global view of the network. However, the architecture of SDN and OpenFlow are susceptible to scalability and reliability issues. In fact, the problem of determining the requisite number of controllers and their locations in SDNs while maximizing the fault tolerance, i.e., Controller Placement Problem (CPP), is NP-hard. Besides, the communication latency between the forwarding nodes and the controllers are usually very high in large-scale SDNs due to sparse deployment. This paper first formulates the CPP as Nonlinear Programming (NLP). Then, we present an efficient greedy heuristic method that employs a local Optimized High Degree and Independent Dominating Set (OHDIDS) strategy to address these issues. In particular, we examine the CPP based on Silhouette analysis, Gap Statistics, and Faster Partitioning Around Medoids (FPAM) techniques. We conduct extensive experiments using Internet Topology Zoo and Mininet network simulators to show the efficacy of the proposed method with various network topologies. The proposed method outperforms the state-of-the-art methods in terms of the minimum number of controllers, average-case and worst-case latency, and reliability. We observed that our method reduces the average communication latency by $$57\%$$ 57 % when two controllers are used rather than a single controller. Moreover, it achieves a performance gain of up to $$19.31\%$$ 19.31 % in terms of average propagation latency.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s11042-021-11545-0,en,Data-driven intelligent decision for multimedia medical management,"['OriginalPaper', '1214: Multimedia Medical Data-driven Decision Making']","Since medical diagnosis runs through the whole journey of human health, the traditional medical diagnosis methods cannot ensure the diagnosis accurate due to the interference of multiple external factors. In response, this paper proposes a multimedia medical management method supported by the data-driven intelligent decision (MMD-DI). This method can be used to predict the survival time of cancer patients under the help of gradient boosting decision tree (GBDT) and hybrid neural network model. First, the feature factors were scanned to match the conditions by GBDT, according to the set value domain; Then the factors were inputted into the neural network. The hybrid neural network was employed to predict the survival time of cancer patients, and it was constructed by combining the convolutional neural network (CNN) and the long short-term memory (LSTM) model. Finally, the stability of the proposed MMD-DI was analyzed, and performance was compared with a series of commonly exploited baseline methods: the mean of cross-validated RMSE (Root Mean Squared Error) evaluation results is 0.183, the mean of cross-validated MAE (Mean Absolute Error) evaluation results is 0.147, the two indicators are both much lower than the commonly exploited baseline methods. A series of experiments proved that MMD-DI has excellent performance and can be used in the multimedia medical management systems.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11071-022-08002-4,en,Visibility graph for time series prediction and image classification: a review,"['ReviewPaper', 'Review']","The analysis of time series and images is significant across different fields due to their widespread applications. In the past few decades, many approaches have been developed, including data-driven artificial intelligence methods, mechanism-driven physical methods, and hybrid mechanism and data-driven models. Complex networks have been used to model numerous complex systems due to its characteristics, including time series prediction and image classification. In order to map time series and images into complex networks, many visibility graph algorithms have been developed, such as horizontal visibility graph, limited penetrable visibility graph, multiplex visibility graph, and image visibility graph. The family of visibility graph algorithms will construct different types of complex networks, including (un-) weighted, (un-) directed, and (single-) multi-layered networks, thereby focusing on different kinds of properties. Different types of visibility graph algorithms will be reviewed in this paper. Through exploring the topological structure and information in the network based on statistical physics, the property of time series and images can be discovered. In order to forecast (multivariate) time series, several variations of local random walk algorithms and different information fusion approaches are applied to measure the similarity between nodes in the network. Different forecasting frameworks are also proposed to consider the information in the time series based on the similarity. In order to classify the image, several machine learning models (such as support vector machine and linear discriminant) are used to classify images based on global features, local features, and multiplex features. Through various simulations on a variety of datasets, researchers have found that the visibility graph algorithm outperformed existing algorithms, both in time series prediction and image classification. Clearly, complex networks are closely connected with time series and images by visibility graph algorithms, rendering complex networks to be an important tool for understanding the characteristics of time series and images. Finally, we conclude in the last section with future outlooks for the visibility graph.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s11069-022-05525-2,en,"Integration of convolutional neural networks for flood risk mapping in Tuscany, Italy","['OriginalPaper', 'Original Paper']","Machine learning-based methodologies have depicted remarkable performance in digital processing of remote sensing imagery. In this work, we propose an integration of hazard susceptibility and vulnerability assessment in flood risk mapping using a CNN—based methodological framework. For this reason, we used nine predictor variables and a flood inventory from past flood events in a part of Tuscany region to train the model. Following a successful learning procedure, the performance of the proposed model was evaluated on a test dataset and depicted a promising prediction accuracy (95%). The analysis of the flood susceptibility map indicated that 4.7 and 2% of the entire study area depict very high and high susceptibility to future flood occurrences, respectively, corresponding to total areas of 44.06 and 19.33 km 2 . Flood risk map depicts those land cover categories that will be severely affected in a future flood event. Among them, a large part of Livorno and a few industrial buildings were highlighted as areas of very high risk.","['Earth Sciences', 'Natural Hazards', 'Hydrogeology', 'Geophysics/Geodesy', 'Geotechnical Engineering & Applied Earth Sciences', 'Civil Engineering', 'Environmental Management']"
doi:10.1007/s40998-022-00515-3,en,An Optimally Tuned Rotation Forest-Based Local Protection Scheme for Detecting High-Impedance Faults in Six-Phase Transmission Line During Nonlinear Loading,"['OriginalPaper', 'Research Paper']","In spite of the higher power transmission capability, the proliferation of six phase system has been hindered due to the challenge toward developing an accurate and reliable protection scheme. The primary challenge pertaining to the increased number of fault types is further enhanced during high-impedance fault (HIF) and nonlinear loading. In this regard a protection scheme for six-phase system is proposed, which is based on the extraction of harmonic information from raw signals followed by formulating the protection tasks as a classification problem and solving it using rotation forest (RoF). The hyperparameters of a set of RoF-based classifier modules have been tuned using grey wolf optimization to achieve high accuracy and selectivity in fault detection and classification. The test results over a wide range of operating scenarios of six-phase system reveal that the proposed scheme is able to reliably execute the protection tasks for all types of faults. With the inclusion of harmonic information, the scheme is able to achieve superior performance for HIF and nonlinear loading as compared to the reported techniques. For the validation dataset, comprising of wide range of operating conditions including stressed scenarios, the proposed scheme is able to achieve a dependability, security and accuracy of 99.46, 99.7 and 99.47%, respectively.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s10579-022-09608-1,en,Redundancy and coverage aware enriched dragonfly-FL single document summarization,"['OriginalPaper', 'Original Paper']","Due to the massive amount of information accessible on the internet, it has become a challenging task for users to discover the desired information. Automatic document summarization has become an emerging technology to address these issues. This allows the users to get the relevant information in a shortened version. However, the summary should have high content coverage and low redundancy to generate a good quality summary. Therefore, an enriched Dragonfly-Fuzzy Logic (FL) Single Document summarization is presented in this paper. Initially, the web document is preprocessed in which some functions such as segmentation, stop word removal, URL removal, stemming, etc. are performed. After preprocessing, the important features such as sentence location, proper nouns, numeric data, cue phrases, etc. are extracted from the web document. Here, the significance of the extracted features is decided by providing weights to each of the features using the Enriched Dragonfly Optimization Algorithm (EDOA). Once the weights are allotted for the features, the importance of the sentence is determined by using the FL system to form a summary. Finally, the sentence similarity in the generated summary is calculated, and then the similar sentences are eliminated from the summary to avoid redundancy issues. The performance of the proposed Dragonfly-FL summarization is tested in the CNN/Daily Mail dataset, and finally, the results are compared with the existing techniques such as MAMHOA, ExDoS, Karci summarization, and regression-based technique, DSN, Semantic approach, and BERTSUMEXT in terms of ROUGE-1, ROUGE-2, and ROUGE-L measures. The observation demonstrates that the proposed technique performs better than the existing techniques with precision, recall, and an F-score of 0.11, 0.05, and 0.01 respectively.","['Linguistics', 'Computational Linguistics', 'Computer Science, general', 'Linguistics, general', 'Language and Literature']"
doi:10.1007/s00530-021-00785-7,en,Multi-modal cyber-aggression detection with feature optimization by firefly algorithm,"['OriginalPaper', 'Special Issue Paper']","Aggressive comments containing offensive images and inappropriate gesture signs together with textual comments have grown exponentially in the recent past on social media. These aggressive contents on social media are affecting the victims negatively causing fear, stress, sleeping problems and even suicide in some cases. Since social media contents are unmoderated, a technical solution with the characteristic of having automatic flagging of these contents considering the text and images together is highly needed. This article presents a deep learning and binary firefly-based optimization-based model to classify the social media posts into high-aggressive, medium-aggressive, and non-aggressive classes. The proposed model considers both text and images together to evaluate the aggression level of a post. In this model, the image features of the posts are extracted using pre-trained VGG-16 model, whereas the textual features are extracted using a three-layered convolutional neural network in parallel. The image and text features are then combined to get a hybrid feature set which is further optimized using a binary firefly optimization algorithm. Our proposed model improves the results by 11% in terms of the weighted F1-score with optimized features by binary firefly algorithm over non-optimized features.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00366-022-01599-5,en,"A novel topology framework for simultaneous topology, size and shape optimization of trusses under static, free vibration and transient behavior","['OriginalPaper', 'Original Article']","This article proposes a novel topology framework for simultaneously optimizing topology, size and shape of truss structures with multiple constraints under static, free vibration and transient responses for the first time. To achieve such a purpose, the topology pseudo-area variable of members is newly proposed discretely assigning to either $$10^{-3}$$ 10 - 3 or 1 to respectively represent the absence or presence of a member. This suggestion aims at not only evading the numerical instability due to the singularity of global stiffness matrix when solving equilibrium equations in finite element analyses but also saving the computational effort owing to the intact preserve of FE model structure. The objective function of this study is to minimize the structural weight. The cross-sectional area of truss members is taken discrete/continuous design variables into account, whilst nodal coordinates are treated as continuous ones. In addition, kinematic stability, displacement, stress, Euler buckling loading, natural frequency and transient behavior are dealt with as constraints. The derivative-free adaptive hybrid evolutionary firefly algorithm is utilized as an optimizer to resolve such optimization problems including mixed continuous-discrete variables. A large number of benchmark examples are tested to verify the validity of the presented paradigm. Obtained outcomes indicate that the present methodology is effective and robust in searching better high-quality optimal solutions against many existing algorithms in the literature.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s12652-021-03226-5,en,Binary Jaya algorithm based on binary similarity measure for feature selection,"['OriginalPaper', 'Original Research']","Feature selection (FS) has become an indispensable data preprocessing task because of the huge amount of high dimensional data being generated by current technologies. These high dimensional data contains irrelevant, redundant, and noisy features that deteriorate classification accuracy. FS reduces dimensionality by removing the unwanted features thus improves classification accuracy. FS can be considered as a binary optimization problem. In order to solve this problem, this work proposes a new wrapper feature selection technique based on the Jaya algorithm. Three binary variants of the Jaya algorithm are proposed, the first and second ones are based on transfer functions namely BJaya-S and BJaya-V. The third variant (BJaya-JS) explores the search space on the basis of the Jaccard Similarity index. In addition, a probability-based local search technique, namely Neighbourhood Search is proposed to balance the exploration and exploitation. The variants of Jaya algorithm are evaluated and the best variant is selected. The best variant is further compared with six state-of-the-art feature selection techniques. All the performances are tested on 18 high dimensional standard UCI datasets. Experimental result comparison shows that the proposed feature selection technique performs better than other competitors.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s12008-022-00869-2,en,A hybrid MCDM approach for parametric optimization of a micro-EDM process,"['OriginalPaper', 'Original Paper']","In modern day manufacturing industries, micro-electrical discharge machining (micro-EDM) has emerged out as an efficient material removal process to produce miniaturized components having varying industrial applications. To explore its fullest machining potential, it is always required to operate the micro-EDM process while setting its various input parameters at their optimal levels. In this paper, four popular multi-criteria decision making (MCDM) techniques, in the form of weighted aggregated sum product assessment, technique for order of preference by similarity to ideal solution, combinative distance-based assessment and complex proportional assessment are separately hybridized with teaching-learning-based optimization (TLBO) algorithm to solve the parametric optimization problems of a micro-EDM process. The polynomial regression (PR) models are considered here as the inputs to these hybrid optimizers. Their optimization performance is subsequently validated against the conventionally adopted weighted sum multi-objective optimization (WSMO) approach at four different weight scenarios. It is revealed that for the micro-EDM process, all the MCDM-PR-TLBO approaches provide better solutions as compared to PR-WSMO-TLBO method for the considered weight scenarios. The best performance of the MCDM-PR-TLBO approaches is achieved when 50% weight is assigned to material removal rate. Moreover, it is also noticed that MCDM-PR-TLBO approaches are less computationally intensive than PR-WSMO-TLBO with approximately 9.61–26.70% saving in computational time.","['Engineering', 'Engineering, general', 'Engineering Design', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Electronics and Microelectronics, Instrumentation', 'Industrial Design']"
doi:10.1007/s11600-022-00942-0,en,The hydrological impact of tropical cyclones on soil moisture using a sensor based hybrid deep learning model,"['OriginalPaper', 'Research Article - Special Issue']","Tropical cyclones that originate from the Indian Ocean affect the Indian Sub-Continent. Heavy rainfall and flooding occur because of these cyclones. South Odisha was affected by Cyclonic Storm Daye in September 2018 and Cyclonic Storm Titli was occurred in August affecting Andhra Pradesh and Odisha as well. The Eastern portion of India was affected by the Cyclonic Storm Fani in April 2019. In May 2020, West Bengal was affected by the Amphan which is a Super Cyclonic Storm and in the same year Tamil Nadu was affected by the very severe Cyclonic Storm Nivar in November 2020. These are just a few of the notable cyclonic events in the Indian Sub-Continent. These cyclonic events cause a dramatic change in a very short time from dry soil to exceptional flooding. In this proposed work, we are attempting to create an observations-driven prediction model to quantify the soil moisture variations daily, predict county-based meteorology and evaluate the cause of cyclones and heavy rainfall in certain areas of India. In our work, we applied a deep learning-based methodology to predict soil moisture. For the prediction model, we fused Feed Forward Neural Networks with the Gated Recurrent Unit (GRU) model and present the prediction results. We have used climatic as well as environmental data published by the Indian Meteorological Department (IMD) Warning from 2011. The collected data is time-series data. Comparisons and the relationship that exists between soil moisture and meteorological data are made and analyzed. The soil moisture of the South Indian states Karnataka, Andhra Pradesh and Tamil Nadu are predicted from weather data using a hybrid deep learning model. The evaluations of the proposed work using Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and R-squared (R 2 ) against Non-hybrid Neural Network models such as Artificial Neural Networks (ANN), Convolutional Neural Networks, and Gated Recurrent Unit (GRU) models is analyzes where our model has given better results.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s40096-021-00424-2,en,Solving Dirichlet boundary problems for ODEs via swarm intelligence,"['OriginalPaper', 'Original Research']","In this paper, we examine the effects of swarm intelligence techniques on the obtaining of numerical solutions of ordinary differential equations (ODEs) with Dirichlet boundary conditions (DBCs) via feed-forward neural networks. Population-based global optimization methods such as artificial bee colony (ABC), ant colony optimization (ACO), gravitational search algorithm (GSA) and particle swarm optimization (PSO) are utilized in order to solve ODEs with DBCs numerically. Furthermore, we hybridize ACO, ABC and GSA with PSO to improve the numerical solution of optimization algorithms. Unlike the approaches in the literature, we use the personal best and global best solutions of PSO for the updating position of employed and onlooker bees in ABC, in the hybrid method that combines PSO and ABC. We also prefer the adaptive number of swarm scheme on the hybrid system of PSO and ACO. In the experiments, we give some different Dirichlet Boundary Problems to indicate the efficiency of optimization methods. Also the results are compared with the well-known traditional methods as shooting method, finite difference method and Lobatto IIIa.","['Mathematics', 'Applications of Mathematics']"
doi:10.1007/s41939-022-00124-x,en,Novel hybrid AOA and ALO optimized supervised machine learning approaches to predict the compressive strength of admixed concrete containing fly ash and micro-silica,"['OriginalPaper', 'Original Paper']","Detecting the impact of admixtures like fly ash and micro-silica on the mechanical property of concrete, especially the compressive strength (CS), earned a lot of attention not only in the concrete industry but also in future extended research and analysis. In this study, two innovative methods of hybrid support vector regression optimized by Arithmetic Optimization Algorithm and Antlion Optimization Algorithm named AOSVR and ALSVR were developed to generate accurately a trustable relationship between the feeding input (eight ingredients) of the model and the target values that optimizers by finding key variables of SVR lead to model precisely. These models applied to perform a prediction process of CS values for 170 High-Performance Concrete (HPC) samples. It can be concluded that the coefficient of determination values showed 0.9872 and 0.9850 for AOSVR and ALSVR, respectively. Moreover, the hybrid AOSVR model outperformed the most premier accuracy in the prediction of CS. Also, using these hybrid models helps diminish the cost of concrete testing and further the analysis of concrete mechanical characterization.","['Engineering', 'Solid Mechanics', 'Characterization and Evaluation of Materials', 'Mechanical Engineering', 'Numerical and Computational Physics, Simulation', 'Mathematical Applications in the Physical Sciences']"
doi:10.1007/s11045-022-00845-9,en,Neural network-based blended ensemble learning for speech emotion recognition,OriginalPaper,"Speech Emotion Recognition (SER) identifies human emotion from short speech signals that enable natural Human Computer Interactions(HCI). Accurate emotion prediction is required to create real-time interactive applications. Inaccurate or wrong predictions may create annoying situations in real-time situations. In addition to linguistic cues, human speech signals consist of numerous hidden features like cepstral, prosodic, spectrograms etc., for determining emotions. Handling only a set of features like cepstral or prosodic or spectrogram for SER does not accurately classify emotion. Machine learning models and artificial neural network architectures used in the existing works of SER individually have their abilities to handle either temporal cues or spatial cues. This research proposes a Neural Network-based Blended Ensemble Learning (NNBEL) model. This model stacks the predictions made by individual neural networks, that are capable of handling both temporal and spatial cues. The proposed model is ensembled with state-of-the-art neural network architecture, viz. 1-Dimensional Convolution Neural Network (1D CNN), Long Short-Term Memory (LSTM), and CapsuleNets. The first two architectures are especially suitable for handling speech-like time-series data, and CapsuleNets are ideal for taking spatial speech cues. Log mel-spectrogram is fed to LSTM and Mel Frequency Cepstral Coefficients (MFCCs) are fed to 1D-CNN and CapsuleNets. The predicted emotions of each of these neural networks are fed to Multi-Layer Perceptron (MLP) at the next level for predicting final emotion. The objective of considering Blended Ensemble learning is to consider the coarse-tuning of classification in the first layer of NNBEL with models namely, 1D-CNN, LSTM, CapsuleNets, and fine-tuning of classification at the second layer with Meta Classifier, MLP. The NNBEL model and individual base models are evaluated on RAVDESS and IEMOCAP datasets. The proposed model has achieved a classification accuracy of 95.3% on RAVDESS and 94% on IEMOCAP. The performance of this model is outstanding over the base models and existing models in the literature. The confusion matrix also shows a clear improvement in distinguishing the emotions.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Artificial Intelligence']"
doi:10.1007/s12161-022-02384-2,en,A Deep Learning Framework for Grocery Product Detection and Recognition,OriginalPaper,"Object detection and recognition are the most important and challenging problems in computer vision. The remarkable advancements in deep learning techniques have significantly accelerated the momentum of object detection/recognition in recent years. Meanwhile, text detection/recognition is also a critical task in computer vision and has gotten more attention from many researchers due to its wide range of applications. This work focuses on detecting and recognizing multiple retail products stacked on the shelves and off the shelves in the grocery stores by identifying the label texts. In this paper, we proposed a new framework is composed of three modules: (a) retail product detection, (b) product-text detection, (c) product-text recognition. In the first module, on-the-shelf and off-the-shelf retail products are detected using the YOLOv5 object detection algorithm. In the second module, we improve the performance of the state-of-the-art text detection algorithm by replacing the backbone network with ResNet50 + FPN and by introducing a new post-processing technique, Width Height based Bounding Box Reconstruction, to mitigate the problem of inaccurate text detection. In the final module, we used a state-of-the-art text recognition model to recognize the retail product’s text information. The YOLOv5 algorithm accurately detects both on-the-shelf and off-the-shelf grocery products from the video frames and the static images. The experimental results show that the proposed post-processing approach improves the performance of the existing methods on both regular and irregular text. The robust text detection and text recognition methods greatly support our proposed framework to recognize the on-the-shelf retail products by extracting product information such as product name, brand name, price, and expiring date. The recognized text contexts around the retail products can be used as the identifier to distinguish the product.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Microbiology', 'Analytical Chemistry']"
doi:10.1007/s41403-022-00363-x,en,Adadelta-Based BPANN Controller for Online Equivalent Parameter Estimation of Squirrel Cage Induction Motor Drive,"['OriginalPaper', 'Original Article']","This paper describes the development of a unique BPANN controller to estimate the equivalent circuit parameters (ECPs) for an indirect field-oriented control (IFOC) voltage source inverter-based induction motor (VSI-IM) drive. The drive controller deals with the challenges for continuous estimation of ECPs during the running condition such that the error in ECP evaluation due to the temperature rise of the drive motor and sudden changes in the loading is minimized. The proposed controller takes the challenge of accurate estimation of ECPs under these two running conditions by mitigating the errors in running speed based on two models. One is the plant model to estimate the ECPs during running conditions while the other one is the reference model. The H – G diagram method is utilized in the reference model to estimate the reference ECPs before starting the motor without performing any physical tests. The technique of the back-propagation algorithm with artificial neural network (BPANN) is utilized in the plant model, the weight and gain parameters of which are tuned with the help of already estimated reference ECPs. The Adadelta rule is utilized for fast tuning of the BPANN weights during starting transient of the motor while stator temperature and other feedback enhance the overall performance with an increase in accuracy of speed regulation. Depending upon the evaluated parameters, the IFOC algorithm determines the required duty ratio and the modulation index for the SVM to control the power switches of this VSI. The results from MATLAB-based simulation and a hardware prototype controller using a DSPIC microcontroller with different running conditions show the efficacy of the proposed algorithm. Besides, a comparison of achieving ECPs with other works also justifies the proposed work.","['Engineering', 'Engineering, general']"
doi:10.1007/s10766-022-00736-3,en,Parallelization of Swarm Intelligence Algorithms: Literature Review,OriginalPaper,"Swarm Intelligence (SI) algorithms are frequently applied to tackle complex optimization problems. SI is especially used when good solutions are requested for NP hard problems within a reasonable response time. And when such problems possess a very high dimensionality, a dynamic nature, or present intrinsic complex intertwined independent variables, computational costs for SI algorithms may still be too high. Therefore, new approaches and hardware support are needed to speed up processing. Nowadays, with the popularization of GPU and multi-core processing, parallel versions of SI algorithms can provide the required performance on those though problems. This paper aims to describe the state of the art of such approaches, to summarize the key points addressed, and also to identify the research gaps that could be addressed better. The scope of this review considers recent papers mainly focusing on parallel implementations of the most frequently used SI algorithms. The use of nested parallelism is of particular interest, since one level of parallelism is often not sufficient to exploit the computational power of contemporary parallel hardware. The sources were main scientific databases and filtered accordingly to the set requirements of this literature review.","['Computer Science', 'Theory of Computation', 'Processor Architectures', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s10899-021-10097-0,en,Using “Markers of Harm” to Track Risky Gambling in Two Cohorts of Online Sports Bettors,"['OriginalPaper', 'Original Paper']","Online gambling poses novel risks for problem gambling, but also unique opportunities to detect and intervene with at-risk users. A consortium of gambling companies recently committed to using nine behavioral ""Markers of Harm'' that can be calculated with online user data to estimate risk for gambling-related harm. The current study evaluates these markers in two independent samples of sports bettors, collected ten years apart. We find over a two-year period that most users never had high enough overall risk scores to indicate that they would have received an intervention. This observation is partly due to characteristics of our samples that are associated with lower risk for gambling-related harm, but might also be due to overly high risk thresholds or flaws in the design of some markers. Users with higher average risk scores had more intraindividual variability in risk scores. Younger age and male gender were not associated with higher average risk scores. The most active users were more likely than other users to have ever exceeded risk thresholds. Several risk scores significantly predicted proxies of gambling-related harm (e.g., account closure). Overall, the current Markers of Harm system has some correctable limitations that future risk detection systems should consider adopting.","['Medicine & Public Health', 'Psychiatry', 'Sociology, general', 'Community and Environmental Psychology', 'Economics, general']"
doi:10.1007/s11042-022-13147-w,en,Deep-EmoRU: mining emotions from roman urdu text using deep learning ensemble,OriginalPaper,"Detecting emotions play a vital role in our lives. In various ways, people convey their feelings, i.e., facial expressions, movements, speech, and text. This study aims to classify the emotions from Roman Urdu’s text. Much research has previously been done on different emotion detection languages, but there is minimal work done in Roman Urdu. There is also a need to explore Roman Urdu, as it is the most widely used social media site for communication. The absence of a benchmark corpus for emotion detection from text is a significant problem for Roman Urdu because language assets are essential for various tasks of natural language processing (NLP). The emotional analysis has many practical applications, such as optimizing product quality, dialog systems, investment patterns, and mental health. In this research, we build a corpus of 18k sentences collected from different domains and annotate it with six other classes to concentrate on the emotional polarity of the Roman Urdu text. We also proposed a Deep-EmoRU model for emotion detection from Roman Urdu text. Our proposed model is based on Long short-term memory (LSTM) and Convolutional neural network (CNN) feature learners. We applied different baseline algorithms like LSTM, Adaboost, XGboost, Random Forest, MLP, SVM, Decision tree, and KNN on our corpus. After experimentation and evaluation, the results showed that our model achieves a better F-measure score than LSTM, KNN, SVM, Adaboost, XGboost, MLP, Decision tree, and Random Forest. We achieve an accuracy of 82.2% and an F-measure of 0.82 on Emotion Detection for Roman Urdu.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00500-022-07437-6,en,An intelligent approach for estimating aeration efficiency in stepped cascades: optimized support vector regression models and mutual information theory,"['OriginalPaper', 'Application of soft computing']","Soft computing (SC) methods have increasingly been used to solve complex hydraulic engineering problems, especially those characterized by high uncertainty. SC approaches have previously proved to be an accurate tool for predicting the aeration efficiency coefficient (E20) in hydraulic structures such as weirs and flumes. In this study, the performance of the standalone support vector regression (SVR) algorithm and three of its hybrid versions, support vector regression–firefly algorithm (SVR-FA), support vector regression–grasshopper optimization algorithm (SVR-GOA), and support vector regression–artificial bee colony (SVR-ABC), is assessed for the prediction of E20 in stepped cascades. Mutual information theory is used to construct input variable combinations for prediction, including the parameters unit discharge ( q ), the total number of steps ( N ), step height ( h ), chute overall length ( L ), and chute inclination ( $$\alpha $$ α ). Entropy indicators, such as maximum likelihood, Jeffrey, Laplace, Schurmann–Grassberger, and minimax, are computed to quantify the epistemic uncertainty associated with the models. Four indices—correlation coefficient (R), Nash–Sutcliffe efficiency (NSE), root mean square error (RMSE), and mean absolute error (MAE)—are employed for evaluating the models’ prediction performance. The models’ outputs reveal that the SVR-FA model (with $$R = 0.947, {\text{NSE}} = 0.888, {\text{RMSE}} = 0.048\;{\text{and}}\;{\text{MAE}} = 0.027$$ R = 0.947 , NSE = 0.888 , RMSE = 0.048 and MAE = 0.027 in testing phase) has the best performance among all the models considered. The input variable combination, including q , N , h , and L , provides the best predictions with the SVR, SVR-FA, and SVR-GOA models. From the uncertainty analysis, the SVR-FA model shows the closest entropy values to the observed ones (3.630 vs. 3.628 for the “classic” entropy method and 3.647 vs. 3.643 on average for the Bayesian entropy method). This study proves that SC algorithms can be highly accurate in simulating aeration efficiency in stepped cascades and provide a valid alternative to the traditional empirical equation.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s13198-021-01497-x,en,Stability analysis of long-span prestressed steel structure of elevated station using edge computing,"['OriginalPaper', 'Original article']","The purpose is to improve the efficiency and accuracy of stability analysis of long-span prestressed steel structure of viaduct, and solve the problem of poor visualization and parameterization of traditional prestressed steel structure stability analysis. The application of edge computing technology in the stability analysis of structural steel is explored. As an effective supplement to cloud computing, edge computing is a technology for data analysis and processing at the near end of information generation. Because of its remarkable advantages in agility and real-time, it is widely used in typical industry application scenarios such as predictive maintenance, energy efficiency management and intelligent manufacturing, but its use in the field of building construction is little known. Therefore, based on the method of steel structure joint stability judgment and the technical characteristics of building information modeling (BIM) and edge computing, a BIM and edge computing long-span prestressed steel structure stability analysis system composed of three-layer structure of data acquisition layer, edge computing layer and comprehensive scheduling layer is composed. With the engineering construction project of the actual elevated station as the research object, the stable bearing capacity of the steel structure is solved through the automatic information collection, calculation and analysis process of the system, and the practical application process of the system is described. It effectively confirms the multiple advantages of edge computing and BIM technology in the stability analysis of prestressed steel structures compared with traditional analysis methods in visualization, plotting and real-time, and expands the possibility of the application of edge computing technology in the field of building construction. It is concluded that the stability analysis process of long-span prestressed steel structure based on edge computing is efficient and convenient, and has strong practicability and application. Meanwhile, it is of great significance to solve the problem of safety collaborative management in steel structure construction, improve the ability of project management and construction efficiency, and finally ensure the smooth progress of the project.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s00530-020-00672-7,en,Multi-input integrative learning using deep neural networks and transfer learning for cyberbullying detection in real-time code-mix data,"['OriginalPaper', 'Special Issue Paper']","Automatic detection of cyberbullying in social media content is a natural language understanding and generic text classification task. The cultural diversities, country-specific trending topics hash-tags on social media, the unconventional use of typographical resources such as capitals, punctuation, emojis and easy availability of native language keyboards add to the variety and volume of user-generated content compounding the linguistic challenges. This research focuses on cyberbullying detection in the code-mix data, specifically the Hinglish, which refers to the juxtaposition of words from the Hindi and English languages. We explore the problem of cyberbullying prediction and propose MIIL-DNN, a multi-input integrative learning model based on deep neural networks. MIIL-DNN combines information from three sub-networks to detect and classify bully content in real-time code-mix data. It takes three inputs, namely English language features, Hindi language features (transliterated Hindi converted to the Hindi language) and typographic features, which are learned separately using sub-networks (capsule network for English, bi-LSTM for Hindi and MLP for typographic). These are then combined into one unified representation to be used as the input for a final regression output with linear activation. The advantage of using this model-level multi-lingual fusion is that it operates with the unique distribution of each input type without increasing the dimensionality of the input space. The robustness of the technique is validated on two datasets created by scraping data from the popular social networking sites, namely Twitter and Facebook. Experimental evaluation reveals that MIIL-DNN achieves superlative performance in terms of AUC-ROC curve on both the datasets.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s11219-022-09586-1,en,Using artificial neural networks to provide guidance in extending PL/SQL programs,OriginalPaper,"Extending legacy systems with new objects for contemporary functionality or technology can lead to architecture erosion. Misplacement of these objects gradually hampers the modular structure, of which documentation is usually missing or outdated. In this work, we aim at addressing this problem for PL/SQL programs, which are highly coupled with databases. We propose a novel approach that employs artificial neural networks to automatically predict the correct placement of a new object among architectural modules. We train a network based on features extracted from the initial version of the source code that is assumed to represent the intended architecture. We use dependencies among the software and database objects as features for this training. Then, given a new object and the list of other objects it uses, the network can predict the architectural module, where the object should be included. We performed two industrial case studies with applications from the telecommunications domain, each of which involves thousands of procedures and database tables. We showed that the accuracy of our approach is 86.7% and 89% for these two applications. The baseline approach that uses coupling and cohesion metrics reaches 55.5% and 57.4% accuracy for the same applications, respectively.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Programming Languages, Compilers, Interpreters', 'Data Structures and Information Theory', 'Operating Systems']"
doi:10.1007/s00530-020-00746-6,en,ParaCap: paraphrase detection model using capsule network,"['OriginalPaper', 'Special Issue Paper']","This paper is concerned withthe problem of paraphrase detection. For a number of applications, the ability to detect similar sentences, such as text mining, summary text, plagiarism detection, authorship authentication, and question answering, is important. Given two phrases, the goal is to detect whether they are identical semantically. This work involves a novel model namely, ParaCap, which uses capsule networks for the investigation of sentences. Capsule networks understand the spatial information (context, language, length of sentences and others) by using the instantiation parameters for the better results as compared to CNNs. For the objective, the Quora Question Pair dataset containing 404291 pairs of Quora Questions is being used. The ParaCap model outperforms many state-of-art methods, and also proves to be comparable to other techniques by achieving the accuracy of 89.19%.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s11600-022-00940-2,en,"Flood susceptibility mapping using advanced hybrid machine learning and CyGNSS: a case study of Nghe An province, Vietnam","['OriginalPaper', 'Research Article - Anthropogenic Geohazards']","Flooding is currently the most dangerous natural hazard. It can have heavy human and material impacts and, in recent years, flooding has tended to occur more frequently, due to changes our species has made to hydrological regimes, and due to climate change. It is of the utmost importance that new models are developed to predict and map flood susceptibility with high accuracy, to support decision-makers and planners in designing more effective flood management strategies. The objective of this study is the development of a new method based on state-of-the-art machine learning and remote sensing, namely random forest (RF), dingo optimization algorithm, a weighted chimp optimization algorithm (WChOA), and particle swarm optimization to build flood susceptibility maps in the Nghe An province of Vietnam. The CyGNSS system was used to collect soil moisture data to integrate into the susceptibility model. A total of 1650 flood locations and 14 conditioning factors were used to construct the model. These data were divided at a ratio of 60/20/20 to train, validate, and test the model, respectively. In addition, various statistical indices, namely root-mean-square error, receiver operation characteristic, mean absolute error, and the coefficient of determination ( R 2 ), were used to assess the performance of the model. The results for all the models were good, with an AUC value of + 0.9. The RF-WChOA model performed best, with an AUC value of 0.99. The proposed models can predict and map flood susceptibility with high accuracy.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s10479-020-03892-2,en,Solving a bi-objective unmanned aircraft system location-allocation problem,"['OriginalPaper', 'Original Research']","In this paper we introduce a bi-objective location-allocation problem for Unmanned Aircraft Systems (UASs) operating in a hostile environment. The objective is to find the locations to deploy UASs and assign Unmanned Aerial Vehicles to regions for surveillance. One of the objectives is to maximize search effectiveness, while the second is the minimization of the threats posed to the UASs. These two objectives are in conflict, because they are affected differently by the proximity between the UAS locations and the target regions. First, we have formulated this problem as a mixed integer nonlinear program. Next, we have developed its linearization which can be solved by a commercial optimizer for small-scale problem instances. To solve large-scale problems, we have adopted a well-known metaheuristic for multi-objective problems, namely the elitist non-dominated sorting genetic algorithm. We have also developed a hybrid approach, which has proven to be more effective than each approach alone.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s00366-021-01335-5,en,Adaptive momentum-based optimization to train deep neural network for simulating the static stability of the composite structure,"['OriginalPaper', 'Original Article']","This article is the first attempt to employ deep learning to estimate the mechanical performance of multi-phase systems. Features of the design-points are obtained with the aid of the fast-converging numerical method used to solve the governing motion equations developed according to the kinematics of shear deformable structures. The optimum values of the parameters involved in the mechanism of the fully-connected neural network are determined through the momentum-based optimizer. The strength of the method applied in this survey comes from the high accuracy besides lower epochs needed to train the multi-layered network. It should be mentioned that the mechanical characteristics of the structure are computed through a two-step micromechanical scheme including the Halpin–Tsai method. The accuracy of the employed approach is examined and verified through the comparison of the results with those published in the literature. The numerical results give the practical hint that increasing the content of the reinforcement phase not always equal to increasing the resistance of the composite structure toward static instability. Thus, designers must choose the weight content of nano or macro-scale reinforcements by considering the shape factors of these materials to boost the strength of the system appropriately.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s13198-021-01189-6,en,Exploration of students' fitness and health management using data mining technology,"['OriginalPaper', 'Original article']","Today, college students' fitness and health have become a major social concern, and the scientific management and planning of college students' fitness and health have become particularly important. The aim is to study the application of Internet of Things (IoT) technology, particularly, data mining (DM) in college students' fitness and health management. First, the current situation is explored for the DM technology in China. Then, the matrix-based Apriori algorithm and the C4.5 decision tree algorithm in the DM field are introduced for association rules mining and classification analysis of college students’ health data, respectively. Afterward, some 2018 college graduates are recruited, and their health status is studied using the combination of the matrix-based Apriori algorithm and the C4.5 decision tree algorithm. The results show that the specific associations of the respondents’ seven health dimensions are mined using the matrix-based Apriori algorithm, then the classification rules of health problems are obtained through the C4.5 decision tree algorithm, and respondents’ health problems are classified. Finally, a fitness and health management system based on matrix-based Apriori and C4.5 decision tree algorithms is established. The results provide a practical reference for schools to master students' health. Thus, the application of IoT technology in college students' fitness and health management can help schools and teachers master students’ health status and prevent college students' health problems scientifically.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s12539-022-00533-z,en,Deep Residual Network for Diagnosis of Retinal Diseases Using Optical Coherence Tomography Images,"['OriginalPaper', 'Original research article']","Diabetic retinopathy occurs due to damage to the blood vessels in the retina, and it is a major health problem in recent years that progresses slowly without recognizable symptoms. Optical coherence tomography (OCT) is a popular and widely used noninvasive imaging modality for the diagnosis of diabetic retinopathy. Accurate and early diagnosis of this disease using OCT images is crucial for the prevention of blindness. In recent years, several deep learning methods have been very successful in automating the process of detecting retinal diseases from OCT images. However, most methods face reliability and interpretability issues. In this study, we propose a deep residual network for the classification of four classes of retinal diseases, namely diabetic macular edema (DME), choroidal neovascularization (CNV), DRUSEN and NORMAL in OCT images. The proposed model is based on the popular architecture called ResNet50, which eliminates the vanishing gradient problem and is pre-trained on large dataset such as ImageNet and trained end-to-end on the publicly available OCT image dataset. We removed the fully connected layer of ResNet50 and placed our new fully connected block on top to improve the classification accuracy and avoid overfitting in the proposed model. The proposed model was trained and evaluated using different performance metrics, including receiver operating characteristic (ROC) curve on a dataset of 84,452 OCT images with expert disease grading as DRUSEN, CNV, DME and NORMAL. The proposed model provides an improved overall classification accuracy of 99.48% with only 5 misclassifications out of 968 test samples and outperforms existing methods on the same dataset. The results show that the proposed model is well suited for the diagnosis of retinal diseases in ophthalmology clinics. Graphical abstract ","['Life Sciences', 'Computer Appl. in Life Sciences', 'Computational Biology/Bioinformatics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Theoretical and Computational Chemistry', 'Theoretical, Mathematical and Computational Physics', 'Computational Science and Engineering']"
doi:10.1007/s00521-022-07611-9,en,A knowledge guided bacterial foraging optimization algorithm for many-objective optimization problems,"['OriginalPaper', 'Original Article']","Despite that evolutionary and swarm intelligence algorithms have achieved considerable success on multi-objective optimization problems, they face huge challenges when dealing with many-objective optimization problems (MaOPs). There is an urgent call for effective evolutionary and swarm intelligence algorithms for MaOPs. Inspired by the satisfactory performance of bacterial foraging optimization (BFO) on the single-objective optimization problems, this paper extends BFO to deal with MaOPs and proposes a knowledge guided BFO for MaOPs (called as KLBFO). Firstly, KLBFO learns promising direction knowledge based on group decision making idea to guide the population to converge toward proper directions. Secondly, KLBFO learns elite knowledge by a new biological mechanism to accelerate the population to converge. Thirdly, KLBFO learns density knowledge by a new diversity management strategy based on orthogonal grid to produce well-distributed solutions. The performance of KLBFO is comprehensively evaluated by comparing it with eight state-of-the-art algorithms on two suites of test problems and one real-world problem. The empirical results have validated the superior performance of KLBFO for MaOPs.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s40194-022-01379-1,en,A roadmap for selection of metal welding process: a review and proposals,"['ReviewPaper', 'Review Article']","This study aims to report the progress and latest status of the “selection of welding process” problem in terms of research, developments, and applications. In addition, it introduces guidelines to serve constructing future expert systems for the problem. Therefore, it presents an extensive literature review on the approaches used to model and solve the problem over 36 years. Hence, several findings and proposed insights are reported. The paper recommends some existing approaches based on their performance in general and literature reporting in addition to simple statistics. A structure for prospected expert systems is proposed. The paper collected and rearranged decision criteria/sub-criteria of the problem, in a manageable form, to construct a modifiable hierarchical scheme. Additional criteria were merged based on recent trends in manufacturing system evaluation such as sustainability and performability. Finally, an agenda is introduced to recognize research opportunities in this area based on prospected industrial and business revolutions.","['Materials Science', 'Metallic Materials', 'Solid Mechanics', 'Theoretical and Applied Mechanics']"
doi:10.1007/s10462-022-10147-y,en,Video super-resolution based on deep learning: a comprehensive survey,OriginalPaper,"Video super-resolution (VSR) is reconstructing high-resolution videos from low resolution ones. Recently, the VSR methods based on deep neural networks have made great progress. However, there is rarely systematical review on these methods. In this survey, we comprehensively investigate 37 state-of-the-art VSR methods based on deep learning. It is well known that the leverage of information contained in video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into seven sub-categories according to the ways of utilizing inter-frame information. Moreover, descriptions on the architecture design and implementation details are also included. Finally, we summarize and compare the performance of the representative VSR methods on some benchmark datasets. We also discuss the applications, and some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding of the VSR techniques based on deep learning.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s40998-022-00512-6,en,Human Detection in Surveillance Videos Based on Fine-Tuned MobileNetV2 for Effective Human Classification,"['OriginalPaper', 'Research Paper']","With the high rate of accidents and crimes around the world, the importance of video surveillance is growing every day and intelligent surveillance systems are being developed to perform surveillance tasks automatically. Detecting human beings accurately in a visual surveillance system is crucial for diverse application areas. The first step in the detection process is to detect moving objects. Then, the moving object could be classified either in the human class or in the non-human class. Human classification is an important process to build effective surveillance system. In this article, an efficient human detection algorithm is proposed by processing the regions of interest (ROI) based on a foreground estimation. In our proposal, we used MobileNetV2 deep convolution neural network, designed to be used in embedded devices, with transfer learning approach to build fine-tuned model for an efficient classification of ROI into human or not human. We train the fine-tuned model on INRIA person dataset using three scenarios. The resulting models were extensively evaluated on INRIA test dataset benchmark and they achieved an F-Score value of 98.35%, 98.72%, and 98.90% which we consider very satisfactory performance. The best fine-tuned model was used for the classification stage which achieved an accuracy of 98.42%, recall of 99.47%, precision of 98.34% and F-Score of 98.90%.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s00521-022-07676-6,en,HFANet: hierarchical feature fusion attention network for classification of glomerular immunofluorescence images,"['OriginalPaper', 'Original Article']","The chronic kidney disease (CKD) accompanied by permanent kidney damage, has become a heavy burden for worldwide public health. Clinically, glomerular immunofluorescence (IF) images are widely-used to reveal the occurrence probability and type of CKD. In histopathological assessment for glomerular IF image, multiple descriptive indicators are used to characterize deposits from different aspects, which suggest associated kidney lesions. In this paper, we design a hierarchical feature fusion attention network (HFANet) to classify two main descriptive indicators, namely fluorescence intensity and distribution pattern. Through the hierarchical feature fusion attention (HFA) module, HFANet supplements deep semantic features using shallow texture features to maximize its feature extraction capability and efficiency of information fusion. Different from directly adding or concatenating, HFANet weighted concatenates the feature maps from different hierarchies to highlight more discriminative regions. Further, by integrating HFANet with the proposed intensity equalization (IE) algorithm, U-Net++, and Grad-CAM, a computer-aided diagnostic system for glomerular IF images is constructed. With this system, the classification accuracy of the fluorescence intensity and distribution pattern reaches 90.48% and 90.87%, respectively. Extensive comparative experiments and ablation studies demonstrate that HFANet outperforms other universal backbones with the help of HFA module, and the classification performance of the devised system is comparable to senior pathologists. The heatmap given by the system, which is similar to the classification evidence used by the clinicians, can be used as diagnostic reference and training material for pathologists. The systematic demonstration video is available in the supplementary material.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12652-021-03271-0,en,Cognitive population initialization for swarm intelligence and evolutionary computing,"['OriginalPaper', 'Original Research']","Cognitive computing has been commonly used to address different forms of optimization issues. Swarm intelligence (SI) and evolutionary computing (EC) are population-based intelligent stochastic search techniques promoted to search for their food from the intrinsic way of bee swarming and human evolution. Initialization of populations is a critical factor in the Particle swarm optimization (PSO) algorithm that significantly affects diversity and convergence. Quasi-random sequences based on cognitive computing are more helpful in initializing the population than applying the random distribution for initialization to maximize diversity and convergence. The capacity of PSO is expanded to make it suitable for the optimization problem by adding new initialization techniques based on cognitive computing using the sequence of low discrepancies. The employed low discrepancies sequences included WELL named WE-PSO to solve the optimization problems in large-scale search spaces. The proposed approach has been tested on fifteen well-known uni-modal and multi-modal benchmark test problems extensively used in the literature. Also, WE-PSO efficiency has been compared to standard PSO, and two other Sobol-based PSO (SOB-PSO) and Halton-based PSO (HAL-PSO) initialization approach. The results were obtained to validate the efficiency and effectiveness of the proposed approach. Mean fitness values obtained using WE-PSO designate that WE-PSO is better than standard techniques in multi-modal problems. The computational results also show that the proposed technique outperformed and has a higher accuracy rate than the classical approaches. Besides, the proposed work’s result offers a foresight of how the proposed initialization approach has a significant effect on the importance of cost function, convergence, and diversity.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s10999-022-09612-x,en,Fracture strength of Graphene at high temperatures: data driven investigations supported by MD and analytical approaches,OriginalPaper,"The extraordinary opto-electronic and mechanical properties of Graphene makes it a popular material for several applications. However, defects like: cracks, and voids are unavoidable during its production, which can lead to poor properties. Furthermore, the fracture properties degrades at higher temperatures. In this study, the fracture strength of Graphene is investigated as a function of temperature, considering the influence of lattice orientation, initial crack size and its orientation. As a first step, an analytical model is developed to estimate the fracture strength of Graphene with respect to temperature, considering the above parameters. Later on, molecular dynamics simulations are performed with an included initial edge crack in ten different sizes and four orientations, at three particular lattice orientations, and operating at thirteen different temperatures. Finally, a deep machine learning model is developed to estimate the fracture strength of defective Graphene. Results from molecular dynamics simulations are used to train the developed deep machine learning model. Furthermore, the training is enhanced using transfer learning, where the weights and biases for the data set considering $$0^\circ$$ 0 ∘ lattice orientation are adopted in training the networks for $$13.9^\circ$$ 13 . 9 ∘ and $$30^\circ$$ 30 ∘ lattice orientations. Results from the developed deep machine learning model are validated by comparing them with the results from the analytical and molecular dynamics models and a good agreement is observed. Thus, a deep machine learning model has been proposed here to estimate the fracture strength of defective Graphene. The developed model serves as a tool for quick estimation fracture strength of defective Graphene.","['Engineering', 'Solid Mechanics', 'Classical Mechanics', 'Characterization and Evaluation of Materials', 'Engineering Design']"
doi:10.1007/s11517-022-02673-2,en,MLRD-Net: 3D multiscale local cross-channel residual denoising network for MRI-based brain tumor segmentation,"['OriginalPaper', 'Original Article']","The precise segmentation of multimodal MRI images is the primary stage of tumor diagnosis and treatment. Current segmentation strategies often underutilize multiscale features, which can easily lead to loss of contextual information, reduction of low-level features and noise interference. To overcome these issues, a 3D multiscale local cross-channel residual denoising network (MLRD-Net) for an MRI-based brain tumor segmentation algorithm is proposed in this paper. Specifically, we employ encoder-decoder structure to connect local and global features, and enhance the receptive field of the network. Random slice operation has been conducted to enhance robustness. Then, residual blocks with pre-activation operation are developed in down-sampling stage, which effectively improves signal propagation along the network and alleviates network overfitting. Finally, the local cross-channel denoising mechanism is established to eliminate unimportant features without dimensionality reduction. Our proposal was evaluated in Brain Tumor Segmentation 2020 dataset (BraTS 2020), obtaining significantly improved results with mean Dice Similarity Coefficient metric of 0.91, 0.79, and 0.73 for the complete, tumor core, and enhancing tumor regions respectively. Besides, we conduct further practice on BraTS 2019, with the mean Dice Similarity Coefficient metric of 0.89, 0.80, and 0.75. Massive experiments demonstrate that our method is powerful and reliable. It increases little model complexity while achieving very competitive performance. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s10489-022-03229-5,en,An optimal-score-based filter pruning for deep convolutional neural networks,OriginalPaper,"Convolutional Neural Networks (CNN) have achieved excellent performance in the processing of high-resolution images. Most of these networks contain many deep layers in quest of greater segmentation performance. However, over-sized CNN models result in overwhelming memory usage and large inference costs. Earlier studies have revealed that over-sized deep neural models tend to deal with abundant redundant filters that are very similar and provide tiny or no contribution in accelerating the inference of the model. Therefore, we have proposed a novel optimal-score-based filter pruning (OSFP) approach to prune redundant filters according to their relative similarity in feature space. OSFP not only speeds up learning in the network but also eradicates redundant filters leading to improvement in the segmentation performance. We empirically demonstrate on widely used segmentation network models (TernausNet, classical U-Net and VGG16 U-Net) and benchmark datasets (Inria Aerial Image Labeling Dataset and Aerial Imagery for Roof Segmentation (AIRS)) that computation costs (in terms of Float Point Operations (FLOPs) and parameters) are reduced significantly, while maintaining or even improving accuracy.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s40747-021-00605-5,en,A metaheuristic-based framework for index tracking with practical constraints,"['OriginalPaper', 'Original Article']","Recently, numerous investors have shifted from active strategies to passive strategies because the passive strategy approach affords stable returns over the long term. Index tracking is a popular passive strategy. Over the preceding year, most researchers handled this problem via a two-step procedure. However, such a method is a suboptimal global-local optimization technique that frequently results in uncertainty and poor performance. This paper introduces a framework to address the comprehensive index tracking problem (IPT) with a joint approach based on metaheuristics. The purpose of this approach is to globally optimize this problem, where optimization is measured by the tracking error and excess return. Sparsity, weights, assets under management, transaction fees, the full share restriction, and investment risk diversification are considered in this problem. However, these restrictions increase the complexity of the problem and make it a nondeterministic polynomial-time-hard problem. Metaheuristics compose the principal process of the proposed framework, as they balance a desirable tradeoff between the computational resource utilization and the quality of the obtained solution. This framework enables the constructed model to fit future data and facilitates the application of various metaheuristics. Competitive results are achieved by the proposed metaheuristic-based framework in the presented simulation.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00521-022-07838-6,en,"Machine learning-based CFD simulations: a review, models, open threats, and future tactics","['ReviewPaper', 'Review']","This review targets various scenarios where CFD could be used and the logical parts needed for exemplary computations. The machine learning aspect with algorithms that have been implemented suggests design parameters to an algorithm that can be used for bodies in flights and different research-based algorithms that have been used and outlines the advantages, disadvantages, and tools used for computing the algorithm. Since fluid behavior is quite erratic, a single algorithm may not be versatile in every case. In some cases, multiple algorithms are combined for successful simulations. The uniqueness of the review lies in the combination of algorithms for every different case with theoretical analysis and disadvantages, which could be avoided by clubbing another algorithm that overcomes the problem. Since ML is not fully mature yet to provide high accuracy without bit preprocessing in the form of the numerical method, this is one of the heavy limitations that are briefly discussed.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s40747-022-00738-1,en,A Lagrangian dual-based theory-guided deep neural network,"['OriginalPaper', 'Original Article']","The theory-guided neural network (TgNN) is a kind of method which improves the effectiveness and efficiency of neural network architectures by incorporating scientific knowledge or physical information. Despite its great success, the theory-guided (deep) neural network possesses certain limits when maintaining a tradeoff between training data and domain knowledge during the training process. In this paper, the Lagrangian dual-based TgNN (TgNN-LD) is proposed to improve the effectiveness of the training process. We convert the original loss function into a constrained form with several items, in which partial differential equations (PDEs), engineering controls (ECs), and expert knowledge (EK) are regarded as constraints, with one Lagrangian variable per constraint. These Lagrangian variables are incorporated to achieve an equitable trade-off between observation data and corresponding constraints, to improve prediction accuracy and training efficiency. To investigate the performance of the proposed method, the original TgNN model with a set of optimized weight values adjusted by ad-hoc procedures is compared on a subsurface flow problem, with their L2 error, R square (R2), and computational time being analyzed. Experimental results demonstrate the superiority of the Lagrangian dual-based TgNN.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11416-022-00416-3,en,ConRec: malware classification using convolutional recurrence,"['OriginalPaper', 'Original Paper']","Today, the extensive reliae on technology has exposed us to a constant threat of sophisticated malware attacks. Various automated malware production techniques have evolved, some of which reuse specific segments of existing malware to produce new malware, making malware detection challenging. In this paper, we propose a Convolutional Recurrence based malware classification technique that exploits the visual recurrences in the grayscale images of the malware samples belonging to the same malware families. Firstly, we convert the malware samples into grayscale images to capture the structural similarities from the malware samples using a Convolutional Neural Network architecture. Then we perform data augmentation to counter the effects of high data imbalance and reduce the class bias, such that training on that dataset would generate a more generalized framework. The augmented dataset is then passed through a VGG16 based feature extractor to extract the visual outliers amongst the malware families. Now, the extracted features are processed by passing them through two stacked BiLSTM layers. The outputs generated by the BiLSTM layers and the VGG16 layer are then merged to perform the final classification of the malware sample into its malware family. The model’s performance is further improved by using proper hyperparameter tuning. We compare the performance of our algorithm against several baseline methods and some contemporary state-of-the-art methods for visual malware detection by utilizing two benchmarked datasets. The obtained experimental results reveal the utility and efficacy of our proposed malware family classification technique.","['Computer Science', 'Computer Science, general']"
doi:10.1007/s11063-022-10879-6,en,Deep Learning Based-Virtual Screening Using 2D Pharmacophore Fingerprint in Drug Discovery,OriginalPaper,"Predicting biological activity and molecular properties is one of the most important goals in the pharmaceutical and bioinformatics field in order to discover potential new drugs. Although machine learning methods have been used in drug discovery for a long time with good efficacy, the use of deep learning has proven its superiority in most cases. In this paper, we present a virtual screening procedure based on deep learning that aims to classify a set of chemical compounds as regards their biological activity on a particular receptor. The molecules are described with 2D pharmacophore fingerprints, which use the coordinates of atoms in 2D space to calculate them. Two deep learning models are proposed, the first is a deep neural network that uses as input data the fingerprint represented as a 1D vector. The second model is a convolutional neural network that uses the same input data after reshaping it into a 2D vector. Our models were trained on a dataset of active and inactive chemical compounds on cyclin A kinase1 receptor a very important protein family. The results have proven that the proposed models are efficient and comparable with some widely used machine learning methods in drug discovery.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11548-022-02688-y,en,HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance,"['OriginalPaper', 'Original Article']","Purpose The success or failure of modern computer-assisted surgery procedures hinges on the precise six-degree-of-freedom (6DoF) position and orientation (pose) estimation of tracked instruments and tissue. In this paper, we present HMD-EgoPose, a single-shot learning-based approach to hand and object pose estimation and demonstrate state-of-the-art performance on a benchmark dataset for monocular red-green-blue (RGB) 6DoF marker-less hand and surgical instrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose framework for performant 6DoF pose estimation on a commercially available optical see-through head-mounted display (OST-HMD) through a low-latency streaming approach. Methods Our framework utilized an efficient convolutional neural network (CNN) backbone for multi-scale feature extraction and a set of subnetworks to jointly learn the 6DoF pose representation of the rigid surgical drill instrument and the grasping orientation of the hand of a user. To make our approach accessible to a commercially available OST-HMD, the Microsoft HoloLens 2, we created a pipeline for low-latency video and data communication with a high-performance computing workstation capable of optimized network inference. Results HMD-EgoPose outperformed current state-of-the-art approaches on a benchmark dataset for surgical tool pose estimation, achieving an average tool 3D vertex error of 11.0 mm on real data and furthering the progress towards a clinically viable marker-free tracking strategy. Through our low-latency streaming approach, we achieved a round trip latency of 199.1 ms for pose estimation and augmented visualization of the tracked model when integrated with the OST-HMD. Conclusion Our single-shot learned approach, which optimized 6DoF pose based on the joint interaction between the hand of a user and a rigid surgical drill, was robust to occlusion and complex surfaces and improved on current state-of-the-art approaches to marker-less tool and hand pose estimation. Further, we presented the feasibility of our approach for 6DoF object tracking on a commercially available OST-HMD.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s11047-021-09856-0,en,Exact Markov chain-based runtime analysis of a discrete particle swarm optimization algorithm on sorting and OneMax,OriginalPaper,"Meta-heuristics are powerful tools for solving optimization problems whose structural properties are unknown or cannot be exploited algorithmically. We propose such a meta-heuristic for a large class of optimization problems over discrete domains based on the particle swarm optimization (PSO) paradigm. We provide a comprehensive formal analysis of the performance of this algorithm on certain “easy” reference problems in a black-box setting, namely the sorting problem and the problem O ne M ax . In our analysis we use a Markov model of the proposed algorithm to obtain upper and lower bounds on its expected optimization time. Our bounds are essentially tight with respect to the Markov model. We show that for a suitable choice of algorithm parameters the expected optimization time is comparable to that of known algorithms and, furthermore, for other parameter regimes, the algorithm behaves less greedy and more explorative, which can be desirable in practice in order to escape local optima. Our analysis provides a precise insight on the tradeoff between optimization time and exploration. To obtain our results we introduce the notion of indistinguishability of states of a Markov chain and provide bounds on the solution of a recurrence equation with non-constant coefficients by integration.","['Computer Science', 'Theory of Computation', 'Evolutionary Biology', 'Processor Architectures', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s00202-022-01604-6,en,Power quality and stability improvement alongside decoupled control of frequency and voltage in doubly-fed induction generator by battery energy storage,"['OriginalPaper', 'Original Paper']","Renewable energy-based power systems, despite their numerous advantages, can lead to problems such as frequency fluctuations, voltage drops, and unstable power outputs. This paper suggests a control method for a doubly-fed induction generator (DFIG), equipped with battery energy storage system (BESS) to frequency, voltage control, and improve the fault-tolerant ability. In the proposed strategy, the main purpose is to provide a method for decoupled control of voltage and frequency of the rotor before injected into the network as well as to improve network stability in the presence of stabilizers and BESS. The proposed control strategy is applied to BESS and the rotor side of the converter of DFIG. In this strategy, each loop controller used a conventional PI controller with additional stabilizers to damp out the oscillations of frequency and voltage. In such a way, under various disturbances condition, system in the presence of a BESS has a fast response with fewer range of oscillation. Finally, parameters of the controllers and stabilizers are adjusted simultaneously by an intelligent algorithm. All the results are obtained by simulation in real-time condition with Simulink of MATLAB 2016b.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s10898-022-01177-5,en,Kernel density estimation based distributionally robust mean-CVaR portfolio optimization,OriginalPaper,"In this paper, by using weighted kernel density estimation (KDE) to approximate the continuous probability density function (PDF) of the portfolio loss, and to compute the corresponding approximated Conditional Value-at-Risk (CVaR), a KDE-based distributionally robust mean-CVaR portfolio optimization model is investigated. Its distributional uncertainty set (DUS) is defined indirectly by imposing the constraint on the weights in weighted KDE in terms of $$\phi $$ ϕ -divergence function in order that the corresponding infinite-dimensional space of PDF is converted into the finite-dimensional space on the weights. This makes the corresponding distributionally robust optimization (DRO) problem computationally tractable. We also prove that the optimal value and solution set of the KDE-based DRO problem converge to those of the portfolio optimization problem under the true distribution. Primary empirical test results show that the proposed model is meaningful.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Real Functions', 'Computer Science, general']"
doi:10.1007/s12008-022-00868-3,en,Multi-objective factors optimization in fused deposition modelling with particle swarm optimization and differential evolution,"['OriginalPaper', 'Original Paper']","The design of any system contemplates the elaboration of a prototype of the entire system or some parts, before the manufacturing phase. Nowadays, rapid prototyping (RP) is widely used by the designers. Achieving good manufacturing performances needs to handle various process parameters. Most works deal with single objective process parameters. The reality is quite different and the processes involve conflicting objectives. This paper addresses the multi-objective factors optimization of the fused deposition modelling (FDM) technology. The problem is converted into a single one using the weighted-sum method and then solved by resorting to two nature-inspired computing techniques, namely particle swarm optimization (PSO) and differential evolution (DE). The results obtained are compared.","['Engineering', 'Engineering, general', 'Engineering Design', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Electronics and Microelectronics, Instrumentation', 'Industrial Design']"
doi:10.1007/s12008-022-00842-z,en,Application of simultaneous evaluation of criteria and alternatives (SECA) method for parametric optimization of hybrid machining processes,"['OriginalPaper', 'Original Paper']","To cope with the ever increasing demands of higher productivity along with better surface finish and dimensional tolerance while machining different hard-to-cut work materials, hybrid machining processes are now being continuously developed considering the advantageous features of two or more non-traditional machining (NTM) processes. These processes involve simultaneous and controlled interaction of material removal mechanisms of the constituent NTM processes. In this paper, an attempt is put forward to explore the application potentiality of simultaneous evaluation of criteria and alternatives method as an efficient multi-objective optimization tool in identifying the optimal parametric intermixes of three hybrid machining processes, i.e. electrochemical discharge machining, abrasive water jet machining and electro-discharge diamond face grinding. It can be noticed that those optimal parametric combinations can lead to 1.34–57.97% improvements in the response values as compared to those derived by the past researchers. Thus, its application is highly recommended as a viable optimization tool in enhancing the capabilities of hybrid machining processes for their wider industrial applications.","['Engineering', 'Engineering, general', 'Engineering Design', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Electronics and Microelectronics, Instrumentation', 'Industrial Design']"
doi:10.1007/s00202-022-01620-6,en,Dynamic stability improvement of power system with DFIG using multi-input backstepping control,"['OriginalPaper', 'Original Paper']","In this paper, the dynamic stability of the power systems is improved through the rotor side convertor voltage control of a number of doubly fed induction generators (DFIGs). The multi-input backstepping method is used to design the control laws. Using the particle swarm optimization algorithm, the proposed control parameters are optimized to achieve a better performance. This optimal control law leads to a significantly improved performance in comparison with linear control methods such as state feedback control which implement optimized pole placement by linear matrix inequality design. The offered method results in a faster convergence rate and also robustness against changes in the operating points. The performance of the presented control scheme is validated in a standard 9-bus IEEE power system, with a DFIG located in bus number 9.","['Engineering', 'Electrical Engineering', 'Power Electronics, Electrical Machines and Networks', 'Energy Policy, Economics and Management']"
doi:10.1007/s40747-022-00751-4,en,A two-stage infill strategy and surrogate-ensemble assisted expensive many-objective optimization,"['OriginalPaper', 'Original Article']","Many optimization problems are expensive in practical applications. The surrogate-assisted optimization methods have attracted extensive attention as they can get satisfyingly optimal solutions in a limited computing resource. In this paper, we propose a two-stage infill strategy and surrogate-ensemble assisted optimization algorithm for solving expensive many-objective optimization problems. In this method, the population is optimized by a surrogate ensemble. Then a two-stage infill strategy is proposed to select individuals for real evaluations. The infill strategy considers individuals with better convergence or greater uncertainty. To calculate the uncertainty, we consider two aspects. One is the approximate variance of the current surrogate ensemble and the other one is the approximate variance of the historical surrogate ensemble. Finally, the population is revised by the recently updated surrogate ensemble. In experiments, we testify our method on two sets of many-objective benchmark problems. The results demonstrate the superiority of our proposed algorithm compared with the state-of-the-art algorithms for solving computationally expensive many-objective optimization problems.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11356-021-16749-3,en,Health assessment of dams under various environmental conditions using structural health monitoring techniques: a state-of-art review,"['ReviewPaper', 'Research on Sustainable Developments for Environment Management']","Over a while, changes in the environment, such as climate, weather, pollution, will have a more significant impact on the structures in different ways. Hence, continuous monitoring of the structures plays a vital role in order to have a predefined prediction of the structural damages that can be caused by the change in the environment. Structural health monitoring (SHM) is critical in determining the life expectancy of civil structures. The advancement of various sensors and data acquisition systems (DAQ) has enabled more accurate prediction of the life span of civil structures subjected to static and dynamic loading conditions. Hence, SHM is a critical area of research to understand the condition and lifetime of structures such as dams. This article provides detailed insight into the base failures in dam structures such as soil erosion, toe erosion, and gully formation. Also, scouring’s impact on the dam structures was discussed in this review article. This review article investigates in detail the detection and analysis of dam structure damage.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10489-022-03295-9,en,Joint intent detection and slot filling using weighted finite state transducer and BERT,OriginalPaper,"Intent detection and slot filling are the two most essential tasks of natural language understanding (NLU). Deep neural models have produced impressive results on these tasks. However, the predictive accuracy of these models heavily depends upon a massive amount of supervised data. In many applications collecting high-quality labeled data is a very expensive and time taking process. This paper proposes WFST-BERT model which augments the fine-tuning of BERT-like architecture with weighted finite-state transducer (WFST) to reduce the need for massive supervised data. The WFST-BERT employs regular expressions (REs) rules to encode domain knowledge and pre-trained BERT model to generate contextual representations of user sentences. In particular, the model converts REs into the trainable weighted finite-state transducer, which can generate decent predictions when limited or no training examples are available. Moreover, BERT contextual representation is combined with WFST and trained simultaneously on supervised data using a gradient descent algorithm. The experimental results on the ATIS dataset show that the F1-Score of the WFST-BERT improved by around 1.8% and 1.3% for intent detection and 0.9%, 0.7% for slot filling tasks as compared to its counterparts RE-NN and JointBERT models in limited data settings. Further, in full data settings, the proposed model generates better recall and F1-score than state-of-the-art models.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10951-022-00750-w,en,Packing-based branch-and-bound for discrete malleable task scheduling,OriginalPaper,"This paper addresses the problem of scheduling chain-like structures of tasks on a single multiprocessor resource. In fact, sub-tasks of unit-time length and predefined size are aggregated to composite tasks that have to be scheduled without preemption, but subject to flexibility concerning resource allocation. This setting most closely resembles the problem of malleable task scheduling, with sub-tasks being the smallest atomic unit of allocation. The specific type of malleability is realized using precedence constraints with minimum and maximum time lags. A bin packing model is established for this scheduling problem and a corresponding, dedicated branch-and-bound algorithm is devised, alongside problem-specific bound tightening, symmetry breaking and dominance concepts. The efficacy of the solution approach is demonstrated based on extensive computational experiments, including randomized instances, adapted benchmark instances from the literature, and a small real-world data set. In comparison to mixed-integer and constraint programming formulations, the new method is able to achieve a considerably higher percentage of optimal solutions at computation times that are up to orders of magnitude smaller.","['Business and Management', 'Operations Research/Decision Theory', 'Calculus of Variations and Optimal Control; Optimization', 'Optimization', 'Artificial Intelligence', 'Supply Chain Management']"
doi:10.1007/s11042-021-11508-5,en,Vehicle type classification using graph ant colony optimizer based stack autoencoder model,"['OriginalPaper', '1220: Visual and Sensory Data Processing for Real Time Intelligent Surveillance System']","In the intelligent transport system, vehicle type classification technology plays a major role. With the growth of video processing and pattern recognition application, a deep learning model is proposed in this research article to improve vehicle type classification under dynamic background. Initially, the original video sequences are collected from MIOvision Traffic Camera Dataset (MIO-TCD), and CDnet2014 dataset. Additionally, the contrast and visible level of the video frames are improved by implementing histogram equalization method. Next, the moving vehicles are detected and tracked using Gaussian Mixture Model (GMM) and Kalman filter. Then, the feature extraction is accomplished using Dual Tree Complex Wavelet Transform (DTCWT), Histogram of Oriented Gradients (HOG), and Local Ternary Pattern (LTP) to extract the texture feature vectors. Further, a new graph clustering-Ant Colony Optimization (ACO) algorithm is proposed to select the active feature vectors for better vehicle type classification. Lastly, the selected active feature vectors are given as the input to stack autoencoder classifier to classify eleven vehicle types in MIO-TCD and four vehicle types in CDnet2014 dataset. In the experimental section, the graph ACO based stack autoencoder model achieved 99.09%, and 89.89% of classification accuracy on both MIO-TCD, and CDnet2014 dataset, which are better related to the existing models like attention based method, improved spatiotemporal sample consistency algorithm, and generative adversarial nets.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-07573-y,en,A hybrid algorithm based on tabu search and generalized network algorithm for designing multi-objective supply chain networks,"['OriginalPaper', 'Original Article']","Recently, substantial progress has been made in developing efficient algorithms for solving combinatorial optimization problems. In this paper, following this direction, the problem of designing supply chains which is an important combinatorial optimization problem is considered. The structural properties of supply chain models are investigated to transform such models into a generalized network optimization model. The transformation to a generalized network optimization model reduces the algorithms' solution time. Moreover, this paper proposes a new efficient hybrid algorithm based on tabu search and generalized network simplex algorithm (GN-TSA) for designing multi-objectives supply chain models. The developed algorithm's parameters are tuned properly, validated, and evaluated. The algorithm's performance is then compared to an exact algorithm embedded in the General Algebraic Modeling System (GAMS) and two metaheuristic algorithms, namely a linear programming simplex algorithm integrated with the tabu search approach (LP-TSA) and simulated annealing. The findings indicated that the GN-TSA obtains solutions very close to the exact algorithm with less computation time. In addition, the proposed algorithm outperforms the LP-TSA and simulated annealing in terms of computation time, while the quality of the solutions is the same as solutions obtained by LP-TSA and better than simulated annealing. On average, the results revealed that the reduction in computational time is more than 26.30% using GN-TSA compared to LP-TSA and simulated annealing.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-021-03048-0,en,Improvement and application of hybrid real-coded genetic algorithm,OriginalPaper,"When solving constrained optimization problems (COPs) with high-dimension and multi-extreme problems, genetic algorithm (GA) has the issue of trapping into local optimum. Therefore, this paper proposes a hybrid real-coded genetic algorithm (HIRCGA). First, a sorting group selection (SGS) is given, which is a simple operation and easy to implement. Second, a combinational crossover (CX) operator is developed. It consists of a heuristic normal distribution and direction crossover based on the optimal individual (HNDDX-BOI) and a sine cosine crossover (SCX), which enhances the exploration ability of the algorithm. Third, an operation of eliminating the similarity of different variables in the same dimension (ES) is added, which significantly avoids premature convergence and maintains the population diversity. Fourth, a combinational mutation (CM) operator is proposed, where the global and local search abilities of HIRCGA are fully considered. Fifth, the chaotic search (CS) based on Tent map is introduced to enhance the search power of HIRCGA. Moreover, 28 benchmark test functions in CEC 2017 and two complex real-world optimization problems are selected to demonstrate the effectiveness and superiority of HIRCGA. The computational results and statistical analysis indicate that HIRCGA can improve the solution accuracy compared with other algorithms. The effectiveness of HIRCGA is verified in theory and practice.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00521-022-07475-z,en,Rubber tapping line detection in near-range images via customized YOLO and U-Net branches with parallel aggregation heads convolutional neural network,"['OriginalPaper', 'Original Article']","Current convolutional neural network structures for image-related tasks lean toward directed acyclic graphs with multiple output nodes. This enables a solution for the rubber tapping line detection that desires various output types, such as bounding boxes, points in pixels, or edges. This paper demonstrates multibranch deep convolutional networks whose outputs are bounding boxes and pixel segmentation masks by adopting YOLOv3 and U-Net structures. This paper proposes the functions of column-wise argmax and column-wise Softmax with redundant mask outputs intended to enhance pixel classification accuracy. Experiments with the networks discovered some novel segmentation loss functions, such as Dice’s coefficient, Focal, and Tversky’s index, having different characters for the tapping line prediction, which were observed by Hausdorff distance and F 1-score. The network with multiple mask predictions can omit their weaknesses and yield higher tapping line detection accuracy compared to every single one. In the context of image processing, the column-wise Softmax and argmax algorithms were superior to the edge-thinning algorithm for detecting line vertices.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-021-02679-7,en,Designing convolutional neural networks with constrained evolutionary piecemeal training,OriginalPaper,"The automated architecture search methodology for neural networks is known as Neural Architecture Search (NAS). In recent times, Convolutional Neural Networks (CNNs) designed through NAS methodologies have achieved very high performance in several fields, for instance image classification and natural language processing. Our work is in the same domain of NAS, where we traverse the search space of neural network architectures with the help of an evolutionary algorithm which has been augmented with a novel approach of piecemeal-training. In contrast to the previously published NAS techniques, wherein the training with given data is considered an isolated task to estimate the performance of neural networks, our work demonstrates that a neural network architecture and the related weights can be jointly learned by combining concepts of the traditional training process and evolutionary architecture search in a single algorithm. The consolidation has been realised by breaking down the conventional training technique into smaller slices and collating them together with an integrated evolutionary architecture search algorithm. The constraints on architecture search space are placed by limiting its various parameters within a specified range of values, consequently regulating the neural network’s size and memory requirements. We validate this concept on two vastly different datasets, namely, the CIFAR-10 dataset in the domain of image classification, and PAMAP2 dataset in the Human Activity Recognition (HAR) domain. Starting from randomly initialized and untrained CNNs, the algorithm discovers models with competent architectures, which after complete training, reach an accuracy of of 92.5% for CIFAR-10 and 94.36% PAMAP2. We further extend the algorithm to include an additional conflicting search objective: the number of parameters of the neural network. Our multi-objective algorithm produces a Pareto optimal set of neural networks, by optimizing the search for both the accuracy and the parameter count, thus emphasizing the versatility of our approach.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1186/s12911-022-02059-2,en,Named entity recognition of Chinese electronic medical records based on a hybrid neural network and medical MC-BERT,"['OriginalPaper', 'Research']","Background Named entity recognition (NER) of electronic medical records is an important task in clinical medical research. Although deep learning combined with pretraining models performs well in recognizing entities in clinical texts, because Chinese electronic medical records have a special text structure and vocabulary distribution, general pretraining models cannot effectively incorporate entities and medical domain knowledge into representation learning; separate deep network models lack the ability to fully extract rich features in complex texts, which negatively affects the named entity recognition of electronic medical records. Methods To better represent electronic medical record text, we extract the text’s local features and multilevel sequence interaction information to improve the effectiveness of electronic medical record named entity recognition. This paper proposes a hybrid neural network model based on medical MC-BERT, namely, the MC-BERT + BiLSTM + CNN + MHA + CRF model. First, MC-BERT is used as the word embedding model of the text to obtain the word vector, and then BiLSTM and CNN obtain the feature information of the forward and backward directions of the word vector and the local context to obtain the corresponding feature vector. After merging the two feature vectors, they are sent to multihead self-attention (MHA) to obtain multilevel semantic features, and finally, CRF is used to decode the features and predict the label sequence. Results The experiments show that the F1 values of our proposed hybrid neural network model based on MC-BERT reach 94.22%, 86.47%, and 92.28% on the CCKS-2017, CCKS-2019 and cEHRNER datasets, respectively. Compared with the general-domain BERT-based BiLSTM + CRF, our F1 values increased by 0.89%, 1.65% and 2.63%. Finally, we analyzed the effect of an unbalanced number of entities in the electronic medical records on the results of the NER experiment.","['Medicine & Public Health', 'Health Informatics', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/s12539-022-00535-x,en,BoT-Net: a lightweight bag of tricks-based neural network for efficient LncRNA–miRNA interaction prediction,"['OriginalPaper', 'Original research article']","Background and objective: Interactions of long non-coding ribonucleic acids (lncRNAs) with micro-ribonucleic acids (miRNAs) play an essential role in gene regulation, cellular metabolic, and pathological processes. Existing purely sequence based computational approaches lack robustness and efficiency mainly due to the high length variability of lncRNA sequences. Hence, the prime focus of the current study is to find optimal length trade-offs between highly flexible length lncRNA sequences. Method The paper at hand performs in-depth exploration of diverse copy padding, sequence truncation approaches, and presents a novel idea of utilizing only subregions of lncRNA sequences to generate fixed-length lncRNA sequences. Furthermore, it presents a novel bag of tricks-based deep learning approach “Bot-Net” which leverages a single layer long-short-term memory network regularized through DropConnect to capture higher order residue dependencies, pooling to retain most salient features, normalization to prevent exploding and vanishing gradient issues, learning rate decay, and dropout to regularize precise neural network for lncRNA–miRNA interaction prediction. Results BoT-Net outperforms the state-of-the-art lncRNA–miRNA interaction prediction approach by 2%, 8%, and 4% in terms of accuracy, specificity, and matthews correlation coefficient. Furthermore, a case study analysis indicates that BoT-Net also outperforms state-of-the-art lncRNA–protein interaction predictor on a benchmark dataset by accuracy of 10%, sensitivity of 19%, specificity of 6%, precision of 14%, and matthews correlation coefficient of 26%. Conclusion In the benchmark lncRNA–miRNA interaction prediction dataset, the length of the lncRNA sequence varies from 213 residues to 22,743 residues and in the benchmark lncRNA–protein interaction prediction dataset, lncRNA sequences vary from 15 residues to 1504 residues. For such highly flexible length sequences, fixed length generation using copy padding introduces a significant level of bias which makes a large number of lncRNA sequences very much identical to each other and eventually derail classifier generalizeability. Empirical evaluation reveals that within 50 residues of only the starting region of long lncRNA sequences, a highly informative distribution for lncRNA–miRNA interaction prediction is contained, a crucial finding exploited by the proposed BoT-Net approach to optimize the lncRNA fixed length generation process. Availability: BoT-Net web server can be accessed at https://sds_genetic_analysis.opendfki.de/lncmiRNA/. Graphic Abstract ","['Life Sciences', 'Computer Appl. in Life Sciences', 'Computational Biology/Bioinformatics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Theoretical and Computational Chemistry', 'Theoretical, Mathematical and Computational Physics', 'Computational Science and Engineering']"
doi:10.1007/s00477-022-02245-8,en,"Landslide hazard, susceptibility and risk assessment (HSRA) based on remote sensing and GIS data models: a case study of Muzaffarabad Pakistan","['OriginalPaper', 'Original Paper']","The notion of this research is based on the two devastating earthquake events that happened on October 8, 2005, and September 24, 2019, in the regions of Azad Kashmir and Muzaffarabad. This study aims to (i) identification of the susceptible zones where landslides can occur in the future; (ii) preparation of landslide inventory maps using vector data, satellite imagery, Shuttle Radar Topographic Mission (STRM) and Advanced Space-borne Thermal Emission and Reflection Radiometer (ASTER) DEM; (iii) implementation of Analytical Hierarchy Process (AHP) model using weighted overlay analysis (WOA). For this purpose, key factors such as land use, faults, slope, contours, soil, and seismology maps are used to develop a landslide hazard zonation map. The output landslide susceptibility map has four susceptibility levels such as low, medium, high, and very high vulnerable zones. The results indicated that a highly susceptible landslide zone is found in the northwestern part of Muzaffarabad, which is a metropolitan region. Moreover, there are 127 active landslides are identified and collectively about 9% of the study area is very highly susceptible to future landslides. Furthermore, research findings are helpful in tactful thinking for future infrastructure development, and ecological protection in high-susceptible landslide regions in Muzaffarabad. It also helps the Government to make strategies based on any specific zones on a priority basis to reduce the casualties and destruction in future landslide events.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s41315-022-00235-1,en,A deep reinforcement learning approach for multi-agent mobile robot patrolling,"['OriginalPaper', 'Regular Paper']","Patrolling strategies primarily deal with minimising the time taken to visit specific locations and cover an area. The use of intelligent agents in patrolling has become beneficial in automation and analysing patterns in patrolling. However, practical scenarios demand these strategies to be adaptive in various conditions and robust against adversaries. Traditional Q-learning based patrolling keeps track of all possible states and actions in a Q-table, making them susceptible to the curse of dimensionality. For multi-agent patrolling to be adaptive in various scenarios represented using graphs, we propose a formulation of the Markov Decision Process (MDP) with state-representations that can be utilised for Deep Reinforcement Learning (DRL) approaches such as Deep Q-Networks (DQN). The implemented DQN can estimate the MDP using a finite length state vector trained with a novel reward function. Proposed state-space representation is independent of the number of nodes in the graph, thereby addressing scalability to graph dimensions. We also propose a reward function to penalise the agents for lack of global coordination while providing immediate local feedback on their actions. As independent policy learners subject to the MDP and reward function, the DRL agents formed a collaborative patrolling strategy. The policies learned by the agents generalise and adapt to multiple behaviours without explicit training or design to do so. We provide empirical analysis that shows the strategy’s adaptive capabilities with changes in agents’ position, non-uniform node visit frequency requirements, changes in a graph structure representing the environment, and induced randomness in the trajectories. DRL patrolling proves to be a promising patrolling strategy for intelligent agents by potentially being scalable, adaptive, and robust against adversaries.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s13198-022-01757-4,en,Rolling element bearing fault diagnosis using supervised learning methods- artificial neural network and discriminant classifier,"['OriginalPaper', 'Original Article']","Bearings are the principal component in the induction motor responsible for 50–60% of faults in an induction motor. Hence, detecting and diagnosing bearing faults in an induction motor is essential for reliable operation. Some soft computing techniques like artificial intelligence-based classifiers are always useful in fault diagnosis. This research diagnoses the bearing fault under three vibration signal conditions: raw vibration signal, filtered vibration signal, and wavelet-based denoised vibration signal. The statistical features such as RMS, kurtosis, standard deviation, variance, etc., are extracted from each condition. The db2 wavelet is selected based on the minimum Shannon entropy criteria for the wavelet denoising. Vibration signal data is collected from the experimental setup for four bearing conditions: healthy, outer race defect, ball defect, and cage defect. Total 1600 samples are collected from 2,000,000 data points for each condition. An artificial neural network and discriminant classifier are trained and tested for fault identification. Two other classifiers from each pedigree, i.e., support vector machine and radial basis function neural network, are also analyzed to compare the classification performance. It is observed that the ANN classifier stands the best among all, with a classification accuracy of 99.58% and a minimum computational time of 1.62 s.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s00366-021-01552-y,en,Topology-based geometry optimization for a new compliant mechanism using improved adaptive neuro-fuzzy inference system and neural network algorithm,"['OriginalPaper', 'Original Article']","In precision engineering, compliant mechanisms are growingly promising mechanisms in designing micro/nano positioners and manipulators due to emerging advantages of free friction, no joint, and decreased assembly. Nevertheless, compliant mechanisms have flexible configurations with nonlinear behaviors, the design, analysis, and optimization are becoming challenges, and a systematic design method is still limited. Therefore, this paper proposes a new multi-phases optimization design method for compliant mechanisms. In the suggested method, the topology optimization is integrated with finite element method, intelligent modeling, and neural network algorithm. First, the solid isotropic material with penalization-based topology is used to design a new compliant mechanism. The numerical simulations are conducted. Next, the parameters of adaptive neuro-fuzzy inference system are optimized by the Taguchi to achieve an improved ANFIS (IANFIS) model. The IANFIS approaches are used to predict behaviors of the developed mechanism. The results confirmed that the developed IANFIS has a highly accurate prediction in comparison with other regression models. Particularly, the metric values of IANFIS models are relatively good. Particularly, the R 2 value is approximately 1 while the MSE, RMSE, and SD values are approximately 0. Last, the neural network algorithm is extended to search the optimal geometry sizes for the compliant mechanism. In the size optimization, two scenarios for are taken into consideration. For the scenario 1, the displacement, rotation angle, parasitic, and stress of the mechanism are found about 1.9977 mm, 0.8232 degrees, 0.1666 mm, and 13.94 MPa, respectively. For the scenario 2, the displacement, rotation angle, the parasitic, and stress are approximately 1.8501 mm, 0.8237 degrees, 0.1429 mm, and 11.8193 MPa, respectively. The results of size optimization showed that the displacement of the mechanism is enhanced by 12.94% and the rotation angle is improved to 4.5E+11% in comparison to the initial topology. The statistic results of Friedman and Kruskal–Wallis found that the accuracy and efficiency of proposed method are superior to those of other methods with p-values less than 0.001. The proposed method is applicable to other industrial systems.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-021-01466-9,en,A novel improved Harris Hawks optimization algorithm coupled with ELM for predicting permeability of tight carbonates,"['OriginalPaper', 'Original Article']","Tight carbonate reservoirs appear to be heterogeneous due to the patchy production of various digenetic properties. Consequently, the permeability calculation of tight rocks is costly, and only a finite number of core plugs in any single reservoir can be estimated. Hence, in the present study, a novel hybrid model constructed by combination of the improved version of the Harris Hawks optimisation (HHO), i.e., IHHO, and extreme learning machine (ELM) is proposed to predict the permeability of tight carbonates using limited number of input variables. The proposed IHHO employs a mutation mechanism to avoid trapping in local optima by increasing the search capabilities. Subsequently, ELM-IHHO, a novel metaheuristic ELM-based algorithm, was developed to predict the permeability of tight carbonates. Experimental results show that the proposed ELM-IHHO attained the most accurate prediction with R 2  = 0.9254 and RMSE = 0.0619 in the testing phase. The result of the proposed model is significantly better than those obtained from other ELM-based hybrid models developed with particle swarm optimisation, genetic algorithm, and slime mould algorithm. The results also illustrate that the proposed ELM-IHHO model outperforms the other benchmark model, such as back-propagation neural nets, support vector regression, random forest, and group method of data handling in predicting the permeability of tight carbonates.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00521-021-05975-y,en,EMOCGAN: a novel evolutionary multiobjective cyclic generative adversarial network and its application to unpaired image translation,"['OriginalPaper', 'S.I. : NCACVIP']","Generative adversarial networks (GANs) have been accepted as powerful models in the field of computer vision, speech and language processing, etc. However, a major concern regarding GANs is the requirement of paired images for image-to-image translation, which is not always possible in the case of real-world applications. Moreover, they also suffer from training instability as well as mode collapse problem. These concerns remain open challenging issues for GANs and become more complex in the case of cyclic GAN. Motivated by evolutionary GAN, we hereby propose a novel evolutionary multiobjective cyclic GAN (EMOCGAN) to address the above stated challenges related to cyclic GAN training for the image-to-image translation. In this work, we have also introduced a new approach for model training by integrating the concept of evolutionary computation, multiobjective optimization, cyclic GAN along with different selection mechanisms. To overcome local optima stagnation, metropolis acceptance criteria and Pareto-based selection on two scores (objective functions) are utilized. Evolutionary concepts in training helped to address the instability and mode collapse problems. Extensive experiments on real-world image datasets show that the EMOCGAN outperforms state-of-the-art method in terms of visually realistic appearance and retaining background information as well as salient objects. Quantitative comparisons of our EMOCGAN with the cyclic GAN also show significantly better scores obtained by our model, as indicated by structural similarity index (SSIM) and universal quality index (UQI). The model demonstrated best efficacy in terms of SSIM on Apple $$\leftrightarrow$$ ↔ Orange, while it shows higher UQI values for Monet $$\leftrightarrow$$ ↔ Picture and Summer $$\leftrightarrow$$ ↔ Winter datasets.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11227-022-04633-x,en,A convolutional neural network intrusion detection method based on data imbalance,OriginalPaper,"With the rapid development of Internet technology, network attacks occur frequently and numerous hidden dangers appear in network security. Therefore, improving the performance of intrusion detection systems to detect and defend against attacks is the key to ensuring network security. However, in the face of complex and massive network data feature information, traditional machine learning methods suffer from data imbalance and feature redundancy, which results in low detection rates, high false alarm rates and poor real-time performance of intrusion detection systems. Therefore, to address these problems, this paper proposes a data imbalance-based Convolutional Neural Network Intrusion Detection Method (CNN-IDMDI). First, an oversampling method is used to solve the data imbalance problem by decomposing the increased number of samples for the few attacks with multiple sampling to form multiple sub-samples. Second, the gradient coordination mechanism and the improved loss function Focal Loss are combined to calculate the loss between the actual and expected values to detect network malicious attacks in high-dimensional and unbalanced data. Finally, the methods in this paper are compared with the current mainstream intrusion detection methods on the NSL-KDD dataset for binary and multi-classification detection. The experimental results show that the method in this paper can effectively improve the effectiveness of CNN intrusion detection and network anomaly. The average accuracy of the CNN intrusion detection method based on data imbalance for binary intrusion detection is 98.73% and the implementation time of the method is 1.42 s, which is 15.45%, 12.76%, and 2.91% higher than the average accuracy of the CNN, the CNN Long Short-Term Memory (CNN-LSTM) and the CNN Neural-induced Support Vector Machine (CNN-NSVM) methods, respectively, and the detection time is saved by 0.82 s, 0.72 s, and 0.61 s, respectively. The average accuracy of the CNN intrusion detection method based on data imbalance for multi-classification intrusion detection is 94.55% and the time required to complete the detection is 2.96 s. This improves the average accuracy by 16.09%, 12.71%, and 3.66% compared with the CNN, CNN-LSTM and CNN-NSVM methods, respectively. It is also quicker, as the time consumption of CNN is 8.84 s, CNN-LSTM is 8.31 s and CNN-NSVM is 6.43 s. Therefore, the CNN-IDMDI method for intrusion detection proposed in this paper has higher accuracy and faster speed.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s10586-022-03644-w,en,"Federated recommenders: methods, challenges and future",OriginalPaper,"Abstract Web users are flooded with information on the internet, and they feel overwhelmed by the different choices they have to make online daily. Recommender systems come to their rescue by suggesting products best aligned with their interests. To achieve this, traditional recommenders transfer users’ personal data from the client to the server and dig for information about the user’s interests and tastes. Moving data to the cloud violates the user confidentiality requirement and poses severe threats to user privacy and security. Moreover, with the tremendous increase in data size, it is no longer possible to collect and process massive data in the cloud. With the emergence of federated learning, numerous innovative recommender models are devised to solve these issues. In these models, the user data never leaves the client-side, and only the inferred results are sent back to the server for aggregating and updating the master model. Hence, the federated recommenders preserve user privacy and save the hassle of transferring enormous data to the cloud. This paper meticulously studies the recently proposed federated recommenders and classifies them based on the enhancements introduced in the prediction model, security scheme, or optimization technique. We identify the challenges faced by current federated recommenders and observe that most issues are inherently due to various aspects of federated learning, such as heterogeneous and non-IID data, malicious users, distributed framework, and non-reliable edge devices. While some emerge due to the coupling of the recommendation process in the federated paradigm. This research summarizes the current limitations, highlights the areas that need improvements, and presents future paths. In short, it paves the way for the development of robust federated recommenders that can handle the challenges of federated learning and, at the same time, generate high-quality recommendations.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00521-022-07593-8,en,Prediction of hydraulic blockage at culverts from a single image using deep learning,"['OriginalPaper', 'Original Article']","Cross-drainage hydraulic structures such as culverts and bridges in urban landscapes are prone to get blocked by the transported debris (e.g., urban, vegetated), which often reduces their hydraulic capacity and triggers flash floods. Unavailability of relevant data from blockage-originated flooding events and complex nature of debris accumulation are highlighted factors hindering the research within the blockage management domain. Wollongong City Council (WCC) blockage conduit policy is the leading formal guidelines to incorporate blockage into design guidelines; however, are criticized by the hydraulic engineers for its dependence on the post-flood visual inspections (i.e., visual blockage) instead of peak floods hydraulic investigations (i.e., hydraulic blockage). Apparently, no quantifiable relationship is reported between the visual blockage and hydraulic blockage; therefore, many consider WCC blockage guidelines invalid. This paper exploits the power of Artificial Intelligence (AI), motivated by its recent success, and attempts to relate visual blockage with hydraulic blockage by proposing a deep learning pipeline to predict hydraulic blockage from an image of the culvert. Two experiments are performed where the conventional pipeline and end-to-end learning approaches are implemented and compared in the context of predicting hydraulic blockage from a single image. In experiment one, the conventional deep learning pipeline approach (i.e., feature extraction using CNN and regression using ANN) is adopted. In contrast, in experiment two, end-to-end deep learning models (i.e., E2E_ MobileNet, E2E_ BlockageNet) are trained and compared with the conventional pipeline approach. Dataset (i.e., Hydraulics-Lab Blockage Dataset (HBD), Visual Hydraulics-Lab Dataset (VHD)) used in this research were collected from laboratory experiments performed using scaled physical models of culverts. E2E_ BlockageNet model was reported best in predicting hydraulic blockage with $$R^2$$ R 2 score of 0.91 and indicated that hydraulic blockage could be interrelated with the visual features at the culvert.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11356-022-22063-3,en,Identifying sensitivity of factor cluster based gully erosion susceptibility models,"['OriginalPaper', 'Research Article']","The present study has attempted to address the issue of sensitivity of different clusters of factors towards gully erosion in the Mayurakshi river basin. Firstly, the gully erosion susceptibility of the basin area has been mapped by integrating using 18 parameters divided into four factor-cluster, viz. erodibility, erosivity, resistance, and topographical cluster, with the help of four machine learning (ML) models such as random forest (RF), gradient boost (GBM), extreme gradient boost (XGB), and support vector machine (SVM). Results show that almost 20% and 25% of the upper catchment of the basin belongs to extreme and high gully erosion susceptibility. Among the applied algorithms, RF is appeared as the best performing model. The spatial association of factor cluster-based models with the final susceptibility model is found the highest for the erosivity cluster, followed by the erodibility cluster. From the sensitivity analysis, it becomes clear that geology and soil texture are dominant contributing factors to gully erosion susceptibility. The geological formation of unclassified granite gneiss and geomorphological formation of denudational origin pediment-pediplain complex is dominant over the entire upper catchment of the basin, and therefore, can be considered regional factors of importance. Since the study has figured out the different grades of susceptible areas with dominant factors and factor cluster, it would be useful for devising planning for gully erosion check measures. From economic particularly food security purpose, it is very essential since it is concerned with precious soil loss and negative effects on agriculture.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s40435-022-00933-5,en,Robustifying adaptive model predictive control for a one-link flexible manipulator using super-twisting integral sliding mode control,OriginalPaper,"Vibration depreciates positioning accuracy and productivity of flexible manipulators and make their modeling and/or controlling a very demanding task. In this paper, an Adaptive Model Predictive Control (AMPC) algorithm is detailed to actively damp a nonlinear one-link flexible manipulator while tracking its rigid body position. The derived controller involves in-loop linearization of the plant, around the actual state and control signal, to adapt the required model over the prediction horizon. The robustness of the proposed control strategy is improved using Super-Twisting Integral Sliding Mode Control (STISMC). The proposed controller compensates for the assumed matched uncertainties and external disturbances affecting the nominal plant, so the robustness is guaranteed. The effectiveness of the active vibration control is appraised via numerical simulation carried out using Matlab/Simulink.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1007/s40031-022-00809-4,en,Fuzzy Inference Model for Short-Term Load Forecasting,"['OriginalPaper', 'Original Contribution']","For planning and operation of an energy management system, load forecasting (LF) is essential. For smooth power system operation (PS), LF enhances the energy-efficient and reliable operation. LF also helps to calculate energy supplied by utilities to meet the load plus the energy lost in the PS. Every day, it is necessary to schedule the power generation for the next day. So, short-term load forecasting (STLF) is used to calculate the power dispatch for the next day. In unit commitment, economic allocation of generation and maintenance schedules, STLF is also used. So, to make the STLF more effective, fuzzy logic (FL) is used here. FL is essential for weather-sensitive and historical load data for forecasting the load. The fuzzy decision rule identifies the nonlinear relationship between the input and output data. The historical load and hourly data like temperature, humidity (relative humidity) and wind speed are used for input data. For the training and testing, the hourly based load data are collected from the state load dispatch and communication center of Rajasthan Vidyut Prasaran Nigam, Jaipur (JVN). The triangular membership function of the fuzzy logic model is used to predict the load. The performance of the work is determined by the mean absolute percentage error (MAPE) and the MAPE value for pre-holiday (Saturday), holiday (Sunday), post-holiday, and working day is 0.37%, 0.24%, 0.09%, and 0.09%, respectively.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s40747-022-00730-9,en,A hybrid multi-objective bi-level interactive fuzzy programming method for solving ECM-DWTA problem,"['OriginalPaper', 'Original Article']","Electronic countermeasure (ECM) has become one of the most significant factors in modern warfare, in the course of combat, the electronic jamming allocation tasks need to be flexibly adjusted with the change of combat stage, which puts forward higher requirements for the modeling and solution method of this kind of problems. To solve the ECM dynamic weapon target assignment (ECM-DWTA) problem, a hybrid multi-target bi-level programming model is established. The upper level takes the sum of the electronic jamming effects in the whole combat stage as an optimization objective, and locally optimizes the ECM weapon (ECM-WP) assignment scheme in each stage. The lower level takes the importance expectation value of the target subjected to interference and combat consumption as double optimization objectives to globally optimize the ECM-WP assignment scheme. Focus on solving this complex model, a hybrid multi-objective bi-level interactive fuzzy programming algorithm (HMOBIF) is proposed, in this method, exponential membership function is used to describe the satisfaction degree of each level. When solving the multi-objective optimization problem composed of membership functions in the upper and lower levels, we use the MOEA/D algorithm to obtain the Pareto Front (PF) solution set, and then each solution in PF is evaluated and selected by the TOPSIS multi-criteria evaluation method. This local and global interactive optimization process of bi-level model is actually the process of executing observation-orientation-decision-action loop in practical combat. According to the current example, we conduct numerical simulation on the parameters in the model and obtain the parameter values suitable for the model solution. The computational experiments on different scale ECM-DWTA problems show that HMOBIF method is superior to four bi-level programming algorithms in terms of performance index, and can better solve ECM-DWTA problems.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s00521-022-07688-2,en,Dynamic memory supported dialog generation model based on commonsense knowledge graph,"['OriginalPaper', 'Original Article']","In daily conversation, humans naturally use the information they already have and remember new information generated during the conversation. It is common for a conversation to start with one concept (or topic) and then move on to broader related concepts. According to this observation, a new dialog generation model, DMDG-K, a dynamic memory-supported dialog generation model, based on a commonsense knowledge graph, is proposed in this paper to improve the practical significance of machine-generated response in a human-machine dialog. In the model, after using BERT to encode the dialogs, the multilayer dynamic memory mechanism based on external storage is adopted to map the semantic information of dialogs into the dynamic memory space for knowledge extraction and realize the knowledge modeling of previous dialogs. By using a two-layer knowledge extraction network, the model obtains the knowledge from ConceptNet, a commonsense knowledge graph, and with the guidance of GAN, the commonsense knowledge injection conversation flow is modeled. The context-attention mechanism is utilized to fuse the conversation flow with the extracted knowledge of previous dialogs to generate the conversation content with more practical semantic information. Extensive experiments were conducted on the public dataset, Reddit, to demonstrate the performance of the proposed model, and the results show that dialogs generated using the DMDG-K model have better knowledge awareness than baseline models.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-022-03300-1,en,Log message anomaly detection with fuzzy C-means and MLP,OriginalPaper,"Log messages are one the most valuable sources of information in the cloud and other software systems. These logs can be used for audits and ensuring system security. Many millions of log messages are produced each day which makes anomaly detection challenging. Automating the detection of anomalies can save time and money as well as improve detection performance. In this paper, an anomaly detection method is proposed using radius-based fuzzy C-means with more clusters than the number of data classes and a multilayer perceptron (MLP) network. The cluster centers and a radius are used to select reliable positive and negative log messages. Moreover, class probabilities are used with an expert to correct the network output for suspect logs. The proposed model is evaluated with three well-known data sets, namely BGL, Openstack and Thunderbird. The results obtained show that this model provides better results than existing methods.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.3758/s13428-021-01739-7,en,Sufficient reliability of the behavioral and computational readouts of a probabilistic reversal learning task,OriginalPaper,"Task-based measures that capture neurocognitive processes can help bridge the gap between brain and behavior. To transfer tasks to clinical application, reliability is a crucial benchmark because it imposes an upper bound to potential correlations with other variables (e.g., symptom or brain data). However, the reliability of many task readouts is low. In this study, we scrutinized the retest reliability of a probabilistic reversal learning task (PRLT) that is frequently used to characterize cognitive flexibility in psychiatric populations. We analyzed data from N  = 40 healthy subjects, who completed the PRLT twice. We focused on how individual metrics are derived, i.e., whether data were partially pooled across participants and whether priors were used to inform estimates. We compared the reliability of the resulting indices across sessions, as well as the internal consistency of a selection of indices. We found good to excellent reliability for behavioral indices as derived from mixed-effects models that included data from both sessions. The internal consistency was good to excellent. For indices derived from computational modeling, we found excellent reliability when using hierarchical estimation with empirical priors and including data from both sessions. Our results indicate that the PRLT is well equipped to measure individual differences in cognitive flexibility in reinforcement learning. However, this depends heavily on hierarchical modeling of the longitudinal data (whether sessions are modeled separately or jointly), on estimation methods, and on the combination of parameters included in computational models. We discuss implications for the applicability of PRLT indices in psychiatric research and as diagnostic tools.","['Psychology', 'Cognitive Psychology']"
doi:10.1007/s40747-022-00726-5,en,New bag-of-feature for histopathology image classification using reinforced cat swarm algorithm and weighted Gaussian mixture modelling,"['OriginalPaper', 'Original Article']","The progress in digital histopathology for computer-aided diagnosis leads to advancement in automated histopathological image classification system. However, heterogeneity and complexity in structural background make it a challenging process. Therefore, this paper introduces robust and reliable new bag-of-feature framework. The optimal visual words are obtained by applying proposed reinforcement cat swarm optimization algorithm. Moreover, the frequency of occurrence of each visual words is depicted through histogram using new weighted Gaussian mixture modelling method. Reinforcement cat swarm optimization algorithm is evaluated on the IEEE CEC 2017 benchmark function problems and compared with other state-of-the-art algorithms. Moreover, statistical test analysis is done on acquired mean and the best fitness values from benchmark functions. The proposed classification model effectively identifies and classifies the different categories of histopathological images. Furthermore, the comparative experimental result analysis of proposed reinforcement cat swarm optimization-based bag-of-feature is performed on standard quality metrics measures. The observation states that reinforcement cat swarm optimization-based bag-of-feature outperforms the other methods and provides promising results.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s12206-022-1132-4,en,Few-shot transfer learning with attention for intelligent fault diagnosis of bearing,"['OriginalPaper', 'Original Article']","The bearing is one of the key components in modern industrial equipment. In the past few years, many studies have been carried out on bearing diagnosis through data-driven methods. However, there are two practical problems. First, under actual working conditions, the lack of fault samples is a major factor that hinders the application of these methods in industrial environments. Second, there is a lack of full utilization of a priori knowledge in the current stage of methods using relational networks for fault diagnosis. It is manifested by the incompleteness of the relational network structure. To address these problems, we present a new diagnosis method based on few-shot learning, which is suitable for the environment where the data is scarce. In this method, we train the model with the data generated by the artificial damaged bearings instead of the data from the real bearing. We experimentally validate the performance improvement of the complete relational network structure. It is able to perform the few-shot learning task better. In addition, we also reduce the global feature discrepancy by introducing an attention mechanism to improve the performance of the model. And the impact of the number of layers of the attention mechanism on the model is also discussed in detail. In this paper, our model performs better under the same experimental conditions compared with other transfer learning models.","['Engineering', 'Mechanical Engineering', 'Vibration, Dynamical Systems, Control', 'Industrial and Production Engineering']"
doi:10.1007/s11071-022-07788-7,en,Modified echo state network for prediction of nonlinear chaotic time series,"['OriginalPaper', 'Original Paper']","In this paper, we focus on the prediction issue of the nonlinear chaotic time series. In particular, we introduce the modified echo state network (M-ESN) to predict the time series of nonlinear chaotic system. Thereinto, to solve the ill-conditioned output weight matrix caused by vast neurons in hidden layer of the ESN, we introduce the hybrid regularized network (HRN) based on the l 2/3 regularization and the l 2 regularization. Thereinto, the l 2/3 regularization plays the role to produce the sparse output weight and import the oracle property to the prediction model, while the l 2 regularization plays the role to shrink the amplitude of the output weight matrix produced by the former to further improve the generalization ability of the HRN. Therefore, the M-ESN produces the sparse output weight matrix, enjoys the oracle property and has well generalization ability for the nonlinear chaotic time series. Besides, considering that the random input weight matrix may affect the whole system performance, we have investigated the plasticity property of the neuron and developed a fine-tuning strategy of the input weight matrix based on the Bienenstock, Cooper, Munro (BCM) theory. The input weight matrix in M-ESN is fine-tuned in the unsupervised training process. We introduce and explain its calculation process in detail. Finally, we evaluate the M-ESN by the time series of the classical nonlinear Lorenz chaotic system, the time series of the nonlinear Lv chaotic system, the time series of the Hindmarsh–Rose neuron model and the time series of the rainfall series in Hefei city in past 50 years. The simulation results indicate the M-ESN has well one-step and multistep prediction performances for nonlinear chaotic time series.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s00477-022-02252-9,en,A Kronecker-based covariance specification for spatially continuous multivariate data,"['OriginalPaper', 'Original Paper']","We propose a covariance specification for modeling spatially continuous multivariate data. This model is based on a reformulation of Kronecker’s product of covariance matrices for Gaussian random fields. The structure holds for different choices of covariance functions with parameters varying in their usual domains. In comparison with classical models from the literature, we used the Matérn correlation function to specify the marginal covariances. We also assess the reparametrized generalized Wendland model as an option for efficient calculation of the Cholesky decomposition, improving the model’s ability to deal with large data sets. The reduced computational time and flexible generalization for increasing number of variables, make it an attractive alternative for modelling spatially continuous data. The proposed model is fitted to a soil chemistry properties dataset, and adequacy measures, forecast errors and estimation times are compared with the ones obtained based on classical models. In addition, the model is fitted to a North African temperature dataset to illustrate the model’s flexibility in dealing with large data. A simulation study is performed considering different parametric scenarios to evaluate the properties of the maximum likelihood estimators. The simple structure and reduced estimation time make the proposed model a candidate approach for multivariate analysis of spatial data.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s00530-022-00954-2,en,Smartphone-based gait recognition using convolutional neural networks and dual-tree complex wavelet transform,"['OriginalPaper', 'Regular Article']","Gait recognition is an efficient way of identifying people from their walking behavior, using inertial sensors integrated into the smartphones. These inertial sensors such as accelerometers and gyroscopes easily collect the gait data used by the existing deep learning-based gait recognition methods. Although these methods specifically, the hybrid deep neural networks, provide good gait feature representation, their recognition accuracy needs to be improved as well as reducing their computational cost. In this paper, a person identification framework from smartphone-acquired inertial gait signals is proposed to overcome these limitations. It is based on the combination of convolutional neural network (CNN) and dual-tree complex wavelet transform (DTCWT), named as CNN–DTCWT. In the proposed framework, global average pooling layer and DTCWT layer are integrated into the CNN to provide robust and highly accurate inertial gait feature representation. Experimental results demonstrate the superiority of the proposed structure over the state-of-the-art models. Tested on three data sets, it achieves higher recognition performance than the state-of-the-art CNN-based, LSTM-based models, and hybrid networks within average recognition accuracy improvements of 1.7–14.95%","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00034-022-02107-2,en,AtResNet: Residual Atrous CNN with Multi-scale Feature Representation for Low Complexity Acoustic Scene Classification,OriginalPaper,"Acoustic Scene Classification (ASC) aims to categorize real-world audio into one of the predetermined classes that identifies the recording environment of the audio. State-of-the-art ASC algorithms have excellent performance in terms of accuracy due to the emergence of deep learning algorithms. In particular, Convolutional Neural Networks (CNN) have set a new benchmark in ASC due to their promising performance. Despite the emergence of new frameworks, the interest in ASC is growing progressively with a shift of focus from enhancing accuracy to reducing model complexity. In this work, we introduce the AtResNet, a residual atrous CNN for low complexity acoustic scene classification. The AtResNet utilizes dilated convolutions and residual connections to reduce the number of model parameters. To further enhance the performance of AtResNet, we introduce a multi-scale feature representation method called multi-scale mel spectrogram (ms2). To compute the ms2, we evaluate the mel spectrogram on the wavelet subbands of the signal. We assessed AtResNet with ms2 on three benchmark datasets in ASC. The results suggest that our method significantly outperformed the CNN-based techniques in addition to a baseline system based on log mel spectrum for signal representation. AtResNet offers a 28.73% reduction in the model parameters against a baseline CNN. Furthermore, the AtResNet has a model size of 81 KB with post-training quantization of network weights. It makes AtResNet suitable for deployment in context-aware devices.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s10776-022-00574-7,en,MLIDS: Machine Learning Enabled Intrusion Detection System for Health Monitoring Framework Using BA-WSN,OriginalPaper,"Health monitoring using Body Area Wireless Sensor Network (BA-WSN) has gained immense popularity due to usability, ubiquitous support, and real-time performance. It is a special kind of Wireless Sensor Network (WSN) that spans over the human body. Although BA-WSN is very useful but it may suffer from security and privacy issues due to compromised sensor nodes by intruders. To design a secure BA-WSN based health monitoring system, it is required to filter out malicious data packets generated by the compromised nodes. An intrusion Detection System (IDS) can be utilized for this purpose. This paper presents a Machine Learning based Intrusion Detection System (MLIDS) for BA-WSN based health monitoring framework. Specialized dataset WSN-DS has been used to train the intrusion detection model. Dataset contains four security attacks such as Blackhole attack, Grayhole attack, Scheduling attack, Flooding attack data as well as normal data packets which are simulated using Network Simulator-2. Five well-known classification algorithms such as Random Forest, kNN, SVM, J48, and Naive Bayes have been applied for the selection and generation of the best model in terms of detection accuracy. Experimental results prove that Random Forest based Intrusion Detection Model has the highest classification accuracy of 99.67%, 98.7%, 92.7%, 98.9%, 99.9% for Blackhole attack, Flooding attack, Scheduling attack, Grayhole attack as well normal packet respectively. Experimental results also show that our achieved results outperform relevant work in terms of accuracy.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s11356-022-23732-z,en,A bibliometric and content analysis of research trends on GIS-based landslide susceptibility from 2001 to 2020,"['ReviewPaper', 'Review Article']","To assess the status of hotspots and research trends on geographic information system (GIS)–based landslide susceptibility (LS), we analysed 1142 articles from the Thomas Reuters Web of Science Core Collection database published during 2001–2020 by combining bibliometric and content analysis. The paper number, authors, institutions, corporations, publication sources, citations, and keywords are noted as sub/categories for the bibliometric analysis. Thematic LS data, including the study site, landslide inventory, conditioning factors, mapping unit, susceptibility models, and mode fit/prediction performance evaluation, are presented in the content analysis. Then, we reveal the advantages and limitations of the common approaches used in thematic LS data and summarise the development trends. The results indicate that the distribution of articles shows clear clusters of authors, institutions, and countries with high academic activity. The application of remote sensing technology for interpreting landslides provides a more convenient and efficient landslide inventory. In the landslide inventory, most of the sample strategies representing the landslides are point and polygon, and the most frequently used sample subdividing strategy is random sampling. The scale effects, lack of geographic consistency, and no standard are key problems in landslide conditioning factors. Feature selection is used to choose the factors that can improve the model’s accuracy. With advances in computing technology and artificial intelligence, LS models are changing from simple qualitative and statistical models to complex machine learning and hybrid models. Finally, five future research opportunities are revealed. This study will help investigators clarify the status of LS research and provide guidance for future research.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s00521-022-07506-9,en,Efficient anomaly detection through surrogate neural networks,"['OriginalPaper', 'S.I. : Cybersecurity Applications of Computational Intelligence']","Anomaly Detection can be viewed as an open problem despite the growing plethora of known anomaly detection techniques. The applicability of various anomaly detectors can vary depending on the application area and problem settings. Especially in the Big Data industrial setting, an important problem is inference speed, which may render even a highly accurate anomaly detector useless. In this paper, we propose to address this problem by training a surrogate neural network based on an auxiliary training set approximating the source anomaly detector output. We show that existing anomaly detectors can be approximated with high accuracy and with application-enabling inference speed. We compare our approach to a number of state-of-the-art algorithms: one class k -nearest-neighbors ( k NN), local outlier factor, isolation forest, auto-encoder and two types of generative adversarial networks. We perform this comparison in the context of an important problem in cyber-security—the discovery of outlying (and thus suspicious) events in large-scale computer network traffic. Our results show that the proposed approach can successfully replace the most accurate but prohibitively slow k NN. Moreover, we observe that the surrogate neural network may even improve the k NN accuracy. Finally, we discuss various implications that the proposed approach can have while reducing the complexity of applied anomaly detection systems.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00500-022-07341-z,en,Design and development of novel hybrid optimization-based convolutional neural network for software bug localization,"['OriginalPaper', 'Application of soft computing']","Software systems are frequently affected by a few defects. In general, the developers make use of the data present in the file, while reporting a bug; thus, they can help to fix the bug by localizing the source code fragments that are required to be improved. This paper intends to implement the bug localization model using the improved deep learning algorithm, which helps to localize the buggy files to enhance the productivity and efficiency of the software quality assurance teams. The datasets such as “Aspect J and SWT” were utilized for assessing the proposed bug localization model. The proposed model involves a few fundamental steps like data pre-processing, word embedding, CNN-based feature detection, and CNN-based classification. Initially, the source files from which the bug has to be localized are given as input. Here, the pre-processing of files is performed by erasing punctuation followed by splitting. Here, the relevant information or features are gathered from the source files using Word2Vec, a bag of n-grams, and the term frequency–inverse document frequency (TF-IDF) technique in the word embedding process. Further, final feature vector extraction is performed by the convolution neural network (CNN). The extracted features are subjected to optimized CNN-based classification to localize the bugs. Here, the number of hidden neurons of CNN is optimized using hybridized cuckoo search-based sea lion optimization (CS-SLnO). The main objective of this work is to introduce a new optimized CNN-based classification of bug localization with hidden neurons optimization using a hybridized optimization algorithm termed CS-SLnO for improving the localization accurateness. Finally, the bug fixing ability of the proposed model is proved and certified through valuable performance analysis. The experimental results shows that the accuracy@1 of the suggested CS-SLnO-CNN is attaining the best performance when compared to other algorithms. It is 85.1% better than DeepLoc, 51.9% better than DeepLocator, 54.3% better than HyLoc, 65.1% better than LR + WE, and 73.5% better than BugLocator. Thus, it can be confirmed that the developed CS-SLnO-CNN is acquiring better bug localization performance when compared to the other deep learning models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00521-022-07701-8,en,CTSC-Net: an effectual CT slice classification network to categorize organ and non-organ slices from a 3-D CT image,"['OriginalPaper', 'Original Article']","Computed tomography (CT) is a non-invasive diagnostic imaging modality that reveals more insight into human organs than conventional X-rays. In general, the CT output is a 3-D image that is formed by combining multiple 2D images or slices together. It is essential to keep in mind that not all of the slices provide significant information to detect tumours. Usually, a 3-D CT image obtained from the CT scanners has a significant number of unwanted non-organ slices in it. Radiologists typically devote a significant amount of time to select the slices with organ from a 3-D CT image. The presence of a tumour is only evident in the organ slice; hence, radiologists must be cautious not to skip any organ slices. This work is evaluated on the LITS, 3DIRCADb and COVID-19 CT datasets. The three datasets collectively contain 22,435 organ slices and 53,661 non-organ slices, and there is a huge gap between the number of organ and non-organ slices. There is a need for the automatic elimination of non-organ slices in 3-D CT volumes to assist the physicians, and hence, this work focuses on the automatic recognition of organ slices from 3-D CT volumes. In this paper, a new deep model called the computed tomography slice classification network (CTSC-Net) is proposed for CT slice classification between organ and non-organ slices. The model is trained on 77,980 CT slices, validated on 9748 slices and tested on 12,571 slices. Nine CNN architectures with different layer settings are trained and tested to arrive at the final optimal model. The performance measures are computed in terms of true positive rate, true negative rate, sensitivity, specificity and accuracy. The 20-layer CTSC-Net achieves a validation accuracy of 95.04% and an overall testing accuracy of 99.96%. The proposed model is compared to eight different pre-trained CNN models, and the results of the proposed CTSC-Net surpassed all the comparable models. The activation feature maps of different layers of the CTSC-Net are visualized to verify the discriminative features learned by the network. Hence, the proposed CTSC-Net can be employed as a computer-aided diagnosis tool to help physicians discard unnecessary non-organ slices from the 3-D CT volume and to speed up the CT diagnosis process.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11356-022-22116-7,en,Advanced operational measure for reducing fuel consumption onboard ships,"['OriginalPaper', 'Research Article']","Environmental issues and targets to reduce greenhouse gas emissions have reinforced attempts to increase energy efficiency for all stakeholders, including the shipping industry. International maritime organization (IMO) has prescribed operational and design measures to develop energy efficiency and decrease ship emissions, and one of the effective operational measures is trim optimization, which is a feasible measure because it does not necessitate ship structural modification or engine advancement, but it can reduce the ship resistance and decrease fuel consumption. The study of trim optimization can be implemented by conducting experimental tests, but it is a difficult, expensive method and consumed more time. Therefore, the present paper proposes a numerical method to predict the optimum trim which achieves the minimum ship resistance and lower fuel consumption. The current paper investigated two types of ships as a case study, bulk carrier, and container ship. The optimization process has studied several trim conditions, ship drafts, and speeds. The results showed that positive trim (trim by bow) have an increasing effect on fuel consumption, while the negative trim (trim by stern) have a decreasing effect on fuel consumption. Fuel-saving based on using optimum trim at each speed is a significant quantity to be benefited as it would reduce the operating costs and increase the energy efficiency.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s11277-021-09076-w,en,Diagnosing COVID-19 from CT Image of Lung Segmentation & Classification with Deep Learning Based on Convolutional Neural Networks,OriginalPaper,"Early-stage exposure and analysis of diseases are life-threatening causes for controlling the spread of COVID-19. Recently, Deep Learning (DL) centered approaches have projected intended for COVID-19 during the initial stage through the Computed Tomography (CT) mechanism is to simplify and aid with the analysis. However, these methodologiesundergocommencing one of the following issues: each CT scan slice treated separately and train and evaluate from the same dataset the strategies for image collections. Independent slice therapy is the identical patient involved in the preparation and set the tests at the same time, which can yield inaccurate outcomes. It also poses the issue of whether or not an individual should compare the scans of the same patient. This paper aims to establish image classifiers to determine whether a patient tested positive or negative for COVID-19 centered on lung CT scan imageries. In doing so, a Visual Geometry Group-16 (VGG-16) and a Convolutional Neural Network (CNN) 3-layer model used for marking. The images are first segmented using K-means Clustering before the classification to increase classification efficiency. Then, the VGG-16 model and the 3-layer CNN model implemented on the raw and segmented data. The impact of the segmentation of the image and two versions are explored and compared, respectively. Various tuning techniques were performed and tested to improve the VGG-16 model's performance, including increasing epochs, optimizer adjustment, and decreasing the learning rate. Moreover, pre-trained weights of the VGG-16 the model added to enhance the algorithm.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s10479-021-04424-2,en,Multi-objective supplier selection process: a simulation–optimization framework integrated with MCDM,"['OriginalPaper', 'Original Research']","As a multi-criteria decision-making (MCDM) problem, supplier selection plays a key role in achieving the objectives of a supply chain system. Multiple strategic, operational, quantitative, and qualitative criteria influence the supplier selection process. A wide spectrum of criteria have been introduced, classified, and used by researchers and practitioners to evaluate the suppliers’ performance; however, measuring and employing all of these criteria is impractical in real-world scenarios due to the budget, time, and information limitations. In this study, a decision support system (DSS) is developed, which helps managers to select a set of most effective criteria for the supplier selection process. This DSS is a threefold integration of MCDM and simulation and optimization. In this framework, the MCDM module incorporates a combination of criteria to select the suppliers. Then, a simulation model is used to evaluate the performance of the supply chain system considering the selected suppliers. Based on the simulation results, a multi-objective metaheuristic algorithm is utilized to find the ideal combinations of the criteria to maximize the supply chain system performance.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1134/S0361768822070027,en,Preventing Vulnerabilities Caused by Optimization of Code with Undefined Behavior,OriginalPaper,"Abstract Sophisticated optimization in modern compilers can sometimes create vulnerabilities in program code as a result of optimization. The source of these vulnerabilities is in code with undefined behavior. Programmers use constructs with undefined behavior while relying on a particular behavior these constructs exhibited before in their practice. However, the compiler does not have to stick to that behavior and may change it if there is a need for code optimization because this behavior is not defined by language standards. This paper describes some approaches to the discovery and elimination of vulnerabilities caused by optimization in the case where the source code is available, but its modification is undesirable or impossible. We propose the concept of a safe compiler (i.e., a compiler that guarantees that no vulnerability is brought into a program in the process of optimization). We describe the implementation of this compiler on top of GCC. The functionality of the safe compiler is implemented at three security levels, the applicability of which is discussed in this paper. The use of the safe compiler is illustrated on real-world codebases with the estimation of possible performance losses.","['Computer Science', 'Computer Science, general', 'Software Engineering/Programming and Operating Systems', 'Operating Systems', 'Software Engineering', 'Artificial Intelligence']"
doi:10.1007/s00530-022-00952-4,en,Automated brain tumor malignancy detection via 3D MRI using adaptive-3-D U-Net and heuristic-based deep neural network,"['OriginalPaper', 'Regular Article']","Using the 3D image from public benchmark sources, the experiment is initiated with pre-processing using skull stripping and contrast enhancement. Further, the segmentation of tumor region is performed by the Adaptive-3-D U-Net (A-3D-U-Net) utilized for the hybridized Butterfly Optimization Algorithm (BOA), and Tunicate Swarm Algorithm (TSA) termed to as Butterfly–Tunicate Swarm Algorithm (B-TSA). The optimal segmentation of tumors is based on solving the multi-objective solution concerning “Structured Similarity Index (SSIM), Mean Square Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Dice Coefficient”. From the segmented tumor region, the numerical features such as “mean, standard deviation, entropy, skewness, kurtosis, energy, contrast, inverse difference moment, directional moment, correlation, coarseness, and texture features like Local Ternary Pattern (LTP), and Local Tetra Pattern (LTrP)” are extracted. In the final stage, the detection of malignancy is performed by heuristic-based deep neural network (HDNN) using the same proposed B-TSA for the parameter optimization. The findings of applying the suggested methodology to 3D-MRI images from the Decathlon dataset demonstrate that the suggested technique is comparable to conventional methods for brain tumor segmentation.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s10973-022-11516-z,en,Thermoeconomic optimization of cascade refrigeration system using computational intelligence techniques,OriginalPaper,"This study presents the thermoeconomic optimization of LiBr-H 2 O, LiCl-H 2 O, (CaCl 2 -LiBr-LiNO 3 )-H 2 O with R290, R123, R1234yf, and R1234ze in the compression-absorption cascade refrigeration system. The detailed thermodynamics and economic analysis have been presented. Nonlinear objective function formulated based on the concept of energy, exergy, economic, and environmental performance is minimized by using five recent computational intelligence techniques. Optimum cascade, evaporator, absorber, generator, condenser, and overlap temperatures and the effectiveness of the solution heat exchanger have been reported for the cascade refrigeration system. The effect of decision variables on the coefficient of performance, total exergy destruction, total heat exchanger area, and the total annual cost of cascade refrigeration system has also been reported within their upper and lower bounds for all absorbent-refrigerant-refrigerant combinations. Among all the studied algorithms, the coyote optimization algorithm reports the minimum total annual cost for all considered absorbent-refrigerant-refrigerant combinations. The minimum and maximum total annual costs are reported as 13,164.76 $ year −1 and 15,406.65 $ year −1 for (CaCl 2 -LiBr-LiNO 3 )-H 2 O-R290 and LiBr-H 2 O-R1234yf, respectively.","['Chemistry', 'Physical Chemistry', 'Analytical Chemistry', 'Polymer Sciences', 'Inorganic Chemistry', 'Measurement Science and Instrumentation']"
doi:10.1007/s11042-022-13484-w,en,End-to-end video compression for surveillance and conference videos,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","The storage and transmission tasks of surveillance and conference videos are an important branch of video compression. Since surveillance and conference videos have strong inter-frame correlation, considerable continuity at the image level and motion level between the consecutive frames exists. However, traditional video codec networks cannot fully use the characteristics of surveillance and conference videos during compression. Therefore, based on the DVC video codec framework, we propose a “MV residual + MV optimization” coding strategy for surveillance and conference videos to further reduce the compression rate and improve the quality of compressed video frames. During the testing stage, the online update strategy is promoted, which adapts the network’s parameters to different surveillance and conference videos. Our contribution is to propose an optical flow residual coding method for videos with strong inter-frame correlation, implement optical flow optimization at decoding end and online update strategy at the encoding end. Experiments show that our method can outperform DVC framework, especially on CUHK Square surveillance video with 1.2dB improvement.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00170-022-10396-9,en,Analytical modeling and multi-objective optimization algorithm for abrasive waterjet milling Ti6Al4V,"['OriginalPaper', 'ORIGINAL ARTICLE']","Abrasive waterjet (AWJ) is a promising method for machining titanium alloy, which is widely used in the aerospace field, but the various process parameters of AWJ make it difficult to achieve a high machining quality. In this research, the main process parameters of AWJ, including the jet pressure, the abrasive flow rate, the stand-off distance, the jet angle, the traverse speed, and the feed rate, were all analyzed by considering their effects on the milling characteristics of Ti6Al4V alloy. Both single and interactive effects of the process parameters were studied, and regression models for predicting the milling depth h , the material erosion rate $$\dot{V}$$ V ˙ , and the X-directional roughness Ra x were established. Furthermore, an ADM-MO-Jaya (adaptive decreasing method multi-objective Jaya) algorithm based on MO-Jaya was proposed to obtain the optimal process parameters, aiming for reaching the minimum Ra x and the maximum h and $$\dot{V}$$ V ˙ at the same time. The results show that the correlation coefficients R 2 of the models are all greater than 0.9, and model terms are relatively significant. The regression models of h , $$\dot{V}$$ V ˙ , and Ra x are generally consistent with the overall trend of the experimental results, and the mean errors are 8.57%, 1.89%, and 10.58%, respectively. The operation efficiency of the ADM-MO-Jaya algorithm is 32% higher than that of the MO-Jaya, and the Pareto front is the most uniform and converges to a curve in the solution space without isolated points. The optimized set of 180 Pareto solutions can be directly selected by the operator for machining without complex process comparisons, which can guide the practical milling of titanium alloy by AWJ.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s10822-022-00486-x,en,Enabling data-limited chemical bioactivity predictions through deep neural network transfer learning,OriginalPaper,"Abstract The main limitation in developing deep neural network (DNN) models to predict bioactivity properties of chemicals is the lack of sufficient assay data to train the network’s classification layers. Focusing on feedforward DNNs that use atom- and bond-based structural fingerprints as input, we examined whether layers of a fully trained DNN based on large amounts of data to predict one property could be used to develop DNNs to predict other related or unrelated properties based on limited amounts of data. Hence, we assessed if and under what conditions the dense layers of a pre-trained DNN could be transferred and used for the development of another DNN associated with limited training data. We carried out a quantitative study employing more than 400 pairs of assay datasets, where we used fully trained layers from a large dataset to augment the training of a small dataset. We found that the higher the correlation r between two assay datasets, the more efficient the transfer learning is in reducing prediction errors associated with the smaller dataset DNN predictions. The reduction in mean squared prediction errors ranged from 10 to 20% for every 0.1 increase in r 2 between the datasets, with the bulk of the error reductions associated with transfers of the first dense layer. Transfer of other dense layers did not result in additional benefits, suggesting that deeper, dense layers conveyed more specialized and assay-specific information. Importantly, depending on the dataset correlation, training sample size could be reduced by up to tenfold without any loss of prediction accuracy. Graphical abstract ","['Chemistry', 'Physical Chemistry', 'Computer Applications in Chemistry', 'Animal Anatomy / Morphology / Histology']"
doi:10.1007/s11063-022-10873-y,en,A Novel Data Augmentation Method for Chinese Character Spatial Structure Recognition by Normalized Deformable Convolutional Networks,OriginalPaper,"In this paper, we propose a novel data augmentation method and a normalized deformable convolutional network for natural image classification and handwritten Chinese character structure recognition. The spatial structure is the basic characteristics of Chinese character, and it plays a very important role in understanding and learning Chinese character. But the convolutional neural networks are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. So, we use the deformable convolutional network to deal with this task. Furthermore, we propose a normalized deformable convolutional network to improve the stability and accuracy of the model. Besides, some traditional data augmentation method could change one Chinese character structure to another, we propose a novel data augmentation method named Matt data augmentation (MDA) to improve the recognition performance. The normalized deformable Resnet with MDA achieve the best accuracy (93.62%) on handwritten Chinese character structure data set. Besides, the CapsuleNet with MDA can also improve to 89.41% test accuracy compared to without MDA (87.75%). Extensive experiments validate the performance of our approach.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s10462-022-10153-0,en,EnvGAN: a GAN-based augmentation to improve environmental sound classification,OriginalPaper,"Several deep learning algorithms have emerged for the automatic classification of environmental sounds. However, the non-availability of adequate labeled data for training limits the performance of these algorithms. Data augmentation is an appropriate solution to this problem. Generative Adversarial Networks (GANs) can successfully generate synthetic speech and sounds of musical instruments for classification applications. In this paper, we present a method for GAN-based augmentation in the context of environmental sound classification. We introduce an architecture named EnvGAN for the adversarial generation of environmental sounds. To validate the quality of the generated sounds, we have conducted subjective and objective evaluations. The results indicate that EnvGAN can produce samples of various domains with an acceptable target quality. We applied this augmentation technique on three benchmark ESC datasets (ESC-10, UrbanSound8K, and TUT Urban Acoustic Scenes development dataset) and used it for training a CNN-based classifier. Experimental results show that this new augmentation method can outperform a baseline method with no augmentation by a relatively wide margin (10–12% on ESC-10, 5–7% on UrbanSound8K, and 4–5% on TUT). In particular, the GAN-based approach reduces the confusion between all pairs of classes on UrbanSound8K. That is, the proposed method is especially suitable for handling class-imbalanced datasets.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s40747-021-00493-9,en,Portfolio optimization model with uncertain returns based on prospect theory,"['OriginalPaper', 'Original Article']","When investing in new stocks, it is difficult to predict returns and risks in a general way without the support of historical data. Therefore, a portfolio optimization model with an uncertain rate of return is proposed. On this basis, prospect theory is used for reference, and then the uncertain return portfolio optimization model is established from the perspective of expected utility maximization. An improved gray wolf optimization (GWO) algorithm is designed because of the complex nonsmooth and nonconcave characteristics of the model. The results show that the GWO algorithm is superior to the traditional particle swarm optimization algorithm and genetic algorithm.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11554-022-01245-9,en,A practical super-resolution method for multi-degradation remote sensing images with deep convolutional neural networks,"['OriginalPaper', 'Original Research Paper']","Recent studies have proved that convolutional neural networks (CNNs) have great potential for image super-resolution (SR) tasks. However, most existing methods rely on paired high-resolution (HR) and low-resolution (LR) images to train the CNN, where the LR images are routinely synthesized by applying predefined degradation operations (e.g., bicubic). Because the degradation process of LR images is usually unknown and more complex than those predefined, these methods suffer a significant performance decrease when applied to real-world SR problems. In addition, a deeper and wider network structure enables superior performance while increasing the network parameters and inference time, making it difficult to process real-time data. Inspired by the above motivations, we present an efficient two-step SR method for multi-degradation remote sensing images. Specifically, we first present a novel kernel estimation framework based on generative adversarial networks that can accurately extract the latent blur kernel from the input LR image without any image priors. We then train an efficient SR deep neural network with paired HR and corresponding LR images degraded with the generated kernels. To better balance network parameters and network performance, the densely connected attention mechanism and multi-scale feature extract blocks are introduced in the SR network by increasing the flow of feature information within the network. Extensive experiments indicate that the proposed method outperforms current methods with desired network parameters and complexity, making it feasible to enable real-time image processing.","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s10614-021-10184-9,en,V-Shaped BAS: Applications on Large Portfolios Selection Problem,OriginalPaper,"The beetle antennae search (BAS) algorithm is a memetic meta-heuristic optimization algorithm capable of solving combinatorial optimization problems. In this paper, the binary version of BAS (BBAS) is modified by adding a V-shaped transfer function. In this way, we introduce the V-shaped transfer function-based binary BAS (VSBAS) algorithm, which is a more effective and efficient version of BBAS in the case of large input data. Applications using real-world data sets on a binary Markowitz-based portfolio selection (BMPS) problem validate the excellent performance of VSBAS on large input data and demonstrate that it is a marvelous alternative against other ordinary memetic meta-heuristic optimization algorithms. Note that, because the meta-heuristic algorithms compared in this paper are directly applicable only to unconstrained optimization, the penalty function method was used to keep their solutions in the feasible district. In order to support and promote the findings of this work, we have constructed a complete MATLAB package for the interested user, which is freely available through GitHub.","['Economics', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Computer Appl. in Social and Behavioral Sciences', 'Operations Research/Decision Theory', 'Behavioral/Experimental Economics', 'Math Applications in Computer Science']"
doi:10.1007/s10619-021-07345-y,en,Detection of Alzheimer’s disease using grey wolf optimization based clustering algorithm and deep neural network from magnetic resonance images,OriginalPaper,"The automated magnetic resonance imaging (MRI) processing techniques are gaining more importance in Alzheimer disease (AD) recognition, because it effectively diagnosis the pathology of the brain. Currently, computer aided diagnosis based on image analysis is an emerging tool to support AD diagnosis. In this research study, a new system is developed for enhancing the performance of AD recognition. Initially, the brain images were acquired from three online datasets and one real-time dataset such as AD Neuroimaging Initiative (ADNI), Minimal Interval Resonance Imaging in AD (MIRIAD), and Open Access Series of Imaging Studies (OASIS) and National Institute of Mental Health and Neuro Sciences (NIMHANS). Then, adaptive histogram equalization (AHE) and grey wolf optimization based clustering algorithm (GWOCA) were applied for denoising and segmenting the brain tissues; grey matter (GM), cerebro-spinal fluid (CSF), and white matter (WM) from the acquired images. After segmentation, the feature extraction was performed by utilizing dual tree complex wavelet transform (DTCWT), local ternary pattern (LTP) and Tamura features to extract the feature vectors from the segmented brain tissues. Then, ReliefF methodology was used to select the active features from the extracted feature vectors. Finally, the selected active feature values were classified into three classes [AD, normal and mild cognitive impairment (MCI)] utilizing deep neural network (DNN) classifier. From the simulation result, it is clear that the proposed framework achieved good performance in disease classification and almost showed 2.2–6% enhancement in accuracy of all four datasets.","['Computer Science', 'Database Management', 'Data Structures', 'Information Systems Applications (incl.Internet)', 'Operating Systems', 'Memory Structures']"
doi:10.1007/s40747-022-00722-9,en,MSAt-GAN: a generative adversarial network based on multi-scale and deep attention mechanism for infrared and visible light image fusion,"['OriginalPaper', 'Original Article']","For the past few years, image fusion technology has made great progress, especially in infrared and visible light image infusion. However, the fusion methods, based on traditional or deep learning technology, have some disadvantages such as unobvious structure or texture detail loss. In this regard, a novel generative adversarial network named MSAt-GAN is proposed in this paper. It is based on multi-scale feature transfer and deep attention mechanism feature fusion, and used for infrared and visible image fusion. First, this paper employs three different receptive fields to extract the multi-scale and multi-level deep features of multi-modality images in three channels rather than artificially setting a single receptive field. In this way, the important features of the source image can be better obtained from different receptive fields and angles, and the extracted feature representation is also more flexible and diverse. Second, a multi-scale deep attention fusion mechanism is designed in this essay. It describes the important representation of multi-level receptive field extraction features through both spatial and channel attention and merges them according to the level of attention. Doing so can lay more emphasis on the attention feature map and extract significant features of multi-modality images, which eliminates noise to some extent. Third, the concatenate operation of the multi-level deep features in the encoder and the deep features in the decoder are cascaded to enhance the feature transmission while making better use of the previous features. Finally, this paper adopts a dual-discriminator generative adversarial network on the network structure, which can force the generated image to retain the intensity of the infrared image and the texture detail information of the visible image at the same time. Substantial qualitative and quantitative experimental analysis of infrared and visible image pairs on three public datasets show that compared with state-of-the-art fusion methods, the proposed MSAt-GAN network has comparable outstanding fusion performance in subjective perception and objective quantitative measurement.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10791-022-09414-x,en,Highlighting exact matching via marking strategies for ad hoc document ranking with pretrained contextualized language models,OriginalPaper,"Pretrained language models (PLMs) exemplified by BERT have proven to be remarkably effective for ad hoc ranking. As opposed to pre-BERT models that required specialized neural components to capture different aspects of query-document relevance, PLMs are solely based on transformers where attention is the only mechanism used for extracting signals from term interactions. Thanks to the transformer’s cross-match attention, BERT was found to be an effective soft matching model. However, exact matching is still an essential signal for assessing the relevance of a document to an information-seeking query aside from semantic matching. We assume that BERT might benefit from explicit exact match cues to better adapt to the relevance classification task. In this work, we explore strategies for integrating exact matching signals using marker tokens to highlight exact term-matches between the query and the document. We find that this simple marking approach significantly improves over the common vanilla baseline. We empirically demonstrate the effectiveness of our approach through exhaustive experiments on three standard ad hoc benchmarks. Results show that explicit exact match cues conveyed by marker tokens are beneficial for BERT and ELECTRA variant to achieve higher or at least comparable performance. Our findings support that traditional information retrieval cues such as exact matching are still valuable for large pretrained contextualized models such as BERT.","['Computer Science', 'Information Storage and Retrieval', 'Natural Language Processing (NLP)', 'Data Mining and Knowledge Discovery', 'Data Structures and Information Theory', 'Pattern Recognition']"
doi:10.1007/s11277-021-08765-w,en,An Optimal Emperor Penguin Optimization Based Enhanced Flower Pollination Algorithm in WSN for Fault Diagnosis and Prolong Network Lifespan,OriginalPaper,"Wireless sensor devices have offered to ascend to numerous WSN applications for cost of deployment, user-friendly interface, transmitting, receiving information and observing the information through the sink hub. Few issues lead to affect the capacity of sink nodes they are, quality of service, high bandwidth demand, high energy consumption, provisioning, compressing techniques, data processing also cross-layer design. If these issues are may affect the entire system, it may create node failure. To overcome these issues,we proposean optimal emperor penguin optimization-based enhanced flower pollination algorithm for fault diagnosis and prolong network lifespan. In optimal emperor penguin optimization (OEPO) strategy is used for automatically identifying the behaviour of active sensor nodes, correcting faulted nodes and to find an optimal alternative solution for routing. Then, we illustrate the enhanced flower pollination algorithm (FPA) is proposed to extend the network's stability period. Using FPA, multi-hop communication between Cluster heads and base station is utilized to accomplish optimal link costs for load balancing of Cluster heads and energy minimization. Analysis and simulation results show that the (OEPO-FPA) proposed algorithm significantly outperforms than existing Trusted cluster based optimal multi-sink repositioning technique in terms of energy consumption,systemlifetime, delay, delivery ratio, throughput and false-positive rate.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s10032-022-00416-5,en,Conv-transformer architecture for unconstrained off-line Urdu handwriting recognition,"['OriginalPaper', 'Special Issue Paper']","Unconstrained off-line handwriting text recognition in general and for Arabic-like scripts in particular is a challenging task and is still an active research area. Transformer-based models for English handwriting recognition have recently shown promising results. In this paper, we have explored the use of transformer architecture for Urdu handwriting recognition. The use of a convolution neural network before a Vanilla full transformer and using Urdu printed text-lines along with handwritten text lines during the training are the highlights of the proposed work. The convolution layers act to reduce the spatial resolutions and compensate for the $$n^{2}$$ n 2 complexity of transformer multi-head attention layers. Moreover, the printed text images in the training phase help the model in learning a greater number of ligatures (a prominent feature of Arabic-like scripts) and a better language model. Our model achieved state-of-the-art accuracy (CER of $$5.31\%$$ 5.31 % ) on publicly available NUST-UHWR dataset (Zia et al. in Neural Comput Appl 34:1–14, 2021).","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s42461-022-00684-z,en,Prediction of Three-Dimensional Fractal Dimension of Hematite Flocs Based on Particle Swarm Optimization Optimized Back Propagation Neural Network,OriginalPaper,"The three-dimensional (3D) fractal dimension is an important parameter to analyze the 3D structure and flocculation effect of the hematite flocs. In this work, the 3D fractal dimension of hematite flocs was predicted by establishing the particle swarm optimization (PSO) algorithm optimized with the back propagation (BP) neural network (PSO-BP). The proposed model considers four factors, namely, the flocculant quantity, flocculation time, stirring speed, and temperature, as the input parameters. We normalize the input data during the preprocessing stage. The BP neural network was optimized by using the PSO algorithm for realizing the 3D fractal dimension. The prediction accuracy and effectiveness of the model were evaluated. The experimental results showed that the predictions performed by the proposed PSO-BP neural network were better than that of BP neural network. In addition, the root mean square error (RMSE), mean squared relative error (MSRE), mean absolute error (MAE), and mean absolute relative error (MARE) of the proposed model are lower, compared with the errors of the BP network. Similarly, the measurement coefficient R 2 of the proposed model is higher, compared with the BP network. The maximum absolute error of the model is 0.0772, the maximum relative error is 0.02405, and the regression coefficient r is 0.98592. These results showed that the proposed model has a good performance and high prediction accuracy. When verifying the practicability and the effectiveness of the proposed PSO-BP model, the prediction results of 10 groups of hematite flocculation experimental data under different conditions showed that the prediction value of the proposed PSO-BP model was closer to the ground truth as compared to the BP model. Therefore, the proposed model is suitable for predicting the 3D fractal dimension of hematite flocculation.","['Engineering', 'Materials Engineering', 'Metallic Materials', 'Mineral Resources']"
doi:10.1007/s10999-022-09607-8,en,Design and modelling methodology for a new magnetorheological damper featuring a multi-stage circumferential flow mode,OriginalPaper,"Combining a multi-stage circumferential flow mode in external valves, novel magnetorheological (MR) dampers have been devised by authors. For characterizing specific advantages generated by the combination of metal materials, magnetic field, non-Newton fluid and structure issues, a composite method including mathematical models and finite element models will be explained gradually. Therefore, beginning with mathematical models of magnetic field in an external valve, magnetic flux densities of circumferential channels at different currents can be obtained firstly. Secondly, establishing a computational fluid dynamics (CFD) model, associations between viscosity of magnetorheological fluid and pressure drops as well as flow rates will be revealed effectively. Finally, considering pressure drops along the entire flow path including an external valve, mathematical models for characterizing damping behaviors can be achieved, and numerical results will be also available based on two kinds of mathematical models and data of a CFD model. Further reflecting through simulated and experimental results, these dampers would be applied in diverse fields with a wide range of adjustment, the small energy consumption and a good versatility.","['Engineering', 'Solid Mechanics', 'Classical Mechanics', 'Characterization and Evaluation of Materials', 'Engineering Design']"
doi:10.1007/s10278-022-00674-z,en,Developing and Validating Multi-Modal Models for Mortality Prediction in COVID-19 Patients: a Multi-center Retrospective Study,"['OriginalPaper', 'Original Paper']","The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain ( N  = 2547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA ( N  = 242) and an academic center in Seoul, Republic of Korea ( N  = 336). The models we developed performed differently across various clinical settings, underscoring the need for a guided strategy when employing machine learning for clinical decision-making. We demonstrated that using features from both the structured electronic health records and chest X-ray imaging data resulted in better 30-day mortality prediction performance across all three datasets (areas under the receiver operating characteristic curves: 0.85 (95% confidence interval: 0.83–0.87), 0.76 (0.70–0.82), and 0.95 (0.92–0.98)). We discuss the rationale for the decisions made at every step in developing the models and have made our code available to the research community. We employed the best machine learning practices for clinical model development. Our goal is to create a toolkit that would assist investigators and organizations in building multi-modal models for prediction, classification, and/or optimization.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s00521-022-07601-x,en,Domain adaptation based on source category prototypes,"['OriginalPaper', 'Original Article']","Unsupervised domain adaptation (UDA), which can transfer knowledge from labeled source domain to unlabeled target domain, needs to access a large number of labeled source data in the process of generalization. However, the data of two domains may not be accessed at the same time due to data privacy protection. To solve this problem, source-data free domain adaptation (SFDA) began to receive attention. However, too little source information will lead to some performance gaps. To balance the issues between UDA and SFDA, a new setting called Prototype-based domain adaptation (Prototype-DA) is proposed, which further improves the practicability of UDA by using source category prototype instead of source data. At the same time, it can also ensure the privacy of source data like SFDA. Specifically, our training process can be divided into two steps. First, the source data is used to pre-train a source model, and the source category prototypes are obtained after the training of source model. Then, to generalize the source model to the target domain, category maximum mean discrepancy (Category-MMD) is defined so that the target data can be aligned with the source category prototypes. In this way, source category prototypes will transfer knowledge to the target domain together with the source model. Through source category prototypes, Prototype-DA can not only achieve the comparable results than the method using source data, but also protect the privacy of source data to some extent. Furthermore, the target category prototypes are constructed and the consistency between the labels of target category prototypes and the classification results is required. This prototype-label consistency regularization, proposed by us for the first time, helps to extract discriminative features in the target domain. Compared with the previous UDA methods and SFDA methods, extensive experiments on multiple public domain adaptation datasets show that Prototype-DA achieves the state-of-the-art results. At the same time, the traditional UDA theory is expanded to our method setting and makes a theoretical analysis to ensure the effectiveness of our method.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s13167-022-00301-5,en,"Predictive, preventive, and personalized management of retinal fluid via computer-aided detection app for optical coherence tomography scans","['OriginalPaper', 'Research']","Aims Computer-aided detection systems for retinal fluid could be beneficial for disease monitoring and management by chronic age-related macular degeneration (AMD) and diabetic retinopathy (DR) patients, to assist in disease prevention via early detection before the disease progresses to a “wet AMD” pathology or diabetic macular edema (DME), requiring treatment. We propose a proof-of-concept AI-based app to help predict fluid via a “fluid score”, prevent fluid progression, and provide personalized, serial monitoring, in the context of predictive, preventive, and personalized medicine (PPPM) for patients at risk of retinal fluid complications. Methods The app comprises a convolutional neural network–Vision Transformer (CNN-ViT)–based segmentation deep learning (DL) network, trained on a small dataset of 100 training images (augmented to 992 images) from the Singapore Epidemiology of Eye Diseases (SEED) study, together with a CNN-based classification network trained on 8497 images, that can detect fluid vs. non-fluid optical coherence tomography (OCT) scans. Both networks are validated on external datasets. Results Internal testing for our segmentation network produced an IoU score of 83.0% (95% CI = 76.7–89.3%) and a DICE score of 90.4% (86.3–94.4%); for external testing, we obtained an IoU score of 66.7% (63.5–70.0%) and a DICE score of 78.7% (76.0–81.4%). Internal testing of our classification network produced an area under the receiver operating characteristics curve (AUC) of 99.18%, and a Youden index threshold of 0.3806; for external testing, we obtained an AUC of 94.55%, and an accuracy of 94.98% and an F1 score of 85.73% with Youden index. Conclusion We have developed an AI-based app with an alternative transformer-based segmentation algorithm that could potentially be applied in the clinic with a PPPM approach for serial monitoring, and could allow for the generation of retrospective data to research into the varied use of treatments for AMD and DR. The modular system of our app can be scaled to add more iterative features based on user feedback for more efficient monitoring. Further study and scaling up of the algorithm dataset could potentially boost its usability in a real-world clinical setting.","['Biomedicine', 'Biomedicine, general', 'Medicine/Public Health, general']"
doi:10.1007/s12204-022-2430-9,en,Non-Line-of-Sight Multipath Detection Method for BDS/GPS Fusion System Based on Deep Learning,OriginalPaper,"Non-line-of-sight (NLOS) multipath effect is the main factor that restricts the application of global navigation satellite system (GNSS) in complex environments, especially in urban canyon. The effective avoidance of NLOS signals can significantly improve the positioning performance of GNSS receiver. In this paper, an NLOS/LOS classification model based on recurrent neural network is proposed to classify satellite signals received in urban canyon environments. The accuracy of classification is 91%, and the recognition rate of NLOS is 89%; the classification performance is better than traditional machine learning classification models such as support vector machine. For BeiDou navigation satellite system/global positioning system (BDS/GPS) fusion system, the least square algorithm and extended Kalman filter are used to estimate the position. The experimental results show that the three-dimensional positioning accuracy after NLOS recognition is improved about 60% on average compared with the traditional methods, and the positioning stability is also improved significantly.","['Engineering', 'Electrical Engineering', 'Materials Science, general', 'Computer Science, general', 'Architecture, general', 'Life Sciences, general']"
doi:10.1007/s11263-022-01676-8,en,A Realism Metric for Generated LiDAR Point Clouds,OriginalPaper,"A considerable amount of research is concerned with the generation of realistic sensor data. LiDAR point clouds are generated by complex simulations or learned generative models. The generated data is usually exploited to enable or improve downstream perception algorithms. Two major questions arise from these procedures: First, how to evaluate the realism of the generated data? Second, does more realistic data also lead to better perception performance? This paper addresses both questions and presents a novel metric to quantify the realism of LiDAR point clouds. Relevant features are learned from real-world and synthetic point clouds by training on a proxy classification task. In a series of experiments, we demonstrate the application of our metric to determine the realism of generated LiDAR data and compare the realism estimation of our metric to the performance of a segmentation model. We confirm that our metric provides an indication for the downstream segmentation performance.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00170-022-10342-9,en,Tool wear state recognition based on WOA-SVM with statistical feature fusion of multi-signal singularity,"['OriginalPaper', 'ORIGINAL ARTICLE']","Tool wear state is the key factor affecting machining quality, machining efficiency, and cutting stability in the cutting process. Serious wear condition will even lead to machining process interruption and machine tool failure. Accurate monitoring of tool wear state has become increasingly important in the intelligent development of manufacturing industry. To monitor tool wear accurately and effectively, a new method based on whale optimization algorithm optimized support vector machine (WOA-SVM) with statistical feature fusion of multi-signal singularity was proposed to recognize the tool wear state. Based on estimating the maximum wavelet transformation module (MWTM), multi-signal denoising and singularity quantitative characterization were carried out. Meanwhile, the probability density transform was performed on the holder (HE) index, and the relevant statistical features were extracted. Random forest algorithm and KPCA algorithm were used for relatively important features screening and dimension reduction fusion of multi-signal singularity features. By establishing the correlation mapping between the fusion features and the tool wear level, a WOA-SVM classification model based on the fusion features was constructed to recognize the tool wear state. The performance of the method proposed was verified based on the milling wear experiment. Results showed that this method can identify the tool wear state efficiently and accurately based on the limited experimental data. Compared with some other classification methods, this method had better classification performance, effectiveness, and feasibility. These findings can be of great significance for evaluating tool condition, replacing tool timely and ensuring machining quality and efficiency.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s40747-022-00746-1,en,An improved artificial bee colony algorithm based on Bayesian estimation,"['OriginalPaper', 'Original Article']","Artificial bee colony (ABC) algorithm was proposed by mimicking the cooperative foraging behaviors of bees. As a member of swarm intelligence algorithms, ABC has some advantages in handling optimization problems. However, it has the exploration capacity over the exploitation capacity, which may lead to slow convergence speed and lower solution accuracy. Hence, to enhance the performance of the algorithm, a novel ABC based on Bayesian estimation (BEABC) is presented in this paper. First, instead of using the fitness ratio, the selection probability in ABC is replaced with a new probability calculated by Bayesian estimation. Second, to help the bees adopt more useful information during updating new food sources, a directional guidance mechanism is designed for onlooker bees and scout bees. Finally, the comprehensive performance of BEABC is evaluated by 24 single-objective test functions. The numerical experiment results indicate that BEABC dominates its peers over most test functions, and the significant statistics show that the significant excellence rate of BEABC is $$76\%$$ 76 % in the overall comparison. In addition, to further test the performance of BEABC, seven multi-objective problems and two real-word optimization problems are solved. The comparison results show that BEABC can achieve better results than other EA competitors.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s42107-022-00483-3,en,Buckling performance evaluation of steel cellular beams strengthened with flange cover plate,"['OriginalPaper', 'Original Paper']","The use of steel cellular beams (beams with circular shape of openings) has been increasing day by day because of its architectural appearance as well as many other advantages such as light in weight, enhancement in flexural strength and incorporation of pipes through the openings. But on the other hand, due to the presence of web openings, there is a sudden drop in the shear as well as flexural strength of the beams. To counteract this problem, there is a necessity to strengthen the cellular beams. Therefore, the objective of the present research work is to evaluate the buckling performance of cellular beams strengthened with flange cover plates. In the present study, the cellular beams are designed as per the Arcelor Mittal, ACB ® and Angelina TM beams specification. The sections considered in the analysis are CB420, CB450 and CB480 which are fabricated from parent section ISMB300 by adopting the expansion ratio as 1.4 h , 1.5 h and 1.6 h . The spacing is varied from 1.1 a , 1.2 a and 1.3 a , where ‘ a ’ is the diameter of the opening. The diameter of openings are changed from 1.0 h , 1.1 h , 1.2 h and 1.3 h where ‘ h ’ is the height of the parent section, i.e. 300 mm. The FE analysis is performed using ABAQUS software. From the analysis results, it observed that by strength enhancement in cellular beam due to strengthening is 36.30% for CB420, 39.16% for CB450 and 43.08% for CB480. The centre-to-centre spacing of openings also plays a governing role on flexural strength of beams. As soon as spacing of openings increases, the strength increases and vice versa. The common modes of failure are excessive bending, local buckling, vierendeel mechanism and web-post buckling. The last two failure modes, i.e. vierendeel mechanism and web-post buckling are the most important failure modes of cellular beams which needs to be carefully addressed.","['Engineering', 'Civil Engineering', 'Building Materials', 'Sustainable Architecture/Green Buildings']"
doi:10.1007/s12530-021-09417-x,en,A semi-supervised deep rule-based classifier for robust finger knuckle-print verification,"['OriginalPaper', 'Original Paper']","Today, biometric recognition systems play an important role in various applications of different domains. Despite remarkable progress, their performance remains insufficient for security applications. Recently, semi-supervised deep rule classifier (SSDRB) is clearly explainable and universal classification tool used to solve different problems of classification or prediction. Thus, in this paper, we propose an effective scheme based SSDRB classifier for personal authentication systems, where, finger knuckle print (FKP) has been exploited. The proposed scheme is data driven and completely automatic. In this scheme, the pertinent and relevant features are extracted from the input finger knuckle image by binarized statistical image features descriptor (BSIF), which are then fed into fuzzy rules based multilayer semi-supervised learning approach based on a deep rule-based (DRB) classifier to decide whether the person is genuine or impostor. The experiments were conducted on the publicly available PolyU-FKP database provided by University of Hong Kong. The results are represented in form of rank-1, equal error rate (EER), cumulative match curve (CMC) and receiver operating characteristic (ROC) curves. The obtained results demonstrate that the proposed SSDRB classifier is a promising tool for the FKP biometric identification systems. Experimental results on the PolyU-FKP database show that the proposed SSDRB achieves lower error rates with an EER of 0.00% and a rank-1 of 99.90% on the FKP single modality outperforming several published methods.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s12065-020-00483-9,en,An algorithm for solving FEVM problem based on SPSO algorithm,"['OriginalPaper', 'Special Issue']","To solve fuzzy expected value model problem that widely exists in fuzzy programming field, a new hybrid intelligence algorithm based on Stochastic Particle Swarm Optimization (SPSO) algorithm is put forward in this article. Fuzzy simulation is used to get training samples for multi-layer (BP) Back Propagation artificial neural networks and multi-layer BP artificial neural networks is used to approximate fuzzy expected value function when SPSO algorithm is used to find the optimal value, the trained multi-layer BP artificial neural networks is used to calculate fuzzy expected function’s fitness value and detail steps are designed. Compared with the hybrid intelligence algorithm based on the classical Genetic Algorithm, the proposed algorithm overcomes some defects, such as taking a long time, computing complexity, easy being immersed in local extremum. The results are justified with the help of two numerical illustrations for fuzzy expected value model problem, the effectiveness of our new approach is demonstrated and it has determinate practical value.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s00521-021-06086-4,en,Predicting image credibility in fake news over social media using multi-modal approach,"['OriginalPaper', 'S.I. : NCACVIP']","Social media are the main contributors to spreading fake images. Fake images are manipulated images altered through software or by other means to change the information they convey. Fake images propagated over microblogging platforms generate misrepresentation and stimulate polarization in the people. Detection of fake images shared over social platforms is extremely critical to mitigating its spread. Fake images are often associated with textual data. Hence, a multi-modal framework is employed utilizing visual and textual feature learning. However, few multi-modal frameworks are already proposed; they are further dependent on additional tasks to learn the correlation between modalities. In this paper, an efficient multi-modal approach is proposed, which detects fake images of microblogging platforms. No further additional subcomponents are required. The proposed framework utilizes explicit convolution neural network model EfficientNetB0 for images and sentence transformer for text analysis. The feature embedding from visual and text is passed through dense layers and later fused to predict fake images. To validate the effectiveness, the proposed model is tested upon a publicly available microblogging dataset, MediaEval (Twitter) and Weibo, where the accuracy prediction of 85.3% and 81.2% is observed, respectively. The model is also verified against the newly created latest Twitter dataset containing images based on India's significant events in 2020. The experimental results illustrate that the proposed model performs better than other state-of-art multi-modal frameworks.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-022-03398-3,en,Multiobjective energy efficient street lighting framework: A data analysis approach,OriginalPaper,"A data analysis approach for designing an energy efficient street lighting framework is proposed to maximize both energy efficiency and uniformity of the system. A multiobjective optimization problem on obtaining energy efficiency is formulated in a comprehensive manner. Three multiobjective evolutionary optimization algorithms such as nondominated sorting genetic algorithm II, strength Pareto evolutionary algorithm 2 and multiobjective differential evolutionary algorithm are used to analyse the approximated Pareto solutions of our proposed model. The performance of considered algorithms are presented and compared with regard to different metrics. The results from the best algorithm, in terms of convergence and diversity, among the algorithms are then validated using DIALux to ensure the recommendation for the standardization in different aspects. The proposed work contributes a comprehensive data analysis on genetic algorithm solutions towards obtaining a multiobjective energy efficient street lighting which is beyond the scope of the existing works. The results obtained by the proposed method are also compared with existing DIALux results. The improvement of energy efficiency obtained by the proposed methodology over existing works is shown in terms of various aspects.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10957-022-02114-y,en,Outer Approximation for Mixed-Integer Nonlinear Robust Optimization,OriginalPaper,"Currently, few approaches are available for mixed-integer nonlinear robust optimization. Those that do exist typically either require restrictive assumptions on the problem structure or do not guarantee robust protection. In this work, we develop an algorithm for convex mixed-integer nonlinear robust optimization problems where a key feature is that the method does not rely on a specific structure of the inner worst-case (adversarial) problem and allows the latter to be non-convex. A major challenge of such a general nonlinear setting is ensuring robust protection, as this calls for a global solution of the non-convex adversarial problem. Our method is able to achieve this up to a tolerance, by requiring worst-case evaluations only up to a certain precision. For example, the necessary assumptions can be met by approximating a non-convex adversarial via piecewise relaxations and solving the resulting problem up to any requested error as a mixed-integer linear problem. In our approach, we model a robust optimization problem as a nonsmooth mixed-integer nonlinear problem and tackle it by an outer approximation method that requires only inexact function values and subgradients. To deal with the arising nonlinear subproblems, we render an adaptive bundle method applicable to this setting and extend it to generate cutting planes, which are valid up to a known precision. Relying on its convergence to approximate critical points, we prove, as a consequence, finite convergence of the outer approximation algorithm. As an application, we study the gas transport problem under uncertainties in demand and physical parameters on realistic instances and provide computational results demonstrating the efficiency of our method.","['Mathematics', 'Calculus of Variations and Optimal Control; Optimization', 'Optimization', 'Theory of Computation', 'Applications of Mathematics', 'Engineering, general', 'Operations Research/Decision Theory']"
doi:10.1007/s40747-022-00716-7,en,Multi-colony ant optimization with dynamic collaborative mechanism and cooperative game,"['OriginalPaper', 'Original Article']","Ant Colony Optimization easily falls into premature stagnation when solving large-scale Travelling Salesmen Problems. To address this problem, a multi-colony ant optimization with dynamic collaborative mechanism and cooperative game is proposed. Firstly, Ant Colony System and Max–Min Ant System form heterogeneous colonies. Secondly, to diversify the solutions of the algorithm, the Shapley value in the cooperative game is applied to share the information by distributing the pheromone payoff of the sub-colonies. In addition, the dynamic collaborative mechanism that contains two methods is designed to enhance the co-evolution of the heterogeneous populations. One, called public path recommendation strategy, is proposed to improve the astringency of Max–Min Ant System . The other is the pheromone fusion mechanism to regulate the pheromone distribution of Ant Colony System when the algorithm falls into stagnation, which can help the algorithm jump out of the local extremum effectively. Finally, the results demonstrate that the proposed methodology can improve the accuracy of solution effectively in solving large-scale TSP instances and has strong competitiveness with other swarm intelligent algorithms.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s40435-022-00947-z,en,A new intelligent adaptation mechanism of MRAS based on a genetic algorithm applied to speed sensorless direct torque control for induction motor,OriginalPaper,"Efficient sensorless approaches for controlled electrical motors reside in their simplicity and robustness, even with a wide range of operating speeds such as the model reference adaptive system. However, the linear tuning method of the PI regulator in the adaptation mechanism of the mentioned observer degrades the motor performance in the presence of external disturbances and parameter variations. This paper proposes an optimized genetic algorithm to automatically tune the PI regulator in the model reference adaptive system based on several execution sequences. A comparative research study between the conventional model reference adaptive system and the same observer based on a genetic algorithm was performed using the MATLAB/SIMULINK simulation platform. The analysis of this study was executed under various operating conditions. The results prove that this strategy can enhance speed and tracking performance while guaranteeing excellent behavior of overshoot and rejection time.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1007/s00170-022-10400-2,en,Surface integrity of Inconel 718 in electrical discharge grinding,"['OriginalPaper', 'ORIGINAL ARTICLE']","Electrical discharge grinding has large potential application prospect in Inconel 718 machining. Surface integrity of Inconel 718 in electrical discharge grinding is characterized by a variety of methods in the paper. The ground surface morphology is observed by scanning electron microscope. Electrical discharge grinding zone and pure grinding zone are generated on the ground surface in electrical discharge grinding. Electrical discharge grinding zone increases with the increases of depth of cut, feed speed, voltage, current, and the ratio of pulse width to pulse interval. Subsurface grain deformation is detected by means of electron backscatter diffraction method. In electrical discharge grinding zone, local misorientations are much less than those in pure grinding zone. Moreover, they tend to distribute along the grain boundaries in pure grinding zone. Moreover, there is almost no recrystallization in both types of zones. Electrolytic corrosion method is adopted to investigate the corrosion resistance. It is found that the corrosion resistance of electrical discharge grinding zone is much poorer than that of pure grinding zone. Furthermore, nano-indentation experiment is executed to explore the nano-mechanical properties. The results show that the hardness and modulus of material from low to high are respectively electrical discharge grinding, bulk, and pure grinding. It also indicates that the recast layer could be removed more easily during subsequent precision machining.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s12599-022-00745-z,en,Design Knowledge for Deep-Learning-Enabled Image-Based Decision Support Systems,"['OriginalPaper', 'Research Paper']","With the ever-increasing societal dependence on electricity, one of the critical tasks in power supply is maintaining the power line infrastructure. In the process of making informed, cost-effective, and timely decisions, maintenance engineers must rely on human-created, heterogeneous, structured, and also largely unstructured information. The maturing research on vision-based power line inspection driven by advancements in deep learning offers first possibilities to move towards more holistic, automated, and safe decision-making. However, (current) research focuses solely on the extraction of information rather than its implementation in decision-making processes. The paper addresses this shortcoming by designing, instantiating, and evaluating a holistic deep-learning-enabled image-based decision support system artifact for power line maintenance at a German distribution system operator in southern Germany. Following the design science research paradigm, two main components of the artifact are designed: A deep-learning-based model component responsible for automatic fault detection of power line parts as well as a user-oriented interface responsible for presenting the captured information in a way that enables more informed decisions. As a basis for both components, preliminary design requirements are derived from literature and the application field. Drawing on justificatory knowledge from deep learning as well as decision support systems, tentative design principles are derived. Based on these design principles, a prototype of the artifact is implemented that allows for rigorous evaluation of the design knowledge in multiple evaluation episodes, covering different angles. Through a technical experiment the technical novelty of the artifact’s capability to capture selected faults (regarding insulators and safety pins) in unmanned aerial vehicle (UAV)-captured image data (model component) is validated. Subsequent interviews, surveys, and workshops in a natural environment confirm the usefulness of the model as well as the user interface component. The evaluation provides evidence that (1) the image processing approach manages to address the gap of power line component inspection and (2) that the proposed holistic design knowledge for image-based decision support systems enables more informed decision-making. The paper therefore contributes to research and practice in three ways. First, the technical feasibility to detect certain maintenance-intensive parts of power lines with the help of unique UAV image data is shown. Second, the distribution system operators’ specific problem is solved by supporting decisions in maintenance with the proposed image-based decision support system. Third, precise design knowledge for image-based decision support systems is formulated that can inform future system designs of a similar nature.","['Business and Management', 'IT in Business', 'Business and Management, general']"
doi:10.1007/s11227-022-04650-w,en,Hybrid feature selection based on SLI and genetic algorithm for microarray datasets,OriginalPaper,"One of the major problems in microarray datasets is the large number of features, which causes the issue of “the curse of dimensionality” when machine learning is applied to these datasets. Feature selection refers to the process of finding optimal feature set by removing irrelevant and redundant features. It has a significant role in pattern recognition, classification, and machine learning. In this study, a new and efficient hybrid feature selection method, called Ga rank&rand , is presented. The method combines a wrapper feature selection algorithm based on the genetic algorithm (GA) with a proposed filter feature selection method, SLI- γ . In Ga rank&rand , some initial solutions are built regarding the most relevant features based on SLI- γ , and the remaining ones are only the random features. Eleven high-dimensional and standard datasets were used for the accuracy evaluation of the proposed SLI- γ . Additionally, four high-dimensional well-known datasets of microarray experiments were used to carry out an extensive experimental study for the performance evaluation of Ga rank&rand . This experimental analysis showed the robustness of the method as well as its ability to obtain highly accurate solutions at the earlier stages of the GA evolutionary process. Finally, the performance of Ga rank&rand was also compared to the results of GA to highlight its competitiveness and its ability to successfully reduce the original feature set size and execution time.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s11063-022-10845-2,en,A New Multi-classifier Ensemble Algorithm Based on D-S Evidence Theory,OriginalPaper,"Classifier ensemble is an important research content of ensemble learning, which combines several base classifiers to achieve better performance. However, the ensemble strategy always brings difficulties to integrate multiple classifiers. To address this issue, this paper proposes a multi-classifier ensemble algorithm based on D-S evidence theory. The principle of the proposed algorithm adheres to two primary aspects. (a) Four probability classifiers are developed to provide redundant and complementary decision information, which is regarded as independent evidence. (b) The distinguishing fusion strategy based on D-S evidence theory is proposed to combine the evidence of multiple classifiers to avoid the mis-classification caused by conflicting evidence. The performance of the proposed algorithm has been tested on eight different public datasets, and the results show higher performance than other methods.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11517-022-02672-3,en,Facilitation of dependent transfers with functional neuromuscular stimulation: a computer simulation study,"['OriginalPaper', 'Original Article']","A two-part simulation process was developed to investigate the facilitation of vertical patient lifts with functional neuromuscular stimulation (FNS) in individuals with spinal cord injury (SCI). First, external lifting forces representing caregiver assistance were applied to a 3D musculoskeletal model representing the patient and optimized to enforce a specific lifting trajectory during a forward dynamic simulation. The process was repeated with and without the activation of the knee, hip, and trunk extensor muscles of the patient model to represent contractions of the paralyzed muscles generated via FNS. Secondly, the spinal compression experienced by a caregiver at the L5/S1 joint while generating these external lifting forces was estimated using a second musculoskeletal model representing the caregiver. Simulation without muscle activation predicted spinal compression in the caregiver model approximately 1.3 × the National Institute for Occupational Safety and Health (NIOSH) recommended “Action Limit.” Comparatively, simulations with two unique patterns of muscle activation both predicted caregiver spinal compressions below NIOSH recommendations. These simulation results support the hypothesis that FNS activation of a patient’s otherwise paralyzed muscles would lower the force output required of a caregiver during a dependent transfer, thus lowering the spinal compression and risk of injury experienced by a caregiver. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s10479-022-05109-0,en,MIP-based solution approaches for multi-site resource-constrained project scheduling,"['OriginalPaper', 'Original Research']","The execution of a project is often distributed among multiple sites. The planning of such a project includes selecting a specific site for the execution of each of the project’s activities and allocating the available resource units to the execution of these activities over time. While some resource units are available at a certain site only, others can be moved across sites. Given the spatial distance between sites, transportation times arise if a resource unit must be transported from one site to another or if the output of an activity must be transported to another site. This planning problem has been introduced in recent literature as the multi-site resource-constrained project scheduling problem. We present a continuous-time model and devise a matheuristic for this planning problem. The continuous-time model uses, among others, binary variables to impose a sequence between activities assigned to the same resource units. In the matheuristic, the binary restrictions on these variables are initially relaxed and iteratively restored for the subset of activities scheduled in the current iteration. We compare the performance of the continuous-time model and the matheuristic to the performance of a discrete-time model and several metaheuristics from the literature using two sets of test instances from the literature. Both the continuous-time model and the matheuristic derive on average superior solutions in shorter average running times than the reference approaches.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s11063-022-10854-1,en,An Efficient Feature Selection for Intrusion Detection System Using B-HKNN and C2 Search Based Learning Model,OriginalPaper,"With the emergence of big data era, the dimensions of data are enhanced exponentially and it becomes a difficult task to handle information of high dimensions in various sectors like text mining, machine learning and data analysis. Redundant and inappropriate feature enhances the complexities in dimensions that further results in poor performances. In the intrusion detection system, the feature selection is considered as one of the most significant processes to improve the performances of the system. Due to high dimensional data, there occurs a drop in accuracy and efficiency. To overcome such drawback, this paper proposes three major phases namely the data pre-processing, feature selection and classification phases. In data-pre processing phase, the input data comprising of various noise signals, high dimensional and redundant data, numerous irrelevant features etc. are extracted. The second phase involves the selection of features using cooperative and competitive (C 2 ) search based learning algorithm. In the classification phase, the extracted features are classified optimally using Bonferroni based Hybrid k-nearest neighbour (B-HkNN) algorithm thereby obtaining an optimal intrusion detection system. Furthermore, the proposed approach based on intrusion detection system is evaluated by the standard CICIDS2017 and ADFA-LD datasets to determine the accuracy and efficiency of the system.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s00530-022-00957-z,en,Facial expression recognition of online learners from real-time videos using a novel deep learning model,"['OriginalPaper', 'Regular Paper']","In every learning setting, in classrooms or online, a student's emotions throughout course involvement play a critical role. It employs disturbing, excite, and eye and head movement patterns to infer important information about a student's mood in an e-learning environment. Researchers from numerous disciplines have been focusing on emotion detection technologies to better understand user engagement, efficacy, and utility of systems that have been established or are being deployed. The goal of this study is to see if students' facial expressions can be used by lecturers to understand students' comprehension levels in a virtual classroom, as well as to determine the influence of facial expressions during lectures and the degree of comprehension displayed by these emotions. The objective is to determine which facial physical behaviours are associated with emotional states and then to determine how these emotional states are related to student understanding. The major purpose of this work is to plan and develop a new deep learning-oriented facial expression recognition (FER) of online learners from real-time videos. For the frames of online learners from real-time videos, the Viola–Jones Face detection algorithm is employed for face detection. Further, the pattern extraction is performed by the optimized local directional Texture Pattern (LDTP) using the hybrid Coyote Optimization Algorithm (COA), and Deer Hunting Optimization Algorithm (DHOA) referred as Coyote–Deer Hunting Optimization (C-DHO). These pattern images are inputted to the convolutional neural network (CNN) for deep feature extraction. Furthermore, the heuristically modified recurrent neural network (HM-RNN) using the same C-DHO is used for the expression recognition. The experimental research reveals that the suggested method aids in the identification of emotions as well as the classification of student participation and interest in the topic, all of which are displayed as feedback to the teacher in terms of improving the learner experience.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s11063-022-10863-0,en,"Lightweight Plant Disease Classification Combining GrabCut Algorithm, New Coordinate Attention, and Channel Pruning",OriginalPaper,"In this paper, a lightweight convolutional neural network is proposed for the classification of plant diseases, containing 63 classes of states for 11 plant species. The different context of experimental data and data in the real environment, insufficient accuracy of the model classification, and oversized model are three main problems of deep learning techniques applied to agricultural production. In this paper, we mainly focus on these three problems. First, the GrabCut algorithm is adopted to unify the background of the experimental data and the real data to black, allowing the trained model to have the same good effect when applied in practice. Then, we propose a new coordinate attention block to improve the classification accuracy of convolutional neural networks and empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models. Finally, channel pruning is applied to the trained model, which reduces the model size and computational effort by 85.19 $$\%$$ % and 92.15 $$\%$$ % respectively with little change in the model accuracy, making it better suited for agricultural platforms with lower memory and computational capacity.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1186/s12859-022-04880-y,en,Robust and accurate prediction of self-interacting proteins from protein sequence information by exploiting weighted sparse representation based classifier,"['OriginalPaper', 'Research']","Background Self-interacting proteins (SIPs), two or more copies of the protein that can interact with each other expressed by one gene, play a central role in the regulation of most living cells and cellular functions. Although numerous SIPs data can be provided by using high-throughput experimental techniques, there are still several shortcomings such as in time-consuming, costly, inefficient, and inherently high in false-positive rates, for the experimental identification of SIPs even nowadays. Therefore, it is more and more significant how to develop efficient and accurate automatic approaches as a supplement of experimental methods for assisting and accelerating the study of predicting SIPs from protein sequence information. Results In this paper, we present a novel framework, termed GLCM-WSRC (gray level co-occurrence matrix-weighted sparse representation based classification), for predicting SIPs automatically based on protein evolutionary information from protein primary sequences. More specifically, we firstly convert the protein sequence into Position Specific Scoring Matrix (PSSM) containing protein sequence evolutionary information, exploiting the Position Specific Iterated BLAST (PSI-BLAST) tool. Secondly, using an efficient feature extraction approach, i.e., GLCM, we extract abstract salient and invariant feature vectors from the PSSM, and then perform a pre-processing operation, the adaptive synthetic (ADASYN) technique, to balance the SIPs dataset to generate new feature vectors for classification. Finally, we employ an efficient and reliable WSRC model to identify SIPs according to the known information of self-interacting and non-interacting proteins. Conclusions Extensive experimental results show that the proposed approach exhibits high prediction performance with 98.10% accuracy on the yeast dataset, and 91.51% accuracy on the human dataset, which further reveals that the proposed model could be a useful tool for large-scale self-interacting protein prediction and other bioinformatics tasks detection in the future.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s11356-022-22052-6,en,Allocating China’s 2025 CO2 emission burden shares to 340 prefecture cities: methods and findings,"['OriginalPaper', 'Research Article']","Peak emission is an important policy/scheme for all the countries to respond greenhouse gas mitigation. The key is how to distribute the emission burden shares to its sub-regions. This study aims to develop a prefecture city leveled CO 2 emission allocation model by integrating multi-indicators method and benchmark method so that China’s 2025 (end year of 14th Five-Year Plan, FYP) CO 2 emission burdens can be allocated to its prefecture cities and provinces. Results show that China’s total CO 2 emission will reach 12 billion tons in 2025. The majority of such emission will occur in the east China due to its more developed economy and dense population. Cities with high emissions are usually allocated more emission quotas, such as Shanghai, Tianjin, Chongqing, Tangshan, Yulin, Suzhou, and Ningbo. The top five provinces with higher CO 2 emission quotas are traditionally high-emission and energy-intensive provinces, including Shandong, Jiangsu, Inner Mongolia, Henan, and Hebei. The national CO 2 emission intensity will decrease by 69.35% in 2025 compared to the 2005 level. The CO 2 emission intensity reduction rates among the 340 Chinese cities is found to be fluctuating significantly from 16 to 74% during the 14th FYP. Finally, policy recommendations are raised for mitigating city level CO 2 emissions by considering the local realities.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s40747-022-00727-4,en,A system for electric vehicle’s energy-aware routing in a transportation network through real-time prediction of energy consumption,"['OriginalPaper', 'Original Article']","To tackle the problem of range anxiety of a driver of an electric vehicle (EV), it is necessary to accurately estimate the power/energy consumption of EVs in real time, so that drivers can get real-time information about the vehicle’s remaining range. In addition, it can be used for energy-aware routing, i.e., the driver can be provided with information that on which route less energy consumption will take place. In this paper, an integrated system has been proposed which can provide reliable and real-time estimate of the energy consumption for an EV. The approach uses Deep Auto-Encoders (DAE), cross-connected using latent space mapping, which consider historical traffic speed to predict the traffic speed at multiple time steps in future. The predicted traffic speed is used to calculate the future vehicle speed. The vehicle speed, acceleration along with wind speed, road elevation, temperature, battery’s SOC, and auxiliary loads are used as input to a multi-channel Convolutional Neural Network (CNN) to predict the energy consumption. The prediction is further fine-tuned using a Bagged Decision Tree (BDT). Unlike other existing techniques, the proposed system can be easily generalized for other vehicles as it is independent of internal vehicle parameters. Comparison with other benchmark techniques shows that the proposed system performs better and has a least mean absolute percentage error of 1.57%.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s40095-022-00497-2,en,Taxonomy of green cloud computing techniques with environment quality improvement considering: a survey,"['OriginalPaper', 'Original Research']","Nowadays, cloud computing is one of the most up-to-date topics conducted by many researchers. The specialists and researchers try to create a new generation of data centers using virtual machines to supply the network service virtually and dynamically. These services will lead everyone to access their required application worldwide via the Internet. Furthermore, the number of datacenters (DC) is growing exponentially. Therefore, a novel concept called green computing has been raised to decrease the negative effect of data centers to protect the environment. Green cloud computing solutions strive to reduce carbon dioxide emissions, energy, power, and water consumption that are harmful to the environment. In this paper, the approaches moving toward green computing are investigated and categorized to help the researchers and specialists in cloud computing expand green cloud computing and improve the environment quality. The ""green cloud computing"" has been searched in this survey. We have searched ACM, IEEE, Elsevier, and Springer and surveyed the papers between 2010 and 2022. This paper is a holistic survey useful for researchers who work on green cloud computing and its environmental influence. This paper can lead researchers to move toward green computing to protect the environment against these days’ environmental issues. These days, environmental issues like climate change make this subject more important than before.","['Energy', 'Renewable and Green Energy']"
doi:10.1007/s00607-022-01100-6,en,A clustering approach for software defect prediction using hybrid social mimic optimization algorithm,"['OriginalPaper', 'Regular Paper']","In this information era, software usage is intertwined with daily routine work and business. Defects in software can cause a severe economic crisis. It is a crucial task in the software industry to be able to predict software defects in advance. Software Defect Prediction (SDP) aims to identify the potential defects based on the software metrics. A software module is a software component(piece of program) that contains one or more procedure. In this study, we propose a clustering approach for grouping the software modules. This work proposes a hybrid elitist self-adaptive multi-population social mimic optimization technique (ESAMP-SMO) for clustering the software defect modules. The objective function (fitness function) of the proposed study minimizes the intra cluster distance and maximizes fault prediction rate. In this study, we used the three popular benchmark NASA datasets (CM1, JM1 and KC1) for the experimental work. The performance comparison analysis shows that the proposed clustering technique outperforms the other competitor approaches.","['Computer Science', 'Computer Science, general', 'Information Systems Applications (incl.Internet)', 'Computer Communication Networks', 'Software Engineering', 'Artificial Intelligence', 'Computer Appl. in Administrative Data Processing']"
doi:10.1007/s40031-022-00777-9,en,Path Planning of Mobile Robot Based on Improved Ant Colony Optimization,"['OriginalPaper', 'Original Contribution']","For the global path planning of mobile robots in static environment, the traditional ant colony optimization (ACO) algorithm has some disadvantages, such as slow convergence speed and easy to fall into local optimum. In order to deal with these problems, a novel improved ACO algorithm is proposed in this paper. First, in order to accelerate the convergence of the traditional ACO algorithm, several coefficients, such as pheromone volatility coefficient, path feasible coefficient, pheromone eliciting factor, expectation eliciting factor, memory parameter, social phobia parameter, personality factor and emotion fluctuation coefficient, were designed in the improved ACO algorithm. Meanwhile, in order to decrease the probability of the traditional ACO algorithm falling into the local optimum, a specific parameter is designed to make the ants evolve continuously, so the ants can reach the target by the shortest path. Finally, three simulation experiments are carried out in static environment to verify the potential of improved ant colony algorithm in mobile robot path planning.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s41315-022-00245-z,en,Proximal policy optimization for formation navigation and obstacle avoidance,"['OriginalPaper', 'Regular Paper']","In this paper, a formation control problem of second-order holonomic agents is considered, where agents navigate around obstacles using proximal policy optimization (PPO)-based deep reinforcement learning (DRL). The formation is allowed to shrink and expand, while maintaining its shape, in order to navigate the geometric centroid of the formation towards the goal. A bearing-based reward function is presented that depends on the bearing error of each agent towards its designated neighbors. The agents share a single policy that is trained in a centralized manner. Distance measurements, state information, error information regarding neighboring agents, and simulation information are used for training the policy in an end-to-end fashion. Simulation results using the proposed approach are compared with that obtained using an angle-based reward function.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s11042-022-11926-z,en,Hand hygiene monitoring and compliance system using convolution neural networks,OriginalPaper,"Hand hygiene monitoring and compliance systems play a significant role in curbing the spread of healthcare associated infections and the COVID-19 virus. In this paper, a model has been developed using convolution neural networks (CNN) and computer vision to detect an individual’s germ level, monitor their hand wash technique and create a database containing all records. The proposed model ensures all individuals entering a public place prevent the spread of healthcare associated infections (HCAI). In our model, the individual’s identity is verified using two-factor authentication, followed by checking the hand germ level. Furthermore, if required the model will request sanitizing/ hand wash for completion of the process. During this time, the hand movements are checked to ensure each hand wash step is completed according to World Health Organization (WHO) guidelines. Upon completion of the process, a database with details of the individual’s germ level is created. The advantage of our model is that it can be implemented in every public place and it is easily integrable. The performance of each segment of the model has been tested on real-time images an validated. The accuracy of the model is 100% for personal identification, 96.87% for hand detection, 93.33% for germ detection and 85.5% for the compliance system respectively.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s12145-022-00816-5,en,Identification of areas at the risk of landslide via the short-time Fourier transform,"['OriginalPaper', 'Research Article']","Landslide is a natural hazard that, next to earthquakes and floods, causes the largest damage to humans. Nowadays, experts take landslide events into serious consideration and they try to map and identify them accurately in time domain. So far, most of the methods used for this purpose have been of the time-domain type with their own drawbacks. Dealing with some functions in a time-domain method is difficult due to the complexity of calculations, frequency behavior analysis and content extraction. Therefore, a frequency-domain format was used in this study. The short-time Fourier transform and weighted overlay methods were applied in order to detect landslides-prone areas, and the results were evaluated using the prediction-area plot. As the intersection point of the P–A plot in the power spectrum-area model demonstrated 82% of landslide events had occurred only in 18% of the district, while the intersection point of the P–A plot in the weighted overlay model showed the occurrence of 70% of landslide events only in 30% of the study area.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s41019-022-00191-7,en,Intent-Aware Data Visualization Recommendation,"['OriginalPaper', 'Research Papers']","This paper proposes a visualization recommender system for tabular data given visualization intents (e.g., “population trends in Italy” and “smartphone market share”). The proposed method predicts the most suitable visualization type (e.g., line, pie, or bar chart) and visualized columns (columns used for visualization) based on statistical features extracted from the tabular data as well as semantic features derived from the visualization intent. To predict the appropriate visualization type, we propose a bi-directional attention (BiDA) model that identifies important table columns using the visualization intent and important parts of the intent using the table headers. To determine the visualized columns, we employ a pre-trained neural language model to encode both visualization intents and table columns and predict which columns are the most likely to be used for visualization. Since there was no available dataset for this task, we created a new dataset consisting of over 100 K tables and their appropriate visualization. Experiments revealed that our proposed methods accurately predicted suitable visualization types and visualized columns.","['Computer Science', 'Database Management', 'Data Mining and Knowledge Discovery', 'Algorithm Analysis and Problem Complexity', 'Systems and Data Security', 'Artificial Intelligence', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences']"
doi:10.1007/s11554-022-01255-7,en,Towards SSD accelerating for embedded environments: a compressive sensing based approach,"['OriginalPaper', 'Original Research Paper']","Since the rise of convolutional neural networks (CNN), deep learning-based computer vision has been a dynamic field of research. Nevertheless, modern CNN architectures have not given sufficient consideration to real−time applications within limited computation settings and always compromise speed and accuracy. To this end, a novel approach to CNN design, based on the emerging technology of compressive sensing (CS), is proposed. For instance, CS networks function in a compression−reconstruction approach as an encoder−decoder neural network. This approach transforms the computer vision problem into a multioutput learning problem by incorporating the CS network into a recognition network for joint training. As to the deployment phase, images are obtained from a CS−acquisition device and fed directly, without reconstruction, to the new recognition network. Following such an approach considerably improves transmission bandwidth and reduces the computational burden. Furthermore, the redesigned CNN holds fewer parameters than its original counterpart, thus reducing model complexity. To validate our findings, object detection using the Single−Shot Detector (SSD) network was redesigned to operate in our CS−based ecosystem using different datasets. The results show that the lightweight CS network offers good performance at a faster running speed. For instance, the number of FLOPS was reduced by 57% compared to the SSD baseline. Furthermore, the proposed CS_SSD achieves a compelling accuracy while being 30% faster than its original counterpart on small GPUs. Code is available at: https://github.com/Bouderbal-Imene/CS-SSD .","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s10489-022-03317-6,en,Improving indoor visual navigation generalization with scene priors and Markov relational reasoning,OriginalPaper,"The problem of visual navigation is the poor generalization to find the given target object in unexplored environment without the help of auxiliary sensors. We propose solving the visual navigation problem by incorporating object spatial scene priors and visible object relational reasoning. To get more accurate ground truth environment priors, we construct specific scene graph priors for indoor navigation, which provides rich object spatial relationships for helping finding the target objects by object relation detection. Furthermore, to imitate human’s reasonability in searching objects, we encode our scene graph priors with Markov model for relational reasoning and fuse them into reinforcement learning policy network, which improves model generalization in novel scenes. Moreover, we perform experiments on the AI2THOR virtual environment and outperform the current most state-of-the-art both in SPL (Success weighted by Path Length) and success rate on average.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-11993-2,en,Cryptography and Tay-Grey wolf optimization based multimodal biometrics for effective security,OriginalPaper,"Biometric recognition is very important for automatically recognizing individuals based on feature vectors from behavioral or physiological characteristics. The biometric recognition systems provide suitable personal recognition approaches for determining individuals. Biometrics are broadly employed in several commercial as well as official identification systems for automatic access control. This paper introduces the model for multi-modal biometric recognition based on the feature level fusion method. The overall procedure of the proposed method involves four steps: pre-processing, feature extraction, recognition feature-level fusion, and Bio-metric recognition. The first step is to input the images into pre-processing steps. Thus, pre-processing three traits, like face, finger knuckle, and the hand vein, is done. Then, the feature extraction is done for each modality to extract the features. After that, the feature level fusion is carried out using Elliptic-curve cryptography (ECC) and the proposed Taylor-Grey Wolf optimization (Tay-GWO). After feature fusion, the Bio-metric recognition is done based on Deep Convolutional Neural Network (DCNN), which Tay-GWO trains. The proposed Tay-GWO is designed by integrating the Taylor series and Grey Wolf Optimization (GWO). The analysis shows that the developed model achieves the maximal accuracy of 94.86%, maximal sensitivity of 96.80%, and specificity of 93.74%, respectively.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s12530-022-09418-4,en,Application of hybrid metaheuristic with Levenberg-Marquardt algorithm for 6-dimensional magnetic localization,"['OriginalPaper', 'Original Paper']","Wireless capsule endoscopies are being developed for gastrointestinal tract examination via magnetic tracking technology. These capsules make it possible to examine the patient’s digestive system without pain and easily diagnose diseases. However, one of the most important problems of these capsules is localization. This localization information includes 3-dimensional position and 3-dimensional orientation data from a set of magnetic sensors. These data must be obtained with the smallest error values. In recent years, metaheuristic algorithms have become popular in many fields due to their flexible nature. In this paper, the performances of frequently used algorithms such as artificial bee colony (ABC), differential evolution (DE), particle swarm optimization (PSO), teaching-learning based optimization (TLBO), genetic algorithm (GA), gravitational search algorithm (GSA), and whale optimization algorithm (WOA) are compared for magnetic localization problems. In addition, hybrid models combined with the Levenberg-Marquardt (LM) algorithm have been developed to increase their performance. In particular, while the PSO+LM algorithm is more successful than other algorithms, an adaptive version of this algorithm has been proposed to improve its performance further. Using the proposed version, the errors in the PSO+LM algorithm are further reduced and thus the localization efficiency of the capsules is increased.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s41403-022-00359-7,en,Improving Frequency Regulation in a Power System Using STATCOM-SMES Combination,"['OriginalPaper', 'Original Article']","The tie-line power oscillation damping provided by Static Synchronous Compensator (STATCOM) has not been explored in load frequency control studies. A linear time invariant model for a STATCOM is developed to regulate area frequencies in the presence of the utility side Automatic Generation Control (AGC) for the maiden time. Super Magnetic Energy Storage (SMES) has an advantage in load levelling applications due to lack of mechanical inertia. The combination of STATCOM-SMES has been implemented in this paper to improve load frequency control and to damp low-frequency oscillations in the tie line. The gains of integral controller in the respective control areas are tuned by Genetic Algorithm. MATLAB/SIMULINK simulations are carried out on a sample case consisting of two areas, each comprising two reheat thermal units. Results with STATCOM and SMES show that transient responses have been enhanced appreciably with respect to overshoots and settling times.","['Engineering', 'Engineering, general']"
doi:10.1007/s13042-022-01622-7,en,Representation learning from noisy user-tagged data for sentiment classification,"['OriginalPaper', 'Original Article']","Sentiment classification aims to identify the sentiment orientation of an opinionated text, which is widely used for market research, product recommendation, and etc. Supervised deep learning approaches are prominent in sentiment classification and have shown the power in representation learning, however such methods suffer from the costly human annotations. Massive user-tagged opinionated texts on the Internet provide a new source for annotation, such as twitter with emoji. However, the texts may contain noisy labels, which may cause ambiguity during training process. In this paper, we propose a novel Weakly-supervised Anti-noise Contrastive Learning framework for sentiment classification, and name it as WACL. We first adopt the supervised contrastive training strategy during the pre-training phase to fully explore potential contrast patterns of weakly-labeled data to learn robust representations. Then we design a simple dropping-layer strategy to remove the top layers from the pre-trained model that are susceptible to noisy data. Last, we add a classification layer on top of the remaining model and fine tune it with labeled data. The proposed framework can learn rich contrastive sentiment patterns in the case of label noise and is applicable to a variety of deep encoders. The experimental results on the Amazon product review, Twitter and SST5 datasets demonstrate the superiority of our method.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s11356-021-15962-4,en,A classic critique on concrete adsorbing pollutants emitted by automobiles and statistical envision using trend analysis,"['ReviewPaper', 'Research on Sustainable Developments for Environment Management']","Globally, road transportation is responsible for about 1/3 of total air pollution, among which Co 2 , No X and So X are major by volume which are directly responsible for ozone layer depletion. As these activities continue to rise, the nature faces the threat of an unprecedented environmental catastrophe. In this survey, the adsorbent and the methods used to adsorb the vehicle emission pollutant directly from the ambient air and enhance the air quality are reviewed. There are extensive number of adsorbents available namely titanium oxide, polyethyleneimine (PEI), activated carbon, and other natural admixtures and methods that adsorb these pollutants such as Co 2 ; specifically, polyethyleneimine (PEI) is a polymer-based product which has a behavior of adsorbing Co 2 directly from ambient air. A carbon-neutral technology for eliminating anthropogenic CO 2 emissions has been proposed, trapping CO 2 from the atmosphere. The major concern including Co 2 adsorption rate and methods to evaluate the volume of adsorption with time has been reviewed. Trend analysis depends on the reason that can foresee what will occur later on by seeing what has happened beforehand. The trend analysis method was used in this study to model the adsorption rate of PEI based on their pore diameter. When it comes to designing regression analysis, R 2 values of 0.2 to 0.3 indicate that statistical tests have no meaningful impact and further research is needed. For adsorption of No X , a lot of adsorbents are used namely sodium bentonite, zeolite, activated carbon, and other natural minerals. Among them, activated carbon enhances the adsorption rate of No x . Vast research has been conducted to analyze the adsorption rate, advantages, disadvantages, and the behavior of different adsorbent with the concrete. Literally for SO x , the adsorbents that are widely used are coal ash, copper oxide, and silicate. The characterization of different adsorbents with their adsorbate can be analyzed by some test methods as follows: Fourier-transform infrared spectroscopy (FTIR), BET analysis, scanning electron microscopy (SEM), field emission scanning electron microscopy (FESEM) analysis, surface area, X-ray diffraction (XRD), porosity analysis, and more according to the requirement. The quantity of adsorbent and adsorbate could be evaluated for facing the real-time constraints through a detailed research by review.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s11665-022-06987-y,en,High-Temperature Deformation Constitutive Model of Zircaloy-4 Based on the Support Vector Regression Algorithm during Hot Rolling,"['OriginalPaper', 'Technical Article']","Due to the small range of plastic deformation temperatures during hot rolling of Zircaloy-4 plates, it is important to determine the appropriate flow behaviors for plate profile control of Zircaloy-4 plates. The developed microstructures and mechanical properties of Zircaloy-4 are evaluated by metallographic observations and Gleeble-3800 thermal simulation tester. To meet the need of data with small-sample properties, the support vector regression (SVR) algorithm is adopted to predict the constitutive model of Zircaloy-4, and the improved particle swarm optimization algorithm (IPSO) is used to optimize parameters of SVR algorithm. Meanwhile, results indicate that the correlation coefficient ( R 2 ) value of zirconium alloy constitutive model is 96.805%. Based on employed algorithm, comparing with modified Arrhenius model, the results show the superiority of IPSO-SVR algorithm. This provides an important theoretical basis for FE simulation of controlling the Zircaloy-4 plate shape during hot rolling process.","['Materials Science', 'Characterization and Evaluation of Materials', 'Tribology, Corrosion and Coatings', 'Quality Control, Reliability, Safety and Risk', 'Engineering Design']"
doi:10.1007/s11045-022-00839-7,en,Early seizure detection in childhood focal epilepsy with electroencephalogram feature fusion on deep autoencoder learning and channel correlations,OriginalPaper,"Recognition of epileptic electroencephalogram (EEG) signals is vital to epileptic seizure detection. Current research on seizure detection mostly focused on generalized seizure analysis. Compared with generalized seizures, childhood focal epilepsy generally originates in one hemisphere of the brain, and the seizure onset patterns vary from patient to patient, making it difficult for analysis. Meanwhile, the frequency, amplitude and rhythm of EEG activities in children of different ages are different, making childhood focal epilepsy detection challenging. In this paper, the channel correlation features (CCF) containing the regional scalp EEG cross correlations and auto-correlations are proposed for children focal epilepsy EEG representation. The spectrum features are also extracted to characterize EEGs in different frequency bands. Further, a convolutional autoencoder (CAE)-based deep feature learning and dimensionality reduction model is proposed for discriminative EEG frequency domain feature extraction. An early seizure detection framework for the childhood focal epilepsy based on the fused CAE and CCF EEG features is finally developed, and an ensemble classification model (ECM) is applied to solve the imbalance issue between ictal, interictal, and preictal. The performance is evaluated on the EEG dataset collected by the Children’s Hospital, Zhejiang University School of Medicine (CHZU). Experiments show that the proposed algorithm can reach to the highest accuracy of 93.57% for the early seizure detection in childhood focal epilepsy.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Artificial Intelligence']"
doi:10.1007/s00366-020-01241-2,en,A novel systematic and evolved approach based on XGBoost-firefly algorithm to predict Young’s modulus and unconfined compressive strength of rock,"['OriginalPaper', 'Original Article']","To design the tunnel excavations, the most important parameters are the engineering properties of rock, e.g., Young’s modulus (E) and unconfined compressive strength (UCS). Numerous researchers have attempted to propose methods to estimate E and UCS indirectly. This task is complex due to the difficulty of preparing and carrying out such experiments in a laboratory. The main aim of the present study is to propose a new and efficient machine learning model to predict E and UCS. The proposed model combines the extreme gradient boosting machine (XGBoost) with the firefly algorithm (FA), called the XGBoost-FA model. To verify the feasibility of the XGBoost-FA model, a support vector machine (SVM), classical XGBoost, and radial basis function neural network (RBFN) were also employed. Forty-five granite sample sets, collected from the Pahang-Selangor tunnel, Malaysia, were used in the modeling. Several statistical functions, such as coefficient of determination ( R 2 ), mean absolute percentage error (MAPE) and root mean square error (RMSE) were calculated to check the acceptability of the methods mentioned above. A review of the results of the proposed models revealed that the XGBoost-FA was more feasible than the others in predicting both E and UCS and could generalize.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11071-022-07803-x,en,Application of the dynamical system method and the deep learning method to solve the new (3+1)-dimensional fractional modified Benjamin–Bona–Mahony equation,"['OriginalPaper', 'Original Paper']","This paper investigates a new (3+1)-dimensional fractional modified Benjamin–Bona–Mahony equation (fmBBM). Two methods are applied to solve the fmBBM equation. On the one hand, we make a detailed analysis on the dynamical behavior of the traveling systems derived from the fmBBM equation. Accordingly, by computing a large number of complicated elliptic integrals, we obtain the exact expressions of all bounded and unbounded traveling wave solutions of the fmBBM equation. On the other hand, a deep neural network is established and trained for the first time to solve the high-dimensional equation. The exact solutions obtained by dynamical system method can just be used to compare with the corresponding numerical solutions given by the deep learning algorithm. The comparison results show that our deep learning algorithm has high accuracy and works well to solve the high-dimensional fractional equation.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s10845-021-01806-y,en,An imbalanced data learning method for tool breakage detection based on generative adversarial networks,OriginalPaper,"Tool breakage in manufacturing procedures can damage machined surfaces and machine tools. It is crucial to detect tool breakage in time and promptly respond to it. Due to the safety restrictions imposed in production, failure samples are significantly scarcer than normal samples, and this disequilibrium results in difficulty of failure detection. Therefore, we propose a new imbalanced data learning method for tool breakage detection. The key strategy is to balance the data distribution by producing valuable artificial samples for the minority class using an adversarial generative oversampling model based on a generative adversarial network (GAN). Unlike previous studies using GAN, we use the discriminator to screen samples generated by the generator and achieve effective oversampling. Multiple classifiers are adopted as the decision-making models to perform tool breakage detection. The proposed method is applied to a set of imbalanced experimental tool breakage data collected in a workshop. Compared with the best results of other oversampling solutions, the proposed method improves the breakage detection rate from 93.6% to 100%, which shows its practicability and validity. Additionally, evaluations are performed based on 12 imbalanced benchmark datasets. The results further substantiate the superiority of the proposed method to existing sampling methods.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11107-022-00982-y,en,Quality of service improvement in fiber-wireless networks using a fuzzy-based nature-inspired algorithm,"['OriginalPaper', 'Original Paper']","Fiber-Wireless (Fi-Wi) networks attract interest as a dependable communication backbone in several additional applications. The cheap cost, dependability, accessibility of Wireless Sensor Networks, and optical fiber networks’ high bandwidth and reliability contribute to it. Although Fi-Wi networks supply good delay performance, they may not be able to fulfill the needs of the smart grid delay-critical applications mentioned earlier. Since the Quality of Service (QoS) is a prerequisite for deploying high-speed Fi-Wi broadband access networks, it is critical to identify and address common issues (such as bandwidth and latency) in high-speed next-generation networks. This study offers a unique strategy for improving QoS in Fi-Wi networks based on a fuzzy-based Cat Swarm Optimization (CSO) algorithm. CSO is a strong metaheuristic swarm-based optimization technique that has garnered a lot of favorable comments since its inception. CSO is a resilient and strong meta-heuristic algorithm based on cat behavior. It has two search modes: seeking and tracing, which may be combined using the mixing ratio parameter. The suggested solution reduces energy usage, packet loss, and latency, according to simulation findings using the MATLAB program. Compared to previous methods, simulations reveal that the suggested method produces relatively excellent outcomes and has a higher performance.","['Computer Science', 'Computer Communication Networks', 'Electrical Engineering', 'Characterization and Evaluation of Materials']"
doi:10.1007/s00521-022-07596-5,en,Optimizing deadline violation time and energy consumption of IoT jobs in fog–cloud computing,"['OriginalPaper', 'Original Article']","Nowadays, Internet of Things (IoT) devices are ubiquitous and their number is growing rapidly. These devices produce massive amount of data which need to be efficiently processed. Since most of the IoT devices are resource constrained in terms of computational capability and power resources, they have to offload their computation jobs to more powerful computing devices. Fog–cloud computing is a promising platform for processing IoT jobs. However, due to the heterogeneity of the computing devices, how to schedule IoT jobs in this environment is a challenging issue. To tackle this issue, in this paper, we first present a system model for the job scheduling problem in fog–cloud computing with the aim of optimizing the total deadline violation time of jobs and the energy consumption of the system. Then, we propose two nature-inspired optimization techniques, grey wolf optimization and grasshopper optimization algorithm to efficiently solve the job scheduling problem in the fog–cloud environment. The performance of the proposed algorithms is evaluated against the state-of-the-art algorithms using various simulation experiments. The results demonstrate that the proposed schedulers are capable of reducing the total deadline violation time about 68% and energy consumption about 22% compared to the second-best results.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s41403-022-00361-z,en,Machine Learning-Based Illumination to Optimize Local Contrast Between Tissues Based on the Human Visual System,"['OriginalPaper', 'Original Article']","Visual inspection during surgeries is one of the most primary aspects when it comes to differentiating the tissues of interest from the rest. The observed colours and patterns of the tissues will depend on the spectral distribution of the surgical light being used; therefore, it becomes necessary that the lights should not only have high colour rendition, but also have adjustable spectral distribution to contrast between the tissues and find the required patterns. LEDs provide a wide range of spectral distribution that can be adjusted dynamically while operating. This research is based on a machine learning approach to estimate the output correlated colour temperature obtained based on the user adjustment of the available light spectrum. It has been found that depending on the age and gender of the medical practitioner, or based on different tissues being operated on, the perception of output light spectrum will vary from individual to individual; thus, this approach provides a way to optimize the settings as per the requirements with decent accuracy. The effectiveness of this method was verified using a spectral irradiance colorimeter and the results were found to be within the acceptable limits.","['Engineering', 'Engineering, general']"
doi:10.1007/s10489-022-03378-7,en,Attacking Bitcoin anonymity: generative adversarial networks for improving Bitcoin entity classification,OriginalPaper,"Classification of Bitcoin entities is an important task to help Law Enforcement Agencies reduce anonymity in the Bitcoin blockchain network and to detect classes more tied to illegal activities. However, this task is strongly conditioned by a severe class imbalance in Bitcoin datasets. Existing approaches for addressing the class imbalance problem can be improved considering generative adversarial networks (GANs) that can boost data diversity. However, GANs are mainly applied in computer vision and natural language processing tasks, but not in Bitcoin entity behaviour classification where they may be useful for learning and generating synthetic behaviours. Therefore, in this work, we present a novel approach to address the class imbalance in Bitcoin entity classification by applying GANs. In particular, three GAN architectures were implemented and compared in order to find the most suitable architecture for generating Bitcoin entity behaviours. More specifically, GANs were used to address the Bitcoin imbalance problem by generating synthetic data of the less represented classes before training the final entity classifier. The results were used to evaluate the capabilities of the different GAN architectures in terms of training time, performance, repeatability, and computational costs. Finally, the results achieved by the proposed GAN-based resampling were compared with those obtained using five well-known data-level preprocessing techniques. Models trained with data resampled with our GAN-based approach achieved the highest accuracy improvements and were among the best in terms of precision, recall and f1-score. Together with Random Oversampling (ROS), GANs proved to be strong contenders in addressing Bitcoin class imbalance and consequently in reducing Bitcoin entity anonymity (overall and per-class classification performance). To the best of our knowledge, this is the first work to explore the advantages and limitations of GANs in generating specific Bitcoin data and “attacking” Bitcoin anonymity. The proposed methods ultimately demonstrate that in Bitcoin applications, GANs are indeed able to learn the data distribution and generate new samples starting from a very limited class representation, which leads to better detection of classes related to illegal activities.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03434-2,en,SentATN: learning sentence transferable embeddings for cross-domain sentiment classification,OriginalPaper,"Cross-domain Sentiment Classification (CDSC) aims to exploit useful knowledge from the source domain to obtain a high-performance classifier on the target domain. Most of the existing methods for CDSC mainly concentrate on extracting domain-shared features, while ignoring the importance of domain-specific features. Besides, these approaches focus on reducing the discrepancy of the source domain and target domain on the word-level. As a result, they cannot fully capture the whole meaning of a sentence, which makes these methods unable to learn enough transferable features. To address these issues, we present a Sentence-level Attention Transfer Network (SentATN) for CDSC, with two distinctive characteristics. Firstly, we design an efficient encoder unit to extract domain-specific features of a sentence. Secondly, SentATN provides a sentence-level adversarial training method, which can better transfer sentiment across domains by capturing complete semantic information of a sentence. Comprehensive experiments have been conducted on extended Amazon review datasets, and the results show that the proposed SentATN performs significantly better than state-of-the-art methods.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00500-022-07392-2,en,Peripheral blood cell classification using modified local-information weighted fuzzy C-means clustering-based golden eagle optimization model,"['OriginalPaper', 'Application of soft computing']","This paper presents a novel medical image processing technique for analyzing different peripheral blood cells such as monocytes, lymphocytes, neutrophils, eosinophils, basophils, and macrophages. However, the existing systems suffered from low accuracy while classifying the different blood cell images and also consume higher processing power. The proposed model consists of two major steps such as segmentation and classification of peripheral blood cells. The modified local-information weighted intuitionistic Fuzzy C-means clustering (MLWIFCM)-based golden eagle optimization algorithm performs the nucleus segmentation. Finally, the peripheral blood cell classes such as Basophil, Lymphocyte, Neutrophil, Monocyte, and Eosinophil are effectively classified using hybrid-parameter RNN-based remora optimization algorithm. The MATLAB R2019b is used as the implementation platform. To analyze the performances of our proposed method, we have taken two datasets; they are BCCD and LISC datasets. Meanwhile, the classification performances were analyzed with the aid of different performance metrics such as mean accuracy, mean intersection over union, mean average precision, and mean BF score values.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00500-022-07687-4,en,Brain tumor detection in MRI images using Adaptive-ANFIS classifier with segmentation of tumor and edema,"['OriginalPaper', 'Fuzzy Systems and Their Mathematics']","The brain is a significant organ that controls all activities of the body parts. A Brain Tumor (BT) is a group of tissues, which are structured by the gradual accumulation of irregular cells. The tasks namely (a) identification, (b) segmentation along with (c) identification of the infected region in BT utilizing MRIs is slow and wearisome. Here, BT Detection (BTD) in MRI images has been proposed utilizing Adaptive-ANFIS (Adaptive-Adaptive Neuro-Fuzzy Inference System) classifier with the segmentation of tumor along with edema. Primarily, the input RGB is transmuted into a Grayscale Image (GSI). During pre-processing, the non-brain tissues are eradicated utilizing the Skull Stripping Algorithm (SSA). Next, the resulted image is segmented into ‘2’ parts: (a) tumor and (b) edema utilizing Modified Region Growing (MRG) and Otsu’s thresholding. Then, as of the segmented “tumor part” image, the DWT, WST, Edge, and color histogram features are extracted. Then, the required features are selected as of the extracted features by employing the MGWO algorithm. After that, those features being selected are given to the Adaptive-ANFIS, which categorizes the tumor as (a) Benign and (b) Malignant. The adaptive process is conducted by the K-Means Clustering (KMC) algorithm. Experiential results examined the performances defined by the proposed as well as the prevailing system regarding specificity, accuracy, sensitivity, recall, and precision.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s40998-022-00528-y,en,Design of Full-Order Neural Observer with Nonlinear Filter Techniques for State Estimation of a Three-Tank Process Control System,"['OriginalPaper', 'Research Paper']","A novel model-based approach to design a full-order state observer for estimating the states of a three-tank process has been attempted in this research study. State estimation has been a methodology that integrates the prediction from exact models pertaining to the system and achieves consistent estimation of the non-measurable variables. This study has attempted to develop a full-order observer for estimation of non-measurable variables of the considered three-tank process control system. Neural observer is designed with the nonlinear state update equation that is structured as the neural network employing radial basis function (RBF) model. Also, nonlinear full-order state observer is designed based on a new recursive likelihood synthesizer (RLS) of the extended Kalman filter (EKF) and classic unscented Kalman filter (UKF) and finally the states are estimated. The likelihood synthesizer determines the covariance and Kalman gains so as to match the real-time process measurements. Three-tank process system (TTPS) is represented by its mathematical model and the developed state estimation techniques are applied for estimating the non-measurable variables. Likelihood synthesizer tends to evaluate the covariance of the initial states and simulation tests confirm the attainment of better results using the new nonlinear filtering techniques. RBF neural observer has resulted in an ARMSE of 4.1629 × 10 –3 , 0.3963 × 10 –3 and 0.1085 × 10 –3 for the measured heights h 1 , h 2 and h 3 , respectively. The new RLS-EKF observer with its recursive determination of the maximum likelihood has attained ARMSE of 2.1982 × 10 –6 , 0.1512 × 10 –6 and 0.0261 × 10 –7 for the measured heights h 1 , h 2 and h 3 , respectively. This novel RLS-EKF has proved to be highly robust and has higher precision than the RBF neural observer and UKF technique as applied for the TTPS model.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s10207-022-00611-9,en,An efficient intrusion detection system for MQTT-IoT using enhanced chaotic salp swarm algorithm and LightGBM,"['OriginalPaper', 'regular contribution']","Internet of Things (IoT) networks are becoming increasingly popular for monitoring critical environments of various types. For the communication of IoT devices, several lightweight protocols have been developed. MQTT (message queuing telemetry transport) is a widely used messaging protocol in IoT using the publish–subscribe technique. The openness of publish–subscribe model and the limited built-in authentication capabilities makes it vulnerable to intruders. Hence, an intrusion detection system (IDS) has a vital role in MQTT-IoT security. This paper proposes an efficient intrusion detection mechanism for the MQTT-IoT networks using an enhanced chaotic salp swarm optimization algorithm (ECSSA) and LightGBM classifier. Traditional IDS uses a lot of irrelevant data and undesirable attributes, resulting in long detection times and low performance. To overcome the limitations, the proposed IDS uses ECSSA for feature selection and LightGBM classifier for better detection accuracy. The experimental verification on MC-IoT, MQTT-IoT-IDS2020, and MQTTset datasets demonstrates that ECSSA and LightGBM improve the overall accuracy rate. The proposed technique outperforms the existing approaches with an accuracy of 99.38%, 98.91%, and 98.35% in the three test sets, MC-IoT, MQTT-IoT-IDS2020, and MQTTset, respectively.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Coding and Information Theory', 'Management of Computing and Information Systems', 'Communications Engineering, Networks']"
doi:10.1007/s10586-022-03640-0,en,Design and implementation of dynamic I/O control scheme for large scale distributed file systems,OriginalPaper,"In this work, we have analyzed the input/output (I/O) activities of Cori, which is a high-performance computing system at the National Energy Research Scientific Computing Center at Lawrence Berkeley National Laboratory. Our analysis results indicate that most users do not adjust storage configurations but rather use the default settings. In addition, owing to the interference from many applications running simultaneously, the performance varies based on the system status. To configure file systems autonomously in complex environments, we developed DCA-IO, a dynamic distributed file system configuration adjustment algorithm that utilizes the system log information to adjust storage configurations automatically. Our scheme aims to improve the application performance and avoid interference from other applications without user intervention. Moreover, DCA-IO uses the existing system logs and does not require code modifications, an additional library, or user intervention. To demonstrate the effectiveness of DCA-IO, we performed experiments using I/O kernels of real applications in both an isolated small-sized Lustre environment and Cori. Our experimental results shows that our scheme can improve the performance of HPC applications by up to 263% with the default Lustre configuration.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s11265-022-01815-x,en,Optimal Speech Intelligibility Improvement for Varying Car Noise Characteristics,OriginalPaper,"The present study proposes a novel method for speech intelligibility improvement by optimally shifting the formants using a trapezoidal voice transformation function. The shaping parameters of this function are determined by maximizing various performance measures using a comprehensive learning particle swarm optimization (CLPSO) algorithm. These measures include the short time objective intelligibility (STOI), perceptual evaluation of speech quality (PESQ) and signal to distortion ration (SDR). The proposed method does not requires a priori knowledge about the noise statistics in designing the voice transformation function. Although, the shaping parameters are obtained at specific SNRs, a Gaussian process (GP) regression model is trained to compute these parameters for arbitrary SNRs. The performance of the proposed method is demonstrated on various databases which include Hearing In Noise Test (HINT) a French database, NOIZEUS (ENGLISH) and CHAINS (ENGLISH) databases at different levels of engine noises arising from a running car at various speeds. The results of the investigation convincingly demonstrate that the proposed approach could improve the speech intelligibility, while preserving the quality.","['Engineering', 'Signal,Image and Speech Processing', 'Circuits and Systems', 'Electrical Engineering', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s10489-022-03265-1,en,Red Deer algorithm based social trust based congestion control in ad hoc social networks,OriginalPaper,"An ad hoc social network (ASNET) explores social connectivity among users of mobile devices which is becoming a main essential forms of internet today. In this ASNET, the security and congestion control is considered as the main issues which degrades the performance of the system. Conventional congestion control and security enhancement methods of ASNETs are do not behave properly in congestion environments and attack conditions. To address this issue, a priority with congestion control based hybrid algorithm with security technique is proposed and designed. The proposed scheduling algorithm exploits the social popularity of sensor nodes to prioritize complete incoming flows which completely reduce the congestion problems in the system. Trap door protocol and Zero knowledge proof protocols are combined which are used to improve the security of the ASNETs networks. Two main objective functions are considered to improve the performance of the network such as congestion control and security enhancement. The congestion control is achieved by optimal scheduling scheme which is attained by applying proposed Red Deer Algorithm (RDA). The proposed method is executed by MATLAB simulator and performances are compared with existing methods such as Atom Search Optimization (ASO), Emperor Penguin Optimization (EPO), Firefly Algorithm (FA), and Particle Swarm Optimization (PSO) algorithm respectively. The performance metrics are delay, drop, throughput, energy consumption, network lifetime, over-head and delivery ratio are determined and compared with the proposed method.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11144-022-02327-2,en,Optimizing process parameter for biodiesel production from avocado peel oil using chicken eggshell biocatalysts using central composite design (CCD),OriginalPaper,"In this study, we investigated the application of waste avocado peel oil (WAPO) as a convenient and abundant source for biodiesel production using calcium oxide (CaO) biocatalysts derived from waste chicken eggshell via transesterification process. Likewise the functional group of extracted waste avocado peel oil and the structure of synthesized CaO biocatalyst were performed by using FT-IR (Fourier Transform Infrared Spectroscopy) and X-ray diffractometer analysis respectively. The physicochemical properties of the avocado peel oil and properties of biodiesel such as flash point, kinematic viscosity, density, cetane number, and acid number were also determined according to international standards. The optimum conditions were at a reaction temperature of 65 °C, a reaction time of 3 h, a catalytic load of 1.2 g, and a ratio of methanol to oil of 5:1 was achieved 94.45% with the desirability of 1 and the corresponding optimized biodiesel yield of 94.89%. The physicochemical properties of the avocado peel oil and the produced biodiesel were characterized using GC–MS to identify the composition of oil and biodiesel compound. The results represented that avocado peel oil can be used as a renewable feedstock source to produce biodiesel.","['Chemistry', 'Catalysis', 'Physical Chemistry', 'Industrial Chemistry/Chemical Engineering']"
doi:10.1007/s12145-022-00825-4,en,"GIS-based hybrid machine learning for flood susceptibility prediction in the Nhat Le–Kien Giang watershed, Vietnam","['OriginalPaper', 'Research Article']","Floods is a natural hazard that occurs over a short time with a high transmission speed. Flood risk management in many countries employs flood susceptibility modeling to mitigate against the damage to the economy and loss of life caused by future floods. The objective of this study is the development of a new approach based on the machine learning algorithms namely Multilayer Perceptron (MLP), Archimedes Optimization Algorithm (AOA), Whale Optimization Algorithm (WOA), Water Cycle Algorithm (WCA), Decision Tree (DT), and Adaboost (ADB) to build flood susceptibility maps for the Nhat Le–Kien Giang watershed of Quang Binh province. In total, 1964 flood locations and 14 conditioning factors were split with a ratio of 70:30 for the training model and the validation model respectively. Various statistical indices – namely root-mean-square error (RMSE), mean absolute error (MAE), coefficient of determination (R 2 ), and area under the ROC curve (AUC-ROC) – were used to evaluate the models. The results show that all the models performed well in building the flood susceptibility map, with an AUC value of more 0.9; the models MLP-WCA (AUC = 0.99) and MLP-AOA (AUC = 0.99) were most successful, followed by MLP-WOA (AUC = 0.98), MLP-ACO (0.98), ADB (0.98), and DT (0.97). The analysis of the flood susceptibility maps shows that the high and very high susceptibility level corresponds to no more than 39% of the study area. The results also show that altitude, slope, rainfall, and land use are most influential on the probability of flood occurrence in the study area. It can be concluded that the machine learning models proposed in this study provide results with high accuracy. The flood susceptibility maps that have been produced will be significant in determining future development strategies, especially in selecting the locations of new urban areas; and the findings of this study may be applied not only to this particular watershed in Vietnam, but also in developing countries around the world.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s11042-021-11539-y,en,A dual-branch neural network for DeepFake video detection by detecting spatial and temporal inconsistencies,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","It has become a research hotspot to detect whether a video is natural or DeepFake. However, almost all the existing works focus on detecting the inconsistency in either spatial or temporal. In this paper, a dual-branch (spatial branch and temporal branch) neural network is proposed to detect the inconsistency in both spatial and temporal for DeepFake video detection. The spatial branch aims at detecting spatial inconsistency by the effective EfficientNet model. The temporal branch focuses on temporal inconsistency detection by a new network model. The new temporal model considers optical flow as input, uses the EfficientNet to extract optical flow features, utilize the Bidirectional Long-Short Term Memory (Bi-LSTM) network to capture the temporal inconsistency of optical flow. Moreover, the optical flow frames are stacked before inputting into the EfficientNet. Finally, the softmax scores of two branches are combined with a binary-class linear SVM classifier. Experimental results on the compressed FaceForensics++ dataset and Celeb-DF dataset show that: (a) the proposed dual-branch network model performs better than some recent spatial and temporal models for the Celeb-DF dataset and all the four manipulation methods in FaceForensics++ dataset since these two branches can complement each other; (b) the use of optical flow inputs, Bi-LSTM and dual-branches can greatly improve the detection performance by the ablation experiments.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-07648-w,en,CADNet157 model: fine-tuned ResNet152 model for breast cancer diagnosis from mammography images,"['OriginalPaper', 'Original Article']","The risk of death incurred by breast cancer is rising exponentially, especially among women. This made the early breast cancer detection a crucial problem. In this paper, we propose a computer-aided diagnosis (CAD) system, called CADNet157, for mammography breast cancer based on transfer learning and fine-tuning of well-known deep learning models. Firstly, we applied hand-crafted features-based learning model using four extractors (local binary pattern, gray-level co-occurrence matrix, and Gabor) with four selected machine learning classifiers (K-nearest neighbors, support vector machine, random forests, and artificial neural networks). Then, we performed some modifications on the Basic CNN model and fine-tuned three pre-trained deep learning models: VGGNet16, InceptionResNetV2, and ResNet152. Finally, we conducted a set of experiments using two benchmark datasets: Digital Database for Screening Mammography (DDSM) and INbreast. The results of the conducted experiments showed that for the hand-crafted features based CAD system, we achieved an area under the ROC curve (AUC) of 95.28% for DDSM using random forest and 98.10% for INbreast using support vector machine with the histogram of oriented gradients extractor. On the other hand, CADNet157 model ( i.e., fine-tuned ResNet152) was the best performing deep model with an AUC of 98.90% (sensitivity: 97.72%, specificity: 100%), and 98.10% (sensitivity: 100%, specificity: 96.15%) for, respectively, DDSM and INbreast. The CADNet157 model overcomes the limitations of traditional CAD systems by providing an early detection of breast cancer and reducing the risk of false diagnosis.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11356-022-21727-4,en,Rainfall prediction using multiple inclusive models and large climate indices,"['OriginalPaper', 'Research Article']","Rainfall prediction is vital for the management of available water resources. Accordingly, this study used large lagged climate indices to predict rainfall in Iran’s Sefidrood basin. A radial basis function neural network (RBFNN) and a multilayer perceptron (MLP) network were used to predict monthly rainfall. The models were trained using the naked mole rat (NMR) algorithm, firefly algorithm (FFA), genetic algorithm (GA), and particle swarm optimization (PSO) algorithm. Large lagged climate indices, as well as three hybrid models, i.e., inclusive multiple model (IMM)-MLP, IMM-RBFNN, and the simple average method (SAM), were then employed to predict rainfall. This paper aims to predict rainfall using large climate indices, ensemble models, and optimized artificial neural network models. Also, the paper considers the uncertainty resources in the modeling process. The inputs were selected using a new input selection method, namely a hybrid gamma test (GT). The GT was integrated with the NMR algorithm to create a new test for determining the best input scenario. Therefore, the main innovations of this study were the introduction of the new ensemble and the new hybrid GT, as well as the new MLP and RBFNN models. The introduced ensemble models of the current study are not only useful for rainfall prediction but also can be used to predict other metrological parameters. The uncertainty of the model parameters and input data were also analysed. It was found that the IMM-MLP model reduced the root mean square error (RMSE) of the IMM-RBFNN, SAM, MLP-NMR, RBFNN-NMR, MLP-FFA, RBFNN-FFA, MLP-PSO, RBFNN-PSO, MLP-GA, and RBFNN-GA, MLP, and RBFNN models by 12%, 25%, 31%, 55%, 60%, 62%, 66%, 69%, 70%, 71%, 72%, and 72%, respectively. The IMMs, such as the IMM-MLP, IMM-RBFNN, and SAM, outperformed standalone models. The uncertainty bound of the multiple inclusive models was narrower than that of the standalone MLP and RBFNN models. The MLP-NMR model decreased the RMSE of the RBFNN-NMR, RBFNN-FFA, RBFNN-PSO, and RBFNN models by 15%, 26%, 37%, 42%, and 45%, respectively. The proposed ensemble models were robust tools for combining standalone models to predict hydrological variables.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10844-022-00712-w,en,Aspect extraction and classification for sentiment analysis in drug reviews,OriginalPaper,"Aspect-based sentiment analysis (ABSA) of patients’ opinions expressed in drug reviews can extract valuable information about specific aspects of a particular drug such as effectiveness, side effects and patient conditions. One of the most important and challenging tasks of ABSA is to extract the implicit and explicit aspects from a text, and to classify the extracted aspects into predetermined classes. Supervised learning algorithms possess high accuracy in extracting and classifying aspects; however, they require annotated datasets whose manual construction is time-consuming and costly. In this paper, first a new method was introduced for identifying expressions that indicate an aspect in user reviews about drugs in English. Then, distant supervision was adopted to automate the construction of a training set using sentences and phrases that are annotated as aspect classes in the drug domain. The results of the experiments showed that the proposed method is able to identify various aspects of the test set with 74.4% F-measure, and outperforms the existing aspect extraction methods. Also, training the random forest classifier on the dataset that was constructed via distant supervision obtained the F-measure of 73.96%, and employing this dataset to fine-tune BERT for aspect classification yielded better F-measure (78.05%) in comparison to an existing method in which the random forest classifier trained on an accurate manually constructed dataset.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s13369-022-06926-y,en,Obstacle Avoidance and Near Time-Optimal Trajectory Planning of a Robotic Manipulator Based on an Improved Whale Optimisation Algorithm,"['OriginalPaper', 'Research Article-Mechanical Engineering']","Aiming at the obstacle avoidance trajectory problem of robotic manipulators in operation space, a combinatorial optimisation solution of the obstacle avoidance path and near time-optimal trajectory planning is proposed. The robotic manipulator and the obstacles are modelled by a swept sphere volume (SSV) envelope and a capsule envelope, respectively. Based on the target offset and the target preference expansion strategy in the repulsion field, an improved RRT* algorithm is adopted to avoid collision to realise obstacle-free path planning at the end-effector level in Cartesian space and the link level in joint space. The thresholds of velocity, acceleration and jerk are set as constraints, and the interval time between nodes is optimised as position information of the improved whale optimisation algorithm. Near time-optimal trajectory planning of robotic manipulators with motion constraints is further completed. The IWOA obtains a high-quality initial population by covering the search space, and the population is updated by the thermal memory (TM) to improve individual quality. Thermal exchange optimisation (TEO) is applied to enhance the exploitation performance of IWOA, and the hierarchical structure strategy is introduced to explore potential optimal solutions. The simulation results show that the method limits the jerk of the robotic manipulator, ensures smooth movement and has higher obstacle avoidance ability and work efficiency than previous methods.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s11814-022-1267-0,en,Numerical investigation and deep learning-based prediction of heat transfer characteristics and bubble dynamics of subcooled flow boiling in a vertical tube,"['OriginalPaper', 'Transport Phenomena']","Subcooled flow boiling presents an enormous ability of heat transfer rate, which is extremely important in the heat-dissipating systems of many industrial applications, such as power plants and internal combustion engines. Using an Euler-Euler-based three-dimensional numerical simulation of subcooled flow boiling in a vertical tube, we investigated different heat transfer quantities (average and local heat transfer coefficient, average and local vapor volume fraction, average and local wall temperature) and bubble dynamics quantities (bubble departure diameter, bubble detachment frequency, bubble detachment waiting time, and nucleation site density) under various boundary conditions (pressure, subcooled temperature, mass flux, heat flux). Numerical results show that an increase in heat flux leads to the increase in all of the physical quantities of interest but the bubble detachment frequency. An entirely opposite behavior is observed when we change the mass flux and inlet subcooled temperature. Furthermore, a rise in pressure reduces all of the target quantities but the wall temperature and bubble detachment frequency. Since numerical simulation of such multiphase flow requires significant computational resources, we also present a deep learning approach, based on artificial neural networks (ANN), to predicting the physical quantities of interest. Prediction results demonstrate that the ANN model is capable of accurately predicting the target quantities with mean absolute errors less than 2.5% and R-squared more than 0.93.","['Chemistry', 'Industrial Chemistry/Chemical Engineering', 'Catalysis', 'Materials Science, general', 'Biotechnology']"
doi:10.1007/s10278-022-00669-w,en,Recognition and Segmentation of Individual Bone Fragments with a Deep Learning Approach in CT Scans of Complex Intertrochanteric Fractures: A Retrospective Study,OriginalPaper,"The characteristics of bone fragments are the main influencing factors for the choice of treatment in intertrochanteric fractures. This study aimed to develop a deep learning algorithm for recognizing and segmenting individual fragments in CT images of complex intertrochanteric fractures for orthopedic surgeons. This study was based on 160 hip CT scans (43,510 images) of complex fractures of three types based on the Evans-Jensen classification (40 cases of type 3 (IIA) fractures, 80 cases of type 4 (IIB)fractures, and 40 cases of type 5 (III)fractures) retrospectively. The images were randomly split into two groups to construct a training set of 120 CT scans (32,045 images) and a testing set of 40 CT scans (11,465 images). A deep learning model was built into a cascaded architecture composed by a convolutional neural network (CNN) for location of the fracture ROI and another CNN for recognition and segmentation of individual fragments within the ROI. The accuracy of object detection and dice coefficient of segmentation of individual fragments were used to evaluate model performance. The model yielded an average accuracy of 89.4% for individual fragment recognition and an average dice coefficient of 90.5% for segmentation in CT images. The results demonstrated the feasibility of recognition and segmentation of individual fragments in complex intertrochanteric fractures with a deep learning approach. Altogether, these promising results suggest the potential of our model to be applied to many clinical scenarios.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s11042-021-11463-1,en,An end to end trained hybrid CNN model for multi-object tracking,"['OriginalPaper', '1220: Visual and Sensory Data Processing for Real Time Intelligent Surveillance System']","A robust MOT (multi-object tracking) is very crucial for computer vision applications such as crowd density estimation and autonomous vehicles. Most of the existing mot approaches perform object tracking in a two task manner such as motion estimation and Re-identification but these approaches pose some drawbacks like the model is not end-to-end trained, the Re-Id required lots of identity switches thus incurred computational overhead and the performance further degrades in complex crowd scenarios. To overcome such drawbacks we are motivated to design an end-to-end trained DNN for MOT. The proposed model utilizes a matching technique that utilizes the relative scale between the boundary boxes and relative position calculates the relative distance between the objects for MOT. To solve the problems, we proposed a matching technique that poses two subtasks to efficiently scale up a single shot DNN tracking approach for an indefinite number of objects in the video frames. The proposed method uses a relative scale and relative position to matching between the detected and targeted objects. The achieved state-of-the-art results of the tasks allow to obtain high accuracy of tracking with detection and surpasses existing state-of-the-art methods by a huge margin on various public datasets.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11042-021-11212-4,en,Transferring knowledge from monocular completion for self-supervised monocular depth estimation,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","Monocular depth estimation is a very challenging task in computer vision, with the goal to predict per-pixel depth from a single RGB image. Supervised learning methods require large amounts of depth measurement data, which are time-consuming and expensive to obtain. Self-supervised methods are showing great promise, exploiting geometry to provide supervision signals through image warping. Moreover, several works leverage on other visual tasks (e.g. stereo matching and semantic segmentation) to further advance self-supervised monocular depth estimation. In this paper, we propose a novel framework utilizing monocular depth completion as an auxiliary task to assist monocular depth estimation. In particular, a knowledge transfer strategy is employed to enable monocular depth estimation to benefit from the effective feature representations learned by monocular depth completion task. The correlation between monocular depth completion and monocular depth estimation could be fully and effectively utilized in this framework. Only unlabeled stereo images are used in the proposed framework, which achieves a self-supervised learning paradigm. Experimental results on publicly available dataset prove that the proposed approach achieves superior performance to state-of-the-art self-supervised methods and comparable performance with supervised methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s12652-021-03568-0,en,New strategy to enhance AC grid performance with SVC device in an optimal location,"['OriginalPaper', 'Original Research']","Voltage and reactive power become one of the most critical aspects to monitor and sustain at an appropriate level in vast alternative current networks to ensure good functionality, load satisfaction, network integrity, and service quality. The network’s spending should be obtained by calculating the load flow or applying more controllable variables to it, defined as the optimization power flow. In this vein, this paper focuses on improving the network’s voltage profile, reactive power generation, and losses. The study begins with load flow estimation in various situations using the Newton–Raphson process, followed by an optimization capacity calculation using the simplified Reduced Gradient power flow method. For each system, four cases are simulated, one with and one without the SVC (Static Var Compensator) device, which is positioned in an ideal position on the IEEE 57 bus network. The confirmation of the data is carried out using the MATLAB program. A distinction is made between the two approaches based on the SVC device’s position and its reactive power generation into the network. The simulation results indicate that the generalized Reduced Gradient optimization approach (GRG-OPF) offers reasonable efficiency, quick iteration in the simulation results, the best power budget with minimal active/reactive losses and the best voltage profile specifically in the case where SVC reactive power is taken as a variant into the GRG-OPF method.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1038/s41592-022-01666-1,en,Detection of m6A from direct RNA sequencing using a multiple instance learning framework,"['OriginalPaper', 'Article']","RNA modifications such as m6A methylation form an additional layer of complexity in the transcriptome. Nanopore direct RNA sequencing can capture this information in the raw current signal for each RNA molecule, enabling the detection of RNA modifications using supervised machine learning. However, experimental approaches provide only site-level training data, whereas the modification status for each single RNA molecule is missing. Here we present m6Anet, a neural-network-based method that leverages the multiple instance learning framework to specifically handle missing read-level modification labels in site-level training data. m6Anet outperforms existing computational methods, shows similar accuracy as experimental approaches, and generalizes with high accuracy to different cell lines and species without retraining model parameters. In addition, we demonstrate that m6Anet captures the underlying read-level stoichiometry, which can be used to approximate differences in modification rates. Overall, m6Anet offers a tool to capture the transcriptome-wide identification and quantification of m6A from a single run of direct RNA sequencing. This work presents m6Anet, which implements a neural-network-based multiple instance learning model to detect m6A modifications from direct RNA sequencing data.","['Life Sciences', 'Life Sciences, general', 'Biological Techniques', 'Biological Microscopy', 'Biomedical Engineering/Biotechnology', 'Bioinformatics', 'Proteomics']"
doi:10.1007/s42107-022-00482-4,en,Performance of cellular steel beam under fire,"['OriginalPaper', 'Original Paper']","The present work is intended to study the behaviour of cellular steel beam (CSB) under the ISO834 fire for three and four-sided exposure conditions. The finite element-based thermo-mechanical analysis is carried out to predict the behaviour of CSB under fire. The present study considers the transient temperature effect, and the geometrical and material nonlinearity. The universal beam section: UB254 × 146 × 43 is used as the parent section for forming CSB. The present study is performed for the beam slenderness (span to depth ratio) equal to 25. The hinge–hinge boundary conditions are used for CSBs at supports. The beam is assumed to be laterally supported along the beam length on the top flange. The various parameters considered for this study include the effect of size and spacing of the openings and different load ratios under uniformly distributed loading conditions. The performance of different CSBs under fire has been compared using in-plane deflections; out-plane displacements, end-reaction forces, and fire-resistance time. The study shows that the effect size and spacing of the openings along with different load ratios and fire exposure condition has a significant effect on the fire-resistance time of CSB under elevated temperature.","['Engineering', 'Civil Engineering', 'Building Materials', 'Sustainable Architecture/Green Buildings']"
doi:10.1007/s00521-022-07625-3,en,P + FELU: Flexible and trainable fast exponential linear unit for deep learning architectures,"['OriginalPaper', 'Original Article']","Activation functions have an important role in obtaining the most appropriate output by processing the information coming into the network in deep learning architectures. Deep learning architectures are widely used in areas such as image processing applications, time series, and disease classification, generally in line with the analysis of large and complex data. Choosing the appropriate architecture and activation function is an important factor in achieving successful learning and classification performance. There are many studies to improve the performance of deep learning architectures and to overcome the disappearing gradient and negative region problems in activation functions. A flexible and trainable fast exponential linear unit (P + FELU) activation function is proposed to overcome existing problems. With the proposed P + FELU activation function, a higher success rate and faster calculation time can be achieved by incorporating the advantages of fast exponentially linear unit (FELU), exponential linear unit (ELU), and rectified linear unit (RELU) activation functions. Performance evaluations of the proposed P + FELU activation function were made on MNIST, CIFAR-10, and CIFAR-100 benchmark datasets. Experimental evaluations have shown that the proposed activation function outperforms the ReLU, ELU, SELU, MPELU, TReLU, and FELU activation functions and effectively improves the noise robustness of the network. Experimental results show that this activation function with “flexible and trainable” properties can effectively prevent vanishing gradient and make multilayer perceptron neural networks deeper.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00607-022-01099-w,en,Item enhanced graph collaborative network for collaborative filtering recommendation,"['OriginalPaper', 'Regular Paper']","Learning vector embeddings of users and items is the core of modern recommender systems. Recently the collaborative filtering recommender systems based on graph convolutional networks, which integrates the bipartite graph of user-item interaction into the embedding process, has achieved significant success. However, such feature as item-item interaction sequence is neglected in the bipartite graph, which limits the ability to model sequential orders for embedding of items. In this work, we propose a novel item-item interaction sequential graph to globally aggregate the hidden interactions sequence among all items. It is derived from the order of all user-item interactions and can give a supplement for user-item interaction modeling in CF. We also propose an item enhanced graph collaborative network (IEGCN) to mix item-item sequences with user-item interactions for collaborative filtering. We performed experiments on three open datasets, and IEGCN shows substantial improvements in recall and normalized discounted cumulative gain when compared with existing mainstream models. Further analysis verifies the importance of item-item sequence graph to improve the recommendation effect.","['Computer Science', 'Computer Science, general', 'Information Systems Applications (incl.Internet)', 'Computer Communication Networks', 'Software Engineering', 'Artificial Intelligence', 'Computer Appl. in Administrative Data Processing']"
doi:10.1007/s11571-022-09787-1,en,Neurodegenerative diseases-Caps: a capsule network based early screening system for the classification of neurodegenerative diseases,"['OriginalPaper', 'Research Article']","The two most generally diagnosed Neurodegenerative diseases are the Alzheimer and Parkinson diseases. So this paper presents a fully automated early screening system based on the Capsule network for the classification of these two Neurodegenerative diseases. In this study, we hypothesized that the Neurodegenerative diseases-Caps system based on the Capsule network architecture accurately performs the multiclass i.e. three class classification into either the Alzheimer class or Parkinson class or Healthy control and delivers better results in comparison other deep transfer learning models. The real motivation behind choosing the capsule network architecture is its more resilient nature towards the affine transformations as well as rotational & translational invariance, which commonly persists in the medical image datasets. Apart from this, the capsule networks overcomes the pooling layers related deficiencies from which conventional CNNs are mostly affected and unable to delivers accurate results especially in the tasks related to image classification. The various Computer aided systems based on machine learning for the classification of brain tumors and other types of cancers are already available. Whereas for the classification of Neurodegenerative diseases, the amount of research done is very limited and the number of persons suffering from this type of diseases are increasing especially in developing countries like India, China etc. So there is a need to develop an early screening system for the correct multiclass classification into Alzheimer’s, Parkinson’s and Normal or Healthy control cases. The Alzheimer disease and Parkinson progression (ADPP) dataset is used in this research study for the training of the proposed Neurodegenerative diseases-Caps system. This ADPP dataset is developed with the aid of both the Parkinson's Progression Markers Initiative (PPMI) and Alzheimer’s disease Neuroimaging Initiative (ADNI) databases. There is no such early screening system exist yet, which can perform the accurate classification of these two Neurodegenerative diseases. For the sake of genuine comparison, other popular deep transfer learning models like VGG19, VGG16, ResNet50 and InceptionV3 are implemented and also trained over the same ADPP dataset. The proposed Neurodegenerative diseases-Caps system deliver accuracies of 97.81, 98, 96.81% for the Alzheimer, Parkinson and Healthy control or Normal cases with 70/30 (training/validation split) and performs way better as compare to the other popular Deep transfer learning models.","['Biomedicine', 'Biomedicine, general', 'Neurosciences', 'Computer Science, general', 'Artificial Intelligence', 'Biochemistry, general', 'Cognitive Psychology']"
doi:10.1007/s40998-022-00524-2,en,Optimal Sizing of Hybrid Renewable Energy System for Electricity Production for Remote Areas,"['OriginalPaper', 'Research Paper']","Today, the world is looking at the adoption of alternative energy resources for electrical power generation, particularly for remote applications. Renewable energy resources are being investigated to meet such demand due to numerous benefits, such as being environmentally friendly, a reliable source of energy, improving public health issues, job creation in rural areas, and so on. In the present work, two intelligent approaches, including a recently developed method named Improved Harmony Search (IHS) and Particle Swarm Optimization (PSO), have been adopted for the optimal sizing of the hybrid renewable energy system to fulfill the electrical load demand of a selected remote site in the Haryana state of India. The problem has been formulated by developing a mathematical model of the hybrid renewable energy system by considering the capital cost, replacement cost, operation and maintenance (O & M) cost, fuel cost, salvage value of various components, and the cost of selling and buying power to and from the utility grid. The optimization of the hybrid model for off-grid and grid-connected mode has been carried out for the minimization of the Net Present Cost (NPC) of the hybrid system by using the MATLAB platform. A comparative analysis of the results obtained by using the IHS and PSO algorithms is also presented in this work.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s11042-022-12930-z,en,Sarcasm detection using deep learning and ensemble learning,OriginalPaper,"Across the globe, there is a noticeable upward trend of incorporating sarcasm in everyday life. This trend can be easily attributed to the frequent use of sarcasm in everyday life, but more specifically to social media and the Internet. This study aims to bridge the gap between human and machine intelligence to recognize and understand sarcastic behavior and patterns. The research is based on using various neural techniques, namely Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Baseline Convolutional Neural Networks (CNN) in an ensemble model to detect sarcasm on the internet. In order to improve the precision of the proposed model, the required dataset is also prepared on different previously trained word-embedding models like fastText, Word2Vec, and GloVe, etc., and their accuracies are compared. The aim is to be able to quantify the overall sentiment of the writer as positive or negative / sarcastic or non-sarcastic to ensure that the correct message is received to the intended audience. The final study revealed that the proposed ensemble model with word embeddings outperformed the other state-of-the-art models and deep learning models considered in this study with an accuracy of around 96% for News Headlines dataset, 73% for Reddit dataset, and amongst our proposed ensemble models, Weighted Average Ensemble gave the highest accuracy of around 99% and 82% for both the datasets respectively. Ensemble model used in our study improvised the stability, precision and predictive power of the proposed model.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00366-022-01753-z,en,Worst case mesh quality in the target matrix optimization paradigm,"['OriginalPaper', 'Original Article']","When considering mesh quality improvement and optimization via node movement, a particularly important goal is to attempt to improve the worst quality in the mesh when that quality is unacceptable. Standard mesh optimization methods often do not address worst case quality and can make it worse. Three fundamental methods for addressing worst case quality in mesh optimization by node movement have appeared in the literature: (1) a shifted barrier approach by Barrera et. al. (Math Comput Simul 46(2):87–102, 1998), (2) a pseudo-barrier approach by Escobar et. al. (Comput Methods Appl Mech Eng 192:2775–2787, 2000), and (3) another barrier approach by Garanzha et. al. (In: IMR 2021-29th international meshing roundtable, 2021). The first two result in ‘simultaneous untangle-optimizers’ while the last addresses worst case quality in terms of the maximum value of the optimization metric. In terms of mesh optimization within the Target Matrix Optimization Paradigm (TMOP), worst case quality is defined by two quantities: the maximum value of the optimization metric ( $$\max \mu$$ max μ ) and the minimum value of the local volume ( $$\min \tau$$ min τ ), both computed over the mesh sample points. In the present paper we show that the methods by the three authors cited above can be applied to the TMOP metrics and used on both linear and high-order element meshes. Unfortunately, the first two methods increase $$\max \tau$$ max τ but do not address $$\max \mu$$ max μ . The third method addresses $$\max \mu$$ max μ , but fails to address $$\min \tau$$ min τ . Using a composition of functions approach, the present work creates new compound metrics that simultaneously increase $$\min \tau$$ min τ and decrease $$\max \mu$$ max μ . This goal can also be accomplished by a two-stage optimization procedure in which the first stage untangles the initial mesh and the second stage decreases $$\max \mu$$ max μ . Although none of these methods provide a guarantee that the worst case quality will be improved to the point that the quality becomes acceptable, it is shown by numerical examples that they can be very effective.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s12652-021-03180-2,en,NACO predicated hybrid model of internet of things and cloud computing to manage immensely colossal data in health accommodations applications,"['OriginalPaper', 'Original Research']","In current trend, there has been a consistent expanding big data analysis in the field of healthcare applications. Multi-criteria optimization problems are one of the challenging tasks when one or more essential criteria are interlinked together with tangible or intangible relations. It causes a total chaos when improving or optimizing a factor it may impacts positively or negatively. In multi criteria optimization problems, inspired algorithms can provide a solution naturally. There is an inflated requirement for the evaluation of existing optimization algorithms and provision of a customized naturally inspired algorithm for ‘IoT-Cloud healthcare monitoring system’. In this propound system, a novel swarm multi-initialization process and multi-swarm optimization algorithm is used to acquire the optimized result and also to overcome the high demand. Data-driven swarm selection procedure enables dynamic multi-criteria optimization. There are three phases exist in the proposed method, they are Nodal Ant Colony Optimization (NACO) IoT-Cloud Swarm Initialization, NACO IoT- Cloud Swarm Optimization and Data-driven IoT-Cloud Swarm Selection. These phases are built with the intension to improve the efficiency and resource utilization by minimizing waiting and execution times. The user interface is designed to monitor the network performance in terms of efficiency, execution time, turn-around time, waiting time and resource utilization and compared with three special optimizers such as Particle swarm optimizer (PSO), Parallel Particle swarm optimization (PPSO) and Genetic Algorithm (GA), Average efficiency of proposed method is increased by 5.78% than the average of existing algorithms. The maximum and minimum resource utilization of the proposed algorithm is 99.99% and 38.84% respectively. This method also gives the better percentage of resource utilization when compared with the existing algorithm. The execution time, turnaround time and waiting time is also very less when compared with above mentioned GA, PSO and PPSO algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s11265-022-01803-1,en,Hyperchaining for LLVM-Based Binary Translators on the x86-64 Platform,OriginalPaper,"Rabbit is an LLVM-based hybrid binary translator with several innovative optimizations (including an extension to traditional block chaining, called hyperchaining) to improve the performance. In addition to platform-independent hyperchaining (indep), Rabbit also includes platform-dependent hyperchaining (dep) on both x86-64 and RISC-V architectures for both direct and indirect branches. The dep optimizations leverage architecture-specific instructions and patches to achieve the same effect as the indep optimization but gains more performance improvements. The experimental results show that the platform-dependent hyperchaining can achieve 1.08x and 1.05x speedup in comparison with platform-independent hyperchaining for direct and indirect branches, respectively. The experimental results also show that platform-dependent hyperchaining incurs little memory space overhead in comparison with platform-independent hyperchaining.","['Engineering', 'Signal,Image and Speech Processing', 'Circuits and Systems', 'Electrical Engineering', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s13748-022-00291-5,en,Exploring how independent variables influence parking occupancy prediction: toward a model results explanation with SHAP values,"['OriginalPaper', 'Regular Paper']","Finding a parking space is a difficult challenge that drivers face on a daily basis in urban neighborhoods around the world. They often report that desirable spaces near to their destination are either unavailable or very expensive, extending further the search time and congesting even more city centers. Intelligent parking solutions can integrally solve this ongoing problem by better managing existing resources. They allow drivers to access real-time information on parking space availability, collected with different detection techniques (crowdsourcing, parking meters, sensors). Some of these systems also encompass opportunistic services, such as forecasting, needed to adapt to unforeseen dynamic situations. Hence, we presented, in this paper, a methodology for predicting car park occupancy rates using four different machine learning algorithms. Each of these methods is trained with four feature sets to exemplify how information quality impacts prediction accuracy. In addition to achieving high accuracy, it is absolutely crucial to interpret model outputs and analyze each individual feature’s importance. That's why we developed an explanation model based on SHAP values. We implemented our proposal exploiting five months of real-time parking data broadcast by Aarhus City Council. Results show that the best-obtained predictions are by far very accurate with a coefficient of determination ( R 2 ) that achieves 0.988 and a mean absolute error that doesn't exceed 2.021%, while requiring a very low computing time that is only 5 s.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Natural Language Processing (NLP)', 'Computational Intelligence', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00779-020-01508-x,en,Modeling and “smart” prototyping human-in-the-loop interactions for AmI environments,"['OriginalPaper', 'Original Paper']","Autonomous capabilities are required in AmI environments in order to adapt systems to new environmental conditions and situations. However, keeping the human in the loop and in control of such systems is still necessary because of the diversity of systems, domains, environments, context situations, and social and legal constraints, which makes full autonomy a utopia within the short or medium term. Human-system integration introduces an important number of challenges and problems that have to be solved. On the one hand, humans should interact with systems even in those situations where their attentional, cognitive, and physical resources are limited in order to perform the interaction. On the other hand, systems must avoid overwhelming the user with unnecessary actions. Therefore, appropriate user-centered methods for AmI development should be used to help designers analyze and design human-in-the-loop interactions in AmI environments. This paper presents a user-centered design method that defines a process with a set of tools and techniques that supports the process steps in order to systematically design, prototype, and validate human-in-the-loop (HiL) solutions. The process starts with the definition of the HiL design, which defines how the system cooperates with the human. This HiL design is built using a conceptual framework that focuses on achieving human-system interactions that get human attention and avoid obtrusiveness. Then, we provide a software infrastructure to generate a prototype based on the HiL design and validate it by having end-users use a web simulator. The feedback data generated during the prototype user validation is gathered and used by a machine learning tool that infers the user’s needs and preferences. Finally, these inferences are used to automatically enhance the human-in-the-loop designs and prototypes. We have validated the proposed method through a twofold perspective: an experiment to analyze the perception of interaction designers regarding their acceptance of the design method and another experiment to evaluate the usefulness of the “smart” prototyping technique. The results obtained point out the acceptability of the proposed method by designers and the useful adaptations provided by the “smart” prototyping technique to achieve a HiL design that adapts well to users’ preferences and needs.","['Computer Science', 'User Interfaces and Human Computer Interaction', 'Computer Science, general', 'Personal Computing', 'Mobile Computing']"
doi:10.1007/s40030-022-00688-4,en,Prediction of Uniaxial Compressive Strength of Rock Using Machine Learning,"['OriginalPaper', 'Original Contribution']","The uniaxial compressive strength of rock is one of the most significant parameters required for analysis of rock mass, its characterization, and design of foundations. Direct determination of the uniaxial compressive strength of rock is time-consuming, expensive, and requires destructive laboratory or field testing. Therefore, indirect methods based on regression analysis are widely used for estimation of the uniaxial compressive strength of rock, which have less accuracy. In this study, machine learning algorithms are used to estimate the uniaxial compressive strength of rock using point load strength, porosity, Schmidt rebound hardness, block punch index, and specific gravity. The performance of each machine learning model is evaluated using statistical parameters, viz., mean absolute error, value account for, and coefficient of determination. It is found that random forest regression is the most suitable model for estimation of uniaxial compressive strength with the minimum mean absolute error of 8.68 MPa and r 2 -score of 0.94.","['Engineering', 'Civil Engineering']"
doi:10.1007/s00521-022-07613-7,en,Development of novel automated language classification model using pyramid pattern technique with speech signals,"['OriginalPaper', 'Original Article']","Language classification using speeches is a complex issue in machine learning and pattern recognition. Various text and image-based language classification methods have been presented. But there are limited speech-based language classification methods in the literature. Also, the previously presented models classified limited numbers of languages, and few are accents. This work presents an automated handcrafted language classification model. The novel pyramid pattern is presented to extract the features extraction. Also, statistical features and maximum pooling are used to generate the features. We have developed our speech-language classification model using two datasets: (i) created a new big speech dataset containing 14,500 speeches in 29 languages, and (ii) used the VoxForge dataset. The neighborhood component analysis method is used to select the most informative 1000 features from the generated features, and these features are classified using a quadratic support vector machine classifier (QSVM). Our developed method yielded 98.87 ± 0.30% and 97.12 ± 1.27% accuracies for our and VoxForge datasets, respectively. Also, geometric mean, average precision, and F1-score evaluation parameters are calculated, and they are presented in the results section. This paper presents an accurate language classification model developed using two big speech-language datasets. Our results indicate the success of the proposed pyramid pattern-based language classification method in classifying various speech languages accurately.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s40747-022-00745-2,en,Deep multi-layer perceptron-based evolutionary algorithm for dynamic multiobjective optimization,"['OriginalPaper', 'Original Article']","Dynamic multiobjective optimization problems (DMOPs) challenge multiobjective evolutionary algorithms (MOEAs) because of the varying Pareto-optimal sets (POS) over time. Research on DMOPs has attracted a great interest from academic, due to widespread applications of DMOPs. Recently, a few learning-based approaches have been proposed to predict new solutions in the following environments as an initial population for a multiobjective evolutionary algorithm. In this paper, we propose an alternative learning-based method for DMOPs, a deep multi-layer perceptron-based predictor to generate an initial population for the MOEA in the new environment. The historical optimal solutions are used to train a deep multi-layer perceptron which then predicts a new set of solutions as the initial population in the new environment. The deep multi-layer perceptron is incorporated with the multiobjective evolutionary algorithm based on decomposition to solve DMOPs. Empirical results demonstrate that our proposed algorithm is effective in tracking varying solutions over time and shows great superiority comparing with state-of-the-art methods.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s42421-022-00059-2,en,A Data-Driven Network Model for Traffic Volume Prediction at Signalized Intersections,"['OriginalPaper', 'Original Paper']","Network-wide traffic prediction at the level of an intersection can benefit transportation systems management and operations. However, traditional traffic modeling approaches relying on mathematical or simulation-based models are either less useful or require higher computational time in predicting high fidelity traffic volumes. In addition, these frameworks need to be modified to ingest large-scale data (such as automated traffic signal performance measures) available from intersections. To overcome these challenges, in this study, a data-driven method based on a deep learning architecture has been developed for network-wide intersection-level traffic prediction. The study has tested two deep learning architectures: Graph Convolutional LSTM ( GCN-LSTM ) and Graph Convolutional Encoder–Decoder LSTM ( GCN- Encoder–Decoder) model to predict intersection-level hourly traffic movement volumes over multiple time steps (e.g., 4-h sequence). Such deep learning architectures capture the spatiotemporal cross correlation among network-wide traffic features while learning the patterns in traffic movement volumes. To test the model performances, we have fused data from multiple sources such as travel demand data, built environment characteristics, etc. We have extracted 1 year (2016) of traffic movement volume data from Seminole County’s automated traffic signal performance measure (ATSPM) database. Experiment results show that the developed GCN-LSTM model outperforms all the other baseline models. The absolute difference between actual and predicted volumes are quite low (GEH < 5); for right turn, through and left turn movement RMSE values are 4.02, 59.37, and 2.47, respectively. The R 2 score of the model is 0.98, which indicates that the model can capture the spatiotemporal variations of traffic movement volumes very well.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1038/s41598-022-25249-4,en,Unsupervised real-world knowledge extraction via disentangled variational autoencoders for photon diagnostics,"['OriginalPaper', 'Article']","We present real-world data processing on measured electron time-of-flight data via neural networks. Specifically, the use of disentangled variational autoencoders on data from a diagnostic instrument for online wavelength monitoring at the free electron laser FLASH in Hamburg. Without a-priori knowledge the network is able to find representations of single-shot FEL spectra, which have a low signal-to-noise ratio. This reveals, in a directly human-interpretable way, crucial information about the photon properties. The central photon energy and the intensity as well as very detector-specific features are identified. The network is also capable of data cleaning, i.e. denoising, as well as the removal of artefacts. In the reconstruction, this allows for identification of signatures with very low intensity which are hardly recognisable in the raw data. In this particular case, the network enhances the quality of the diagnostic analysis at FLASH. However, this unsupervised method also has the potential to improve the analysis of other similar types of spectroscopy data.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40747-022-00747-0,en,A decomposition-based many-objective evolutionary algorithm with optional performance indicators,"['OriginalPaper', 'Original Article']","Evolutionary algorithms (EAs) have shown excellent performance for solving optimization problems with multiple objectives as they can get a set of compromising solutions on a single run. However, when the number of objectives increases, an efficient selection is significant to find a good set of solutions. In this paper, a decomposition-based many-objective evolutionary algorithm with optional performance indicators is proposed, in which the decomposition strategy is utilized to convert a many-objective optimization problem into a set of single-objective optimization problems, and the criterion to select a solution for the next generation along each reference is randomly set to convergence or diversity performance. The performance of the proposed method is evaluated on two sets of benchmark problems, and the experimental results showed the efficiency of the proposed method compared with seven state-of-the-art MaOEAs.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1038/s41598-022-25027-2,en,Computer-aided diagnosis through medical image retrieval in radiology,"['OriginalPaper', 'Article']","Currently, radiologists face an excessive workload, which leads to high levels of fatigue, and consequently, to undesired diagnosis mistakes. Decision support systems can be used to prioritize and help radiologists making quicker decisions. In this sense, medical content-based image retrieval systems can be of extreme utility by providing well-curated similar examples. Nonetheless, most medical content-based image retrieval systems work by finding the most similar image, which is not equivalent to finding the most similar image in terms of disease and its severity. Here, we propose an interpretability-driven and an attention-driven medical image retrieval system. We conducted experiments in a large and publicly available dataset of chest radiographs with structured labels derived from free-text radiology reports (MIMIC-CXR-JPG). We evaluated the methods on two common conditions: pleural effusion and (potential) pneumonia. As ground-truth to perform the evaluation, query/test and catalogue images were classified and ordered by an experienced board-certified radiologist. For a profound and complete evaluation, additional radiologists also provided their rankings, which allowed us to infer inter-rater variability, and yield qualitative performance levels. Based on our ground-truth ranking, we also quantitatively evaluated the proposed approaches by computing the normalized Discounted Cumulative Gain (nDCG). We found that the Interpretability-guided approach outperforms the other state-of-the-art approaches and shows the best agreement with the most experienced radiologist. Furthermore, its performance lies within the observed inter-rater variability.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10208-021-09532-w,en,Robust Group Synchronization via Cycle-Edge Message Passing,OriginalPaper,"We propose a general framework for solving the group synchronization problem, where we focus on the setting of adversarial or uniform corruption and sufficiently small noise. Specifically, we apply a novel message passing procedure that uses cycle consistency information in order to estimate the corruption levels of group ratios and consequently solve the synchronization problem in our setting. We first explain why the group cycle consistency information is essential for effectively solving group synchronization problems. We then establish exact recovery and linear convergence guarantees for the proposed message passing procedure under a deterministic setting with adversarial corruption. These guarantees hold as long as the ratio of corrupted cycles per edge is bounded by a reasonable constant. We also establish the stability of the proposed procedure to sub-Gaussian noise. We further establish exact recovery with high probability under a common uniform corruption model.","['Mathematics', 'Numerical Analysis', 'Economics, general', 'Applications of Mathematics', 'Linear and Multilinear Algebras, Matrix Theory', 'Math Applications in Computer Science', 'Computer Science, general']"
doi:10.1007/s00530-022-01028-z,en,Recent advancements of deep learning in detecting breast cancer: a survey,"['OriginalPaper', 'Regular Paper']","Breast cancer is a deadly disease which is most commonly diagnosed in women. It spreads worldwide as the number of cases is increasing each day. The significant increase in cancer cases leads researchers to develop imaging tools for its detection. However, false-positive rates in manual detections are more, which may be due to human error, time taking process or some other issues. If these cancers are not detected at an early stage, they can cause the death of the patient. Therefore, several machine learning and deep learning-based methodologies have come into existence, which can be used to detect breast cancer in early stages. Nowadays, deep learning-based methods are trending, therefore, this paper discusses some important work done by researchers using deep learning. This survey provides an in-depth study of various deep learning models used for breast cancer detection. The details about mammography, histopathology and ultrasound datasets are also given which are being used for comparative analysis of the developed methods. This survey is not only on one imaging modality instead covering three modalities and thus giving a wider study of various methods. Important works from various years have been discussed along with recent works based on vision transformer. Therefore, this paper presents all the necessary information to researchers working in this area and they can think of new ideas based on it to carry out further work.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s10489-022-03157-4,en,Gated residual feature attention network for real-time Dehazing,OriginalPaper,"Images captured under complicated weather conditions, such as haze, often suffer from a noticeable degradation and hamper its practical application. Traditional dehazing methods use various hand-crafted priors to get a clear image; in such cases, the performance is limited owing to unconstrained environment. In order to restore the haze-free image directly, we propose an end-to-end Gated Residual Feature Attention Network (GRFA-Net) that leverages the haze representations through feature restacking and propagation. We design a Feature Attention Residual Block (FARB) as the core of feature extraction, which employs the residual block to extract hierarchical features, and followed by a novel Feature Attention Module (FAM) that adaptively captures the inter-dependencies from channel- and spatial-wise perspectives. Furthermore, we utilize a group structure (GS) to enlarge the receptive field and merge different multi-level features via the gate fusion module (GFM), respectively. Extensive experiments demonstrate that our GRFA-Net can obtain results that are comparable or even better than previous state-of-the-art methods in terms of quantitative and qualitative evaluation metrics. Furthermore, we reduce the computational complexity considerably and obtain a real-time FPS. The code is available: https://github.com/leandepk/GRFA-Net.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10032-022-00409-4,en,Combination of explicit segmentation with Seq2Seq recognition for fine analysis of children handwriting,"['OriginalPaper', 'Special Issue Paper']","We consider the task of analyzing children handwriting in the context of a dictation task. The objective is to detect orthographic and phonological errors. To achieve this goal, we extend an existing handwriting analysis engine, based on an explicit segmentation of the handwritten input, originally developed for children copying exercises. We present a new approach, based on the combination of this analysis engine with a deep learning word recognition approach in order to improve both the recognition and segmentation performance. Explicit segmentation needs prior knowledge, and the deep network recognition predictions are a reliable approximation of the ground truth which can guide the analysis process. We propose to combine multiple prior knowledge strategies to further improve the analysis performance. Furthermore, we exploit the deep network approximate implicit segmentation to optimize the existing analysis process in terms of complexity.","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00521-022-07489-7,en,Neural network-based error handler in natural language processing,"['OriginalPaper', 'Original Article']","Grammar checking is one of the important applications of Natural Language Processing. Though the work in this area has been started decades before, the requirement of full-fledged grammar checking is still a demanding task. The recent revolution of Internet requires the computers not only deal with English Language but also in regional languages. People, who do not know English, tend to interact with computers through their regional language. Tamil is one such regional language which is recognized as classical (Semmozhi) language. Grammar checker application has been implemented for languages like English, Urdu, Punjabi, etc. But as far as Tamil is concerned, grammar checker is very scarce. There are many approaches to develop a grammar checker application. It can be statistical based, rule based or deep learning based. The proposed method involves hybrid approach to develop a Tamil grammar checker as Tamil has lot of grammatical features. In the proposed work, we concentrated on spell checking, consonant (Punarchi) error handling, long component letter error and subject–verb agreement errors. To tackle all these errors, combination of neural network approach as well as rule-based approach is proposed in this paper.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11590-021-01845-7,en,The multi-depot k-traveling repairman problem,"['OriginalPaper', 'Original Paper']","In this paper, we study the multi-depot k -traveling repairman problem. This problem extends the traditional traveling repairman problem to the multi-depot case. Its objective, similar to the single depot variant, is the minimization of the sum of the arrival times to customers. We propose two distinct formulations to model the problem, obtained on layered graphs. In order to find feasible solutions for the largest instances, we propose a hybrid genetic algorithm where initial solutions are built using a splitting heuristic and a local search is embedded into the genetic algorithm. The efficiency of the mathematical formulations and of the solution approach are investigated through computational experiments. The proposed models are scalable enough to solve instances up to 240 customers.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Computational Intelligence', 'Numerical and Computational Physics, Simulation']"
doi:10.1007/s10489-022-04114-x,en,Multi-instance learning based on spatial continuous category representation for case-level meningioma grading in MRI images,OriginalPaper,"Meningiomas have the highest incidence rate of all primary intracranial and central nervous system tumors. Accurate preoperative grading of meningiomas is extraordinarily meaningful for treatment strategy and patient prognosis. Magnetic resonance imaging (MRI) is the most common method for meningioma grading. Existing methods are typically two-stage models and require image-level classifications or region of interest (ROI) annotations for assistant diagnosis, thereby adding massive manual annotations and time costs. Meanwhile, most of these methods use only a single MRI slice, which may lose the overall meningioma information and are inconsistent with the actual clinical diagnosis process. To address the above problems, a multi-instance learning (MIL) method based on spatial continuous category representation is proposed for case-level meningioma grading. It considers the MRI case and corresponding slices as a bag and instances, respectively, and requires only a case-level label to diagnose a patient. To make the most of the serialization characteristics of MRI images, this method selects continuous instance-feature sequences under each category that are most suspected to contain tumors and further integrates these sequences into bag-level features for classification. In addition, an end-to-end meningioma grading architecture is designed to support the proposed MIL method, which directly takes the original MRI images of the patient as input and outputs the meningioma grade prediction. To train and evaluate the proposed method, datasets with different slice thicknesses are constructed. The experimental results demonstrate that our method performs satisfactorily compared with other related methods for meningioma grading.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00500-022-07506-w,en,FIR digital filter design based on improved artificial bee colony algorithm,"['OriginalPaper', 'Optimization']","The traditional swarm intelligence optimization algorithm is prone to fall into local optimal solutions in finite impulse response (FIR) digital filter design and has slow convergence speed. In order to optimize the design of FIR filter, a FIR digital filter design method based on improved bee colony (ABC) algorithm is proposed. This improved ABC algorithm can adaptively adjust the step size of the selected neighborhood of nectar source location. At the same time, the information of the global optimal solution is used to guide the search of candidate solution, which improves the global search ability of the algorithm. The improved ABC algorithm can balance the conflict between local search ability and global search ability, so it can achieve better optimization effect. The time and space complexity of the algorithm is analyzed in detail. Then, the improved ABC algorithm is used to design three typical FIR digital filters, namely low-pass, band-pass and band-stop filter. The performance of the designed filter is tested by simulation experiments. The experimental results show that compared with other state-of-the-art optimization algorithms, the proposed FIR filter design method has achieved better effect and performance. Meanwhile, the proposed design method has shorter optimization time. The superiority of the proposed method is verified.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11277-021-09071-1,en,A Machine Learning Based Detection and Mitigation of the DDOS Attack by Using SDN Controller Framework,OriginalPaper,"Recently, SDN has arisen as a new network platform that offers unparalleled programming that enables network operators to dynamically customize and control their networks. The attackers aim to paralyse the logical plane, the brain of the network that offers several advantages, by using the SDN controller. However, the control plane is the desirable target of security attacks on the opponents because of its characteristics. One of the most common threats is the DDOS attacks to drain network capacity by sending them heavy traffic, causing network congestion. SDN is a common area of investigation for SDN defenceand DDoS threat identification and prevention in the SDN context has been introduced to many researchers since the proposed SDN attacks. Nevertheless, security risks must be adequately secured. In this paper we suggest a discrete scalable memory based support vector machine algorithm for DDoS threat and SDN mitigation architecture for attack detection. By starting the process of attack detection the input data can gets pre-processed by using Spark standardization technique in which the missing values are replaced and the unwanted data are removed. Then the feature extractions are done using semantic multilinear component analysis algorithm. The classifier is responsible for predicting target and for this a novel discrete scalable memory based support vector machine (DSM-SVM) algorithm is used which provides high accuracy of attack prediction. Followed by attack detection the mitigation process was done, here the mitigation server can identify the threat by intelligently dropping malicious bot traffic and absorbing the rest of the traffic. Here the suggested mechanism achieves attack traffic mitigation and benign traffic dropping. We have evaluated the whole process on KDD dataset. The proposed network model was trained and then used in an SDN threat detection and mitigation environment as part of the assessment process. The entire experiment is run on a VMware-based Ubuntu virtual machine. Weka will utilize our suggested classifier model for training and evaluation, while Mininet uses a RYU controller to establish an SD Network. The findings demonstrate that the mechanism presented exceeds the other algorithms examined, by expressing 99.7% accuracy especially concerning training and testing time over KDD dataset.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s10994-022-06219-3,en,BT-Unet: A self-supervised learning framework for biomedical image segmentation using barlow twins with U-net models,OriginalPaper,"Deep learning has brought the most profound contribution towards biomedical image segmentation to automate the process of delineation in medical imaging. To accomplish such task, the models are required to be trained using huge amount of annotated or labelled data that highlights the region of interest with a binary mask. However, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this problem, a self-supervised learning framework, BT-Unet is proposed that uses the Barlow Twins approach to pre-train the encoder of a U-Net model via redundancy reduction in an unsupervised manner to learn data representation. Later, complete network is fine-tuned to perform actual segmentation. The BT-Unet framework can be trained with a limited number of annotated samples while having high number of unannotated samples, which is mostly the case in real-world problems. This framework is validated over multiple U-Net models over diverse datasets by generating scenarios of a limited number of labelled samples using standard evaluation metrics. With exhaustive experiment trials, it is observed that the BT-Unet framework enhances the performance of the U-Net models with significant margin under such circumstances.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s10489-022-03472-w,en,TD-Net:unsupervised medical image registration network based on Transformer and CNN,OriginalPaper,"Medical image registration is a fundamental task in computer-aided medical diagnosis. Recently, researchers have begun to use deep learning methods based on convolutional neural networks (CNN) for registration, and have made remarkable achievements in medical image registration. Although CNN based methods can provide rich local information on registration, their global modeling ability is weak to carry out the long distance information interaction and restrict the registration performance. The Transformer is originally used for sequence-to-sequence prediction. Now it also achieves great results in various visual tasks, due to its strong global modeling capability. Compared with CNN, Transformer can provide rich global information, in contrast, Transformer lacks of local information. To address Transformer lacks local information, we propose a hybrid network which is similar to U-Net to combine Transformer and CNN, to extract global and local information (at each level). Specifically, CNN is first used to obtain the feature maps of the image, and the Transformer is used as encoder to extract global information. Then the results obtained by Transformer encoding are connected to the upsampling process. The upsampling uses CNN to integrate local information and global information. Finally, the resolution is restored to the input image, and obtain the displacement field after several convolution layers. We evaluate our method on brain MRI scans. Experimental results demonstrate that our method improves the accuracy by 1 % compared with the state-of-the-art approaches.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00530-022-00956-0,en,BERT-based semi-supervised domain adaptation for disastrous classification,"['OriginalPaper', 'Regular Paper']","Currently, with the rapid development of social media platforms represented by twitter, multimodal content on social media provides critical information during major disastrous events. Different from traditional text news, major disastrous events posted on Twitter often attach videos or images. Although many studies have shown that both image and text contents are useful for humanitarian aid and disaster response, this research focuses on analyzing multimodal information in social media to effectively analyze sudden disastrous events. We propose an end-to-end model, the BERT-based semi-supervised domain adaptation for multimodal disastrous events classification (BSSDA). BSSDA is composed of two main modules: a classifier with minimax entropy domain adaptation and a multimodal feature extractor. First, we perform multimodal feature extraction on our three modalities (image, text and image description). Here, we generate image description with the NeuralTalk and extract both image description features and text features with BERT, and extract image features with pretrained VGG. Then, the multimodal features are concatenated and fed to the classifier with minimax entropy domain adaptation module. The purpose of adding the domain adaptation module is to map the multimodal features of different disastrous events to the same feature space by using a good amount of unlabeled data of the unforeseen real-time disastrous event. Our experimental results of three different types of disastrous events show that our BSSDA model has significant improvements for disastrous events classification.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s41095-021-0264-2,en,High fidelity virtual try-on network via semantic adaptation and distributed componentization,"['OriginalPaper', 'Research Article']","Image-based virtual try-on systems have significant commercial value in online garment shopping. However, prior methods fail to appropriately handle details, so are defective in maintaining the original appearance of organizational items including arms, the neck, and in-shop garments. We propose a novel high fidelity virtual try-on network to generate realistic results. Specifically, a distributed pipeline is used for simultaneous generation of organizational items. First, the in-shop garment is warped using thin plate splines (TPS) to give a coarse shape reference, and then a corresponding target semantic map is generated, which can adaptively respond to the distribution of different items triggered by different garments. Second, organizational items are componentized separately using our novel semantic map-based image adjustment network (SMIAN) to avoid interference between body parts. Finally, all components are integrated to generate the overall result by SMIAN. A priori dual-modal information is incorporated in the tail layers of SMIAN to improve the convergence rate of the network. Experiments demonstrate that the proposed method can retain better details of condition information than current methods. Our method achieves convincing quantitative and qualitative results on existing benchmark datasets.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s11063-022-10740-w,en,EFRNet: Efficient Feature Reuse Network for Real-time Semantic Segmentation,OriginalPaper,"Semantic segmentation is a kind of dense prediction task, which has high requirements on the prediction accuracy and inference speed in mobile terminals. To reduce the computational burden of the segmentation network and supplement the missing spatial information of high-level features, an efficient feature reuse network (EFRNet) is proposed in two steps: a Multi-scale Bottleneck module is designed to extract multi-scale features, and a lightweight backbone is designed based on the MB module; then, features of different depths are integrated through efficient feature reuse model. Experiments on Cityscapes datasets demonstrate that the proposed EFRNet achieves an impressive balance between speed and precision. Specifically, without any pre-trained model and post-processing, it achieves 75.58% Mean IoU on the Cityscapes test dataset with the speed of 118 FPS on a single RTX 2080Ti GPU.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s44212-022-00015-z,en,Traffic flow prediction using bi-directional gated recurrent unit method,"['OriginalPaper', 'Original Article']","Traffic flow prediction plays an important role in intelligent transportation systems. To accurately capture the complex non-linear temporal characteristics of traffic flow, this paper adopts a Bi-directional Gated Recurrent Unit (Bi-GRU) model in traffic flow prediction. Compared to Gated Recurrent Unit (GRU), which can memorize information from the previous sequence, this model can memorize the traffic flow information in both previous and subsequent sequence. To demonstrate the model’s performance, a set of real case data at 1-hour intervals from 5 working days was used, wherein the dataset was separated into training and validation. To improve data quality, an augmented dickey-fuller unit root test and differential processing were performed before model training. Four benchmark models were used, including the Autoregressive Integrated Moving Average (ARIMA), Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (Bi-LSTM), and GRU. The prediction results show the superior performance of Bi-GRU. The Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE), and Mean Absolute Error (MAE) of the Bi-GRU model are 30.38, 9.88%, and 23.35, respectively. The prediction accuracy of LSTM, Bi-LSTM, GRU, and Bi-GRU, which belong to deep learning methods, is significantly higher than that of the traditional ARIMA model. The MAPE difference of Bi-GRU and GRU is 0.48% which is a small prediction error value. The results show that the prediction accuracy of the peak period is higher than that of the low peak. The Bi-GRU model has a certain lag on traffic flow prediction.","['Geography', 'Geographical Information Systems/Cartography', 'Landscape/Regional and Urban Planning', 'Computer Appl. in Social and Behavioral Sciences']"
doi:10.1007/s11042-022-13481-z,en,FDDL-Net: frequency domain decomposition learning for speckle reduction in ultrasound images,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","Image decomposition is a useful operation that benefits a number of low-level vision tasks. However, this conventional wisdom is not well studied in deep learning, and almost no existing deep learning-based methods consider the fact that the extracted feature map from a convolution layer consists of different frequency information. We propose an end-to-end frequency domain decomposition learning network (FDDL-Net) to remove speckle noise from ultrasound images. FDDL-Net leverages frequency domain decomposition at the feature level to learn structure and detail information from ultrasound images via an interactive dual-branch framework. According to the properties of speckle noise, the median filter is utilized in the high-frequency branch of the network to remove the noise effectively. In addition, information from the two branches is exchanged interactively, so that valuable features from different frequencies are fully exploited for speckle reduction. Compared with state-of-the-art methods, FDDL-net demonstrates superior noise reduction and feature preservation (0.89 and 30.92 for SSIM and PSNR metrics respectively), attributing to the dual-branch interaction of the network.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11135-021-01299-0,en,"Bio, psycho, or social: supervised machine learning to classify discursive framing of depression in online health communities",OriginalPaper,"Supervised machine learning on textual data has successful industrial/business applications, but it is an open question whether it can be utilized in social knowledge building outside the scope of hermeneutically more trivial cases. Combining sociology and data science raises several methodological and epistemological questions. In our study the discursive framing of depression is explored in online health communities. Three discursive frameworks are introduced: the bio-medical, psychological, and social framings of depression. ~80 000 posts were collected, and a sample of them was manually classified. Conventional bag-of-words models, Gradient Boosting Machine, word-embedding-based models and a state-of-the-art Transformer-based model with transfer learning, called DistilBERT were applied to expand this classification on the whole database. According to our experience ‘discursive framing’ proves to be a complex and hermeneutically difficult concept, which affects the degree of both inter-annotator agreement and predictive performance. Our finding confirms that the level of inter-annotator disagreement provides a good estimate for the objective difficulty of the classification. By identifying the most important terms, we also interpreted the classification algorithms, which is of great importance in social sciences. We are convinced that machine learning techniques can extend the horizon of qualitative text analysis. Our paper supports a smooth fit of the new techniques into the traditional toolbox of social sciences.","['Social Sciences', 'Methodology of the Social Sciences', 'Social Sciences, general']"
doi:10.1186/s13018-022-03408-7,en,Artificial intelligence and machine learning on diagnosis and classification of hip fracture: systematic review,"['ReviewPaper', 'Systematic Review']","Background In the emergency room, clinicians spend a lot of time and are exposed to mental stress. In addition, fracture classification is important for determining the surgical method and restoring the patient's mobility. Recently, with the help of computers using artificial intelligence (AI) or machine learning (ML), diagnosis and classification of hip fractures can be performed easily and quickly. The purpose of this systematic review is to search for studies that diagnose and classify for hip fracture using AI or ML, organize the results of each study, analyze the usefulness of this technology and its future use value. Methods PubMed Central, OVID Medline, Cochrane Collaboration Library, Web of Science, EMBASE, and AHRQ databases were searched to identify relevant studies published up to June 2022 with English language restriction. The following search terms were used [All Fields] AND ("", ""[MeSH Terms] OR (""""[All Fields] AND ""bone""[All Fields]) OR ""bone fractures""[All Fields] OR ""fracture""[All Fields]). The following information was extracted from the included articles: authors, publication year, study period, type of image, type of fracture, number of patient or used images, fracture classification, reference diagnosis of fracture diagnosis and classification, and augments of each studies. In addition, AI name, CNN architecture type, ROI or important region labeling, data input proportion in training/validation/test, and diagnosis accuracy/AUC, classification accuracy/AUC of each studies were also extracted. Results In 14 finally included studies, the accuracy of diagnosis for hip fracture by AI was 79.3–98%, and the accuracy of fracture diagnosis in AI aided humans was 90.5–97.1. The accuracy of human fracture diagnosis was 77.5–93.5. AUC of fracture diagnosis by AI was 0.905–0.99. The accuracy of fracture classification by AI was 86–98.5 and AUC was 0.873–1.0. The forest plot represented that the mean AI diagnosis accuracy was 0.92, the mean AI diagnosis AUC was 0.969, the mean AI classification accuracy was 0.914, and the mean AI classification AUC was 0.933. Among the included studies, the architecture based on the GoogLeNet architectural model or the DenseNet architectural model was the most common with three each. Among the data input proportions, the study with the lowest training rate was 57%, and the study with the highest training rate was 95%. In 14 studies, 5 studies used Grad-CAM for highlight important regions. Conclusion We expected that our study may be helpful in making judgments about the use of AI in the diagnosis and classification of hip fractures. It is clear that AI is a tool that can help medical staff reduce the time and effort required for hip fracture diagnosis with high accuracy. Further studies are needed to determine what effect this causes in actual clinical situations.","['Medicine & Public Health', 'Orthopedics', 'Surgical Orthopedics']"
doi:10.1007/s11042-022-13115-4,en,Unpaired image-to-image translation with improved two-dimensional feature,OriginalPaper,"With the feature-level constraints, unpaired image translation is challenging in generating poor realistic images, which focuses on convolutional feature extraction, ignoring the SVD feature extraction. To address this limitation, the Unpaired Image-to-image Translation with Improved Two-dimensional Feature (UNTF) is proposed. Specifically, in our method the novel feature extraction module consists two part: the SVD feature extraction and the convolutional feature extraction. The SVD feature maps were built by Two-Dimensional Feature which transform 1-D features into 2-D features to cascade with convolutional features. In up-sampling module sub-pixel convolution is used to replace transposed convolution. What’s more, the proposed feature loss can stabilize the training process of generator. Finally, the proposed network was verified by ablation study and state-of-the-art methods. Experiments on image translation, image illustration, and image restoration show that both the image clarity index (EGF) and experts agree that the proposed method is superior to the existing methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1038/s41586-022-05306-8,en,An integrated imaging sensor for aberration-corrected 3D photography,"['OriginalPaper', 'Article']","Planar digital image sensors facilitate broad applications in a wide range of areas 1 – 5 , and the number of pixels has scaled up rapidly in recent years 2 , 6 . However, the practical performance of imaging systems is fundamentally limited by spatially nonuniform optical aberrations originating from imperfect lenses or environmental disturbances 7 , 8 . Here we propose an integrated scanning light-field imaging sensor, termed a meta-imaging sensor, to achieve high-speed aberration-corrected three-dimensional photography for universal applications without additional hardware modifications. Instead of directly detecting a two-dimensional intensity projection, the meta-imaging sensor captures extra-fine four-dimensional light-field distributions through a vibrating coded microlens array, enabling flexible and precise synthesis of complex-field-modulated images in post-processing. Using the sensor, we achieve high-performance photography up to a gigapixel with a single spherical lens without a data prior, leading to orders-of-magnitude reductions in system capacity and costs for optical imaging. Even in the presence of dynamic atmosphere turbulence, the meta-imaging sensor enables multisite aberration correction across 1,000 arcseconds on an 80-centimetre ground-based telescope without reducing the acquisition speed, paving the way for high-resolution synoptic sky surveys. Moreover, high-density accurate depth maps can be retrieved simultaneously, facilitating diverse applications from autonomous driving to industrial inspections. A meta-imaging sensor detects an extra-fine 4D light field distribution using a vibrating microlens array, enabling high-resolution 3D photography up to a gigapixel with fast aberration correction, demonstrated on a telescope aimed at the Moon.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07014-x,en,Dynamic GAN for high-quality sign language video generation from skeletal poses using generative adversarial networks,"['OriginalPaper', 'Focus']","The recent advancements of unsupervised deep generative models have produced incredible results in image and video generation tasks. However, existing approaches still pose huge challenges in the high-quality video generation process. The generated videos consist of blurred effects and poor video quality. In this paper, we introduce a novel generative framework named dynamic generative adversarial networks (dynamic GAN) model for regulating the adversarial training and generating photo-realistic high-quality sign language videos. The proposed model uses skeletal poses information and person images as input and produces high-quality videos. In generator phase, the proposed model uses U-Net-like network to generate target frames from skeletal poses. Further, the generated samples are classified using the VGG-19 framework to identify its word class. The discriminator network classifies the real and fake samples as well as concatenates the resultant frames and generates the high-quality video output. Unlike, existing approaches the proposed novel framework produces photo-realistic video quality results without employing any animation or avatar approaches. To evaluate the model performance qualitatively and quantitatively, the proposed model has been evaluated using three benchmark datasets that yield plausible results. The datasets are RWTH-PHOENIX-Weather 2014T dataset, and our self-created dataset for Indian Sign Language (ISL-CSLTR), and the UCF-101 Action Recognition dataset. The proposed model achieves average 28.7167 PSNR score, 0.921 average SSIM score, 14 average FID score and 8.73 ± 0.23 average inception score which are relatively higher than existing approaches.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1186/s42400-022-00121-0,en,Unleashing the power of pseudo-code for binary code similarity analysis,"['OriginalPaper', 'Research']","Code similarity analysis has become more popular due to its significant applicantions, including vulnerability detection, malware detection, and patch analysis. Since the source code of the software is difficult to obtain under most circumstances, binary-level code similarity analysis (BCSA) has been paid much attention to. In recent years, many BCSA studies incorporating AI techniques focus on deriving semantic information from binary functions with code representations such as assembly code, intermediate representations, and control flow graphs to measure the similarity. However, due to the impacts of different compilers, architectures, and obfuscations, binaries compiled from the same source code may vary considerably, which becomes the major obstacle for these works to obtain robust features. In this paper, we propose a solution, named UPPC (Unleashing the Power of Pseudo-code), which leverages the pseudo-code of binary function as input, to address the binary code similarity analysis challenge, since pseudo-code has higher abstraction and is platform-independent compared to binary instructions. UPPC selectively inlines the functions to capture the full function semantics across different compiler optimization levels and uses a deep pyramidal convolutional neural network to obtain the semantic embedding of the function. We evaluated UPPC on a data set containing vulnerabilities and a data set including different architectures (X86, ARM), different optimization options (O0-O3), different compilers (GCC, Clang), and four obfuscation strategies. The experimental results show that the accuracy of UPPC in function search is 33.2% higher than that of existing methods.","['Computer Science', 'Computer Science, general']"
doi:10.1007/s10334-022-01017-3,en,Right ventricular strain and volume analyses through deep learning-based fully automatic segmentation based on radial long-axis reconstruction of short-axis cine magnetic resonance images,"['OriginalPaper', 'Research Article']","Objective We propose a deep learning-based fully automatic right ventricle (RV) segmentation technique that targets radially reconstructed long-axis (RLA) images of the center of the RV region in routine short axis (SA) cardiovascular magnetic resonance (CMR) images. Accordingly, the purpose of this study is to compare the accuracy of deep learning-based fully automatic segmentation of RLA images with the accuracy of conventional deep learning-based segmentation in SA orientation in terms of the measurements of RV strain parameters. Materials and methods We compared the accuracies of the above-mentioned methods in RV segmentations and in measuring RV strain parameters by Dice similarity coefficients (DSCs) and correlation coefficients. Results DSC of RV segmentation of the RLA method exhibited a higher value than those of the conventional SA methods (0.84 vs. 0.61). Correlation coefficient with respect to manual RV strain measurements in the fully automatic RLA were superior to those in SA measurements (0.5–0.7 vs. 0.1–0.2). Discussion Our proposed RLA realizes accurate fully automatic extraction of the entire RV region from an available CMR cine image without any additional imaging. Our findings overcome the complexity of image analysis in CMR without the limitations of the RV visualization in echocardiography.","['Medicine & Public Health', 'Imaging / Radiology', 'Computer Appl. in Life Sciences', 'Solid State Physics', 'Biomedical Engineering and Bioengineering', 'Health Informatics']"
doi:10.1007/s10237-022-01616-y,en,Analysis of control strategies for VIVA OpenHBM with active reflexive neck muscles,"['OriginalPaper', 'Original Paper']","Modeling muscle activity in the neck muscles of a finite element (FE) human body model can be based on two biological reflex systems. One approach is to approximate the Vestibulocollic reflex (VCR) function, which maintains the head orientation relative to a fixed reference in space. The second system tries to maintain the head posture relative to the torso, similar to the Cervicocolic reflex (CCR). Strategies to combine these two neck muscle controller approaches in a single head-neck FE model were tested, optimized, and compared to rear-impact volunteer data. The first approach, Combined-Control, assumed that both controllers simultaneously controlled all neck muscle activations. In the second approach, Distributed-Control, one controller was used to regulate activation of the superficial muscles while a different controller acted on deep neck muscles. The results showed that any muscle controller that combined the two approaches was less effective than only using one of VCR- or CCR-based systems on its own. A passive model had the best objective rating for cervical spine kinematics, but the addition of a single active controller provided the best response for both head and cervical spine kinematics. The present study demonstrates the difficulty in completely capturing representative head and cervical spine responses to rear-impact loading and identified a controller capturing the VCR reflex as the best candidate to investigate whiplash injury mechanisms through FE modeling.","['Engineering', 'Theoretical and Applied Mechanics', 'Biomedical Engineering and Bioengineering', 'Biological and Medical Physics, Biophysics']"
doi:10.1007/s00521-022-07608-4,en,A walk in the black-box: 3D visualization of large neural networks in virtual reality,"['OriginalPaper', 'Original Article']","Within the last decade Deep Learning has become a tool for solving challenging problems like image recognition. Still, Convolutional Neural Networks (CNNs) are considered black-boxes, which are difficult to understand by humans. Hence, there is an urge to visualize CNN architectures, their internal processes and what they actually learn. Previously, virtual realityhas been successfully applied to display small CNNs in immersive 3D environments. In this work, we address the problem how to feasibly render large-scale CNNs, thereby enabling the visualization of popular architectures with ten thousands of feature maps and branches in the computational graph in 3D. Our software ”DeepVisionVR” enables the user to freely walk through the layered network, pick up and place images, move/scale layers for better readability, perform feature visualization and export the results. We also provide a novel Pytorch module to dynamically link PyTorch with Unity, which gives developers and researchers a convenient interface to visualize their own architectures. The visualization is directly created from the PyTorch class that defines the Pytorch model used for training and testing. This approach allows full access to the network’s internals and direct control over what exactly is visualized. In a use-case study, we apply the module to analyze models with different generalization abilities in order to understand how networks memorize images. We train two recent architectures, CovidResNet and CovidDenseNet on the Caltech101 and the SARS-CoV-2 datasets and find that bad generalization is driven by high-frequency features and the susceptibility to specific pixel arrangements, leading to implications for the practical application of CNNs. The code is available on Github https://github.com/Criscraft/DeepVisionVR .","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12206-022-1103-9,en,Intelligent identification of mortar void in ballastless slab track using the wheelset acceleration combined with CNN-SVM,"['OriginalPaper', 'Original Article']","Mortar void is a hidden defect in ballastless slab track difficult to be efficiently identified by traditional detection methods. This paper is dedicated to proposing a new detection method to identify the mortar void position and length using the vehicle response combined with the hybrid convolutional neural network-support vector machine (CNN-SVM) classifier. The vertical wheelset accelerations with different mortar void conditions are collected from a vehicle-track coupled dynamics simulation model. The first components decomposed from wheel-set accelerations by local mean decomposition and their envelopes are utilized as the training data due to their sensitivity to mortar void. To improve the identification precision, the scope descent method is proposed to determine the range influenced by mortar void (IMVR) and samples are labeled according to IMVR. Meanwhile, identification results are post processed based on the mortar void characteristics. The results show that over 90 % mortar void conditions with the length of 0.65 m are detected correctly and the identification has a higher precision with the mortar void length greater than 0.95 m. The proposed technology of mortar void detection using the wheelset accelerations with the hybrid CNN-SVM classifier provides reference for engineering application, which is of great significance to relieve the pressure of health monitoring of railway track.","['Engineering', 'Mechanical Engineering', 'Vibration, Dynamical Systems, Control', 'Industrial and Production Engineering']"
doi:10.1007/s10973-022-11694-w,en,Artificial neural network modeling of the Casson fluid flow over unsteady radially stretching sheet with Soret and Dufour effects,OriginalPaper,"The Dufour and Soret effects on the flow of a Casson fluid about an unsteady radially stretched sheet are analyzed. The system of nonlinear ordinary differential equations is obtained from the equations governing the flow by employing similarity transformations. The solution to these nonlinear ordinary differential equations is calculated using artificial neural networks. The trial functions employ a multilayer perceptron neural network with programmable parameters (biases and masses). In order to fulfill the governing equations, the ADAMS (adaptive moment estimation algorithm) optimization technique is used to calculate the trial solution’s adjustable parameters. The results suggest that the artificial neural network-based method gives significant accuracy and that the solution’s efficacy increases as the number of neurons in the neural network increases. Also, the computations of skin friction and heat transfer coefficient using the current method are compared with the values obtained by the Runge–Kutta fourth-order method. Further, the impact of relevant parameters on the physical quantities is displayed through graphs. Finally, a comparison using existing literature is made to back up our findings, and an excellent correlation is discovered, affirming our findings. According to the current computation, raising the Soret number improves the Nusselt number and drops the Sherwood number, whereas improving the Dufour number diminishes the Nusselt number and enhances the Sherwood number.","['Chemistry', 'Physical Chemistry', 'Analytical Chemistry', 'Polymer Sciences', 'Inorganic Chemistry', 'Measurement Science and Instrumentation']"
doi:10.1007/s12198-022-00247-9,en,Expediting airport security queues through advanced lane assignment,OriginalPaper,"Excessive airport security wait times during peak operational periods have been well-documented in crowdsourced data and well-publicized among the news media. While serving a paramount purpose, airport security checkpoints are capacity constrained and frequently stressed, leading to passenger dissatisfaction and system limitations. To alleviate air travelers’ wasted wait time during the security screening process, an innovative queue management technique is explored. Passengers currently flow to Transportation Security Administration (TSA) screening lanes at terminal checkpoints via a First-Come, First-Serve (FCFS) discipline. However, repeated variations in passenger characteristics and screening times may cause this service discipline to suffer small inefficiencies that aggregately distort resource utilization and throughput speed. This paper proposes an Advance Lane Assignment System (ALAS) in which passengers are directed to specific screening lanes upon arrival to a terminal checkpoint using real-time, autonomous, feedback control. Leveraging existing Bluetooth© technology to assess lane flow rates, control logic can convey lane assignments to passengers at identification authentication gates. System feasibility was analyzed through discrete, dynamic, and probabilistic simulations of a multilane, multiphase queue model with varying traffic intensities and control logic. Basic, discrete-time Proportional-Integral-Derivative (PID) control was found to offer a 12% reduction in average passenger waiting times over the baseline FCFS discipline.","['Business and Management', 'Logistics', 'Economic Policy', 'Innovation/Technology Management', 'R & D/Technology Policy', 'Law of the Sea, Air and Outer Space']"
doi:10.1007/s00500-021-06473-y,en,An intrusion detection system for wireless sensor networks using deep neural network,"['OriginalPaper', 'Focus']","Wireless sensor network comprises of a large number of sensor nodes to acquire and transmit data to the central location. However, due to resource constrained nodes, deployment strategies and communication channel introduce numerous security challenges to the wireless sensor networks. So, it is essential to detect unauthorized access to improve the security features of wireless sensor networks. Network intrusion detection systems provide such services to the network and it becomes inevitable for any communication network. Machine learning (ML) techniques are widely used in intrusion detection systems; however, the performance of ML techniques is not satisfactory while handling imbalanced attacks. To solve this and to improve the performance, this research work proposed an intrusion detection system based on deep neural network (DNN). Cross-correlation process is used to select the optimal features from the dataset and the selected parameters are used as building blocks for deep neural network structure to find intrusions. The experimental results confirmed that the proposed DNN performs better than conventional machine learning models such as support vector machine, decision tree, and random forest and efficiently identifies the attacks.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s12539-022-00524-0,en,TP-DDI: A Two-Pathway Deep Neural Network for Drug–Drug Interaction Prediction,"['OriginalPaper', 'Original research article']","Adverse drug–drug interactions (DDIs) can severely damage the body. Thus, it is essential to accurately predict DDIs. DDIs are complex processes in which many factors can cause interactions. Rather than merely considering one or two of the factors, we design a two-pathway drug–drug interaction framework named TP-DDI that uses multimodal data for DDI prediction. TP-DDI effectively explores the combined effect of a topological structure-based pathway and a biomedical object similarity-based pathway to obtain multimodal drug representations. For the topology-based pathway, we focus on drug chemistry structures through the self-attention mechanism, which can capture hidden critical relationships, especially between pairs of atoms at remote topological distances. For the similarity-based pathway, our model can emphasize useful biomedical objects according to the channel weights. Finally, the fusion of multimodal data provides a holistic view of DDIs by learning the complementary features. On a real-world dataset, experiments show that TP-DDI can achieve better performance than the state-of-the-art models. Moreover, we can find the most critical substructures with certain interpretability in the newly predicted DDIs. Graphical abstract ","['Life Sciences', 'Computer Appl. in Life Sciences', 'Computational Biology/Bioinformatics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Theoretical and Computational Chemistry', 'Theoretical, Mathematical and Computational Physics', 'Computational Science and Engineering']"
doi:10.1007/s10489-022-03217-9,en,SARNet: Spatial Attention Residual Network for pedestrian and vehicle detection in large scenes,OriginalPaper,"With the development of high-resolution camera technology, the shooting scene coverage has reached the square kilometer level, thousands of people can be observed at the same time, and the faces of people from a hundred meters away are clearly recognizable. The images captured by high-resolution cameras are very different from those captured by conventional cameras. In the face of many detection targets in high-resolution images, large differences in target scales due to spatial position, as well as difficulties in extracting features and poor detection results caused by target overlap and concealment phenomena, this paper proposes a multi-target detection method SARNet that combined with spatial attention optimization feature extraction. Use spatial attention to optimize the backbone network, expand the local receptive field, thereby enhance the representation ability, and enhance the feature extraction ability of small targets; the different scale features of the dilated feature pyramid network are subjected to the deformable region of interest pooling operation, which effectively improves the different scales detection accuracy. The experimental results show that the method proposed in this paper can get 51.9% mAP on the PANDA dataset, which is superior to the existing detection algorithms. At the same time, experimental verification of pedestrians and vehicles on the COCO2017 dataset fully proves the feasibility of the method in this paper.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10994-022-06194-9,en,Representation learning for clustering via building consensus,OriginalPaper,"In this paper, we focus on unsupervised representation learning for clustering of images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be close in the representation space (exemplar consistency), and/or similar images must have similar cluster assignments (population consistency). We define an additional notion of consistency, consensus consistency , which ensures that representations are learned to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a single clustering algorithm. We define a clustering loss by executing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, consensus clustering using unsupervised representation learning (ConCURL), improves upon the clustering performance of state-of-the-art methods on four out of five image datasets. Furthermore, we extend the evaluation procedure for clustering to reflect the challenges encountered in real-world clustering tasks, such as maintaining clustering performance in cases with distribution shifts. We also perform a detailed ablation study for a deeper understanding of the proposed algorithm. The code and the trained models are available at https://github.com/JayanthRR/ConCURL_NCE .","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s11042-022-13102-9,en,Identify videos with facial manipulations based on convolution neural network and dynamic texture,OriginalPaper,"Recent facial manipulation techniques based on deep learning can create a highly realistic face by changing expression, attributes, identity, or creating an entire face synthesis, that called recently Deep-Fake. With the rapid appearance of such applications, they have raised great security concerns. Therefore, corresponding forensic techniques are proposed to tackle this issue. However, existing techniques are either based on complex deep networks with a binary classification that are unable to distinguish between facial manipulation types or rely on fragile hand-crafted features with unsatisfactory results. To overcome these issues, we propose a learning-based detection method by creating an uncomplicated CNN network called FMD-Net relying on the dynamic textures as input. Moreover, it is able to distinguish between facial manipulation types such as Deepfake, Face2Face, FaceSwap, and NeuralTexture. By using dynamic textures of each video shot, motion and appearance features are combined which helped the network learn manipulation artifacts and provides a robust performance at various compression levels. We conduct extensive experiments on various benchmark datasets (FaceForensics++, DFDC, and Celeb-DF) to empirically demonstrate the superiority and effectiveness of the proposed method with both binary classification and multi-classification against the state-of-the-art methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40747-022-00741-6,en,Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text,"['OriginalPaper', 'Original Article']","The increase in people’s use of mobile messaging services has led to the spread of social engineering attacks like phishing, considering that spam text is one of the main factors in the dissemination of phishing attacks to steal sensitive data such as credit cards and passwords. In addition, rumors and incorrect medical information regarding the COVID-19 pandemic are widely shared on social media leading to people’s fear and confusion. Thus, filtering spam content is vital to reduce risks and threats. Previous studies relied on machine learning and deep learning approaches for spam classification, but these approaches have two limitations. Machine learning models require manual feature engineering, whereas deep neural networks require a high computational cost. This paper introduces a dynamic deep ensemble model for spam detection that adjusts its complexity and extracts features automatically. The proposed model utilizes convolutional and pooling layers for feature extraction along with base classifiers such as random forests and extremely randomized trees for classifying texts into spam or legitimate ones. Moreover, the model employs ensemble learning procedures like boosting and bagging. As a result, the model achieved high precision, recall, f1-score and accuracy of 98.38%.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11554-022-01241-z,en,DDH-YOLOv5: improved YOLOv5 based on Double IoU-aware Decoupled Head for object detection,"['OriginalPaper', 'Original Research Paper']","YOLOv5 is a high-performance real-time object detector that plays an important role in one-stage detectors. However, there are two problems with the design of the YOLOv5 head. The common branch of classification task and regression task of the YOLOv5 head will hurt the training process, and the correlation between classification score and localization accuracy is low. We propose a Double IoU-aware Decoupled Head (DDH) and apply it to YOLOv5. The improved model is named DDH-YOLOv5, which substantially improves the localization accuracy of the model without significantly increasing FLOPS and parameters. Extensive experiments on dataset PASCAL VOC2007 show that DDH-YOLOv5 has good performance. Compared with YOLOv5, DDH-YOLOv5m and DDH-YOLOv5l proposed in this paper achieve 2.4 $$\%$$ % and 1.3 $$\%$$ % improvement in Average Precision (AP), respectively. Compared with Deformable DETR, which is known for its fast-converging, DDH-YOLOv5 completely outperforms Deformable DETR on COCO2017 Val with half of FLOPS and only a quarter of epochs.","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s12194-022-00673-3,en,Classifying presence or absence of calcifications on mammography using generative contribution mapping,"['OriginalPaper', 'Research Article']","The purpose of this study was to verify the efficacy of generative contribution mapping (GCM), an explainable deep learning model for images, in classifying the presence or absence of calcifications on mammography. The learning dataset consisted of 303 full-field digital mammography (FFDM) images labeled with microcalcifications obtained from the public INbreast database without extremely dense images. FFDM images were divided into calcification and non-calcification patch images using a sliding window method with 25% overlap. The patch images of the mediolateral oblique (MLO) and craniocaudal (CC) views were divided into a training set of 70%, a validation set of 10%, and a testing set of 20%. The classification performance of GCM classifiers was evaluated and compared with that of EfficientNet classifiers. Visualization maps of GCM highlighted regions of interest more clearly than EfficientNet’s gradient-weighted class activation maps. The results showed that GCM classifiers yielded an accuracy of 0.92 (CC), 0.91 (MLO), and an area under the receiver operating characteristic curve of 0.92 (CC), 0.94 (MLO). In conclusion, GCM could accurately classify the presence or absence of calcifications on mammograms and explain intuitively reasonable grounds for their classification with visualization maps highlighting regions of interest.","['Medicine & Public Health', 'Imaging / Radiology', 'Nuclear Medicine', 'Radiotherapy', 'Medical and Radiation Physics']"
doi:10.1007/s00256-022-04081-x,en,Ensemble deep learning model for predicting anterior cruciate ligament tear from lateral knee radiograph,"['OriginalPaper', 'Scientific Article']","Objective To develop an ensemble deep learning model (DLM) predicting anterior cruciate ligament (ACL) tears from lateral knee radiographs and to evaluate its diagnostic performance. Materials and methods In this study, 1433 lateral knee radiographs (661 with ACL tear confirmed on MRI, 772 normal) from two medical centers were split into training ( n  = 1146) and test sets ( n  = 287). Three single DLMs respectively classifying radiographs with ACL tears, abnormal lateral femoral notches, and joint effusion were developed. An ensemble DLM predicting ACL tears was developed by combining the three DLMs via stacking method. The sensitivities, specificities, and area under the receiver operating characteristic curves (AUCs) of the DLMs and three radiologists were compared using McNemar test and Delong test. Subgroup analysis was performed to identify the radiologic features associated with the sensitivity. Results The sensitivity, specificity, and AUC of the ensemble DLM were 86.8% (95% confidence interval [CI], 79.9–92.0%), 89.4% (95% CI, 83.4–93.8%), and 0.927 (95% CI, 0.891–0.954), achieving diagnostic performance comparable with that of a musculoskeletal radiologist ( P  = 0.193, McNemar test; P  = 0.131, Delong test). The AUC of the ensemble DLM was significantly higher than those of non-musculoskeletal radiologists ( P  = 0.043, P  < 0.001). The sensitivity of the DLM was higher than that of the radiologists in the absence of an abnormal lateral femoral notch or joint effusion. Conclusion The diagnostic performance of the ensemble DLM in predicting lateral knee radiographs with ACL tears was comparable to that of a musculoskeletal radiologist.","['Medicine & Public Health', 'Imaging / Radiology', 'Orthopedics', 'Pathology', 'Nuclear Medicine']"
doi:10.1007/s11042-022-12892-2,en,Semantic segmentation and detection of satellite objects using U-Net model of deep learning,OriginalPaper,"Deep learning methods are used to analyze satellite images. These satellite images contain many constructed and natural objects, but these are not entirely visible and detectable with naked eyes. Because human eyes can only see and detect the light that falls in the visible range. These satellite images fall beyond the visual scope, thus rendering it impossible for a human. However, after the application of pre-processing techniques and methods of image processing, it can be seen. So, we apply deep learning methods to classify and detect different objects in satellite images and segment them according to their classes. These methods also count class-wise objects using segmentation techniques. Here, only ten predefined classes are considered and classify all objects of a satellite image into these classes. For this, we use the U-Net model of deep learning of image segmentation. The Kaggle dataset of the DSTL competition is used to segment them according to their classes and count their numbers. We measured the performance of models in terms of the Jaccard index, dice coefficient, accuracy, and loss at the time of training and testing. To prove the model’s superiority, we compared it with others’ scores in terms of the Jaccard index. The motivation behind this work is to apply deep learning techniques in satellite imaging analysis for well-being. Because with the help of satellite images, we can know changes in flora and fauna of an area in a particular range of time. This technique also helps monitor disaster management efficiently in flood, fire, and natural calamities where physical presence is impossible. The satellite can monitor all-region effectively. Based on satellite image analysis, accurate and fast decisions can be made economically and efficiently. Thus this research will be beneficial for humankind.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00354-021-00144-0,en,Stacking Ensemble-Based Intelligent Machine Learning Model for Predicting Post-COVID-19 Complications,OriginalPaper,"The recent outbreak of novel coronavirus disease (COVID-19) has resulted in healthcare crises across the globe. Moreover, the persistent and prolonged complications of post-COVID-19 or long COVID are also putting extreme pressure on hospital authorities due to the constrained healthcare resources. Out of many long-lasting post-COVID-19 complications, heart disease has been realized as the most common among COVID-19 survivors. The motivation behind this research is the limited availability of the post-COVID-19 dataset. In the current research, data related to post-COVID complications are collected by personally contacting the previously infected COVID-19 patients. The dataset is preprocessed to deal with missing values followed by oversampling to generate numerous instances, and model training. A binary classifier based on a stacking ensemble is modeled with deep neural networks for the prediction of heart diseases, post-COVID-19 infection. The proposed model is validated against other baseline techniques, such as decision trees, random forest, support vector machines, and artificial neural networks. Results show that the proposed technique outperforms other baseline techniques and achieves the highest accuracy of 93.23%. Moreover, the results of specificity (95.74%), precision (95.24%), and recall (92.05%) also prove the utility of the adopted approach in comparison to other techniques for the prediction of heart diseases.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11554-022-01242-y,en,M-YOLO: an object detector based on global context information for infrared images,"['OriginalPaper', 'Original Research Paper']","Object detection is an important task in computer vision. While visible (VS) images are adequate for detecting objects in most scenarios, infrared (IR) images can extend the capabilities of object detection to night-time or occluded objects. For IR images, we proposes an infrared object detector based on global context information. Combined with the lightweight network (MobileNetV2) to extract features, therefore the detector is named M-YOLO. Then, dedicated to enhancing the global information perception capability of the model, this paper proposes a global contextual information aggregation model. To preserve multi-scale information and enhance expressiveness of features, a top-down and bottom-up parallel feature fusion method is proposed. Only two detection heads are used to implement a lightweight model, which improves detection accuracy and speed. We use the self-built IR dataset (GIR) and the public IR dataset (FLIR) to verify the superiority of the model. Compared with YOLOv4 (78.1%), the average accuracy of M-YOLO (83.4%) is improved by 5.3% on the FLIR dataset. The detection time (4.33 ms) is less, with a detection speed of 30.6 FPS. On the GIR dataset, the detection accuracy (76.1%) is 6.4% higher than that of YOLOv4 (69.7%), and the detection time (6.84 ms) is lower. Our method improves the performance of IR object detection. The method is able to detect IR ground targets in complex environments, and the detection speed meets the real-time requirements.","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s41019-022-00199-z,en,Joint Attention Networks with Inherent and Contextual Preference-Awareness for Successive POI Recommendation,"['OriginalPaper', 'Research Paper']","Nowadays recording and sharing personal lives using mobile devices on the Internet is becoming increasingly popular, and successive POI recommendation is gaining growing attention from academia and industry. In mobile scenarios, multiple influencing factors including the diversity of user preferences, the changeability of user behavior and the dynamic of spatiotemporal context bring great challenges to the POI recommender system. In order to accurately capture both the stable and the contextual preferences of mobile users in dynamic contexts, we propose a fusion framework JANICP (Joint Attention Networks with Inherent and Contextual Preferences) for successive POI recommendation by jointly training an offline/nearline user inherent interest perception model and an online user contextual interest prediction model. The offline model is trained based on the global historical behavior data to achieve stable interest representation, while the online model is trained based on the instantly selected context-sensitive data to achieve dynamic interest perception. An attention aggregation and matching module is used to fully connect the two kinds of preference representations and generate the final POI recommendation. Extensive experiments were conducted on three real datasets and experimental results show that the proposed JANICP outperforms existing state-of-the-art methods.","['Computer Science', 'Database Management', 'Data Mining and Knowledge Discovery', 'Algorithm Analysis and Problem Complexity', 'Systems and Data Security', 'Artificial Intelligence', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences']"
doi:10.1007/s00530-021-00771-z,en,Deep learning based cyber bullying early detection using distributed denial of service flow,"['OriginalPaper', 'Special Issue Paper']","Cyber-bullying has been on the rise especially after the explosive widespread of various cyber-attacks. Various types of techniques have been used to tackle cyber-bullying. These techniques focused primarily on data traffic for monitoring malicious activities. This research proposes a methodology where we can detect early Denial of service (DoS) and Distributed Denial of Service (DDoS) attacks. First, we formulate the problem in a practical scenario by comparing flow and non-flow-based datasets using Mann Whitney U statistical test . Flow and non-flow-based datasets and Artificial Neural Network (ANN) and Support Vector Machine (SVM) is used for classification. To keep original features, we use variance, correlation, ¾ quartile method to eliminate the unimportant features. The forward selection wrapper method for feature selection is used to find out the best features. To validate the proposed methodology, we take multiple DoS and DDoS single flow and validate it on 10%, 20%, 30%, 40%, and 50%. For validation, the experimental results show + 90% accuracy on the early 10% flow.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s11042-022-13497-5,en,HandGCNN model for gesture recognition based voice assistance,"['OriginalPaper', '1220: Visual and Sensory Data Processing for Real Time Intelligent Surveillance System']","Communication plays an important role in today’s world. Before the evolution of the verbal communication, sign language was the only way of communication used by our ancestors. Later on, the verbal communication started evolving and different people from different region started to speak different languages. But there are some groups of people who cannot express themselves with verbal language; instead they use sign language to communicate. To bridge the gap between those people who use sign language for communication with those who use verbal language, a system is designed that recognizes the gestures of the sign language, interprets it and converts it into verbal language. Various researches have been carried out by capturing the hand signs of the speech impaired people through sensors like leap motion sensors and camera. This research works focusses on improving the gesture capturing through camera and process them through deep learning models. This work focussed on creating a hand gesture dataset “HandG” that includes 20,600 images for 10 classes (2060 images per category) using digital camera and image augmentation. A novel Convolution Neural Networks (CNN) based model, termed as “HandGCNN”, is proposed achieving a high prediction accuracy of 99.13%. A real-time system with webcam being the input receptor unit is built which recognises the signal and generates the audio relevant to that. The generated audio will serve as voice assistance for impaired people.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11517-022-02693-y,en,Impact of repetitive transcranial magnetic stimulation on the directed connectivity of autism EEG signals: a pilot study,"['OriginalPaper', 'Original Article']","To compare the differences in directed connectivity between typically developing (TD) and autism spectrum disorder (ASD) children and identify the potential effects of repetitive transcranial magnetic stimulation (rTMS) on brain connectivity and behavior of children with ASD; 26 TD children (18 males/8 females; the average age was 6.34 ± 0.45) and 30 ASD children (21 males/9 females; the average age was 6.42 ± 0.17) participated in the experiment. ASD children were divided randomly into an experimental group and a control group. The experimental group received 18 rTMS sessions (twice a week for nine weeks), whereas the control group received the same procedures with sham stimulation. Directed transfer function (DTF) was used to calculate the effective connectivity as a way of investigating differences between ASD and TD children while simultaneously evaluating the effectiveness of rTMS for ASD. The results illustrate that the DTF of TD children in the frontal lobe (Fp1, Fp2, F7, F8) and temporal lobe (T7, T8) is higher than that of ASD children in all frequency bands; however, the DTF of ASD children is higher than TD in the midline (Fz, Cz), central lobe (C3, C4), and parietal lobe (P3, P4). In the experimental group of ASD children, the effective connectivity decreased from O1 to T7 and from P7 to Fp1 in the alpha band and from Pz to T8 in the gamma band after stimulation. Significant changes in Autism Behavior Checklist (ABC) scores were also found in social behaviors. Effective connectivity derived from DTF distinguishes ASD from TD children. rTMS provides changes in connectivity and behavior, suggesting its potential use as a viable treatment option for ASD individuals. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s10462-022-10342-x,en,Generative Adversarial Networks based on optimal transport: a survey,OriginalPaper,"Optimal transport theory provides a distance to find the cheapest way to convey an object from one place to another, based on a certain cost. Optimal transport thus defines a set of geometric tools with interesting properties in terms of coupling and correspondence between probability distributions. Recent theoretical and algorithmic advances in this theory generate interesting methods for data science. Bearing this in mind, Wasserstein Generative Adversarial Networks (WGAN) make it possible to generate complex data with a high degree of realism in addition to real data which may be limited in certain contexts where their accessibility is restricted. This paper presents a literature review of recent developments in optimal transport-based data science in some practical and theoretical contexts, for solving machine learning problems. In the theoretical developments, we will appreciate the extension of WGANs coupled with conditions, autoencoders, and transfer learning. We made a critical evaluation of prevalent WGANs by synthesizing and comparing information between them to improve understanding of their respective impact. The practical context shows prominent applications in the fields of industry, health, and safety. Finally, challenges are discussed, and the conclusion presents the benefits of WGAN and prospective analyses.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11219-022-09587-0,en,Transferability of machine learning models learned from public intrusion detection datasets: the CICIDS2017 case study,OriginalPaper,"Intrusion detection is a primary concern in any modern computer system due to the ever-growing number of intrusions. Machine learning represents an effective solution to detect and prevent network intrusions. Many existing intrusion detection approaches capitalize on machine learning models learned on the top of individual public datasets and achieve detection accuracy close to 1. These highly performing detectors strongly depend on the training data, which may not be representative of real-life production environments. This paper aims to explore this proposition in the context of denial of service attacks. Different intrusion detectors learned on the top of CICIDS2017 (an established public dataset widely used as a benchmark) are tested against an unseen, although closely related, dataset. The test dataset is based on the same mixture of denial of service attacks in CICIDS2017 and some additional variants. The results indicate that the perfect detection figures obtained in the context of a public dataset may not transfer in practice.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Programming Languages, Compilers, Interpreters', 'Data Structures and Information Theory', 'Operating Systems']"
doi:10.1007/s10589-022-00404-9,en,Modeling design and control problems involving neural network surrogates,OriginalPaper,"We consider nonlinear optimization problems that involve surrogate models represented by neural networks. We demonstrate first how to directly embed neural network evaluation into optimization models, highlight a difficulty with this approach that can prevent convergence, and then characterize stationarity of such models. We then present two alternative formulations of these problems in the specific case of feedforward neural networks with ReLU activation: as a mixed-integer optimization problem and as a mathematical program with complementarity constraints. For the latter formulation we prove that stationarity at a point for this problem corresponds to stationarity of the embedded formulation. Each of these formulations may be solved with state-of-the-art optimization methods, and we show how to obtain good initial feasible solutions for these methods. We compare our formulations on three practical applications arising in the design and control of combustion engines, in the generation of adversarial attacks on classifier networks, and in the determination of optimal flows in an oil well network.","['Mathematics', 'Optimization', 'Operations Research, Management Science', 'Operations Research/Decision Theory', 'Statistics, general', 'Convex and Discrete Geometry']"
doi:10.1007/s40747-021-00478-8,en,An enhanced group teaching optimization algorithm for multi-product disassembly line balancing problems,"['OriginalPaper', 'Original Article']","Big data have been widely studied by numerous scholars and enterprises due to its great power in making highly reliable decisions for various complex systems. Remanufacturing systems have recently received much attention, because they play significant roles in end-of-life product recovery, environment protection and resource conservation. Disassembly is treated as a critical step in remanufacturing systems. In practice, it is difficult to know the accurate data of end-of-life products such as disassembly time because of their various usage processes, leading to the great difficulty of making effective and reliable decisions. Thus, it is necessary to model the disassembly process with stochastic programming method where the past collected data are fitted into stochastic distributions of parameters by applying big data technology. Additionally, designing and applying highly efficient intelligent optimization algorithms to handle a variety of complex problems in the disassembly process are urgently needed. To achieve the global optimization of disassembling multiple products simultaneously, this work studies a stochastic multi-product disassembly line balancing problem with maximal disassembly profit while meeting disassembly time requirements. Moreover, a chance-constrained programming model is correspondingly formulated, and then, an enhanced group teaching optimization algorithm incorporating a stochastic simulation method is developed by considering this model’s features. Via performing simulation experiments on real-life cases and comparing it with five popularly known approaches, we verify the excellent performance of the designed method in solving the studied problem.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11042-021-11521-8,en,Denoising of brain magnetic resonance images using a MDB network,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","The denoising of brain magnetic resonance images could be important for the medical image analysis. Many algorithms have been proposed for this task, especially the deep learning ones which show great success compared with the classical image processing algorithms. Though satisfied results it achieve, they may fail to consider the contextual and attentive features during the feature learning process, and those ones could provide essential and complementary information for the feature encoding, and the poor learning of them could hinder the model to achieve a better performance. To address this challenge, in this paper, we propose a multi-dilated block (MDB) which aims to extract more contextual and attentive features during the feature extraction stage. The whole network is based on DnCNN, and the MDB is placed in the middle stage of the network to learn the contextual and attentive representations. Moreover, for improving the similarity between the noisy image and the denoised one from feature-level, we propose a perceptual loss which is able to further boost the performance of the MDB network. To validate the effectiveness of our proposed method, we conduct extensive experiments on the brain magnetic resonance images to compare the peak signal to noise ratio and structural similarity index, and the final experimental results demonstrate that our propose method could predict a higher resolution image compared with other ones.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-07551-4,en,A Vision-based inventory method for stacked goods in stereoscopic warehouse,"['OriginalPaper', 'Original Article']","Inventory of stacked goods in the stereoscopic warehouse is important for modern logistics. Currently, this inventory task is completed by counting manually. With the advance of industry 4.0 and deep learning technology, automatic inventory based on machine vision comes true, greatly saving labor and material costs. In this work, we firstly collected WSGID, an image dataset about wine boxes stacked in a stereoscopic winey warehouse. Moreover, we presented an automatic inventory method based on machine vision, consisting of a stacked goods surface detecting model and a prior-based quantity calculating algorithm. To get a better detecting performance, we introduced STCNet, an improved detection network based on Swin Transformer. The final results of 86.7 mAP, 82.8 mAP, and 85.9 mAP on three sub-datasets are achieved and are higher than the baselines. To count the quantity of goods after detection, we proposed an adaptive and robust calculating algorithm. Our method got an accuracy of 85.71 on the largest sub-dataset. Extensive experiments on the WSGID and COCO benchmark demonstrate the effectiveness of our approach. Our work indicates that the machine vision method successfully facilitates inventory for stacked goods in the stereoscopic warehouse.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11042-022-14210-2,en,Dual UNet low-light image enhancement network based on attention mechanism,OriginalPaper,"Low-light image enhancement has been an important research direction in the field of image processing. Recently, U-Net networks have shown better promise in low-light image enhancement. However, because of the semantic gap and the lack of connection between global contextual information in the U-shaped network, it leads to problems such as inaccurate color information in the enhanced images. To address the above problems, this paper proposes a Dual UNet low-light image enhancement network (DUAMNet) based on an attention mechanism. Firstly, the local texture features of the original image are extracted using the Local Binary Pattern(LBP) operator, and the illumination invariance of the LBP operator better maintains the texture information of the original image. Next, use the Brightness Enhancement Module(BEM). In the BEM module, the outer U-Net network captures feature information at different levels and luminance information of different regions, and the inner densely connected U-Net++ network enhances the correlation of feature information at different levels, mines more hidden feature information extracted by the encoder, and reduces the feature semantic gap between the encoder and decoder. The attention module Convolutional Block Attention Module(CBAM) is introduced in the decoder of U-Net++ network. CBAM further enhances the ability to model the global contextual information linkage and effectively improves the network’s attention to the weak light region. The network adopts a progressive recursive structure. The entire network includes four recursive units, and the output of the previous recursive unit is used as the input of the next recursive unit. Comparative experiments are conducted on seven public datasets, and the results are analyzed quantitatively and qualitatively. The results show that despite the simple structure of the network in this paper, the network in this paper outperforms other methods in image quality compared to other methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1038/s41592-022-01675-0,en,A large-scale neural network training framework for generalized estimation of single-trial population dynamics,"['OriginalPaper', 'Brief Communication']","Achieving state-of-the-art performance with deep neural population dynamics models requires extensive hyperparameter tuning for each dataset. AutoLFADS is a model-tuning framework that automatically produces high-performing autoencoding models on data from a variety of brain areas and tasks, without behavioral or task information. We demonstrate its broad applicability on several rhesus macaque datasets: from motor cortex during free-paced reaching, somatosensory cortex during reaching with perturbations, and dorsomedial frontal cortex during a cognitive timing task. AutoLFADS models neural population activity via a deep learning-based approach with automated hyperparameter optimization.","['Life Sciences', 'Life Sciences, general', 'Biological Techniques', 'Biological Microscopy', 'Biomedical Engineering/Biotechnology', 'Bioinformatics', 'Proteomics']"
doi:10.1007/s11269-022-03346-3,en,Improved Convolutional Neural Network and its Application in Non-Periodical Runoff Prediction,OriginalPaper,"Due to the influence of human regulation and storage factors, the runoff series monitored at the hydro-power stations often show the characteristics of non-periodicity which increases the difficulty of forecasting. The prediction model based on the neural network can avoid the interference of the non-periodicity by focusing on the relationship between rainfall input and runoff output. However, the physical correlation of the rainfall-runoff and the complexity of the neural network still flaw the subdivision research. In this paper, an improved convolutional neural network (CNN) was innovatively constructed to model runoff prediction, which contains effective layers design and adaptive activation function. The long-term and irregular observation data collected by the Zhexi reservoir were used for training and validation. In addition, the models based on traditional artificial neural networks and ordinary CNN were applied to the forecast simulation for contrast. Evaluation results using real data indicated that the improved CNN model performs better in these acyclic series, with over 0.9 correlation coefficient values and under 185 root means square error values during the validation, meanwhile averting the gradient vanishing and negative discharge problems occurring in other models. Numerous indicators and plots prove the excellent effect and reliability of the model forecast. Considering the robustness and validity of the neural network, this research and verification are of significance to non-periodic reservoir inflow prediction.","['Earth Sciences', 'Hydrogeology', 'Hydrology/Water Resources', 'Geotechnical Engineering & Applied Earth Sciences', 'Atmospheric Sciences', 'Civil Engineering', 'Environment, general']"
doi:10.1007/s10182-021-00421-9,en,Scoring predictions at extreme quantiles,"['OriginalPaper', 'Original Paper']","Prediction of quantiles at extreme tails is of interest in numerous applications. Extreme value modelling provides various competing predictors for this point prediction problem. A common method of assessment of a set of competing predictors is to evaluate their predictive performance in a given situation. However, due to the extreme nature of this inference problem, it can be possible that the predicted quantiles are not seen in the historical records, particularly when the sample size is small. This situation poses a problem to the validation of the prediction with its realization. In this article, we propose two non-parametric scoring approaches to assess extreme quantile prediction mechanisms. The proposed assessment methods are based on predicting a sequence of equally extreme quantiles on different parts of the data. We then use the quantile scoring function to evaluate the competing predictors. The performance of the scoring methods is compared with the conventional scoring method and the superiority of the former methods are demonstrated in a simulation study. The methods are then applied to analyze cyber Netflow data from Los Alamos National Laboratory and daily precipitation data at a station in California available from Global Historical Climatology Network.","['Statistics', 'Statistics, general', 'Statistics for Business, Management, Economics, Finance, Insurance', 'Probability Theory and Stochastic Processes', 'Econometrics']"
doi:10.1007/s11518-022-5545-5,en,A Hybrid Evolutionary Under-sampling Method for Handling the Class Imbalance Problem with Overlap in Credit Classification,OriginalPaper,"Credit risk assessment is an important task of risk management for financial institutions. Machine learning-based approaches have made promising progress in credit risk assessment by treating it as imbalanced binary classification tasks. However, few efforts have been made to deal with the class overlap problem that accompanies imbalances simultaneously. To this end, this study proposes a Tomek link and genetic algorithm (GA)-based under-sampling framework (TEUS) to address the class imbalance and overlap issues in binary credit classification by eliminating majority class instances with considering multi-perspective factors. TEUS first determines boundary majority instances with Tomek link, then take the distance from each majority instance to its nearest boundary as the radius and assigns the density of opposite class samples within the radius as the overlap potential of that majority instance. Second, TEUS weighs each non-borderline majority instance based on its information contribution in estimating class labels. After partitioning non-borderline majority instances into subgroups according to overlap potential and information contribution, TEUS applies GA to select samples from subgroups and merge them with the minority samples into a new training set. Innovatively, the design of the fitness function in GA and the grouping of the non-borderline majority not only trade off the multi-perspective characteristics of instances but also help reduce the computational complexity of the sampling optimization search. Numerical experiments on real-world credit data sets demonstrate the effectiveness of the proposed TEUS.","['Engineering', 'Complexity', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Operations Research/Decision Theory']"
doi:10.1007/s10489-022-03279-9,en,A complete framework for aspect-level and sentence-level sentiment analysis,OriginalPaper,"Aspect-Based Sentiment Analysis (ABSA) and Sentence-Based Sentiment Analysis (SBSA) stand for two highly coupled study fields. Basically, the features required at the sentence level influence and depend on the aspect level and vice versa. Nevertheless, a few approaches have considered the correlation between these two tasks. This research work is interested in both aspect and sentence levels. It starts with the ABSA which is in turn divided into two strongly coupled tasks, namely the aspect extraction and the aspect sentiment classification. Indeed, integrating highly coupled tasks into an integrated model can lead to more significant performance improvement rather than in the case of separate models, which is also confirmed through the proposed ABSA model. The latter represents a unified-trained model based on deep learning techniques for extracting the aspects along with their sentiment polarities. Later on, the emphasis would be put on SBSA, which is a complex study, especially with the existence of opinions that include several aspects with opposing polarities. From this perspective, a combination of deep learning and fuzzy logic techniques was elaborated to address this issue. The hybrid model achieved satisfactory performance compared to the Bert model.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10115-022-01796-0,en,A new method of ensemble learning:  case of cryptocurrency price prediction,"['OriginalPaper', 'Regular Paper']","This work proposes a novel method of ensemble learning for time series prediction. Different machine learning-based models have been integrated, and a combined prediction model has been created. The objective of the ensemble model is that it must outperform all other individual models that are used to construct the ensemble model in terms of producing excellent predictions. The field of cryptocurrencies has been selected as the domain of this work where the focus is to predict the cryptocurrency prices using the proposed model. A new regression model is proposed and implemented in this work. Different machine learning techniques have been adopted and integrated to form a combined prediction model. The machine learning models include deep neural networks, support vector regression, and decision trees. The regression scheme has to be implemented on each machine learning model separately as well as their performance is also to be improved. The combined prediction model requires optimal weights generation for integration, and therefore, time complexity is a concern. A large set of experiments have been carried out on various cryptocurrencies and the results are displayed. Real-world data has been used here and a comparison is also performed. It is observed that the combined prediction model outperforms other models resulting in excellent predictions capturing most of the nonstationary movements in the data.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s10863-022-09947-2,en,"Synthesis, biological evaluation and molecular modeling studies of novel 1,2,3-triazole-linked menadione-furan derivatives as P2X7 inhibitors",OriginalPaper,"The P2X7 receptor (P2X7R) is an ion channel that promotes the passage of ions through the membrane through brief stimulation once activated by ATP, its endogenous opener. However, prolonged stimulation with ATP, which occurs in pathological processes, opens a nonselective pore in the plasma membrane, allowing the passage of large molecules and leading to cytokine release or even cell death. In this sense, the search for new inhibitors for this receptor has attracted a great deal of attention in recent years. Considering the booming of biomass upgrading reactions in recent years and the continued efforts to synthesize biologically active molecules containing the 1,2,3-triazole ring, in the present work, we aimed to investigate whether triazole-linked menadione-furan derivatives could present P2X7R inhibitory activity. The novel compounds were tested for their inhibitory activity on ATP-induced dye uptake in peritoneal macrophages. Some have shown promising results, having displayed IC 50 values lower than that of the P2X7R inhibitor BBG. Molecular docking studies also indicated that the active compounds bind to an allosteric site on P2X7R, presenting potential P2X7R inhibition.","['Chemistry', 'Bioorganic Chemistry', 'Biochemistry, general', 'Animal Anatomy / Morphology / Histology', 'Animal Biochemistry', 'Organic Chemistry']"
doi:10.1007/s11263-022-01673-x,en,OASIS: Only Adversarial Supervision for Semantic Image Synthesis,OriginalPaper,"Despite their recent successes, generative adversarial networks (GANs) for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Previously, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limited the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity and with a better alignment to their input label maps, making the use of the perceptual loss superfluous. Furthermore, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image editing. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve a strong improvement in image synthesis quality over prior state-of-the-art models across the commonly used ADE20K, Cityscapes, and COCO-Stuff datasets using only adversarial supervision. In addition, we investigate semantic image synthesis under severe class imbalance and sparse annotations, which are common aspects in practical applications but were overlooked in prior works. To this end, we evaluate our model on LVIS, a dataset originally introduced for long-tailed object recognition. We thereby demonstrate high performance of our model in the sparse and unbalanced data regimes, achieved by means of the proposed 3D noise and the ability of our discriminator to balance class contributions directly in the loss function. Our code and pretrained models are available at https://github.com/boschresearch/OASIS .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11590-022-01901-w,en,Individual and cooperative portfolio optimization as linear program,"['OriginalPaper', 'Original Paper']","We derive a linear program for minimization, subject to a linear constraint, of an arbitrary positively homogeneous convex functional, whose dual set is given by linear inequalities, possibly involving auxiliary variables. This allows to reduce to linear programming individual and cooperative portfolio optimization problems with arbitrary deviation measures whose risk envelopes are given by a finite number of linear constraints. Earlier, such linear programs were known only for individual porfolio optimization problems with special examples of deviation measures, such as mean absolute deviation or CVaR deviation.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Computational Intelligence', 'Numerical and Computational Physics, Simulation']"
doi:10.1007/s00521-022-07681-9,en,CNN-LSTM and clustering-based spatial–temporal demand forecasting for on-demand ride services,"['OriginalPaper', 'Original Article']","Passenger demand forecasting is of great importance to the on-demand ride systems. With the accurate forecasting of demand, it can be determined from which regions and when the passengers demand a vehicle. In this way, passenger and vehicle waiting times, fuel costs of vehicles can be reduced. In the literature, various models such as time series, long short-term memory (LSTM), convolutional neural network (CNN), and hybrid of CNN-LSTM are used for demand forecasting in on-demand ride service systems. These models forecast demands by considering temporal and spatial data separately or together. In models that use spatial and spatial–temporal data, generally, the city is divided into zones in the form of a grid. This partitioning method has some disadvantages, such as misleading the forecasting accuracy by considering regions without demand and ignoring the geographical conditions. In this study, two new models, ConvLSTM2D-clustering and CNN-LSTM-clustering are proposed to overcome these disadvantages and make more accurate and robust forecasts. The proposed models use clustering instead of grid partitioning in dividing the city into zones and take time-of-day, time-of-week variables into account in forecasting as well as passenger demand. The presented models have been used in the passenger demand forecasting of Turkcell Technology Company, which provides on-demand ride services for its employees in Istanbul, Turkey. Experimental results, validated on real-world data provided by Turkcell, show that the proposed models partition the city more effectively and achieve 14–55% better short- and long-term forecasting performances than the compared models in terms of mean squared error.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07561-2,en,Leveraging greater relations for improving multi-choice reading comprehension,"['OriginalPaper', 'Original Article']","Remarkable success has been achieved in the last few years on machine reading comprehension tasks. In previous works, long-range dependencies were captured by explicitly attending to all the tokens and modeling the relations between the question and each sentence. However, a great deal of important information regarding token-level and sentence-level relations in the passage, which are useful to infer the answer, were ignored in these works. We observed that the contextual information between the token-level and sentence-level in the same passage plays a vital role in reading comprehension tasks. To address this problem, we proposed a multi-stage maximization attention (MMA) network, which is used to capture the important relations in the passage from different levels of granularity at its hierarchical nature. By utilizing MMA as a module, we integrated two sentence-level question-aware matching mechanisms to infer the answer: (1) Co-matching is used to match the passage with the question and the candidate answer. (2) Sentence-level hierarchical attention is used to identify the importance of sentences conditioned on the question and the option. In addition, inspired by how humans solve multi-choice reading comprehension questions, the passage sentence selection strategy is fused into our model to select the most salient sentences to guide the model to infer the answer. The proposed model is evaluated on three multi-choice reading comprehension datasets RACE, Dream and MultiRC. Significance tests demonstrated the improvement of existing MRC models. A series of analyses were also conducted to interpret the effectiveness of the proposed model.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10614-021-10144-3,en,Early Warning of Chinese Yuan’s Exchange Rate Fluctuation and Value at Risk Measure Using Neural Network Joint Optimization Algorithm,OriginalPaper,"The aims are to analyze the fluctuation forecast of exchange rate markets, including the Chinese Yuan (CNY), and discuss applying the neural network model in the Value at Risk (VaR) measure. Therefore, six exchange rate markets are selected as the research objects, with the CNY exchange rate market as the main body, to analyze and explain the exchange rate fluctuation risks. Second, based on the overview of the neural network model, the Deep Belief Network (DBN), Multilayer Perceptron (MLP), and Long Short-Term Memory Network (LSTM) are introduced, and a VaR method based on risk measurement is proposed. Finally, based on the number of excess days (Exc) and the Kupiec test, the VaR measure results under different models are analyzed. Results demonstrate that the CNY exchange rate market’s historical data are relatively concentrated, with minor fluctuations, and the overall change is a sharp right shift. Compared with the benchmark model Generalized AutoRegressive Conditional Heteroskedasticity, the three neural network models show excellent risk measurement performance for different exchange rate markets. Based on Exc, the DBN model has the optimal risk forecast performance. In the CNY exchange rate market, the Exc values corresponding to the DBN and LSTM models are small, and the forecast performance is fair. Based on the Kupiec test, in addition to the Great Britain Pound exchange rate market, the three neural network models perform well in measuring the risks of the other five exchange rate markets. Besides, the MLP model has the optimal performance in measuring the CNY risks. Hence, the neural network models have excellent applicability in measuring the risks of exchange rate markets.","['Economics', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Computer Appl. in Social and Behavioral Sciences', 'Operations Research/Decision Theory', 'Behavioral/Experimental Economics', 'Math Applications in Computer Science']"
doi:10.1007/s11063-022-11093-0,en,Using Cartesian Genetic Programming Approach with New Crossover Technique to Design Convolutional Neural Networks,OriginalPaper,"In image classification problems, Convolutional Neural Networks (CNNs) are deep neural networks that include a variety of different layers aimed at classifying images. Until today, the most promising and state-of-the-art method in image recognition tasks is CNN. Tuning the deep network with a large number of hyperparameters to maximize performance would be an excruciating task that requires lots of time and engineering efforts. To construct that high-performance architecture, experts should go through a lot of trial and error. Neural Architecture Search is a way to automatically fabricate an accurate network architecture. An evolutionary algorithm called Cartesian Genetic Programming (CGP) with a new crossover operation based on the multiple Sequence Alignment algorithm is proposed in this paper to construct an appropriate neural network without the burden of building manually. This new method has a remarkable improvement over a standard CGP only by adding a crossover operator. The datasets for training on the proposed method were CIFAR-10 and CIFAR-100. The results show that it achieves a good balance between accuracy and the number of trainable parameters compared to the other state-of-the-art methods.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s00521-022-07642-2,en,Assessing vascular complexity of PAOD patients by deep learning-based segmentation and fractal dimension,"['OriginalPaper', 'Original Article']","The assessment of vascular complexity in the lower limbs provides relevant information about peripheral artery occlusive diseases (PAOD), thus fostering improvements both in therapeutic decisions and prognostic estimation. The current clinical practice consists of visually inspecting and evaluating cine-angiograms of the interested region, which is largely operator-dependent. We present here an automatic method for segmenting the vessel tree and compute a quantitative measure, in terms of fractal dimension (FD), of the vascular complexity. The proposed workflow consists of three main steps: ( i ) conversion of the cine-angiographies to single static images with a broader field of view, ( ii ) automatic segmentation of the vascular trees, and ( iii ) calculation and assessment of FD as complexity index. In particular, this work defines (1) a method to reduce the inter-observer variability in judging vascular complexity in cine-angiography images from patients affected by peripheral artery occlusive disease (PAOD), and (2) the use of Fractal Dimension as a metric of shape complexity of vascular tree. The inter-class correlation coefficient ( ICC ) is computed as inter-observer agreement metric and to account for possible systematic error, that depends on the experience of the raters. The automatic segmentation of vascular tree achieved an Area Under the Curve mean value of $$0.77~\pm ~0.07$$ 0.77 ± 0.07 , with a min-max range of $$0.57-0.87$$ 0.57 - 0.87 . Absolute operator agreement was higher over the segmented image ( $$ICC=0.96$$ I C C = 0.96 ) compared to the video ( $$ICC=0.76$$ I C C = 0.76 ) and the a broader field of view image ( $$ICC=0.92$$ I C C = 0.92 ). Fractal Dimension computed on both manual segmented images (ground truths) and automatically showed a good correlation with the clinical score (0.85 and 0.75, respectively). Experimental analyses suggest that extracting the vascular tree from cine-angiography can substantially improve the reliability of visual assessment of vascular complexity in PAOD. Results also reveal the effectiveness of FD in evaluating complex vascular tree structures.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07719-y,en,Boundary regression-based reep neural network for thyroid nodule segmentation in ultrasound images,"['OriginalPaper', 'Original Article']","Due to the noises captured in ultrasound device and image reconstruction process, the edges of thyroid nodule are usually not distinctive and it is very difficult for existing approaches to well segment them in ultrasound images. While deep neural networks like U-Net have been successfully applied in many medical image segmentation tasks, their segmentation performances on ultrasound images are still not satisfactory. To address this issue, we propose in this paper a boundary field regression branch to provide useful boundary information to help improve the segmentation performance of existing networks. Without requirement of additional labeling costs, our approach firstly generates boundary field heatmap from available segmentation masks, which are then used as a supervision to train the regression branch. As a general architecture, our branch can be integrated with all encoder-decoder like segmentation networks. A dataset consisting of 3169 images from 2004 patients is used for experiments. We integrate our branch with U-Net, Attention U-Net, U-Net++ and DeepLabv3+; consistent improvements of Dice metrics were observed. The memory and computation costs required by adding our branch are marginal as well.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00799-022-00337-y,en,VIVA: visual information retrieval in video archives,OriginalPaper,"Video retrieval methods, e.g., for visual concept classification, person recognition, and similarity search, are essential to perform fine-grained semantic search in large video archives. However, such retrieval methods often have to be adapted to the users’ changing search requirements: which concepts or persons are frequently searched for, what research topics are currently important or will be relevant in the future? In this paper, we present VIVA, a software tool for building content-based video retrieval methods based on deep learning models. VIVA allows non-expert users to conduct visual information retrieval for concepts and persons in video archives and to add new people or concepts to the underlying deep learning models as new requirements arise. For this purpose, VIVA provides a novel semi-automatic data acquisition workflow including a web crawler, image similarity search, as well as review and user feedback components to reduce the time-consuming manual effort for collecting training samples. We present experimental retrieval results using VIVA for four use cases in the context of a historical video collection of the German Broadcasting Archive based on about 34,000 h of television recordings from the former German Democratic Republic (GDR). We evaluate the performance of deep learning models built using VIVA for 91 GDR specific concepts and 98 personalities from the former GDR as well as the performance of the image and person similarity search approaches.","['Computer Science', 'Database Management', 'Information Systems and Communication Service']"
doi:10.1007/s11265-022-01801-3,en,TAFFNet: Two-Stage Attention-Based Feature Fusion Network for Surface Defect Detection,OriginalPaper,"It is important to detect surface defects for controlling product quality and prolonging equipment life. However, detecting surface defects quickly and accurately is still a great challenge due to the complexity of the environment and surface defects. Aiming at the issue, this paper proposes a two-stage attention-based feature fusion network (TAFFNet) to make full use of each level feature for surface defect segmentation. Specifically, the network uses Resnet50 as the backbone network to obtain features, and then extracts multi-scale feature by the atrous convolution feature extraction module. In order to make the features at all levels contain more defect information, the attention-based adjacent feature fusion module is applied to fuse features with adjacent layers; then use the attention-based high-level feature fusion module to merge features with all upper layers, so all level features not only contain multi-scale context but also obtain more defect details. Finally, all features are cascaded together to achieve accurate segmentation of surface defects. In addition, TAFFNet uses a hybrid loss function to overcome blurry boundaries. The experimental results on three surface defects datasets (SD900, MT, and CFD) show that the proposed network outperforms other 13 methods in terms of PR curve, F 1 , MAE, and mIoU.","['Engineering', 'Signal,Image and Speech Processing', 'Circuits and Systems', 'Electrical Engineering', 'Image Processing and Computer Vision', 'Pattern Recognition', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s10589-022-00402-x,en,A stabilized sequential quadratic semidefinite programming method for degenerate nonlinear semidefinite programs,OriginalPaper,"In this paper, we propose a new sequential quadratic semidefinite programming (SQSDP) method for solving degenerate nonlinear semidefinite programs (NSDPs), in which we produce iteration points by solving a sequence of stabilized quadratic semidefinite programming (QSDP) subproblems, which we derive from the minimax problem associated with the NSDP. Unlike the existing SQSDP methods, the proposed one allows us to solve those QSDP subproblems inexactly, and each QSDP is feasible. One more remarkable point of the proposed method is that constraint qualifications or boundedness of Lagrange multiplier sequences are not required in the global convergence analysis. Specifically, without assuming such conditions, we prove the global convergence to a point satisfying any of the following: the stationary conditions for the feasibility problem, the approximate-Karush–Kuhn–Tucker (AKKT) conditions, and the trace-AKKT conditions. Finally, we conduct some numerical experiments to examine the efficiency of the proposed method.","['Mathematics', 'Optimization', 'Operations Research, Management Science', 'Operations Research/Decision Theory', 'Statistics, general', 'Convex and Discrete Geometry']"
doi:10.1007/s11227-022-04646-6,en,Particle swarm optimization-based empirical mode decomposition predictive technique for nonstationary data,OriginalPaper,"Real-world nonstationary data are usually characterized by high nonlinearity and complex patterns due to the effects of different exogenous factors that make prediction a very challenging task. An ensemble strategically combines multiple techniques and tends to be robust and more precise compared to a single intelligent algorithmic model. In this work, a dynamic particle swarm optimization-based empirical mode decomposition ensemble is proposed for nonstationary data prediction. The proposed ensemble implements an environmental change detection technique to capture concept drift occurring and the intrinsic nonlinearity in time series, hence improving prediction accuracy. The proposed ensemble technique was experimentally evaluated on electric time series datasets. The obtained results show that the proposed technique improves prediction accuracy and it outperformed several state-of-the-art techniques in several cases. For future work direction, a detailed empirical analysis of the proposed technique can be considered such as the effect of the cost of prediction errors, and the technique's search capability.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s00521-022-07496-8,en,Gaussian-based probability fusion for person re-identification with Taylor angular margin loss,"['OriginalPaper', 'Original Article']","Person re-identification (ReID) aims to match the specific pedestrians in the public environment with cross-domain cameras. Posture change, occlusion, and viewpoint change complicate ReID. Representation learning and metric learning have been focused on in previous researches as a key to solving the problem of ReID task by adaptive learning emphasized character feature representation. This paper proposed a Gaussian-based probability fusion mechanism and an improved Taylor Angular Margin softmax loss function, that the methods can be joined to learn more discriminative features and then better embedding space with categories weights. Different from the existing network attention intervention methods that by guided measurement quality fuse with prediction, the probability fusion structure can generate fine-grained recognition features via attention learning, and then send these features into the proposed Taylor angular margin loss. This proposed loss can push the inter-class margin and mining the intra-class variance for more effective performance. Massive experiments on Market-1501, DukeMTMC-reID, and MSMT17 show predominant performance promotion. It achieves 90.5% mAP and 96.4% Rank-1 on Market-1501, 80.5% mAP and 89.8% Rank-1 on DukeMTMC-reID 59.7% mAP and 82.5% Rank-1 on MSMT17. Moreover, excellent improvements are shown on occlusion datasets, 55.6% mAP and 65.1% Rank-1 on Occluded-DukeMTMC, 74.8% mAP and 80.5% Rank-1 on Occluded-ReID.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00784-022-04646-z,en,Artificial intelligence system for training diagnosis and differentiation with molar incisor hypomineralization (MIH) and similar pathologies,"['OriginalPaper', 'Original Article']","Objectives Molar incisor hypomineralization (MIH) is a difficult-to-diagnose developmental disorder of the teeth, mainly in children and adolescents. Due to the young age of the patients, problems typically occur with the diagnosis of MIH. The aim of the present technical note was to investigate whether a successful application of a neural network for diagnosis of MIH and other different pathologies in dentistry is still feasible. Materials and methods For this study, clinical pictures of four different pathologies were collected ( n  = 462). These pictures were categorized in caries ( n  = 118), MIH ( n  = 115), amelogenesis imperfecta ( n  = 112) and dental fluorosis ( n  = 117). The pictures were anonymized and a specialized dentist taking into account all clinical data did the diagnosis. Then, well-investigated picture classifier neural networks were selected. All of these were convolutional neural networks (ResNet34, ResNet50, AlexNet, VGG16 and DenseNet121). The neural networks were pre-trained and transfer learning was performed on the given datasets. Results For the vgg16 network, the precision is the lowest with 83.98% as for the dense121 it shows the highest values with 92.86%. Comparing the different pathologies between the investigated neural networks, there is no trend detectable. Conclusion In the long term, an implementation of artificial intelligence for the detection of specific dental pathologies is conceivable and sensible. Clinical relevance Finally, this application can be integrated in the area of training and teaching in order to teach dental students as well as general practitioners for MIH and similar dental pathologies.","['Dentistry', 'Dentistry']"
doi:10.1007/s40031-022-00785-9,en,Deep Retinal Image Analysis and Classification Using Deer Hunting Optimization-Based Tandem Pulse Coupled Neural Network,"['OriginalPaper', 'Original Contribution']","Retinal eye diseases lead to vision loss and visual deficiency. Various kinds of human eye diseases are arteriosclerosis, diabetic retinopathy, hypertension, and glaucoma. Diabetic retinopathy is the form of the injured retina which is occurred by diabetes. The improper treatment without proper observations leads to permanent blindness. Therefore, it is necessary to detect the disease at an earlier stage to protect their vision. The main aim of this research is to detect diabetic retinopathy early from the fundus images. The proposed approach detects diabetic retinopathy through four major stages, namely pre-processing, segmentation, feature extraction, and classification. The proposed classifier uses the deep neural network for classifying the diabetic retinopathy (DR) infected images and normal images. Here, the proposed approach utilizes features such as coherence, edge features, shape features, local binary pattern, and Gray Level Co-occurrence Matrix (GLCM) features from the segmented output. The performance measures like sensitivity, specificity, accuracy, precision, F1-measure and processing time are utilized for the estimation of the proposed classifier and compared to other approaches. From the performance evaluation, it is noted that the proposed diabetic retinopathy detection has obtained an accuracy of 99.42%, a sensitivity of 98.84%, and a specificity of 96.49% that outperforming other state-of-art methods.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s11042-022-13480-0,en,Flow-MotionNet: A neural network based video compression architecture,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","The growth of superfluous video content over the internet led to the emergence of highly proficient video compression techniques. These novel techniques make optimal use of the available varying bandwidths to deliver quality video content. The traditional techniques of video compression are mainly based on block designs and remove the redundancies using Discrete Cosine Transforms. Although these techniques perform well but these are not adaptive to the varying bandwidth. A number of learning based video compression schemes have been developed during previous years. Though some are performing efficiently but these are not adaptable for mobile usage because of their flexibility lack for varying reconstruction quality with varying bandwidth. In this paper, a lightweight learning-based video compression architecture has been proposed that attempts to allow variation in quality of the reconstructed video with the amount of data sent, without requiring separate low-resolution versions of the same video. The proposed model is a amalgamation of three tiny networks namely frame autoencoder, flow autoencoder and motion extension network. The performance analysis reveals a significant improvement in visual quality of the video frames but in tradeoff with frame reconstruction time. The results have also been compared to some state-of-the-art techniques including H.264 in terms of SSIM and PSNR.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00170-022-10455-1,en,Tool wear prediction based on convolutional bidirectional LSTM model with improved particle swarm optimization,"['OriginalPaper', 'ORIGINAL ARTICLE']","Accurate tool wear prediction is crucial for preventive maintenance on time. Most of the existing data-driven prediction methods still need complex feature engineering, which reduces the prediction accuracy and efficiency. To address this problem, a tool wear prediction model based on Improved Particle Swarm Optimization (IPSO) Convolutional Neural Network (CNN) and Bidirectional long short-term memory (BiLSTM) network is proposed. Firstly, the cutting force, vibration, and acoustic emission signals are taken as the input features of the model. CNN is used for high-dimensional feature extraction in the raw signal. Then, the BiLSTM with long-term memory and time-series processing ability is used to model time-series data. Besides, IPSO is employed to enhance the prediction accuracy of the hybrid model. Finally, the fully connected layer obtains the tool wear prediction results. IEEE PHM 2010 challenge data was used to illustrate and validate this model. The experimental results show that the hybrid model has an average prediction error of 7.06% on this data set, with an average absolute error (MAE) and an average root mean square error (RMSE) of 1.49 and 1.81, respectively. The prediction performance of the proposed hybrid model outperforms other related deep learning models.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s10489-022-03395-6,en,A novel intrinsic measure of data separability,OriginalPaper,"In machine learning, the performance of a classifier depends on both the classifier model and the separability/complexity of datasets. To quantitatively measure the separability of datasets, in this study, we propose an intrinsic measure – the Distance-based Separability Index (DSI), which is independent of the classifier model. We then formally show that the DSI can indicate whether the distributions of datasets are identical for any dimensionality. DSI can measure separability of datasets because we consider the situation in which different classes of data are mixed in the same distribution to be the most difficult for classifiers to separate. And, DSI is verified to be an effective separability measure by comparing it to state-of-the-art separability/complexity measures using synthetic datasets and real datasets (CIFAR-10/100). Having demonstrated the DSI’s ability to compare distributions of samples, our other studies show that it can be used in other separability-based applications, such as measuring the performance of generative adversarial networks (GANs) and evaluating the results of clustering methods.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10489-022-03451-1,en,Video summarization with u-shaped transformer,OriginalPaper,"In recent years, supervised video summarization has made tremendous progress with treating it as a sequence-to-sequence learning task. However, traditional recurrent neural networks (RNNs) have limitations in sequence modeling of long sequences, and the use of a transformer for sequence modeling requires a large number of parameters. We propose an efficient U-shaped transformer for video summarization tasks in this paper to address this issue, which we call “Uformer”. Precisely, Uformer consists of three key components: embedding, Uformer block, and prediction head. First of all, the image features sequence is represented by the pre-trained deep convolutional network, then represented by a liner embedding. The image feature sequence differences are also represented by another liner embedding and concatenate together to form a two-stream embedding feature in the embedding component. Secondly, we stack multiple transformer layers into a U-shaped block to integrate the representations learned from the previous layers. Multi-scale Uformer can not only learn longer sequence information but also reduce the number of parameters and calculations. Finally, prediction head regression the localization of the keyframes and learning the corresponding classification scores. Uformer combine with non-maximum suppression (NMS) for post-processing to get the final video summarization. We improved the F-score from 50.2% to 53.9% by 3.7% on the SumMe dataset and improved F-score from 62.1% to 63.0% by 0.9% on the TVSum dataset. Our proposed model with 0.85M parameters which are only 32.32% of DR-DSN’s parameters.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10994-022-06227-3,en,Variance reduction on general adaptive stochastic mirror descent,OriginalPaper,"In this work, we propose a simple and generalized algorithmic framework for applying variance reduction to adaptive mirror descent algorithms for faster convergence. We introduce the SVRAMD algorithm, and provide its general convergence analysis in both the nonsmooth nonconvex optimization problem and the generalized P–L conditioned nonconvex optimization problem. We prove that variance reduction can reduce the gradient complexity of all adaptive mirror descent algorithms that satisfy a mild assumption and thus accelerate their convergence. In particular, our general theory implies that variance reduction can be applied to different algorithms with their distinct choices of the proximal function, such as gradient descent with time-varying step sizes, mirror descent with $$L_1$$ L 1 mirror maps, and self-adaptive algorithms such as AdaGrad and RMSProp. Moreover, the proved convergence rates of SVRAMD recover the existing rates without complicated algorithmic components, which indicates their optimality. Extensive experiments validate our theoretical findings.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s10489-022-04351-0,en,TransG-net: transformer and graph neural network based multi-modal data fusion network for molecular properties prediction,OriginalPaper,"Molecular properties prediction is an important task in the field of materials, especially in computational drug and materials discovery. Deep learning (DL) is one of the most popular methods for molecular properties prediction due to its ability to establish quantitative relationships between molecular representations and target properties. In order to improve the performance of DL algorithms, it is crucial to select appropriate representation of molecules. Molecular graph has become one of the choices as it can be easily input into graph neural network (GNN)-based DL models for learning. However, model performance is limited if molecular representation is only used because it only contains atomic information, bond information, and adjacency relationships between atoms. Therefore, we use molecular mass spectrum as another representation to provide supplement information which is not contained in the graph data. In this paper, a transformer-based model, named Mass Spectrum Transformer (MST), is proposed to perform quantitative analysis of molecular spectra, then it is combined with the graph neural network to form a multi-modal data fusion model TransG-Net for accurate molecular properties prediction. Several feature fusion methods are adopted and the best method is chosen to further enhance the performance of the model. A multi-modal dataset is collected in this paper which is composed of molecular graph data and spectra. Data augmentation is performed to simulate the experimentally measured molecular spectra for the generalizability of the model. Experimental results show that MST outperforms previous best mass spectrum-based methods for molecular properties prediction. In addition, TransG-Net combining MST and GNN achieves better performance than state-of-the-art well-designed message passing models, which proves the effectiveness of our multi-modal data fusion method.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00366-021-01380-0,en,Load carrying capacity assessment of thin-walled foundations: an ANFIS–PNN model optimized by genetic algorithm,"['OriginalPaper', 'Original Article']","A proper and reliable estimation of bearing capacity of thin-walled foundations is of importance and necessary for accurate design of these structures. This study proposes a new hybrid intelligent technique, i.e., adaptive neuro-fuzzy inference system (ANFIS)–polynomial neural network (PNN) optimized by the genetic algorithm (GA), called ANFIS–PNN–GA, for prediction of bearing capacity of the thin-walled foundations. In fact, in ANFIS–PNN–GA system, GA was used to optimize the ANFIS–PNN structure. To achieve the aim of this study, a series of data samples were collected from literature. After establishing the database, many ANFIS–PNN–GA models were constructed and proposed to estimate the bearing capacity of the aforementioned foundations. To show capability of this advance hybrid model, two pre-developed models i.e., ANFIS and PNN were also built to predict bearing capacity. The performance prediction of the proposed models were evaluated through the use of several performance indices, e.g., correlation coefficient ( R ) and mean square error (MSE). The R values of (0.9825, 0.9071, and 0.9928) and (0.8630, 0.7595 and 0.9241) were obtained for training and testing data of the ANFIS, PNN and ANFIS–PNN–GA, models, respectively. Accordingly, because of the role of GA as a practical optimization algorithm in improving the efficiency of both PNN and ANFIS models, results obtained by the ANFIS–PNN–GA model are more accurate compared to other implemented methods. The proposed advance hybrid model can be introduced as a new and applicable technique for solving problems in field of geotechnics and civil engineering.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00366-020-01267-6,en,Improved Levenberg–Marquardt backpropagation neural network by particle swarm and whale optimization algorithms to predict the deflection of RC beams,"['OriginalPaper', 'Original Article']","The aim of this study is to develop a novel computer-aided method for the prediction of the deflection of reinforced concrete beams (DRCB) under concentrated loads. To this end, in the present work, a Levenberg–Marquardt-based backpropagation novel neural network model, optimized by the whale optimization algorithm (WOA), called WOA-LMBPNN, has been developed. Specifically, a neural network, using the Levenberg–Marquardt backpropagation training algorithm with multiple hidden layers, was optimized by the WOA, aiming to obtain higher accuracy in predicting DRCB. For the training of the models, 120 experiments with the geometrical and mechanical properties of concrete beams were compiled using were used as the input parameters. Seven datasets with different number of input variables were investigated to evaluate the effect of the input variables on DRCB. For comparison purposes, another swarm optimization algorithm (i.e., particle swarm optimization—PSO) was also used to optimize the LMBPNN model (i.e., PSO-LMBPNN model). The results obtained by the PSO-LMBPNN and WOA-LMBPNN models are then compared based on the different datasets. Finally, the results revealed the effective role of the WOA, as well as the efficiency and robustness of the new hybrid WOA-LMBPNN model in predicting DRCB.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s10278-022-00661-4,en,Deep CTS: a Deep Neural Network for Identification MRI of Carpal Tunnel Syndrome,OriginalPaper,"Carpal tunnel syndrome (CTS) is a common peripheral nerve disease in adults; it can cause pain, numbness, and even muscle atrophy and will adversely affect patients’ daily life and work. There are no standard diagnostic criteria that go against the early diagnosis and treatment of patients. MRI as a novel imaging technique can show the patient’s condition more objectively, and several characteristics of carpal tunnel syndrome have been found. However, various image sequences, heavy artifacts, small lesion characteristics, high volume of imagine reading, and high difficulty in MRI interpretation limit its application in clinical practice. With the development of automatic image segmentation technology, the algorithm has great potential in medical imaging. The challenge is that the segmentation target is too small, and there are two categories of images with the proximal border of the carpal tunnel as the boundary. To meet the challenge, we propose an end-to-end deep learning framework called Deep CTS to segment the carpal tunnel from the MR image. The Deep CTS consists of the shape classifier with a simple convolutional neural network and the carpal tunnel region segmentation with simplified U-Net. With the specialized structure for the carpal tunnel, Deep CTS can segment the carpal tunnel region efficiently and improve the intersection over union of results. The experimental results demonstrated that the performance of the proposed deep learning framework is better than other segmentation networks for small objects. We trained the model with 333 images, tested it with 82 images, and achieved 0.63 accuracy of intersection over union and 0.17 s segmentation efficiency, which indicate great promise for the clinical application of this algorithm.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s11277-021-08766-9,en,An Enhanced Crow Search Inspired Feature Selection Technique for Intrusion Detection Based Wireless Network System,OriginalPaper,"Recent development of cognitive computing driven evolutionary techniques improve the overall quality of service and user experience in wireless communication network. This Paper consists of a feature selection method based on improvement of Crow Search Algorithm which has been used in Intrusion Detection System to limit the size of the dataset with which the system is working with and getting better results. Since IDS deals with a large data, the crucial task of IDS is to keep efficient features which represents the whole data and there is no duplicity and irrelevancy. The previous model that was proposed used the crow search algorithm in the intrusion detection system (CSA-IDS) as a model to find the optimal feature’s subset and random forest as a judgement on features that are produced by the CSA-IDS. The KDD and UNSW datasets are used to evaluate the earlier proposed model. The proposed model achieved an accuracy of 99.84% for attack detection using UNSW datasets. Similarly, R2L and U2R attacks have detected accuracy of 99.97% for NSL-KDD dataset. The development of proposed model improve the overall communication services and feature selection in wireless communication network. The outcome proves that the subset of features that are obtained by using CSA-IDS fetches higher accuracy rate using a smaller number of features.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s11431-022-2174-9,en,Eco-geotechnics for human sustainability,"['ReviewPaper', 'Review']","As a result of climate change and increasing engineering activities, soil-related disasters such as slope failures and sandstorms have become more frequent worldwide. These disasters have caused not only loss of life, but also have led to serious economic losses as well as ecological and environmental damage. To sustain mankind, a new discipline, eco-geotechnics, has rapidly become established and developed in recent years. It integrates scientific knowledge from soil mechanics, rock mechanics, ecology, biology, and atmospheric science to develop cross-disciplinary theories and carry out experiments to tackle grand world challenges such as the effects of climate change. Through the development of eco-geotechnics, various eco-friendly technologies have been developed to mitigate sandstorms and to improve the performance of earthen structures such as embankments, slopes and landfill covers. This state-of-the-art review introduces and discusses the important advances in the field of eco-geotechnics, covering theoretical developments, laboratory testing, centrifuge modelling, field monitoring and engineering applications. Finally, the research gaps and future needs of eco-geotechnics are highlighted and discussed.","['Engineering', 'Engineering, general']"
doi:10.1007/s10489-022-03281-1,en,Word-level human interpretable scoring mechanism for novel text detection using Tsetlin Machines,OriginalPaper,"Recent research in novelty detection focuses mainly on document-level classification, employing deep neural networks (DNN). However, the black-box nature of DNNs makes it difficult to extract an exact explanation of why a document is considered novel. In addition, dealing with novelty at the word level is crucial to provide a more fine-grained analysis than what is available at the document level. In this work, we propose a Tsetlin Machine (TM)-based architecture for scoring individual words according to their contribution to novelty. Our approach encodes a description of the novel documents using the linguistic patterns captured by TM clauses. We then adapt this description to measure how much a word contributes to making documents novel. Our experimental results demonstrate how our approach breaks down novelty into interpretable phrases, successfully measuring novelty.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11548-022-02764-3,en,Uncertainty estimation for margin detection in cancer surgery using mass spectrometry,"['OriginalPaper', 'Original Article']","Purpose Rapid evaporative ionization mass spectrometry (REIMS) is an emerging technology for clinical margin detection. Deployment of REIMS depends on construction of reliable deep learning models that can categorize tissue according to its metabolomic signature. Challenges associated with developing these models include the presence of noise during data acquisition and the variance in tissue signatures between patients. In this study, we propose integration of uncertainty estimation in deep models to factor predictive confidence into margin detection in cancer surgery. Methods iKnife is used to collect 693 spectra of cancer and healthy samples acquired from 91 patients during basal cell carcinoma resection. A Bayesian neural network and two baseline models are trained on these data to perform classification as well as uncertainty estimation. The samples with high estimated uncertainty are then removed, and new models are trained using the clean data. The performance of proposed and baseline models, with different ratios of filtered data, is then compared. Results The data filtering does not improve the performance of the baseline models as they cannot provide reliable estimations of uncertainty. In comparison, the proposed model demonstrates a statistically significant improvement in average balanced accuracy (75.2%), sensitivity (74.1%) and AUC (82.1%) after removing uncertain training samples. We also demonstrate that if highly uncertain samples are predicted and removed from the test data, sensitivity further improves to 88.2%. Conclusions This is the first study that applies uncertainty estimation to inform model training and deployment for tissue recognition in cancer surgery. Uncertainty estimation is leveraged in two ways: by factoring a measure of input noise in training the models and by including predictive confidence in reporting the outputs. We empirically show that considering uncertainty for model development can help improve the overall accuracy of a margin detection system using REIMS.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s13748-022-00293-3,en,Improving graph prototypical network using active learning,"['OriginalPaper', 'Regular Paper']","Due to the growth of using various devices and applications in modern life, the amount of data available is skyrocketing, but labeling all of this data is beyond the reach of data scientists. Thus, it is necessary to categorize data with a small amount of labeled data. In fact, it should be possible to prioritize data for labeling. To achieve this goal in this study, we have used few-shot learning with active learning and also used the power of graph convolutional networks in classifying data with a graphical structure. To implement the proposed model, we use two graph convolutional networks in parallel to calculate the embedding and the importance of each node. Using the output of both networks, we create prototypes of classes, and then, we classify them according to the distance of each node of these prototypes. We have also used active learning to select data more intelligently, which improves the overall model performance. As well as this, we have tested our proposed model in the field of electronic commerce for tagging goods in big online stores, which encounter a large number of diverse products, where high accuracy categorization in a short time without the interference of human factor and with the help of artificial intelligence is needed to reduce costs. The results of implementing the model on the Amazon dataset and its comparison with the state-of-the-art models in this field show the superiority of our method.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Natural Language Processing (NLP)', 'Computational Intelligence', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10489-022-03508-1,en,Automatic channel pruning via clustering and swarm intelligence optimization for CNN,OriginalPaper,"As convolutional neural networks (CNNs) have become increasingly deeper in recent years, the requirements for the quantity of data and hardware resources have gradually increased. CNN also reveals salient redundancy in several tasks. The existing magnitude-based pruning methods are efficient, but the performance of the compressed network is unpredictable. While the accuracy loss after pruning based on the structure sensitivity is relatively slight, the process is time-consuming, and the algorithm complexity is notable. To fully combine the advantages of the two types of methods, we propose a novel automatic channel pruning method (ACP). Specifically, we first perform layerwise channel clustering via the similarity of the feature maps to perform preliminary pruning on the network. Then, a population initialization method is introduced to transform the pruned structure into a candidate population. Finally, we conduct iterative searching and optimization based on particle swarm optimization (PSO) to find the optimal compressed structure. The compact network is then retrained to mitigate the accuracy loss from pruning. Our method is evaluated against several state-of-the-art CNNs on three different classification datasets CIFAR-10/100 and ILSVRC-2012. On the ILSVRC-2012, when removing 64.36% parameters and 63.34% floating-point operations (FLOPs) of ResNet-50, the Top-1 and Top-5 accuracy drops are less than 0.9%. Moreover, we demonstrate that without harming the overall performance, it is possible to compress the SSD by more than 50% on the target detection dataset PASCAL VOC. This further verifies that the proposed method can also be applied to other CNNs and application scenarios.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00530-020-00742-w,en,Detection of hate speech in Arabic tweets using deep learning,"['OriginalPaper', 'Special Issue Paper']","Nowadays, people are communicating through social networks everywhere. However, for whatever reason it is noticeable that verbal misbehaviors, such as hate speech is now propagated through the social networks. One of the most popular social networks is Twitter which has gained widespread in the Arabic region. This research aims to identify and classify Arabic tweets into 5 distinct classes: none, religious, racial, sexism or general hate. A dataset of 11 K tweets was collected and labelled and SVM model was used as a baseline to be compared against 4 deep learning models: LTSM, CNN + LTSM, GRU and CNN + GRU. The results show that all the 4 deep learning models outperform the SVM model in detecting hateful tweets. Although the SVM achieves an overall recall of 74%, the deep learning models have an average recall of 75%. However, adding a layer of CNN to LTSM enhances the overall performance of detection with 72% precision, 75% recall and 73% F1 score.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00521-021-05981-0,en,Effective training of convolutional neural networks for age estimation based on knowledge distillation,"['OriginalPaper', 'S.I. : NCACVIP']","Age estimation from face images can be profitably employed in several applications, ranging from digital signage to social robotics, from business intelligence to access control. Only in recent years, the advent of deep learning allowed for the design of extremely accurate methods based on convolutional neural networks (CNNs) that achieve a remarkable performance in various face analysis tasks. However, these networks are not always applicable in real scenarios, due to both time and resource constraints that the most accurate approaches often do not meet. Moreover, in case of age estimation, there is the lack of a large and reliably annotated dataset for training deep neural networks. Within this context, we propose in this paper an effective training procedure of CNNs for age estimation based on knowledge distillation, able to allow smaller and simpler “student” models to be trained to match the predictions of a larger “teacher” model. We experimentally show that such student models are able to almost reach the performance of the teacher, obtaining high accuracy over the LFW+, LAP 2016 and Adience datasets, but being up to 15 times faster. Furthermore, we evaluate the performance of the student models in the presence of image corruptions, and we demonstrate that some of them are even more resilient to these corruptions than the teacher model.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10844-022-00709-5,en,Improving knowledge-based dialogue generation through two-stage knowledge selection and knowledge selection-guided pointer network,OriginalPaper,"Existing End-to-End neural models for dialogue generation tend to generate generic and uninformative responses. Recently, knowledge-based dialogue models have been developed to generate more informative responses by leveraging external knowledge. However, it is still challenging for the models to select appropriate knowledge from an external knowledge base and generate responses coherent with the context and knowledge. In this paper, we propose a new method that uses two-stage knowledge selection to get proper knowledge for response generation without the guidance of ground-truth knowledge. Specifically, in the first stage the model selects knowledge according to the relevance between the context and candidate knowledge from a global perspective. During response generation, dynamic knowledge attention is performed to capture the knowledge relevant to the current decoding state, which is the second stage. Furthermore, we incorporate a knowledge selection-guided pointer network into the decoder to copy words from the captured knowledge. Experimental results on DuConv and Wizard-of-Wikipedia datasets demonstrate that our model can generate more coherent and informative responses than baselines do.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s00521-022-07656-w,en,Context-awareness trust management model for trustworthy communications in the social Internet of Things,"['OriginalPaper', 'Original Article']","The social Internet of Things (SIoT) is the next generation of the Internet of Things network. It entails the evolution of intelligent devices into social ones, aiming at building interactions with people in order to link groups and develop their own social context. Because a high volume of data is shared throughout the network’s diverse nodes, security measures are essential to ensure that users may interact safely. Trust management (TM) models have been presented in the literature to avoid detrimental interactions and preserve a system’s optimal functioning. In reality, given the SIoT context of nodes varies over time, a TM mechanism must contain methods for evaluating the level of trustworthiness. Existing methods, on the other hand, continue to lack effective solutions for addressing contextual SIoT attributes that define the network node while assessing trust. The utmost objective of this paper is to perform an in-depth analysis of contextual trust-awareness based on the defined TM model “CTM-SIoT” in order to more precisely detect malicious SIoT nodes to maintain safe network connections. As part of our trust evaluation process, machine learning techniques are employed to study the behavior of nodes. Our objective is to limit contacts with aggressive and unskilled service providers. Experimentation was carried out using the Cooja simulator on a simulated SIoT dataset based on real social data. With an F -measure value of up to 1, we validated the Artificial Neural Network’s suitability as a classifier for our issue statement. When compared to other conventional trust classification methods, the findings demonstrated that handling contextual SIoT characteristics inside our TM model enhanced the performance of a TM mechanism with a 0.037% rise in F -measure and a 0.13% drop in FPR, in identifying malicious nodes even for a system with 50% of malicious transactions.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1134/S1064230722060089,en,Frame Regularization of a Convolutional Neural Network in Image-Classification Problems,"['OriginalPaper', 'ARTIFICIAL INTELLIGENCE']","Abstract The problem of regularization of the parameters of a neural network is considered in order to increase the efficiency of using their redundancy and increase resistance to implementations of input data that are not contained in the training set. A representation of the system of weight vectors of the neural-network layer as a frame in the space of weights is proposed and regularization is introduced in the form of a penalty for noncompliance with the sufficient condition of the frame. The proposed method imposes less restrictions on the weights of the model than existing methods for increasing efficiency based on orthogonalization. The method is generalized to convolutional layers in the block-Toeplitz representation and is applicable to convolutional neural networks. A computational experiment on CIFAR-10, CIFAR-100 and SVHN datasets shows the superiority of the proposed regularization method in terms of classification accuracy, generalization ability and resistance to adversarial attacks compared to the basic approaches.","['Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00354-022-00176-0,en,Internet of Medical Things-Based COVID-19 Detection in CT Images Fused with Fuzzy Ensemble and Transfer Learning Models,OriginalPaper,"One of the most difficult research areas in today's healthcare industry to combat the coronavirus pandemic is accurate COVID-19 detection. Because of its low infection miss rate and high sensitivity, chest computed tomography (CT) imaging has been recommended as a viable technique for COVID-19 diagnosis in a number of recent clinical investigations. This article presents an Internet of Medical Things (IoMT)-based platform for improving and speeding up COVID-19 identification. Clinical devices are connected to network resources in the suggested IoMT platform using cloud computing. The method enables patients and healthcare experts to work together in real time to diagnose and treat COVID-19, potentially saving time and effort for both patients and physicians. In this paper, we introduce a technique for classifying chest CT scan images into COVID, pneumonia, and normal classes that use a Sugeno fuzzy integral ensemble across three transfer learning models, namely SqueezeNet, DenseNet-201, and MobileNetV2. The suggested fuzzy ensemble techniques outperform each individual transfer learning methodology as well as trainable ensemble strategies in terms of accuracy. The suggested MobileNetV2 fused with Sugeno fuzzy integral ensemble model has a 99.15% accuracy rate. In the present research, this framework was utilized to identify COVID-19, but it may also be implemented and used for medical imaging analyses of other disorders.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s10909-022-02719-7,en,Application of Deep Learning to the Evaluation of Goodness in the Waveform Processing of Transition-Edge Sensor Calorimeters,OriginalPaper,"Optimal filtering is the crucial technique for the data analysis of transition-edge-sensor (TES) calorimeters to achieve their state-of-the-art energy resolutions. Filtering out the ‘bad’ data from the dataset is important because it otherwise leads to the degradation of energy resolutions, while it is not a trivial task. We propose a neural network-based technique for the automatic goodness tagging of TES pulses, which is fast and automatic and does not require bad data for training.","['Physics', 'Condensed Matter Physics', 'Characterization and Evaluation of Materials', 'Magnetism, Magnetic Materials']"
doi:10.1007/s11081-021-09680-6,en,Robust vehicle routing under uncertainty via branch-price-and-cut,"['OriginalPaper', 'Research Article']","This paper contemplates how branch-price-and-cut solvers can be employed along with the robust optimization paradigm to address parametric uncertainty in the context of vehicle routing problems. In this setting, given postulated uncertainty sets for customer demands and vehicle travel times, one aims to identify a set of cost-effective routes for vehicles to traverse, such that the vehicle capacities and customer time window constraints are respected under any anticipated demand and travel time realization, respectively. To tackle such problems, we propose a novel approach that combines cutting-plane techniques with an advanced branch-price-and-cut algorithm. Specifically, we use deterministic pricing procedures to generate “partially robust” vehicle routes and then utilize robust versions of rounded capacity inequalities and infeasible path elimination constraints to guarantee complete robust feasibility of routing designs against demand and travel time uncertainty. In contrast to recent approaches that modify the pricing algorithm, our approach is both modular and versatile. It permits the use of advanced branch-price-and-cut technologies without significant modification, while it can admit a variety of uncertainty sets that are commonly used in robust optimization but could not be previously employed in a branch-price-and-cut setting.","['Mathematics', 'Optimization', 'Engineering, general', 'Systems Theory, Control', 'Environmental Management', 'Operations Research/Decision Theory', 'Financial Engineering']"
doi:10.1007/s12190-022-01723-0,en,Combating COVID-19 crisis and predicting the second wave in Europe: an Age-structured modeling,"['OriginalPaper', 'Original Research']","We employ an age-structured susceptible-infected-quarantined-recovered model to simulate the progression of COVID-19 in France, Spain, and Germany. In the absence of a vaccine or conventional treatment, non-pharmaceutical interventions become more valuable, so our model takes into account the efficacy of official social distancing and lockdown measures. Using data from February to July 2020, we make useful predictions for the upcoming months, and further simulate the effect of lifting the lockdown at a later stage. A control model is also proposed and conditions for optimality are also obtained using optimal control theory. Motivated by the recent surge in cases in France and Spain, we also examine the possibility of a second wave of the pandemic. We conclude that further measures need to be taken in these two countries, while Germany is on its way to mitigating the disease.","['Mathematics', 'Computational Mathematics and Numerical Analysis', 'Mathematical and Computational Engineering', 'Theory of Computation', 'Mathematics of Computing']"
doi:10.1007/s40747-022-00733-6,en,A wavelet convolutional capsule network with modified super resolution generative adversarial network for fault diagnosis and classification,"['OriginalPaper', 'Original Article']","The study of fault diagnosis and classification has gained tremendous attention in various aspects of modern industry. However, the performance of traditional fault diagnosis technique solely depends on handcrafted features based on expert knowledge which is difficult to pre-design and has failed in several applications. Deep learning (DL) has achieved remarkable performance in hierarchical feature extraction and learning distinctive feature of dataset from related distribution. However, the challenge associated with DL models is that max-pooling operation usually leads to loss of spatial details during high-level feature extraction. Another concern is the low quality characteristics of 2D time-frequency image which is mostly caused by the presence of noise and poor resolution. This paper proposes a modified wavelet convolutional capsule network with modified enhanced super resolution generative adversarial network plus for fault diagnosis and classification. It uses continuous wavelet transform to convert raw data signals to 2D time-frequency images and applies super resolution generative adversarial technique to enhance the quality of the time-frequency images and finally, the convolutional capsule network learns the extracted high-level features without loss of spatial details for the diagnosis and classification of faults. We validated our proposed model on the famous motor bearing dataset from the Case Western Reserve University. The experimental results show that our proposed fault diagnostic model obtains higher diagnosis accuracy of 99.84% outweighing most traditional deep learning models including state-of-the-art methods.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10877-022-00822-4,en,A model-based approach to generating annotated pressure support waveforms,"['OriginalPaper', 'Original Research']","Large numbers of asynchronies during pressure support ventilation cause discomfort and higher work of breathing in the patient, and are associated with an increased mortality. There is a need for real-time decision support to detect asynchronies and assist the clinician towards lung-protective ventilation. Machine learning techniques have been proposed to detect asynchronies, but they require large datasets with sufficient data diversity, sample size, and quality for training purposes. In this work, we propose a method for generating a large, realistic and labeled, synthetic dataset for training and validating machine learning algorithms to detect a wide variety of asynchrony types. We take a model-based approach in which we adapt a non-linear lung-airway model for use in a diverse patient group and add a first-order ventilator model to generate labeled pressure, flow, and volume waveforms of pressure support ventilation. The model was able to reproduce basic measured lung mechanics parameters. Experienced clinicians were not able to differentiate between the simulated waveforms and clinical data (P = 0.44 by Fisher’s exact test). The detection performance of the machine learning trained on clinical data gave an overall comparable true positive rate on clinical data and on simulated data (an overall true positive rate of 94.3% and positive predictive value of 93.5% on simulated data and a true positive rate of 98% and positive predictive value of 98% on clinical data). Our findings demonstrate that it is possible to generate labeled pressure and flow waveforms with different types of asynchronies.","['Medicine & Public Health', 'Anesthesiology', 'Intensive / Critical Care Medicine', 'Statistics for Life Sciences, Medicine, Health Sciences']"
doi:10.1007/s10462-022-10169-6,en,Uncertainty modeling in multi-objective vehicle routing problem under extreme environment,OriginalPaper,"Assumption of fuzziness in the vehicle routing problems under extreme conditions is necessary for modelers, because there are usually insufficient objective input data. In extreme situations, the complexity of the description of vehicles’ movement on routes may cause by two poles: the imprecision of movement time and the uncertainty of the possibility of movement on roads . Traditionally, a fuzzy value has been used to represent the data’s impreciseness; hence, only one pole of expert’s information is taken in the aggregation results. The main objective of this paper is to present an efficient way for fuzzy vehicle routing modeling to minimize the decision-making risks in the optimal planning of routes network and from distribution centers to demand points. To address this, a new two-stage possibilistic bi-criteria vehicle routing problem (VRP) is presented under extreme conditions. In the first stage, the sample of so-called “promising” closed routes are selected based on a “constructive” approach using a simulation algorithm. The expected times of the vehicle movement between demand points are taken as fuzzy triangular numbers. In the second stage, based on Choquet integral’s, a bi-criteria partitioning model for the fuzzy VRP has been constructed. The constraint approach has been defined to obtain the optimal solution of the model. For numerical experiments, a parallel algorithm is created based on D. Knuth’s algorithm of dancing links. An example is presented with the results of our approach for the VRP, where all Pareto-optimal solutions are found from the promising routes.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11263-022-01677-7,en,CRCNet: Few-Shot Segmentation with Cross-Reference and Region–Global Conditional Networks,OriginalPaper,"Few-shot segmentation aims to learn a segmentation model that can be generalized to novel classes with only a few training images. In this paper, we propose a Cross-Reference and Local–Global Conditional Networks (CRCNet) for few-shot segmentation. Unlike previous works that only predict the query image’s mask, our proposed model concurrently makes predictions for both the support image and the query image. Our network can better find the co-occurrent objects in the two images with a cross-reference mechanism, thus helping the few-shot segmentation task. To further improve feature comparison, we develop a local-global conditional module to capture both global and local relations. We also develop a mask refinement module to refine the prediction of the foreground regions recurrently. Experiments on the PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art performance.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11517-022-02670-5,en,Enhancing the feasibility of cognitive load recognition in remote learning using physiological measures and an adaptive feature recalibration convolutional neural network,"['OriginalPaper', 'Original Article']","The precise assessment of cognitive load during a learning phase is an important pathway to improving students’ learning efficiency and performance. Physiological measures make it possible to continuously monitor learners’ cognitive load in remote learning during the COVID-19 outbreak. However, maintaining a good balance between performance and computational cost is still a major challenge in advancing cognitive load recognition technology to real-world applications. This paper introduced an adaptive feature recalibration (AFR) convolutional neural network to overcome this challenge by capturing the most discriminative physiological features (EEG and eye-tracking). The results revealed that the optimal average classification accuracy of the feature combination obtained by the AFR method reached 95.56% with only 60 feature dimensions. Additionally, compared with the best result of the conventional correlation-based feature selection (CFS) method, the introduced AFR algorithm achieved higher accuracy and cheaper computational cost, as well as a 2.06% improvement in accuracy and a 51.21% reduction in feature dimension, which is more in line with the requirements of low delay and real-time performance in practical BCI applications. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s11554-022-01249-5,en,FAM: focal attention module for lesion segmentation of COVID-19 CT images,"['OriginalPaper', 'Original Research Paper']","The novel coronavirus pneumonia (COVID-19) is the world’s most serious public health crisis, posing a serious threat to public health. In clinical practice, automatic segmentation of the lesion from computed tomography (CT) images using deep learning methods provides an promising tool for identifying and diagnosing COVID-19. To improve the accuracy of image segmentation, an attention mechanism is adopted to highlight important features. However, existing attention methods are of weak performance or negative impact to the accuracy of convolutional neural networks (CNNs) due to various reasons (e.g. low contrast of the boundary between the lesion and the surrounding, the image noise). To address this issue, we propose a novel focal attention module (FAM) for lesion segmentation of CT images. FAM contains a channel attention module and a spatial attention module. In the spatial attention module, it first generates rough spatial attention, a shape prior of the lesion region obtained from the CT image using median filtering and distance transformation. The rough spatial attention is then input into two 7 × 7 convolution layers for correction, achieving refined spatial attention on the lesion region. FAM is individually integrated with six state-of-the-art segmentation networks (e.g. UNet, DeepLabV3+, etc.), and then we validated these six combinations on the public dataset including COVID-19 CT images. The results show that FAM improve the Dice Similarity Coefficient (DSC) of CNNs by 2%, and reduced the number of false negatives (FN) and false positives (FP) up to 17.6%, which are significantly higher than that using other attention modules such as CBAM and SENet. Furthermore, FAM significantly improve the convergence speed of the model training and achieve better real-time performance. The codes are available at GitHub ( https://github.com/RobotvisionLab/FAM.git ).","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s11548-022-02737-6,en,Evaluating different combination methods to analyse ultrasound and shear wave elastography images automatically through discriminative convolutional neural network in breast cancer imaging,"['OriginalPaper', 'Original Article']","Purpose Ultrasound (US) and Shear Wave Elastography (SWE) imaging are non-invasive methods used for breast lesion characterization. While US and SWE images provide both morphological information, SWE visualizes in addition the elasticity of tissue. In this study a Discriminative Convolutional Neural Network (DCNN) model is applied to US and SWE images and their combination to classify the breast lesions into malignant or benign cases. Furthermore, it is identified whether analysing only the region of the elastogram or including the surrounding B-mode image gives a superior performance. Methods The dataset used in this study consists of 746 images obtained from 207 patients comprising 486 malignant and 260 benign breast lesions. From each image the US and SWE image was extracted, once including only the region of the elastogram and once including also the surrounding B-mode image. These four datasets were applied individually to a DCNN to determine their predictive capability. Each the best US and SWE dataset were used to examine different combination methods with DCNN. The results were compared to the manual assessment by an expert radiologist. Results The combination of US and SWE images with the surrounding B-mode image using two ensembled DCNN models achieved best results with an accuracy of 93.53 %, sensitivity of 94.42 %, specificity of 90.75 % and area under the curve (AUC) of 96.55 %. Conclusion This study showed that using the whole US and SWE images through DCNN was superior to methods, in which only the region of elastogram was used. Combining breast cancer US and SWE images with two ensembled DCNN models in parallel improved the results. The accuracy, sensitivity and AUC of the best combination method were significantly superior to the results of using a single dataset through DCNN and to the results of the expert radiologist.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s13296-022-00604-3,en,Cyclic Behavior of WUF-W Connections Predicted using FE Analyses with Accurate Material Hardening Models,OriginalPaper,"Welded unreinforced flange-welded web (WUF-W) connections are one of prequalified connections specified in AISC 358-16 (2016) for special and intermediate moment frames. The cyclic behavior of the WUF-W connections varies according to many different parameters such as panel zone strength ratio, beam span-to depth ratio, access hole geometry, and beam-column strength ratio. The cyclic behavior of the connections has been often investigated using experimental tests. The cyclic behavior of the connections has been often investigated using experimental tests. However, it is difficult to conduct tests considering all combinations of individual design and detail parameters because of excessive cost and time. The objective of this study was to construct the FE model with an accurate material model for predicting the cyclic behavior of WUF-W connections. The material model was constructed with a combination of one isotropic and three kinematic hardening models based on combined hardening model. The particle swarm optimization was used to precisely determine the constituent parameter values of the material model for steel materials. The cyclic behavior of WUF-W connections was accurately simulated using the proposed FE model. Strain distribution and local flange bucking shape were also precisely predicted using the model.","['Engineering', 'Civil Engineering', 'Solid Mechanics', 'Materials Science, general']"
doi:10.1007/s10844-022-00724-6,en,Entity-aware answer sentence selection for question answering with transformer-based language models,OriginalPaper,"The Answer Sentence Selection (AS2) task is defined as the task of ranking the candidate answers for each question based on a matching score. The matching score is the probability of being a correct answer for a given question. Detecting the question class and matching it with the named entities of the answer sentence to narrow down the search space was used in primary question answering systems. We used this idea in the state-of-the-art text matching models namely, Transformer-based language models. In this paper, we proposed two different architectures: Ent-match and Ent-add, while using two different question classifiers: Convolutional Neural Network-based (CNN-based) and rule-based. The proposed models outperform the state-of-the-art AS2 model, namely TANDA and RoBERTa-base on both TREC-QA and Wiki-QA datasets. Using Wiki-QA, the Ent-add (CNN-based) model outperforms the TANDA model by 2.1% and 1.9% improvement over Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) metrics, respectively. Over the TREC-QA dataset the Ent-match (CNN-based) model outperformed the TANDA model with 1.5% and 1.4% improvement over MAP and MRR, respectively.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s10489-022-03454-y,en,Lightweight global-locally connected distillation network for single image super-resolution,OriginalPaper,"As convolutional neural networks (CNNs) have been commonly applied to ill-posed single image super-resolution (SISR) task, most previous CNN-based methods made significant progress in terms of both high signal-to-noise ratios (PSNR) and structural similarity (SSIM). However, with the layers in those networks going deeper and deeper, they require more and more computing power, fail to consider distilling the feature maps. In this paper, we propose a lightweight global-locally connected distillation network, GLCDNet. Specifically, we propose a wide activation shrink-expand convolutional block whose filter channels will first shrink then expand to aggregate more information. This information will concatenate with feature maps of the previous blocks to further explore shallow information. Thus, the block will exploit statistics within most feature channels while refining useful information of features. Furthermore, together with the global-local connection method, our network is robust to benchmark datasets with high processing speed. Comparative results demonstrate that our GLCDNet achieves superior performance while keeping the parameters and speed balanced.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-021-11241-z,en,An EM-based optimization of synthetic reduced nearest neighbor model towards multiple modalities representation with human interpretability,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","A convenient, accurate and well-known way toward any supervised task is using Nearest Neighbor approach or its variants. However, there has been little attempt toward improving interpretability by human and providing a classical optimization of Synthetic Reduced Nearest Neighbor. To tackle these issues, this paper provides a novel optimization of Synthetic Reduced Nearest Neighbor based on Expectation Maximization (EM-SRNN). Reduced Nearest Neighbor model consists of a subset of the samples from the trainset that has similar accuracy to Nearest Neighbor. Synthetic Reduced Nearest Neighbor relaxes the model to learn K synthetic samples (or prototypes/centroids) in the space of dataset. Therefore, inspired by EM algorithm for K-means, we propose a novel optimization based on EM algorithm to learn EM-SRNN by iterating over the centroids of the model and assignment of training samples to the centroids. The first step consists of optimizing the position of each centroid based on the assignment of the samples to the centroid and the second step consists finding optimal assignments and labels of the centroids. The EM-SRNN is interpretable since the centroids exist inside the space of training samples. Additionally, the centroids represent the multiple modalities (or sub-clusters) of the classes that are interpretable by human. These properties make this type of interpretability unique, hence, making this model suitable for various studies that are related to interpretability by human, such as image processing and epidemiological studies. In this paper, analytical aspects of problem are explored and it is shown that computational complexity of proposed optimization is linear over size of the trainset. Finally, EM-SRNN shows superior or similar performance when compared with several other interpretable and similar state-of-the-art models, such as trees and kernel SVMs.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11263-022-01678-6,en,Multi-Object Tracking and Segmentation Via Neural Message Passing,OriginalPaper,"Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and Multiple Object Tracking and Segmentation (MOTS) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such structured domain is not trivial. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks. By operating directly on the graph domain, our method can reason globally over an entire set of detections and exploit contextual features. It then jointly predicts both final solutions for the data association problem and segmentation masks for all objects in the scene while exploiting synergies between the two tasks. We achieve state-of-the-art results for both tracking and segmentation in several publicly available datasets. Our code is available at https://github.com/ocetintas/MPNTrackSeg","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00521-022-07627-1,en,A cost-effective test case selection and prioritization using hybrid battle royale-based remora optimization,"['OriginalPaper', 'Original Article']","Due to the reason, that many test cases in the software industry can be reused, selection and prioritizing techniques are essential during the regression and validation testing phases. However, time and project-specific limits must be observed. We present a hybrid optimization strategy to solve test case selection and prioritizing challenges in this research. The proposed method is the hybridization of Battle Royale Optimization (BRO) and Remora Optimization (RO) algorithm, which is named as hybrid Battle Royale-based Remora Optimization (HBR 2 O) algorithm. We focus on issues with pseudo-polynomial time complexity and low memory usage, which may be used to tackle problems like selection-prioritization and selection across collections of test suites or test cases. Dynamic programming optimization approaches require a large amount of memory, yet memory is limited. As a result of the decreased memory usage, the selection method includes a larger number of test cases. During the selection of software test cases, the proposed method’s effectiveness is validated using multiple state-of-the-art techniques, resulting in improved performance.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10844-022-00711-x,en,Multi-stage music separation network with dual-branch attention and hybrid convolution,OriginalPaper,"In this paper, we propose a lightweight multi-stage network for monaural vocal and accompaniment separation. We design a dual-branch attention (DBA) module to obtain the correlation of each position pair and that among the channels of feature maps, respectively. The square CNN (i.e. the size of the filter is k × k ) shares the weights of each of the square areas in feature maps that which makes its ability of feature extraction limited. In order to address it, we propose a hybrid convolution (HC) block based on hybrid convolutional mechanism instead of square CNN to capture the dependencies along with the time dimension and the frequency dimension respectively. The ablation experiments demonstrate that the DBA module and HC block can assist in improving the separation performance. Experimental results show that our proposed network achieves outstanding performance on the MIR-1K dataset only with fewer parameters, and competitive performance compared with state-of-the-arts on DSD100 and MUSDB18 datasets.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s00234-022-02978-x,en,Validation of a machine learning software tool for automated large vessel occlusion detection in patients with suspected acute stroke,"['OriginalPaper', 'Diagnostic Neuroradiology']","Purpose CT angiography (CTA) is the imaging standard for large vessel occlusion (LVO) detection in patients with acute ischemic stroke. Stroke SENS LVO is an automated tool that utilizes a machine learning algorithm to identify anterior large vessel occlusions (LVO) on CTA. The aim of this study was to test the algorithm’s performance in LVO detection in an independent dataset. Methods A total of 400 studies (217 LVO, 183 other/no occlusion) read by expert consensus were used for retrospective analysis. The LVO was defined as intracranial internal carotid artery (ICA) occlusion and M1 middle cerebral artery (MCA) occlusion. Software performance in detecting anterior LVO was evaluated using receiver operator characteristics (ROC) analysis, reporting area under the curve (AUC), sensitivity, and specificity. Subgroup analyses were performed to evaluate if performance in detecting LVO differed by subgroups, namely M1 MCA and ICA occlusion sites, and in data stratified by patient age, sex, and CTA acquisition characteristics (slice thickness, kilovoltage tube peak, and scanner manufacturer). Results AUC, sensitivity, and specificity overall were as follows: 0.939, 0.894, and 0.874, respectively, in the full cohort; 0.927, 0.857, and 0.874, respectively, in the ICA occlusion cohort; 0.945, 0.914, and 0.874, respectively, in the M1 MCA occlusion cohort. Performance did not differ significantly by patient age, sex, or CTA acquisition characteristics. Conclusion The Stroke SENS LVO machine learning algorithm detects anterior LVO with high accuracy from a range of scans in a large dataset.","['Medicine & Public Health', 'Neuroradiology', 'Imaging / Radiology', 'Neurology', 'Neurosurgery', 'Neurosciences']"
doi:10.1038/s42256-022-00569-2,en,Deep transfer operator learning for partial differential equations under conditional shift,"['OriginalPaper', 'Article']","Transfer learning enables the transfer of knowledge gained while learning to perform one task (source) to a related but different task (target), hence addressing the expense of data acquisition and labelling, potential computational power limitations and dataset distribution mismatches. We propose a new transfer learning framework for task-specific learning (functional regression in partial differential equations) under conditional shift based on the deep operator network (DeepONet). Task-specific operator learning is accomplished by fine-tuning task-specific layers of the target DeepONet using a hybrid loss function that allows for the matching of individual target samples while also preserving the global properties of the conditional distribution of the target data. Inspired by conditional embedding operator theory, we minimize the statistical distance between labelled target data and the surrogate prediction on unlabelled target data by embedding conditional distributions onto a reproducing kernel Hilbert space. We demonstrate the advantages of our approach for various transfer learning scenarios involving nonlinear partial differential equations under diverse conditions due to shifts in the geometric domain and model dynamics. Our transfer learning framework enables fast and efficient learning of heterogeneous tasks despite considerable differences between the source and target domains. A promising area for deep learning is in modelling complex physical processes described by partial differential equations (PDEs), which is computationally expensive for conventional approaches. An operator learning approach called DeepONet was recently introduced to tackle PDE-related problems, and in new work, this approach is extended with transfer learning, which transfers knowledge obtained from learning to perform one task to a related but different task.","['Engineering', 'Engineering, general']"
doi:10.1007/s10614-021-10097-7,en,The Relationship Between Economic Growth and Electricity Consumption: Bootstrap ARDL Test with a Fourier Function and Machine Learning Approach,OriginalPaper,"In this study, the relationship between electricity and growth of the economy is investigated by applying the newly-developed bootstrap autoregressive-distributed lag test with a Fourier function to examine both the causality and cointegration for China, India, and the United States (US). While it is not possible to detect a long-term cointegration relation among the economy's electricity and growth, the study findings demonstrate the contingency of the causality. The ensemble method in machine learning performs better than conventional methods as electricity is an independent indicator for forecast economics. Concerning the US, previous electricity consumption has a positive impact on the current nature of economic growth. In contrast, the consumption of electricity is negatively affected by the development of the economy. However, for China and India, positive and negative feedback can be observed, respectively. Due to the increased awareness of the environment's adverse effects, China should promote technologies that conserve energy and boost energy efficiency to achieve sustainable development in both environmental and economic terms. In India's context, broadening access to electricity has significance for residents in rural areas and enhances economic growth. It is recommended that policy-makers promote innovative technologies in the US, as the abundant natural and human resources can make valuable contributions to the society and development of the economy.","['Economics', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Computer Appl. in Social and Behavioral Sciences', 'Operations Research/Decision Theory', 'Behavioral/Experimental Economics', 'Math Applications in Computer Science']"
doi:10.1007/s10278-022-00668-x,en,Automatic Liver Segmentation Using EfficientNet and Attention-Based Residual U-Net in CT,OriginalPaper,"This paper proposes a new network framework, which leverages EfficientNetB4, attention gate, and residual learning techniques to achieve automatic and accurate liver segmentation. First, we use EfficientNetB4 as the encoder to extract more feature information during the encoding stage. Then, an attention gate is introduced in the skip connection to eliminate irrelevant regions and highlight features of a specific segmentation task. Finally, to alleviate the problem of gradient vanishment, we replace the traditional convolution of the decoder with a residual block to improve the segmentation accuracy. We verified the proposed method on the LiTS17 and SLiver07 datasets and compared it with classical networks such as FCN, U-Net, attention U-Net, and attention Res-U-Net. In the Sliver07 evaluation, the proposed method achieved the best segmentation performance on all five standard metrics. Meanwhile, in the LiTS17 assessment, the best performance is obtained except for a slight inferior on RVD. The proposed method’s qualitative and quantitative results demonstrated its applicability in liver segmentation and proved its good prospect in computer-assisted liver segmentation.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s11042-022-13496-6,en,Unsupervised anomalous event detection in videos using spatio-temporal inter-fused autoencoder,"['OriginalPaper', '1220: Visual and Sensory Data Processing for Real Time Intelligent Surveillance System']","Automatic detection, localization and interpretation of an unusual event in a sequence of video is a challenging task due to its equivocal and complex nature. The development of deep neural networks have paved the way for more efficient recognition and analysis of anomalous events in video data. With the introduction of convolutional neural network (CNN) and Long short-term memory (LSTM), the spatial and temporal features extraction became easier. In this paper, we propose an end-to-end trainable Inter-fused Autoencoder (IFA) which is designed using the assemblage of CNN and LSTM layers to detect the unwonted events in a video sequence. The proposed architecture is capable of exploiting both the spatial and temporal variation of video data. The reconstruction error is computed in terms of both MSE and PSNR for each testing video. A comparison is also carried out between MSE and PSNR to show that which assessment technique is better for a reconstructive model for recreating the video sequence. A well-optimized threshold is calculated which decides the fate of testing event i.e. either usual or unusual event. Using benchmark datasets, multiple experiments were carried out to demonstrate the efficacy of proposed architecture.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s12652-021-03185-x,en,Detecting IoT botnets based on the combination of cooperative game theory with deep and machine learning approaches,"['OriginalPaper', 'Original Research']","An Internet of Things (IoT) botnet is a collection of infected smart devices that are remotely managed by a botmaster. The injection of multiple attacks into the infrastructure, high permeability, and vulnerability of IoT security interfaces are the most significant challenges in the field of IoT security. Therefore, it is essential to provide an efficient solution that can detect intrusion into the IoT infrastructure in the shortest time. In this paper, cooperative game theory in combination with three approaches—long short term memory (LSTM), Autoencoder, and support vector machine (SVM)—are applied to detect IoT botnet attacks. Proposed approaches based on the selection of effective features using cooperative game theory and shapely value on data set gathered from five IoT devices infected with botnets and using SVM, LSTM, and Autoencoder to identify IoT botnet traffic. Compared to the results of the best method presented on the same data set, the proposed approach improved 11.624% in accuracy, 11.629% in the recall, and 154.41 s in learning time in SVM. Also in LSTM, 0.245% in accuracy, 0.250% in the recall, and 222.72 s improved learning time. In addition, the approach of using Autoencoder has overall good performance and remarkable speed in identifying botnet traffic. Based on the results, the performance of the proposed approach in classifying IoT botnets is very promising. Therefore, it can help IoT providers to identify IoT attacks more accurately and faster so that they may make the proper decisions for detection and prevention of botnet attacks.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Robotics and Automation', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s12145-022-00889-2,en,"Landslide identification using machine learning techniques: Review, motivation, and future prospects","['ReviewPaper', 'Review']","The WHO (World Health Organization) study reports that, between 1998-2017, 4.8 million people have been affected by landslides with more than 18000 deaths. The prevailing climate change and erratic high intensity rainfall are expected to trigger more landslides, which can increase the death rates per year in future. Therefore, evolving successful mechanisms to identify and predict landslides is critical in risk reduction and post-disaster management activities. With the applications of Machine Learning , the success rates of landslide identification have been improved significantly. This review paper presents the results of data analysis on the papers published for the last three decades on varying degrees of reliability and success rate on the theme “Machine Learning for landslide identification, mitigation, and prediction”. The analyses show how the reliability and accuracy of the landslide prediction model have improved considerably with the tools available in Machine Learning. Though many conventional tools such as statistical packages are available, the Machine Learning algorithms gave a robust dimension for a reliable landslide risk analysis, modeling prediction tools and post-disaster damage identification, This paper recommends a multi-modal framework for characterising landslides in all aspects using Machine Learning techniques that could outperform single modal approaches. Open research problems and future research dimensions by integrating landsat data and Machine Learning for landslide studies are also discussed in this work which would be beneficial for researchers in this field and also to the community at large across the globe.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s10462-022-10165-w,en,3DDACNN: 3D dense attention convolutional neural network for point cloud based object recognition,OriginalPaper,"Recently, deep CNN-based methods have achieved significant success in solving various 2D computer vision issues. However, directly processing 3D point clouds with CNNs remains a challenging problem due to their irregular characteristic, which results in the comprehensive performance far from optimal. In this paper, we propose a novel trainable architecture for 3D point cloud based object recognition from the perspective of depth of network and attention mechanism for the first time. We first transform the input point cloud into regular volumetric representation using binary occupancy grid strategy. The output is then fed into our proposed 3D Dense-Attention CNN framework, dubbed as $$\mathbf{3DDACNN }$$ 3 DDACNN , to obtain features with enhanced representation power. Extensive experiments on highly challenging datasets demonstrate the effectiveness of our proposed model, which can achieve remarkable performance.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11263-022-01679-5,en,The Family of Onion Convolutions for Image Inpainting,OriginalPaper,"Recently deep learning methods have achieved great success in image inpainting problem. However, reconstructing continuities of complex structures with non-stationary textures remains a challenging task for computer vision. In this paper the family of onion convolutions is presented, the concept of which arises from a connection between patch-based techniques and attention mechanisms . The onion convolutions are building blocks designed for the iterative completion of the missing region from its boundary to the center. It allows to continuously propagate structures and textures from the known region to the missing one and meet human criteria on high-quality image completions. As qualitative and quantitative comparisons show, our method with onion convolutions outperforms state-of-the-art methods by producing more realistic, visually plausible and semantically coherent results.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00530-022-01026-1,en,2C-Net: integrate image compression and classification via deep neural network,"['OriginalPaper', 'Regular Paper']","Providing effective support for intelligent vision tasks without image reconstruction can save numerous computational costs in the era of big data. With the help of the Deep Neural Network (DNN), integrating image compression and intelligent vision tasks at a feature representation level becomes a new promising approach. But how to perform non-linear transformation for image compression and extract image patterns for intelligent vision tasks simultaneously within a shared DNN remains an open problem. In this paper, a versatile framework is studied to explore the common feature representations for both image compression and classification. A fully shared latent representation is extracted in a more compact way to support compression and classification task. The General Feature Extraction and Feature-Analytic Classifier are proposed to generate and utilize shared latent representation. Then, the whole framework is joint optimized by considering multiple factors (i.e., rate, quality, and accuracy). Extensive experiments are carried out to validate that the proposals can improve the performance of both learning-based image compression and classification. The results show that the proposed method outperforms the conventional codecs like BPG and JPEG2000 in compression efficiency, while achieving acceptable accuracy on different image classification datasets without image reconstruction.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s40747-022-00744-3,en,Star topology convolution for graph representation learning,"['OriginalPaper', 'Original Article']","We present a novel graph convolutional method called star topology convolution (STC). This method makes graph convolution more similar to conventional convolutional neural networks (CNNs) in Euclidean feature spaces. STC learns subgraphs which have a star topology rather than learning a fixed graph like most spectral methods. Due to the properties of a star topology, STC is graph-scale free (without a fixed graph size constraint). It has fewer parameters in its convolutional filter and is inductive, so it is more flexible and can be applied to large and evolving graphs. The convolutional filter is learnable and localized, similar to CNNs in Euclidean feature spaces, and can share weights across graphs. To test the method, STC was compared with the state-of-the-art graph convolutional methods in a supervised learning setting on nine node properties prediction benchmark datasets: Cora, Citeseer, Pubmed, PPI, Arxiv, MAG, ACM, DBLP, and IMDB. The experimental results showed that STC achieved the state-of-the-art performance on all these datasets and maintained good robustness. In an essential protein identification task, STC outperformed the state-of-the-art essential protein identification methods. An application of using pretrained STC as the embedding for feature extraction of some downstream classification tasks was introduced. The experimental results showed that STC can share weights across different graphs and be used as the embedding to improve the performance of downstream tasks.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s41348-022-00660-1,en,Comparison of various deep convolutional neural network models to discriminate apple leaf diseases using transfer learning,"['OriginalPaper', 'Original Article']","Plant diseases are the major factor behind production loss in agriculture. The traditional manual methods for disease detection in plants involve expert knowledge that may be biased. The modern computing techniques and image processing can assist non-experts in plant disease management. Recently, deep learning techniques have observed remarkable success in image-based health assessment of plants. In this paper, the state-of-the-art pre-trained convolutional neural network (CNN) models are fine-tuned to detect and diagnose the diseases in apple crop using digital images. The experiments are performed on a publicly available dataset PlantVillage. The dataset consists of three classes of apple diseases including Scab, Black Rot, and Cedar Rust, and one class of Healthy leaves. The experimental results on ten well-known CNN models DenseNet201, DenseNet169, InceptionV3, InceptionResNetV2, MobileNet, MobileNetV2, ResNet50, VGG16, VGG19, and Xception showed that deep learning techniques can accurately discriminate the apple diseases. DenseNet201 outperformed the other models with an accuracy of 98.75%. The high accuracy shows that CNN-based methods could be a useful alternative to the conventional methods.","['Life Sciences', 'Plant Sciences', 'Plant Pathology', 'Plant Physiology']"
doi:10.1007/s00521-022-07633-3,en,An integrated spatiotemporal-based methodology for deepfake detection,"['OriginalPaper', 'Original Article']","Rapid advances in deep learning models have made it easier for public and crackers to generate hyper-realistic deepfake videos in which faces are swapped. Such deepfake videos may constitute a significant threat to the world if they are misused to blackmail public figures and to deceive systems of face recognition. As a result, distinguishing these fake videos from real ones has become fundamental. This paper introduces a new deepfake video detection method. You Only Look Once (YOLO) face detector is used to detect faces from video frames. A proposed hybrid method based on proposing two different feature extraction methods is applied to these faces. The first feature extraction method, a proposed Convolution Neural Network (CNN), is based on the Histogram of Oriented Gradient (HOG) method. The second one is an ameliorated XceptionNet CNN. The two extracted sets of features are merged together and fed as input to a sequence of Gated Recurrent Units (GRUs) to extract the spatial and temporal features and then individuate the authenticity of videos. The proposed method is trained on the CelebDF-FaceForencics++ (c23) dataset and evaluated on the CelebDF test set. The experimental results and analysis confirm the superiority of the suggested method over the state-of-the-art methods.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00034-022-02075-7,en,Cross-Lingual Self-training to Learn Multilingual Representation for Low-Resource Speech Recognition,OriginalPaper,"Representation learning or pre-training has shown promising performance for low-resource speech recognition which suffers from the data shortage. Recently, self-supervised methods have achieved surprising performance for speech pre-training by effectively utilizing large amount of un-annotated data. In this paper, we propose a new pre-training framework, Cross-Lingual Self-Training (XLST), to further improve the effectiveness for multilingual representation learning. Specifically, XLST first trains a phoneme classification model with a small amount of annotated data of a non-target language and then uses it to produce initial targets for training another model on multilingual un-annotated data, i.e., maximizing frame-level similarity between the output embeddings of two models. Furthermore, we employ the moving average and multi-view data augmentation mechanisms to better generalize the learned representations. Experimental results on downstream speech recognition tasks for 5 low-resource languages demonstrate the effectiveness of XLST. Specifically, leveraging additional 100 h of annotated English data for pre-training, the proposed XLST achieves a relative 24.8% PER reduction over the state-of-the-art self-supervised methods.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s12650-022-00849-4,en,Deep learning-assisted segmentation of bubble image shadowgraph,"['OriginalPaper', 'Regular Paper']","Global thresholding parameters for the semantic segmentation of bubbles from experimental bubble image shadowgraph were implemented. Traditional image processing algorithms for experimental visualization of multiphase flows require very rigorous and time-consuming trial by error of applying thresholding to be able to obtain the bubble statistics. More so, due to the varying flow conditions and lighting system during experimentation, it is impossible to apply a global threshold for in the post-processing the results of visualized flows. BIMSNet (modified U-Net architecture) was trained with bubble shadowgraph images obtained from experiments with varying flows and lightning conditions and developed global threshold parameters (binarization threshold) to semantically segment clustered bubbles with irregular shapes. The variation of pixel intensity of the sequence of images was taken into consideration in training the network. The average dice coefficient score (accuracy) of the network on the validation dataset was 99.3% with a 1.2% loss. Evaluation of the trained network on the test dataset gave an average precision and dice coefficient score of 99.73%, respectively. The detection of bubbles with the trained model when compared with the local average adaptive threshold image extraction process yields a higher bubble detection rate with less amount of misdetection and eliminates the trial-by-error method of obtaining the threshold limits for the binarization of images when post-processing images. Graphical abstract ","['Engineering', 'Engineering Fluid Dynamics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Classical and Continuum Physics', 'Engineering Thermodynamics, Heat and Mass Transfer']"
doi:10.1007/s00366-022-01716-4,en,An isogeometric analysis-based topology optimization framework for 2D cross-flow heat exchangers with manufacturability constraints,"['OriginalPaper', 'Original Article']","Heat exchangers (HXs) have gained increasing attention due to the intensive demand of performance improving and energy saving for various equipment and machines. As a natural application, topology optimization has been involved in the structural design of HXs aiming at improving heat exchange performance (HXP) and meanwhile controlling pressure drop (PD). In this paper, a novel multiphysics-based topology optimization framework is developed to maximize the HXP for 2D cross-flow HXs, and concurrently limit the PD between the fluid inlet and outlet. In particular, an isogeometric analysis solver is developed to solve the coupled steady-state Navier–Stokes and heat convection–diffusion equations. Non-body-fitted control mesh is adopted instead of dynamically remeshing the design domain during the evolution of the boundary interface. The method of moving morphable voids is employed to represent and track boundary interface between the hot and the remaining regions. In addition, various constraints are incorporated to guarantee manufacturability of the optimized structures with respect to practical considerations in additive manufacturing, such as removing sharp corners, controlling channel perimeters, and minimizing overhangs. To implement the iterative optimization process, the method of moving asymptotes is employed. Numerical examples show that the HXP of the optimized structure is greatly improved compared with its corresponding initial design, and the PD between the fluid inlet and outlet is controlled concurrently. Moreover, a smooth boundary interface between the channel and the cold fluid, and improved manufacturability are simultaneously obtained for the optimized structures.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s40747-022-00731-8,en,Student achievement prediction using deep neural network from multi-source campus data,"['OriginalPaper', 'Original Article']","Finding students at high risk of poor academic performance as early as possible plays an important role in improving education quality. To do so, most existing studies have used the traditional machine learning algorithms to predict students’ achievement based on their behavior data, from which behavior features are extracted manually thanks to expert experience and knowledge. However, owing to an increase in the varieties and overall volume of behavioral data, it has become more and more challenging to identify high-quality handcrafted features. In this paper, we propose an end-to-end deep learning model that automatically extracts features from students’ multi-source heterogeneous behavior data to predict academic performance. The key innovation of this model is that it uses long short-term memory networks to capture inherent time-series features for each type of behavior, and it takes two-dimensional convolutional networks to extract correlation features among different behaviors. We conducted experiments with four types of daily behavior data from students of the university in Beijing. The experimental results demonstrate that the proposed deep model method outperforms several machine learning algorithms.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s42044-022-00116-7,en,Avocado fruit disease detection and classification using modified SCA–PSO algorithm-based MobileNetV2 convolutional neural network,"['OriginalPaper', 'Research']","Classification of diseases in fruits is a comparatively multifaceted problem due to varieties, which include irregular shapes, colors, and textures. Deep learning plays a vital role in the classification of diseases from fruits. The agricultural productivity of fruit highly depends on the economic growth of Ethiopia. The production of avocado fruit enriches the economy of farmers in Ethiopia. Improper care causes severe possessions on avocado fruit productivity. The automatic classification of avocado diseases from the healthy fruit will reduce the extensive work of monitoring the big farms. This research presents a Modified Sine Cosine Algorithm–Particle Swarm Optimization (MSCA–PSO)-based MobileNetV2 Convolutional Neural Network (CNN) model for detecting and classifying avocado fruit diseases into healthy and non-healthy diseases from the images. This research considers the Avocado fruit disease image database as input to the proposed model. It is proposed to identify and detect the diseases by taking the high-resolution image database “Fruits 360 dataset” and real images collected from different agricultural farms in Ethiopia. Further, to show the robustness of the proposed hybrid MSCA–PSO algorithm, three benchmark functions, such as the Rastrigin function, Griewanks’s function, and Sphere function, are considered, and the results are presented. The proposed Modified SCA–PSO-based MobileNetV2 convolutional neural network model obtained an accuracy of 98.42% compared to the conventional CNN models, and the comparison results are presented.","['Computer Science', 'Software Engineering/Programming and Operating Systems', 'Theory of Computation', 'Mathematics of Computing', 'Artificial Intelligence', 'Computer Systems Organization and Communication Networks', 'Data Structures and Information Theory']"
doi:10.1007/s10596-022-10170-6,en,Design of optimal operational parameters for steam-alternating-solvent processes in heterogeneous reservoirs – A multi-objective optimization approach,"['OriginalPaper', 'Original Paper']","Steam injection is a widely-used process for heavy oil and bitumen recovery. However, a significant drawback is the excessive energy requirement, water consumption, and CO 2 generation. The Steam Alternating Solvent (SAS) process has been proposed as an eco-friendlier alternative to the existing steam-based methods. It involves injecting steam and solvent (i.e. propane) in separate cycles. The interplay between reservoir heterogeneity and the complex physical mechanisms of heat and mass transfer has made optimizing its design parameters quite challenging. This work aims to develop a hybrid Multi-Objective Optimization (MOO) scheme for determining the optimal operational parameters using the Pareto dominance concept while considering several conflicting objectives (i.e. RF , steam, and solvent injection) under several heterogeneous scenarios. A 2-D base homogenous reservoir model is built according to the Fort McMurray formation in the Athabasca region in Alberta, Canada. Next, shale barriers with varying proportions, lengths, and locations are superimposed onto the base model. Then, a sensitivity analysis is performed to assess the controllable operational parameters’ impact and formulate several objective functions; proxy models are introduced to speed up the objective function evaluations. Finally, different Multi-Objective Evolutionary Algorithms (MOEAs) are applied to establish the optimal ranges to operate the selected decision variables. Different optimal operating strategies are needed depending on the shale barrier distribution. Injector bottom-hole pressure, steam trap, and producer gas rate significantly impact the model response. Injecting high propane concentration over short durations is recommended. The length of the steam injection phase seems to be more sensitive to the reservoir heterogeneities; extended steam injection is needed for the more heterogeneous models. This paper is the first work comparing different MOEAs to optimize the SAS process using multiple heterogeneous models.","['Earth Sciences', 'Earth Sciences, general', 'Geotechnical Engineering & Applied Earth Sciences', 'Hydrogeology', 'Mathematical Modeling and Industrial Mathematics', 'Soil Science & Conservation']"
doi:10.1007/s11761-022-00342-8,en,Hybrid deep learning model for attack detection in internet of things,"['OriginalPaper', 'Original Research']","Internet of things (IoT) provides a new application, which helps the existing networks communicate with smart technologies. Things are now becoming increasingly connected to the Internet, and lots of new gadgets are being created at a faster rate. Since these interconnected smart objects are capable of interacting with one another in undefended surroundings, the entire communication ecology needs solutions related to security at various levels. Unlike the existing networks, IoT technology has its own set of features, including various network protocol requirements and a variety of resource constraints. To launch different attacks, the attacker takes many security vulnerabilities in the IoT system. The growth in cyber-attacks has rendered it important to address the consequences implied in the IoT. This paper intends to introduce a novel attack detection model. Originally, the input data are preprocessed, from which the most relevant features are extracted that include raw features, statistical features, and higher-order statistical features. The extracted features are subjected to the classification process. More importantly, the extracted raw features are directly given to the long short-term memory (LSTM), and the extracted statistical and higher-order statistical features are subjected to the deep reinforcement learning (DRL) for the classification process. Then, the average of both LSTM and DRL provides the detected output in an effective manner. To improve the performance of detection results, the weight of LSTM is optimized by the self-improved battle royale optimization (SIBRO) algorithm. At the end, the performance of the presented scheme is compared to the existing approaches in terms of different metrics like “F-measure, specificity, NPV, accuracy, FNR, sensitivity, precision, FPR, and MCC,” respectively.","['Computer Science', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'e-Commerce/e-business', 'Computer Appl. in Administrative Data Processing', 'Management of Computing and Information Systems', 'IT in Business']"
doi:10.1007/s11554-022-01252-w,en,Real-time traffic sign detection based on multiscale attention and spatial information aggregator,"['OriginalPaper', 'Original Research Paper']","Traffic sign detection, as an important part of intelligent driving, can effectively guide drivers to regulate driving and reduce the occurrence of traffic accidents. Currently, the deep learning-based detection methods have achieved very good performance. However, existing network models do not adequately consider the importance of lower-layer features for traffic sign detection. The lack of information on the lower-layer features is a major obstacle to the accurate detection of traffic signs. To solve the above problems, we propose a novel and efficient traffic sign detection method. First, we remove a prediction branch of the YOLOv3 network model to reduce the redundancy of the network model parameters and improve the real-time performance of detection. After that, we propose a multiscale attention feature module. This module fuses the feature information from different layers and refines the features to enhance the Feature Pyramid Network. In addition, we introduce a spatial information aggregator. This enables the spatial information of the lower-layer feature maps to be fused into the higher-layer feature maps. The robustness of our proposed method is further demonstrated by experiments on GTSDB, CCTSDB2021 and TT100k datasets. Specifically, the average execution time on CCTSDB2021 demonstrates the excellent real-time performance of our method. The experimental results show that the method has better accuracy than the original YOLOv3 and YOLOv5 network models.","['Computer Science', 'Image Processing and Computer Vision', 'Multimedia Information Systems', 'Computer Graphics', 'Pattern Recognition', 'Signal,Image and Speech Processing']"
doi:10.1007/s11063-022-10872-z,en,A dark image enhancement method based on multiscale features and dilated residual networks,OriginalPaper,"In low-light scenes, due to the limitations of ambient light and camera hardware equipment, the images captured by imaging devices often have low brightness, low contrast, high noise and loss of detail, which can cause great interference in face recognition, video surveillance and other application scenarios. Accordingly, a dark image enhancement method based on multi-scale features and dilated residual networks is proposed to solve the above problem. The input to the network is the V channel of the image HSV color space. The network uses a multiscale feature extractor to extract shallow features from the image, then a new dilated residual network constructed in this paper is used to extract deep features from the image, and finally the enhanced V-component is obtained by a single-channel convolutional layer. The final enhanced low-illumination image is obtained by component fusion in this paper. The experimental results show that compared with the existing mainstream algorithms, the algorithm in this paper has good subjective evaluation, natural image enhancement, no distortion in color, and high network robustness. In terms of objective metrics, the PSNR, SSIM, MSE, image mean value and image information entropy of the algorithms in this paper are significantly improved over other algorithms. Among them, in the dataset LOL with reference images, PSNR, SSIM and MSE are improved by about 61.55, 9.42 and 861.7% respectively. In the datasets Exdark and DARK FACE without reference images, the image mean and image information entropy are improved by about 20.728 and 2.31% respectively.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11633-022-1339-y,en,YOLOP: You Only Look Once for Panoptic Driving Perception,"['OriginalPaper', 'Research Article']","A panoptic driving perception system is an essential part of autonomous driving. A high-precision and real-time perception system can assist the vehicle in making reasonable decisions while driving. We present a panoptic driving perception network (you only look once for panoptic (YOLOP)) to perform traffic object detection, drivable area segmentation, and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. Our model performs extremely well on the challenging BDD100K dataset, achieving state-of-the-art on all three tasks in terms of accuracy and speed. Besides, we verify the effectiveness of our multi-task learning model for joint training via ablative studies. To our best knowledge, this is the first work that can process these three visual perception tasks simultaneously in real-time on an embedded device Jetson TX2(23 FPS), and maintain excellent accuracy. To facilitate further research, the source codes and pre-trained models are released at https://github.com/hustvl/YOLOP .","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s00158-022-03457-w,en,Multi-objective robust optimization using adaptive surrogate models for problems with mixed continuous-categorical parameters,"['OriginalPaper', 'Research Paper']","Explicitly accounting for uncertainties is paramount to the safety of engineering structures. Optimization which is often carried out at the early stage of the structural design offers an ideal framework for this task. When the uncertainties are mainly affecting the objective function, robust design optimization is traditionally considered. This work further assumes the existence of multiple and competing objective functions that need to be dealt with simultaneously. The optimization problem is formulated by considering quantiles of the objective functions which allows for the combination of both optimality and robustness in a single metric. By introducing the concept of common random numbers, the resulting nested optimization problem may be solved using a general-purpose solver, herein the non-dominated sorting genetic algorithm (NSGA-II). The computational cost of such an approach is however a serious hurdle to its application in real-world problems. We therefore propose a surrogate-assisted approach using Kriging as an inexpensive approximation of the associated computational model. The proposed approach consists of sequentially carrying out NSGA-II while using an adaptively built Kriging model to estimate the quantiles. Finally, the methodology is adapted to account for mixed categorical-continuous parameters as the applications involve the selection of qualitative design parameters as well. The methodology is first applied to two analytical examples showing its efficiency. The third application relates to the selection of optimal renovation scenarios of a building considering both its life cycle cost and environmental impact. It shows that when it comes to renovation, the heating system replacement should be the priority.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1007/s11517-022-02674-1,en,Neural-Symbolic Ensemble Learning for early-stage prediction of critical state of Covid-19 patients,"['OriginalPaper', 'Original Article']","Recently, Artificial Intelligence (AI) and Machine Learning (ML) have been successfully applied to many domains of interest including medical diagnosis. Due to the availability of a large quantity of data, it is possible to build reliable AI systems that assist humans in making decisions. The recent Covid-19 pandemic quickly spread over the world causing serious health problems and severe economic and social damage. Computer scientists are actively working together with doctors on different ML models to diagnose Covid-19 patients using Computed Tomography (CT) scans and clinical data. In this work, we propose a neural-symbolic system that predicts if a Covid-19 patient arriving at the hospital will end in a critical condition. The proposed system relies on Deep 3D Convolutional Neural Networks (3D-CNNs) for analyzing lung CT scans of Covid-19 patients, Decision Trees (DTs) for predicting if a Covid-19 patient will eventually pass away by analyzing its clinical data, and a neural system that integrates the previous ones using Hierarchical Probabilistic Logic Programs (HPLPs). Predicting if a Covid-19 patient will end in a critical condition is useful for managing the limited number of intensive care at the hospital. Moreover, knowing early that a Covid-19 patient could end in serious conditions allows doctors to gain early knowledge on patients and provide special treatment to those predicted to finish in critical conditions. The proposed system, entitled Neural HPLP, obtains good performance in terms of area under the receiver operating characteristic and precision curves with values of about 0.96 for both metrics. Therefore, with Neural HPLP, it is possible not only to efficiently predict if Covid-19 patients will end in severe conditions but also possible to provide an explanation of the prediction. This makes Neural HPLP explainable, interpretable, and reliable. Graphical abstract Representation of Neural HPLP. From top to bottom, the two different types of data collected from the same patient and used in this project are represented. This data feeds the two different machine learning systems and the integration of the two systems using Hierarchical Probabilistic Logic Program.","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s00366-021-01332-8,en,Prediction of ground vibration intensity in mine blasting using the novel hybrid MARS–PSO–MLP model,"['OriginalPaper', 'Original Article']","The present paper's primary goal is to propose a novel hybrid model with high reliability to predict peak particle velocity (PPV)—a ground vibration evaluation unit in mine blasting. This model is based on the coupling of the multivariate adaptive regression splines (MARS), particle swarm optimization (PSO), and multi-layer perceptron neural networks (MLP). To this end, a strategy of stacking the MARS models was applied. Multiple MARS models were developed first with different hyper-parameters. Subsequently, the outcome predictions from these MARS models were merged as a new data set. The MLP model was then developed based on the newly generated data set, called the MARS–MLP model. To improve the accuracy and reduction of the MARS–MLP model's error, the PSO algorithm was applied in terms of optimization of the MARS–MLP's weights, called the MARS–PSO–MLP model. The proposed MARS–PSO–MLP model was then compared with the stand-alone MARS, MLP, empirical models, and the hybrid PSO–MLP model (without stacking MARS models) using the same data set. The results revealed that the proposed strategies could significantly boost the MARS and MLP models' performance with the PSO algorithm's effective help. The proposed MARS–PSO–MLP model yielded the highest accuracy and reliability with a root-mean-squared error (RMSE) of 1.569, mean absolute error (MAE) of 1.017, and squared-correlation ( R 2 ) of 0.902. In comparison, the stand-alone models (i.e., MARS and MLP) and the hybrid model of PSO–MLP provided lower performances with an RMSE of 1.582 to 1.704, MAE of 0.941 to 1.427, and R 2 of 0.871 to 0.891. In contrast, poor performance with an RMSE of 5.059, MAE of 3.860, and R 2 of 0.127 was found for the empirical model, and it is not a reliable method to predict PPV in this study. This work's findings also indicated that explosive charge per delay, monitoring distance, spacing, powder factor, and burden have significant effects on PPV, the incredibly explosive charge per delay, and monitoring distance. Remarkable, the stemming variable has a minimal impact on PPV, and its role in the modeling of PPV is not exact.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11042-022-13287-z,en,Automated universal fractures detection in X-ray images based on deep learning approach,OriginalPaper,"At present, bone fracture is a common clinical disease, while the missed diagnosis or misdiagnosis of fracture is harmful to the recovery of patients. Fracture diagnosis often needs the X-ray image as an assistive tool and many fracture detection CAD systems on X-ray images have been explored. However, the majority of existing works mainly focus on detecting fractures in a specific human body part. It’s desirable and feasible to propose a more practical system that can detect various anatomical region fractures ideally due to their similar general fracture characteristics. In this paper, a universal fracture detection CAD system has been developed by us on X-ray images based on the deep learning method. Firstly, we design an image preprocessing method to improve the poor quality of these X-ray images and employ several data augmentation strategies to enlarge the used dataset. Secondly, based on our modified Ada-ResNeSt backbone network and the AC-BiFPN detection method, we propose our automatic fracture detection system. Finally, we establish a private universal fracture detection dataset MURA-D based on the public dataset MURA. As demonstrated by our comprehensive experiments, compared with other popular detectors, our method achieved a higher detection AP of 68.4% with an acceptable inference speed of 122 ms per image on the MURA-D test set, achieving promising results among the state-of-the-art detectors.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11263-022-01681-x,en,Delving into Inter-Image Invariance for Unsupervised Visual Representations,OriginalPaper,"Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since no pair annotations are available. In this work, we present a comprehensive empirical study to better understand the role of inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. To facilitate the study, we introduce a unified and generic framework that supports the integration of unsupervised intra- and inter-image invariance learning. Through carefully-designed comparisons and analysis, multiple valuable observations are revealed: 1) online labels converge faster and perform better than offline labels; 2) semi-hard negative samples are more reliable and unbiased than hard negative samples; 3) a less stringent decision boundary is more favorable for inter-image invariance learning. With all the obtained recipes, our final model, namely InterCLR, shows consistent improvements over state-of-the-art intra-image invariance learning methods on multiple standard benchmarks. We hope this work will provide useful experience for devising effective unsupervised inter-image invariance learning. Code: https://github.com/open-mmlab/mmselfsup .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00521-021-05972-1,en,Fully neural object detection solutions for robot soccer,"['OriginalPaper', 'S.I. : NCACVIP']","RoboCup is one of the major global AI events, gathering hundreds of teams from the world’s best universities to compete in various tasks ranging from soccer to home assistance and rescue. The commonality of these three seemingly dissimilar tasks is that in order to perform well, the robot needs to excel at the all major AI tasks: perception, control, navigation, strategy and planning. In this work, we focus on the first of these by presenting what is—to our knowledge—the first fully neural vision system for the Nao robot soccer. This is a challenging task, mainly due to the limited computational capabilities of the Nao robot. In this paper, we propose two novel neural network architectures for semantic segmentation and object detection that ensure low-cost inference, while improving accuracy by exploiting the properties of the environment. These models use synthetic transfer learning to be able to learn from a low number of hand-labeled images. The experiments show that our models outperform state-of-the-art methods such as Tiny YOLO at a fraction of the cost.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10032-022-00408-5,en,Textline alignment on the image domain,"['OriginalPaper', 'Special Issue Paper']","Editing and publishing a historical manuscript involves a research phase to recover the original manuscript and reconstruct the transmission of its text based on the relations between its surviving copies. Manuscript alignment , which aims to locate the shared and the different text among a set of copies of the same manuscript, is essential for this phase. In this paper, we present an alignment algorithm for historical handwritten documents that works directly on the image domain due to the absence of an accurate handwritten text recognition (HTR) system for handwritten historical documents and the necessity to visualize the original manuscripts in parallel to examine features beyond the transcribed text. Our approach extracts subwords, estimates the similarity among these subwords, and establishes an alignment among them. We extract subwords from textlines images and convert them into sequences of subword images. It estimates the similarity between two subwords using a Siamese network model and applies Longest Common Subsequence (LCS) to establish the alignment between two image sequences. We have implemented our algorithm, trained the Siamese model, and evaluate its performance using textline images from historical documents. Our algorithm outperformed the state-of-the-art by large margins. Unlike the state-of-the-art, the framework builds the alignment from scratch without requiring any prior knowledge concern subwords boundaries. In addition, we build a new dataset for textline alignment for historical documents, which include ten pairs of pages taken from two copies of two Arabic manuscripts and annotated at the subword level.","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s00354-022-00172-4,en,COVID-19 Detection on Chest X-ray Images with the Proposed Model Using Artificial Intelligence and Classifiers,OriginalPaper,"Coronavirus disease-2019 (COVID-19) is a serious infectious disease that is spreading rapidly all over the world. Scientists are looking for alternative diagnostic methods to detect and control the disease early. Artificial intelligence applications are promising in the COVID-19 epidemic. This paper proposes a hybrid approach for diagnosing COVID-19 on chest X-ray images and differentiation from other viral pneumonia. The model we propose consists of three steps. In the first step, classification was made using the MobilenetV2, Efficientnetb0, and Darknet53 deep models. In the second step, the feature maps of the images in the Chest X-ray data set were extracted separately for each architecture using the MobilenetV2, Efficientnetb0, and Darknet53 architectures. NCA method was preferred to reduce the size of these feature maps obtained. The feature maps obtained after dimension reduction were classified in the classic machine learning classifiers. In the third step, the feature maps obtained from each architecture were combined. After dimension reduction was applied to these combined features by applying the NCA method, this feature map is classified in the classifiers. The model we proposed was tested on two different data sets. The accuracy values obtained in these data sets are 99.05 and 97.1%, respectively. The obtained accuracy values show that the model is successful.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11042-022-13256-6,en,Residual UNet with spatial and channel attention for automatic magnetic resonance image segmentation of rectal cancer,OriginalPaper,"The precise segmentation of rectal tumors is a key step in the diagnosis and treatment of rectal cancer. This paper aims to study the automatic segmentation task of rectal tumors based on deep learning methods, and proposes a residual UNet network model that combines spatial attention and channel attention. The model uses residual convolution for feature extraction, and uses squeeze-and-excitation module and attention gating module to focus on more useful features. In this study, we established a rectal tumor dataset for model evaluation, and used a combination of two-class cross-entropy and DICE loss function in the training process. Comparative experiments show that the DICE similarity coefficient is 0.8476, the Hausdorff distance reaches 9.5622, the prediction accuracy of the model is 0.9938, and the evaluation indicators are better than the segmentation results of UNet and AttUNet, which can effectively segment the rectal tumor area, and the combined loss function can also improve the segmentation accuracy by about 15% to a certain extent.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10278-022-00666-z,en,Analyzing Transfer Learning of Vision Transformers for Interpreting Chest Radiography,"['OriginalPaper', 'Original Paper']","Limited availability of medical imaging datasets is a vital limitation when using “data hungry” deep learning to gain performance improvements. Dealing with the issue, transfer learning has become a de facto standard, where a pre-trained convolution neural network (CNN), typically on natural images (e.g., ImageNet), is finetuned on medical images. Meanwhile, pre-trained transformers, which are self-attention-based models, have become de facto standard in natural language processing (NLP) and state of the art in image classification due to their powerful transfer learning abilities. Inspired by the success of transformers in NLP and image classification, large-scale transformers (such as vision transformer) are trained on natural images. Based on these recent developments, this research aims to explore the efficacy of pre-trained natural image transformers for medical images. Specifically, we analyze pre-trained vision transformer on CheXpert and pediatric pneumonia dataset. We use CNN standard models including VGGNet and ResNet as baseline models. By examining the acquired representations and results, we discover that transfer learning from the pre-trained vision transformer shows improved results as compared to pre-trained CNN which demonstrates a greater transfer ability of the transformers in medical imaging.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s00419-022-02273-4,en,A viscoelastic Mooney–Rivlin model for adhesive curing and first steps toward its calibration based on photoelasticity measurements,"['OriginalPaper', 'Original']","The transition of polymer adhesives from an initially liquid to a fully cured viscoelastic state is accompanied by three phenomenological effects, namely an increase in stiffness and viscosity in conjunction with a decrease in volume (curing shrinkage). Under consideration of these phenomena, some of us (Hossain et al. in Computational Mechanics 46:363-375, 2010) have devised a generic, viscoelastic finite strain framework for the simulation of the curing process of adhesives, which renders a thermodynamically consistent model regardless of the selected free energy density. In the present work, this generic curing framework is modified by means of more precise integration schemes and is applied to a hyperelastic Mooney–Rivlin material based on an additive volumetric-isochoric split of the strain energy density. The benefit of this decomposition is directly related to the distinct material responses of various polymers to volumetric and isochoric deformations [ 4 ]. The resulting Mooney–Rivlin curing model provides the foundation for implementing a user-defined material subroutine ( UMAT ) in Abaqus requiring the Cauchy stress and a non-standard formulation of the tangent operator. To this end, the corresponding transformations are presented. Additionally, a first attempt to determine the evolution of the curing-dependent material parameters through optimization with respect to a photoelasticity measurement is presented. A subset of the material properties, which reflect the emergence of shrinkage stresses inside a ceramic-epoxy composite after its fabrication, is determined via inverse parameter identification. However, due to a lack of experimental data and some rather strong assumptions made on the physics involved, this demonstration can currently be considered only as a proof-of-concept.","['Engineering', 'Theoretical and Applied Mechanics', 'Classical Mechanics']"
doi:10.1007/s00521-022-07602-w,en,Daytime sea fog monitoring using multimodal self-supervised learning with band attention mechanism,"['OriginalPaper', 'Original Article']","Sea fog is a dangerous weather phenomenon that seriously affects maritime traffic and other operations at sea. The conventional sea fog detection methods are not only difficult to make full advantage of the multispectral information of cloud images, but also deficient in the exploration of deep-level semantic information, leading to poor detection results. In this paper, we proposed a multimodal self-supervised convolutional neural network incorporating intra-modal band attention mechanism (MSCNN-IBAM) based on multispectral images of Himawari-8. MSCNN-IBAM uses independent branches to extract features from different modality cloud images and characterize the importance of each band through attention mechanisms. Simultaneously, multimodal self-supervised learning and supervised learning are effectively combined to optimize the model by constructing a two-tuple trainset. Experimental results show the accuracy, precision, recall, and F1 score of the proposed method as 97.72%, 95.84%, 96.54%, and 96.08%, respectively, which have the competitive performance and acceptable computational efficiency. And the additional analysis of sea fog cases shows that the proposed method is not only effective in identifying sea fog, but also has the ability to locate sea fog regions.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11042-020-10482-8,en,ChroNet: A multi-task learning based approach for prediction of multiple chronic diseases,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","Chronic diseases (such as diabetes, hypertension, etc) are generally of long duration and slow progression. These diseases may be implied in electronic medical records (EMR), and one chronic disease may be accompanied by another. Recently, many methods have been proposed for chronic disease prediction and early detection. However, previous methods mainly focused on predicting one individual disease, thus possibly neglecting potential correlations among multiple diseases. In this paper, we propose a new framework for chronic disease prediction which can take into account possible correlations among multiple chronic diseases, called ChroNet. We propose a Multi-task Learning (MTL) based framework, for multiple chronic disease prediction. First, based on the characteristics of EMR, we introduce a novel approach for data embedding, including Content Embedding and Spatial Embedding. Then, an MTL convolutional neural network (CNN) is designed to perform multiple chronic disease prediction simultaneously. We collect a dataset from 5 local hospitals, involving 48953 patients’ records. Then we conduct abundant experiments for hypertension and type 2 diabetes prediction, based on our dataset. For both hypertension and type 2 diabetes prediction, our proposed framework outperforms known single-task models (with the same CNN layers yet a single branch). Further, our MTL-based framework outperforms several most commonly used traditional machine learning models and convolutional neural networks. Theoretically, our framework can capture general features of different diseases and focus its attention on those features that actually matter for each disease. The performance superiority in experiments indicates that our framework may be able to capture more detailed characteristics of medical structural data after specific embedding, comparing with known single-task models.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40262-022-01180-9,en,DeepIDC: A Prediction Framework of Injectable Drug Combination Based on Heterogeneous Information and Deep Learning,"['OriginalPaper', 'Original Research Article']","Background and Objective In clinical practice, injectable drug combination (IDC) usually provides good therapeutic effects for patients. Numerous clinical studies have directly indicated that inappropriate IDC generates adverse drug events (ADEs). The clinical application of injections is increasing, and many injections lack relevant combination information. It is still a significant need for experienced clinical pharmacists to participate in evidence-based drug decision making, monitor medication safety, and manage drug interactions. Meanwhile, a large number of injection pairs and dosage combinations limit exhaustive screening. Here, we present a prediction framework, called DeepIDC, that can expediently screen the feasibility of IDCs using heterogeneous information with deep learning. This is the first specific prediction framework to identify IDCs. Methods Since the interaction between the injected drugs may occur in the direct physical and chemical reactions at the time of mixing or may be the indirect interaction of their drug targets and pathways, we used molecular fingerprints, drug-target associations, and drug-pathway associations to convert injections into a string of digital vectors. Then, based on these injection vectors, we combined a bidirectional long short-term memory and a feed-forward neural network to build a prediction model for accurate and instructive prediction of IDC. Results In three realistic evaluation scenarios, DeepIDC has achieved ideal prediction results. Furthermore, compared with the other five machine-learning methods, the proposed predictor is more efficient and robust. Among the top 30 potential IDCs of each IDC class predicted by DeepIDC, we found that 9 cases were experimentally verified in the literature or available on Drug.com. Conclusion The information we extracted in vivo and in vitro can effectively characterize injectable drugs. DeepIDC developed based on deep learning algorithm provides a valuable unified framework for new IDC discovery, which can make up for the lack of IDC information and predict potential IDC events.","['Medicine & Public Health', 'Pharmacotherapy', 'Pharmacology/Toxicology', 'Internal Medicine']"
doi:10.1007/s10479-022-05085-5,en,Multiobjective centralized DEA approach to Tokyo 2020 Olympic Games,"['OriginalPaper', 'Original Research']","There exist two types of Data Envelopment Analysis (DEA) approaches to the Olympic Games: conventional and fixed-sum outputs (FSO). The approach proposed in this paper belongs to the latter category as it takes into account the total number de medals of each type awarded. Imposing these constraints requires a centralized DEA perspective that projects all the countries simultaneously. In this paper, a multiobjective FSO approach is proposed, and the Weighted Tchebychef solution method is employed. This approach aims to set all output targets as close as possible to their ideal values. In order to choose between the alternative optima, a secondary goal has been considered that minimizes the sum of absolute changes in the number of medals, which also renders the computed targets to be as close to the observed values as possible. These targets represent the output levels that could be expected if all countries performed at their best level. For certain countries, the targets are higher than the actual number of medals won while, for other countries, these targets may be lower. The proposed approach has been applied to the results of the Tokyo 2020 Olympic Games and compared with both FSO and non-FSO DEA methods.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s00521-022-07653-z,en,Automatic segmentation of COVID-19 from computed tomography images using modified U-Net model-based majority voting approach,"['OriginalPaper', 'Original Article']","The coronavirus disease (COVID-19) is an important public health problem that has spread rapidly around the world and has caused the death of millions of people. Therefore, studies to determine the factors affecting the disease, to perform preventive actions and to find an effective treatment are at the forefront. In this study, a deep learning and segmentation-based approach is proposed for the detection of COVID-19 disease from computed tomography images. The proposed model was created by modifying the encoder part of the U-Net segmentation model. In the encoder part, VGG16, ResNet101, DenseNet121, InceptionV3 and EfficientNetB5 deep learning models were used, respectively. Then, the results obtained with each modified U-Net model were combined with the majority vote principle and a final result was reached. As a result of the experimental tests, the proposed model obtained 85.03% Dice score, 89.13% sensitivity and 99.38% specificity on the COVID-19 segmentation test dataset. The results obtained in the study show that the proposed model will especially benefit clinicians in terms of time and cost.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10596-022-10168-0,en,Lithology identification in carbonate thin section images of the Brazilian pre-salt reservoirs by the computational vision and deep learning,"['OriginalPaper', 'Original Paper']","Currently, the computer vision area, which represents one of the subfields of artificial intelligence and machine learning, has been widely used to process data in the oil and gas industry. In this context, the detection of specific properties inside carbonate rocks in different datasets from petroleum reservoirs represents a considerable challenge, that consumes enormous resources and time. Therefore, the automatic separation of the lithologies within rocks of reservoirs has attracted the increasing attention of many research groups. The consistent classification of these lithologies is the main factor for the construction of reliable depositional, diagenetic, and reservoir models. This work deals with this last issue by presenting the development of a technique for the automatic classification of carbonate thin sections obtained from plane-polarized and cross-polarized microscopy images corresponding to natural rocks belonging to the Brazilian pre-salt reservoir. Our proposed model transforms the analyzed images into structured data by defining texture parameters (Haralick parameters), and Wavelets transforms. Later, a stacked autoencoder neural network is used to eliminate images with anomalies and/or distortions in order to define relevant characteristics of the data. This stage is followed by supervised classifier called multilayer feed-forward neural network. The definition of the model’s hyperparameters is tuned by Bayesian optimization and the Gaussian process. For training and testing of the network, images of 570 thin sections were used (each image obtained with plane-polarized and cross-polarized light) totaling 1140 images. Our model reported an accuracy of 83% for the test samples, confirming the validity of the proposed model in the automatic classification of carbonate rocks.","['Earth Sciences', 'Earth Sciences, general', 'Geotechnical Engineering & Applied Earth Sciences', 'Hydrogeology', 'Mathematical Modeling and Industrial Mathematics', 'Soil Science & Conservation']"
doi:10.1007/s11235-022-00965-4,en,Adaptive modulation and coding using deep recurrent neural network,OriginalPaper,"Adaptive Modulation and Coding (AMC) is a promising technique to increase the average spectral efficiency of communication links. This research proposes a novel AMC method based on a supervised deep learning approach to maximize the average spectral efficiency of OFDM wireless systems while the bit error rate (BER) remains under a predefined threshold. The proposed method consists of a one-dimensional convolutional network that performs feature extraction and a long short-term memory network that learns the behavior of the channel. Input features are the magnitudes and phases of the estimated channel frequency response in the pilot subcarriers and signal-to-noise ratio. Datasets of various fading channel responses were generated using WINNER II. The proposed method was compared with previous methods based on different criteria, including average spectral efficiency, BER, the accuracy of predictions, the average delay of each prediction, and model complexity. The simulation results confirmed the superiority of the proposed AMC method.","['Business and Management', 'IT in Business', 'Computer Communication Networks', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes']"
doi:10.1007/s00521-021-05982-z,en,Synthetic Data generation using DCGAN for improved traffic sign recognition,"['OriginalPaper', 'S.I. : NCACVIP']","Traffic sign detection and recognition perform a vital function in real-world driver guidance applications, including driver assistance systems. Research into vision-based traffic sign detection (TSD) and traffic sign recognition (TSR) has gained considerable attention in the scientific community, led mainly by three variables: identification, monitoring, and classification. In addition, TSR provides valuable details and alerts for smart cars including advanced driving assistance (ADAS) and cooperative intelligent transport systems (CITS). Our work will generate high-quality synthetic prohibitory sign images using deep convolutional generative adversarial networks (DCGAN). This paper analyzes and discusses CNN models incorporating different backbone architectures and feature extractors, focusing on Resnet 50 and Densenet for object detection. Assessment of the models provides important information, including mean average accuracy (mAP), workspace capacity, detection period, and the amount of billion floating-point operations (BFLOPS). The maximum average accuracy is 92% (Densenet DCGAN), led by 91% (Resnet 50 DCGAN), 88% (Densenet), and 63% (Resnet 50). We find when using the original image and a synthetic image, accuracy increases, while detection time falls. Our findings show that combining original images and synthetic images in the dataset for training can improve intersection over union (IoU) and traffic sign recognition performance.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11263-022-01683-9,en,Hybrid Warping Fusion for Video Frame Interpolation,OriginalPaper,"Video frame interpolation aims to synthesize new intermediate frames between existing ones, which is an important task in video enhancement. A classic direction in this field is flow-based which estimates motions in the form of optical flow, warps the frames, and synthesizes the final results. In this work, we explicitly investigate the warping step and propose a way to combine the strength from using both forward and backward warping. Our method, named HWFI, introduces hybrid warping fusion for frame interpolation. We also include edge information explicitly in our pipeline and employ channel attention in our synthesis network. Compared to the latest state-of-the-art method that only uses forward warping, our method produces better results with higher quality, especially in edge regions. Extensive experiments show that our method can obtain the best results qualitatively and quantitatively on multiple benchmark datasets.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s41095-022-0279-3,en,Self-supervised coarse-to-fine monocular depth estimation using a lightweight attention module,"['OriginalPaper', 'Research Article']","Self-supervised monocular depth estimation has been widely investigated and applied in previous works. However, existing methods suffer from texture-copy, depth drift, and incomplete structure. It is difficult for normal CNN networks to completely understand the relationship between the object and its surrounding environment. Moreover, it is hard to design the depth smoothness loss to balance depth smoothness and sharpness. To address these issues, we propose a coarse-to-fine method with a normalized convolutional block attention module (NCBAM). In the coarse estimation stage, we incorporate the NCBAM into depth and pose networks to overcome the texture-copy and depth drift problems. Then, we use a new network to refine the coarse depth guided by the color image and produce a structure-preserving depth result in the refinement stage. Our method can produce results competitive with state-of-the-art methods. Comprehensive experiments prove the effectiveness of our two-stage method using the NCBAM.","['Computer Science', 'Computer Graphics', 'User Interfaces and Human Computer Interaction', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s10514-022-10058-5,en,Method of evolving junction on optimal path planning in flows fields,OriginalPaper,"We propose an algorithm using method of evolving junctions to solve the optimal path planning problems with piece-wise constant flow fields. In such flow fields, we prove that the optimal trajectories, with respect to a convex Lagrangian in the objective function, must be formed by piece-wise constant velocity motions. Taking advantage of this property, we transform the infinite dimensional optimal control problem into a finite dimensional optimization and use intermittent diffusion to solve the problems. The algorithm is proven to be complete. At last, we demonstrate the performance of the algorithm with various simulation examples.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00521-022-07537-2,en,Calliar: an online handwritten dataset for Arabic calligraphy,"['OriginalPaper', 'Original Article']","Calligraphy is an essential part of the Arabic heritage and culture. It has been used in the past for the decoration of houses and mosques. Usually, such calligraphy is designed manually by experts with aesthetic insights. In the past few years, there has been a considerable effort to digitize such type of art by either taking a photograph of decorated buildings or drawing them using digital devices. The latter is considered an online form where the drawing is tracked by recording the apparatus movement, an electronic pen, for instance, on a screen. In the literature, there are many offline datasets with diverse Arabic styles for calligraphy. However, there is no available online dataset for Arabic calligraphy. In this paper, we illustrate our approach for collecting and annotating an online dataset for Arabic calligraphy called Calliar, which consists of 2,500 sentences. Calliar is annotated for stroke, character, word, and sentence-level prediction. We also propose various baseline models for the character classification task. The results we achieved highlight that it is still an open problem.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00530-020-00747-5,en,Multimodal cyberbullying detection using capsule network with dynamic routing and deep convolutional neural network,"['OriginalPaper', 'Special Issue Paper']","Cyberbullying is the use of information technology networks by individuals’ to humiliate, tease, embarrass, taunt, defame and disparage a target without any face-to-face contact. Social media is the 'virtual playground' used by bullies with the upsurge of social networking sites such as Facebook, Instagram, YouTube and Twitter. It is critical to implement models and systems for automatic detection and resolution of bullying content available online as the ramifications can lead to a societal epidemic. This paper presents a deep neural model for cyberbullying detection in three different modalities of social data, namely textual, visual and info-graphic (text embedded along with an image). The all-in-one architecture, CapsNet–ConvNet, consists of a capsule network (CapsNet) deep neural network with dynamic routing for predicting the textual bullying content and a convolution neural network (ConvNet) for predicting the visual bullying content. The info-graphic content is discretized by separating text from the image using Google Lens of Google Photos app. The perceptron-based decision-level late fusion strategy for multimodal learning is used to dynamically combine the predictions of discrete modalities and output the final category as bullying or non-bullying type. Experimental evaluation is done on a mix-modal dataset which contains 10,000 comments and posts scrapped from YouTube, Instagram and Twitter. The proposed model achieves a superlative performance with the AUC–ROC of 0.98.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1186/s12859-022-05070-6,en,ENTAIL: yEt aNoTher amyloid fIbrils cLassifier,"['OriginalPaper', 'Research']","Background This research aims to increase our knowledge of amyloidoses. These disorders cause incorrect protein folding, affecting protein functionality (on structure). Fibrillar deposits are the basis of some wellknown diseases, such as Alzheimer, Creutzfeldt–Jakob diseases and type II diabetes. For many of these amyloid proteins, the relative precursors are known. Discovering new protein precursors involved in forming amyloid fibril deposits would improve understanding the pathological processes of amyloidoses. Results A new classifier, called ENTAIL, was developed using over than 4000 molecular descriptors. ENTAIL was based on the Naive Bayes Classifier with Unbounded Support and Gaussian Kernel Type, with an accuracy on the test set of 81.80%, SN of 100%, SP of 63.63% and an MCC of 0.683 on a balanced dataset. Conclusions The analysis carried out has demonstrated how, despite the various configurations of the tests, performances are superior in terms of performance on a balanced dataset.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s11517-022-02677-y,en,Integrated Bayesian and association-rules methods for autonomously orienting COVID-19 patients,"['OriginalPaper', 'Original Article']","The coronavirus infection continues to spread rapidly worldwide, having a devastating impact on the health of the global population. To fight against COVID-19, we propose a novel autonomous decision-making process which combines two modules in order to support the decision-maker: (1) Bayesian Networks method–based data-analysis module, which is used to specify the severity of coronavirus symptoms and classify cases as mild, moderate, and severe, and (2) autonomous decision-making module–based association rules mining method. This method allows the autonomous generation of the adequate decision based on the FP-growth algorithm and the distance between objects. To build the Bayesian Network model, we propose a novel data–based method that enables to effectively learn the network’s structure, namely, MIGT-SL algorithm. The experimentations are performed over pre-processed discrete dataset. The proposed algorithm allows to correctly generate 74%, 87.5%, and 100% of the original structure of ALARM, ASIA, and CANCER networks. The proposed Bayesian model performs well in terms of accuracy with 96.15% and 94.77%, respectively, for binary and multi-class classification. The developed decision-making model is evaluated according to its utility in solving the decisional problem, and its accuracy of proposing the adequate decision is about 97.80%. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s11042-022-13122-5,en,A new deep learning architecture for dehazing of aerial remote sensing images,OriginalPaper,"A major problem in most aerial remote image processing applications is the presence of haze in images. It is a phenomenon by which particles in the atmosphere disperse light, thus altering the quality of the overall image. This can be detrimental to the performance of vision-based algorithms such as those concerned with object detection. There have been numerous attempts using traditional image processing techniques as well as using deep learning approaches to eliminate this haze. In most cases, models tend to make assumptions on the nature of haze that are rarely true in reality. In this paper, we propose an end-to-end deep learning architecture that can dehaze aerial remote sensing images efficiently with minimal deviation from the ground truth. Many of the assumptions made in other models are eliminated and the relationship between hazed and dehazed images is directly computed. The proposed model is based on the observation that identifying structural and statistical portions separately from an image and using those features to reconstruct the image can give a realistic dehazed image. It also makes use of information exposed by different color spaces to achieve this using lesser computation. The experimental quantitative and qualitative results of the proposed architecture are compared with recent benchmark dehaze models on NYU hazy dataset and real-world hazy images. Experimental results yield that the proposed architecture outperforms benchmark models on test aerial remote sensing images.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-07612-8,en,Multi-scale error feedback network for low-light image enhancement,"['OriginalPaper', 'Original Article']","Low-light image enhancement is a challenging task because brightness, contrast, noise and other factors must be considered simultaneously. However, most of the existing studies focus on improving illumination, and it is difficult to obtain natural images when the images of complex scenes are enhanced. To address this issue, we propose a neural network—a multi-scale error feedback network (MSEFN)—to enhance low-light images. The proposed network consists of an error feedback encoder module (EFEM), an error feedback decoder module (EFDM) and a feature integration module (FIM). As the main component of EFEM and EFDM, the error feedback feature extraction module can effectively retain spatial information by using the shuffle attention fusion block (SAFB) to fuse the acquired multi-scale features and nonadjacent features. FIM has the ability to capture contextual information that can compensate for the lack of global features in the network. Furthermore, the local uneven illumination (LUI) dataset and polynomial loss function constructed in this paper make our network more stable. Extensive experiments demonstrate that the proposed network outperforms state-of-the-art methods both qualitatively and quantitatively. The LUI dataset is publicly available at: https://github.com/Qyizos/LUI-dataset .","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11633-022-1371-y,en,Video Polyp Segmentation: A Deep Learning Perspective,"['OriginalPaper', 'Research Article']","We present the first comprehensive video polyp segmentation (VPS) study in the deep learning era. Over the years, developments in VPS are not moving forward with ease due to the lack of a large-scale dataset with fine-grained segmentation annotations. To address this issue, we first introduce a high-quality frame-by-frame annotated VPS dataset, named SUN-SEG, which contains 158 690 colonoscopy video frames from the well-known SUN-database. We provide additional annotation covering diverse types, i.e., attribute, object mask, boundary, scribble, and polygon. Second, we design a simple but efficient baseline, named PNS+, which consists of a global encoder, a local encoder, and normalized self-attention (NS) blocks. The global and local encoders receive an anchor frame and multiple successive frames to extract long-term and short-term spatial-temporal representations, which are then progressively refined by two NS blocks. Extensive experiments show that PNS+ achieves the best performance and real-time inference speed (170 fps), making it a promising solution for the VPS task. Third, we extensively evaluate 13 representative polyp/object segmentation models on our SUN-SEG dataset and provide attribute-based comparisons. Finally, we discuss several open issues and suggest possible research directions for the VPS community. Our project and dataset are publicly available at https://github.com/GewelsJI/VPS .","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s11263-022-01684-8,en,NormAttention-PSN: A High-frequency Region Enhanced Photometric Stereo Network with Normalized Attention,OriginalPaper,"Photometric stereo aims to recover the surface normals of a 3D object from various shading cues, establishing the relationship between two-dimensional images and the object geometry. Traditional methods usually adopt simplified reflectance models to approximate the non-Lambertian surface properties, while recently, photometric stereo based on deep learning has been widely used to deal with non-Lambertian surfaces. However, previous studies are limited in dealing with high-frequency surface regions, i.e. , regions with rapid shape variations, such as crinkles, edges, etc. , resulted in blurry reconstructions. To alleviate this problem, we present a normalized attention-weighted photometric stereo network, namely NormAttention-PSN, to improve surface orientation prediction, especially for those complicated structures. In order to address these challenges, in this paper, we (1) present an attention-weighted loss to produce better surface reconstructions, which applies a higher weight to the detail-preserving gradient loss in high-frequency areas, (2) adopt a double-gate normalization method for non-Lambertian surfaces, to explicitly distinguish whether the high-frequency representation is stimulated by surface structure or spatially varying reflectance, and (3) adopt a parallel high-resolution structure to generate deep features that can maintain the high-resolution details of surface normals. Extensive experiments on public benchmark data sets show that the proposed NormAttention-PSN significantly outperforms traditional calibrated photometric stereo algorithms and state-of-the-art deep learning-based methods.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s10845-021-01777-0,en,A mixed adversarial adaptation network for intelligent fault diagnosis,OriginalPaper,"Behind the brilliance of the deep diagnosis models, the issue of distribution discrepancy between source training data and target test data is being gradually concerned for catering to more practical and urgent diagnostic requirements. Consequently, advanced domain adaptation algorithms have been introduced to the field of fault diagnosis to address this problem. However, in performing domain adaptation, most existing diagnosis methods only focus on the minimization of marginal distribution divergences and do not consider conditional distribution differences at the same time. In this paper, we present a mixed adversarial adaptation network (MAAN) based intelligent framework for cross-domain fault diagnosis of machinery. In this approach, differences in marginal distribution and conditional distribution are reduced together by the adversarial learning strategy, moreover, a simple adaptive factor is also endowed to dynamically weigh the relative importance of two distributions. Extensive experiments on two kinds of mechanical equipment, i.e. planetary gearbox and rolling bearing, are built to validate the proposed method. Empirical evidence demonstrates that the proposed model outperforms popular deep learning and deep domain adaptation diagnosis methods.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11600-022-00759-x,en,Identifying region specific seasonal crop for leaf borne diseases by utilizing deep learning techniques,"['OriginalPaper', 'Research Article - Special Issue']","India economy depends on agriculture with severe climatic changes and a heavy infestation of diseases depleting food crop yield substantially. Rapid identification and real-time infestation feedback that affects plants are accomplished through computer vision and IoT, thereby providing a reliable system for farmers to increase the season’s growth yield. With LSTM, CNN provides an efficient way of identifying diseases specific leaf in plants through image recognition techniques. An extensive collection of plant leaf images is trained to recognize season-specific diseases like early blight and late blight, leaf mold, and yellow leaf curl. The proposed CNN model identifies the infestation with high accuracy and precision with significantly fewer training epochs. The proposed model provides an efficient way of identifying leaf borne infestation pertained to a particular agricultural region. Furthermore, there is a need to increase and improve different region-specific infestations that arise due to climatic and seasonal changes.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s12539-022-00536-w,en,scVAEBGM: Clustering Analysis of Single-Cell ATAC-seq Data Using a Deep Generative Model,"['OriginalPaper', 'Original research article']","A surge in research has occurred because of current developments in single-cell technologies. Above all, single-cell Assay for Transposase-Accessible Chromatin with high throughput sequencing (scATAC-seq) is a popular approach of analyzing chromatin accessibility differences at the level of single cell, either within or between groups. As a result, it is critical to examine cell heterogeneity at a previously unseen level and to identify both recognized and unknown cell types. However, with the ever-increasing number of cells engendered by technological development and the characteristics of the data, such as high noise, sparsity and dimension, challenges in distinguishing cell types have emerged. We propose scVAEBGM, which integrates a Variational Autoencoder (VAE) with a Bayesian Gaussian-mixture model (BGM) to process and analyze scATAC-seq data. This method combines and takes benefits of a Bayesian Gaussian mixture model to estimate the number of cell types without determining the cluster number in a beforehand. In other words, the size of the clusters is inferred from the data, thus avoiding biases introduced by subjective assessments when manually determining the size of the clusters. Additionally, the method is more robust to noise and can better represent single-cell data in lower dimensions. We also create a further clustering strategy. It is indicated by experiments that further clustering based on the already completed clustering can improve the clustering accuracy again. We test on six public datasets, and scVAEBGM outperforms various dimension reduction baselines. In downstream applications, scVAEBGM can reveal biological cell types. Graphical abstract ","['Life Sciences', 'Computer Appl. in Life Sciences', 'Computational Biology/Bioinformatics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Theoretical and Computational Chemistry', 'Theoretical, Mathematical and Computational Physics', 'Computational Science and Engineering']"
doi:10.1007/s11664-022-09980-2,en,"The Structural, Electronic and Optical Properties of the AlAs/InP/CdS Heterotrilayer: A First-Principles Study","['OriginalPaper', 'Original Research Article']","The structural stability, electrical properties (energy band structure, density of states) and optical properties of AlAs/InP/CdS multilayer heterojunctions have been systematically investigated using first-principles calculations based on density functional theory. The A-type structure in this paper is the most stable, with a direct band gap of 0.477 eV. The heterojunction is strongly bonded due to the aggregation of electrons among all three layers of atoms, the formation of van der Waals forces between the atoms and the formation of orbital hybridization among some of the atoms. The optoelectronic properties of multilayer heterojunctions are superior to those of single layers. AlAs/InP/CdS multilayer heterojunctions have high absorption peaks in the UV and visible regions and also have good absorption capability in the infrared. The AlAs/InP/CdS multilayer heterojunction has a stable structure and excellent optoelectronic properties, and has great potential and broad application prospects in the fields of visible light absorption, photocatalysis, infrared light and ultraviolet light detection.","['Materials Science', 'Optical and Electronic Materials', 'Characterization and Evaluation of Materials', 'Electronics and Microelectronics, Instrumentation', 'Solid State Physics']"
doi:10.1007/s11869-022-01252-6,en,An ensemble LSTM-based AQI forecasting model with decomposition-reconstruction technique via CEEMDAN and fuzzy entropy,OriginalPaper,"Air quality affects people’s daily life. Air quality index (AQI) is an essential indicator for controlling air pollution and ensuring public health, whose accurate forecasting can provide timely air pollution warnings and remind people to take protective measures against air pollution in advance. To address this issue, this paper developed a new ensemble learning model for AQI forecasting. In this study, (1) the signal decomposition technique complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) is introduced to decompose the nonlinear and nonstationary AQI history data series into several more regular and more stable subseries firstly. (2) Fuzzy entropy (FE) is selected as the feature indicator to recombine the subseries with similar trends to avoid the problem of over-decomposition and reduce the computing time. (3) An ensemble long short-term memory (LSTM) neural network is established to forecast each reconstructed subseries, whose values are superimposed to predict the AQI value eventually. To validate the predicting performance of the proposed model, daily AQI data of Wuhan, China, dating from January 1, 2019, to February 28, 2022, is used as the experiment case. And comparative analysis is made between the proposed model and other common-used forecasting models. Benchmarking results of the numerical study demonstrate that the proposed model is superior to the other forecasting models with better AQI prediction accuracy.","['Environment', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Health Promotion and Disease Prevention']"
doi:10.1038/s41597-022-01814-4,en,SOMAS: a platform for data-driven material discovery in redox flow battery development,"['OriginalPaper', 'Data Descriptor']","Aqueous organic redox flow batteries offer an environmentally benign, tunable, and safe route to large-scale energy storage. The energy density is one of the key performance parameters of organic redox flow batteries, which critically depends on the solubility of the redox-active molecule in water. Prediction of aqueous solubility remains a challenge in chemistry. Recently, machine learning models have been developed for molecular properties prediction in chemistry and material science. The fidelity of a machine learning model critically depends on the diversity, accuracy, and abundancy of the training datasets. We build a comprehensive open access organic molecular database “Solubility of Organic Molecules in Aqueous Solution” (SOMAS) containing about 12,000 molecules that covers wider chemical and solubility regimes suitable for aqueous organic redox flow battery development efforts. In addition to experimental solubility, we also provide eight distinctive quantum descriptors including optimized geometry derived from high-throughput density functional theory calculations along with six molecular descriptors for each molecule. SOMAS builds a critical foundation for future efforts in artificial intelligence-based solubility prediction models. Measurement(s) quantum descriptors • molecular descriptors • physical descriptors Technology Type(s) Computation • experiment","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10579-022-09604-5,en,POMET: a corpus for poetic meter classification,"['OriginalPaper', 'Original Paper']","The availability of appropriate research corpora is a fundamental concern in music information retrieval research. This paper addresses the design, development, and evaluation of a poetic corpus, POMET, for the meter estimation task. Poems, which communicate through rhythm and apparent meaning, have a vital role in many literary traditions. Metrical rhythm generally involves periodic arrangements of sequences of stressed and unstressed syllables in each line of poems. It has already been proved that poetry’s aesthetic and emotional perception can be studied well using poetic meter analysis. A corpus with eight meters is designed and recorded in a studio environment for Malayalam, one of the prominent languages in South India. Using deep neural network architectures, a pilot evaluation is performed with musical texture features and spectrograms. We hope that the corpus can be used as a benchmark dataset for poetic meter estimation, rhythmic analysis, and corpus-based prosody analysis.","['Linguistics', 'Computational Linguistics', 'Computer Science, general', 'Linguistics, general', 'Language and Literature']"
doi:10.1007/s41315-022-00243-1,en,Light-weight behavior-based continuous authentication for personalized mobile robot,"['OriginalPaper', 'Regular Paper']","Personal robots contain a lot of private information about users, causing a high security risk. However, the installation of new biometric sensors tends to be costly, and even if they are installed, many of them authenticate one of the users only once before starting the use of the personal robot. For sensor-less and continuous authentication, behavior-based continuous authentication (BCA) has been proposed, which utilizes a series of user-dependent motion data measured by the personal robot. To distinguish the user behavior, long short-term memory (LSTM), which is excellent in analyzing and predicting time-series data, is often applied, but it is known to be computationally expensive. Since BCA should not interfere with the original tasks of the personal robot, the authors develop a new light-weight model, the so-called recurrent unit with forget gate and memory trace (RGaM), to minimize the computational cost and memory usage of BCA. RGaM has a minimum number of gates and a memory trace approximately derived from a fractional-order leaky integrator model, which suppresses the divergence of stored memories and provides long-term memory. This paper demonstrates the light-weight BCA using RGaM on a force-driven mobile robot optimized for individuals. The results suggested that RGaM, which could be implemented at about half the memory usage of LSTM, successfully achieved a higher level of user authentication than other models. In particular, RGaM achieved a 100% recall on the test dataset, and always succeeded in finding fraudulent users.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s12145-022-00885-6,en,FCD-R2U-net: Forest change detection in bi-temporal satellite images using the recurrent residual-based U-net,"['OriginalPaper', 'Research']","Forest changes caused by fires, clear-cuts, and Land Use/Land Cover (LULC) changes have negatively affected the climate, wildlife, and global ecosystem. By monitoring the forest changes, managers and planners can make appropriate decisions to preserve these natural areas. In this regard, this paper presents a novel deep learning-based forest change detection method including two main steps: (1) producing a new difference image enabling a more efficient distinction of changed and unchanged areas, and (2) generating a reliable forest change map by applying the recurrent residual-based U-Net deep neural network (R2U-Net) on the difference image. The recently introduced forest fused difference image (FFDI) is first improved by modifying its weighted angular operator as well as applying the fast local laplacian filter (FLLF) to generate an enhanced forest fused difference image (EFFDI). R2U-Net is subsequently used to segment the EFFDI into the changed and unchanged regions because it preserves their geometric shapes more effectively than other U-net variants. To assess the efficacy of the presented method, experimental results were conducted on four bi-temporal images acquired by the Sentinel 2 and Landsat 8 satellite sensors. The qualitative and quantitative results demonstrated the effectiveness of the proposed EFFDI in reflecting the true forest changes from the background. Moreover, compared with the other conventional U-Net-based models, including U-Net, ResU-Net, and U-Net ++, forest changes and their geometrical details were better preserved by R2U-Net. Furthermore, the proposed approach outperformed the other state-of-the-art supervised and unsupervised change detection methods in terms of quantitative and qualitative results, demonstrating its high potential for forest change detection applications.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s11042-021-11020-w,en,An enhanced self-attention and A2J approach for 3D hand pose estimation,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","Three dimensional (3D) hand pose estimation is the task of estimating the 3D location of hand keypoints. In recent years, this task has received much research attention due to its diverse applications in human-computer interaction and virtual reality. To the best of our knowledge, there has been limited studies that model self-attention in 3D hand pose estimation despite its use in various computer vision tasks. Hence, we propose augmenting convolution with self-attention to capture long-range dependencies in a depth image. In addition, motivated by a recent work which uses anchor points set on a depth image, we extend anchor points to the depth dimension to regress 3D hand joint locations. Validation experiments using the proposed approaches are performed on various hand pose datasets, and we obtain performances that are comparable to other state-of-the-art methods. The results demonstrate the potential of these approaches in a hand-based recognition system.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00034-022-02106-3,en,Optimal Near-End Speech Intelligibility Improvement Using CLPSO-Based Voice Transformation in Realistic Noisy Environments,OriginalPaper,"The proposed work attempts to improve the near-end intelligibility of speech at very low signal-to-noise ratios (SNRs). Additionally, the prerequisite of noise statistics that existing intelligibility improvement methods require is not a limitation of the proposed approach. To this end, the shaping parameters of the voice transformation function (VTF) are optimized. This optimization of the shaping parameters of the VTF corresponds to the combined modification that includes formant shifting, nonuniform time scaling, smoothing, and energy re-distributions in comprehensive learning particle swarm optimization (CLPSO) framework. The optimal parameters of the combined modifications are obtained by jointly maximizing the short time objective intelligibility, perceptual evaluation of speech quality and signal-to-distortion ratio metrics being used as the cost function in CLPSO. The outcome at the end is an improvement in intelligibility that is significantly higher than the ones obtained by applying these methods individually, while preserving the quality. As a side result, a Gaussian process regression is also employed to estimate the shaping parameters of VTF at arbitrary SNRs—other than the ones which were used during CLPSO training.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s12145-022-00872-x,en,Geospatial modeling using hybrid machine learning approach for flood susceptibility,"['OriginalPaper', 'Research Article']","Advanced methods for flood susceptibility mapping are required to minimize hazards in the watershed. Here, Partial Least Square-Structural Equation Model (PLS-SEM) was introduced to analyze the impact of flood influencing factors. PLS-SEM integrated with four Machine Learning (ML) methods as Multi-Layer Perceptron Neural Network (MLPNN), K Nearest Neighbor (KNN), Support Vector Machine (SVM) and Radial Basis Function Neural network (RBFN). In addition, significant flood influencing factors from PLS-SEM analysis was taken as the input of ML models. Then SVM, MLPNN, KNN, and RBFN integrated with the PLS-SEM classifier to develop hybrid models for constructing FSM. The performance of models is assessed in terms of standard statistical methods. The performance of the achieved model is good having AUROC > 0.8 and PLS-SEM-SVM (AUROC = 0.978) perform superior than others. Thus, hybrid SVM model can be best utilized for flood susceptibility. This study provides the importance of mechanism for flood influencing factors and extends the application of proposed hybrid ML models to minimize flood risk.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s00422-022-00940-x,en,Optimum trajectory learning in musculoskeletal systems with model predictive control and deep reinforcement learning,"['OriginalPaper', 'Original Article']","From the computational point of view, musculoskeletal control is the problem of controlling high degrees of freedom and dynamic multi-body system that is driven by redundant muscle units. A critical challenge in the control perspective of skeletal joints with antagonistic muscle pairs is finding methods robust to address this ill-posed nonlinear problem. To address this computational problem, we implemented a twofold optimization and learning framework to be specialized in addressing the redundancies in the muscle control . In the first part, we used model predictive control to obtain energy efficient skeletal trajectories to mimick human movements. The second part is to use deep reinforcement learning to obtain a sequence of stimulus to be given to muscles in order to obtain the skeletal trajectories with muscle control. We observed that the desired stimulus to muscles is only efficiently constructed by integrating the state and control input in a closed-loop setting as it resembles the proprioceptive integration in the spinal cord circuits. In this work, we showed how a variety of different reference trajectories can be obtained with optimal control and how these reference trajectories are mapped to the musculoskeletal control with deep reinforcement learning. Starting from the characteristics of human arm movement to obstacle avoidance experiment, our simulation results confirm the capabilities of our optimization and learning framework for a variety of dynamic movement trajectories. In summary, the proposed framework is offering a pipeline to complement the lack of experiments to record human motion-capture data as well as study the activation range of muscles to replicate the specific trajectory of interest. Using the trajectories from optimal control as a reference signal for reinforcement learning implementation has allowed us to acquire optimum and human-like behaviour of the musculoskeletal system which provides a framework to study human movement in-silico experiments. The present framework can also allow studying upper-arm rehabilitation with assistive robots given that one can use healthy subject movement recordings as reference to work on the control architecture of assistive robotics in order to compensate behavioural deficiencies. Hence, the framework opens to possibility of replicating or complementing labour-intensive, time-consuming and costly experiments with human subjects in the field of movement studies and digital twin of rehabilitation.","['Biomedicine', 'Neurosciences', 'Computer Appl. in Life Sciences', 'Neurobiology', 'Bioinformatics', 'Complex Systems']"
doi:10.1007/s10732-022-09503-6,en,Modeling and evolutionary algorithm for solving a multi-depot mixed vehicle routing problem with uncertain travel times,OriginalPaper,"This paper deals with a multi-depot mixed vehicle routing problem under uncertain travel times (MDMVRP-UT), where there are several different depots and a number of identical vehicles. A vehicle can come back to any of the depots after its service is completed. A light-robust-optimization model is set up to control the total travel time within a preset value and to minimize the total travel time as much as possible. Then an effective evolutionary algorithm (EA) is proposed to solve the light-robust-optimization model. In the proposed EA, two constructive heuristics, namely a random customer sequence-based heuristic and a minimum spanning tree-based heuristic, are presented according to the problem-specific knowledge to generate a high-quality initial population with a certain level of diversity. A destruction and construction-based reproduction operator is provided to give birth to high-quality feasible offspring. A pairwise interchange based local search method is proposed to enhance the local exploitation capability. A hybrid selection operator and a population updating method are employed to remain the diversity of the population. The effectiveness of the proposed EA is verified by comprehensive experiments based on the well-known benchmark instances in the literature.","['Mathematics', 'Operations Research, Management Science', 'Operations Research/Decision Theory', 'Artificial Intelligence', 'Calculus of Variations and Optimal Control; Optimization']"
doi:10.1007/s10957-022-02122-y,en,SABRINA: A Stochastic Subspace Majorization-Minimization Algorithm,OriginalPaper,"A wide class of problems involves the minimization of a coercive and differentiable function F on $${\mathbb {R}}^N$$ R N whose gradient cannot be evaluated in an exact manner. In such context, many existing convergence results from standard gradient-based optimization literature cannot be directly applied and robustness to errors in the gradient is not necessarily guaranteed. This work is dedicated to investigating the convergence of Majorization-Minimization (MM) schemes when stochastic errors affect the gradient terms. We introduce a general stochastic optimization framework, called StochAstic suBspace majoRIzation-miNimization Algorithm SABRINA that encompasses MM quadratic schemes possibly enhanced with a subspace acceleration strategy. New asymptotical results are built for the stochastic process generated by SABRINA . Two sets of numerical experiments in the field of machine learning and image processing are presented to support our theoretical results and illustrate the good performance of SABRINA with respect to state-of-the-art gradient-based stochastic optimization methods.","['Mathematics', 'Calculus of Variations and Optimal Control; Optimization', 'Optimization', 'Theory of Computation', 'Applications of Mathematics', 'Engineering, general', 'Operations Research/Decision Theory']"
doi:10.1007/s13762-022-04422-2,en,Cost-based model for optimal waste-load allocation and pollution loading losses in river system: simulation–optimization approach,"['OriginalPaper', 'Original Paper']","The development of agricultural, industrial, and urban activities creates an increase in pollution loading in rivers that may violate water quality standards which in its turn results in damages to the river systems. An efficient model of waste-load allocation plays a significant role in improving the water quality of the rivers. In this paper, a cost-based waste-load allocation model (C-WLA) is applied to guarantee the optimal management of both costs and river water quality. To determine the optimal loading pattern and threshold limits, the trade-off between treatment cost and pollution loss from the river is considered. In this regard, the MIKE11 model is coupled with the Particle Swarm Optimization (PSO) algorithm and applied to the Karoon River in Iran to demonstrate its practicality and efficiency. The sum of water treatment cost and pollution loading loss is minimized and the monthly optimal treatment percentages and threshold limit were determined. Then, different strategies under various operations of the river system are given with insights into the impacts of the trade-off policy between costs and losses for the discharger’s TDS removal. The results demonstrated that the C-WLA model can achieve optimal management of pollution load in various operating of the river system. In addition, it is also shown that the proposed method is expected to offer better decision support to reasonable waste-load allocation where it can encourage dischargers to improve loading performance. Graphical abstract ","['Environment', 'Environment, general', 'Environmental Science and Engineering', 'Environmental Chemistry', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution', 'Soil Science & Conservation', 'Ecotoxicology']"
doi:10.1007/s11063-022-10875-w,en,CSA-Net: Deep Cross-Complementary Self Attention and Modality-Specific Preservation for Saliency Detection,OriginalPaper,"The multi-modality or multi-stream-based convolution neural network is the recent trend in saliency computation, which is receiving tremendous research interest. The previous models used modality-based independent fusion or cross-modality-based complementary fusion to find saliency that leads to incurring inconsistency or distribution loss of salient points and regions. Most existing models did not effectively utilize accurate localization of high-level semantic and contextual features. The proposed model collectively uses the above two methods and a precise deep localization model to target the abovementioned challenges. Specifically, CSA-Net comprises four essential features: non-complementarity, cross-complementary, intra-complementary, and deep localized improved high-level features. The designed $$2\times 3$$ 2 × 3 encoder and decoder streams produce these essential features and assure modality-specific saliency preservation. The cross and intra- complementary fusion are deeply guided by proposed novel, cross-complementary self-attention to produce fused saliency. The attention map is computed by two-stage additive fusion based on a Non-Local network. A novel, Optimal Selective Saliency, has been proposed to find two similar saliencies among three steam-wise saliencies. The experimental analysis demonstrates the effectiveness of the proposed $$2\times 3$$ 2 × 3 stream network and attention map. The experimental results show better performance in comparison with fourteen closely related state-of-the-art methods.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s00330-022-08863-8,en,Using deep learning to safely exclude lesions with only ultrafast breast MRI to shorten acquisition and reading time,"['OriginalPaper', 'Imaging Informatics and Artificial Intelligence']","Objectives To investigate the feasibility of automatically identifying normal scans in ultrafast breast MRI with artificial intelligence (AI) to increase efficiency and reduce workload. Methods In this retrospective analysis, 837 breast MRI examinations performed on 438 women from April 2016 to October 2019 were included. The left and right breasts in each examination were labelled normal (without suspicious lesions) or abnormal (with suspicious lesions) based on final interpretation. Maximum intensity projection (MIP) images of each breast were then used to train a deep learning model. A high sensitivity threshold was calculated based on the detection trade - off (DET) curve on the validation set. The performance of the model was evaluated by receiver operating characteristic analysis of the independent test set. The sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) with the high sensitivity threshold were calculated. Results The independent test set consisted of 178 examinations of 149 patients (mean age, 44 years ± 14 [standard deviation]). The trained model achieved an AUC of 0.81 (95% CI: 0.75–0.88) on the independent test set. Applying a threshold of 0.25 yielded a sensitivity of 98% (95% CI: 90%; 100%), an NPV of 98% (95% CI: 89%; 100%), a workload reduction of 15.7%, and a scan time reduction of 16.6%. Conclusion This deep learning model has a high potential to help identify normal scans in ultrafast breast MRI and thereby reduce radiologists’ workload and scan time. Key Points • Deep learning in TWIST may eliminate the necessity of additional sequences for identifying normal breasts during MRI screening. • Workload and scanning time reductions of 15.7% and 16.6%, respectively, could be achieved with the cost of 1 (1 of 55) false negative prediction.","['Medicine & Public Health', 'Imaging / Radiology', 'Diagnostic Radiology', 'Interventional Radiology', 'Neuroradiology', 'Ultrasound', 'Internal Medicine']"
doi:10.1007/s10489-022-03292-y,en,Attention based trajectory prediction method under the air combat environment,OriginalPaper,"In close-range air combat, highly reliable trajectory prediction results can help pilots to win victory to a great extent. However, traditional trajectory prediction methods can only predict the precise location that the target aircraft may reach, which cannot meet the requirements of high-precision, real-time trajectory prediction for highly maneuvering targets. To this end, this paper proposes an attention-based convolution long sort-term memory (AttConvLSTM) network to calculate the arrival probability of each space in the reachable area of the target aircraft. More specifically, by segmenting the reachable area, the trajectory prediction problem is transformed into a classification problem for solution. Second, the AttConvLSTM network is proposed as an efficient feature extraction method, and combined with the multi-layer perceptron (MLP) to solve this classification problem. Third, a novel loss function is designed to accelerate the convergence of the proposed model. Finally, the flight trajectories generated by experienced pilots are used to evaluate the proposed method. The results indicate that the mean absolute error of the proposed method is no more than 45.73m, which is of higher accuracy compared to other state-of-the-art algorithms.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-13617-1,en,Ocular diseases classification using a lightweight CNN and class weight balancing on OCT images,"['OriginalPaper', '1181: Multimedia-based Healthcare Systems using Computational Intelligence']","Optical coherence tomography (OCT) is a non-invasive technique to capture cross-sectional volumes of the human retina. OCT images are used for the diagnosis of various ocular diseases. However, OCT datasets generally suffer from the problem of class imbalance. This work aims to leverage CNN capability for OCT images classification in the presence of class imbalance. A lightweight convolutional neural network (CNN) with class weight balancing (CWB) is proposed for OCT image classification. Training of CNN is done while penalizing the classes having a higher number of samples using the CWB method. The performance of the proposed method is evaluated on spectral-domain OCT (SD-OCT) images from two publicly available datasets, namely, ZhangLab and Duke. The performance of the proposed method is evaluated on the confusion matrix-based parameters like accuracy, sensitivity, specificity, and F 1 − s c o r e . The proposed method achieved 99.17% and 98.46% accuracy for ZhangLab and Duke datasets, respectively. It is observed that the proposed method performs better as compared to most of the state-of-the-art OCT classification methods The generalizability and interpretability of the proposed method are also evaluated to improve the understanding of the CNN model.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00521-022-07630-6,en,Wi-Fi signal-based human action acknowledgement using channel state information with CNN-LSTM: a device less approach,"['OriginalPaper', 'Original Article']","Human action acknowledgment is an abundant and significant area for machine learning-based researchers due to the level of accuracy in identifying human actions. Due to the rapid growth of technologies in the machine and deep learning techniques, wireless sensors, handy Internet of Things (IoT) devices, and Wireless Fidelity (Wi-Fi), the activity recognition process is made effective with higher accuracy. By using those booming technologies and preserving the privacy of the test person we propose a novel human action recognition model that uses the channel state information (CSI) from Wi-Fi and the most prominent machine learning model, CNN with LSTM. Initially, CSI is introduced, the changes in CSI signals are assessed, and the obtained data samples are made as input to the CNN-LSTM model. To make the recognition more accurate, we also incorporated Kalman filters for noise removal and smoothed the data sample. Furthermore, we have used an image segmentation procedure to identify the initial and end times of all the activities considered and to fragment the image obtained, which is further fed as input to the CNN-LSTM model. Getting a dataset for the experiment is a herculean task. Hence a self-collected dataset is used to assess, or model proposed. Finally, the results obtained are verified and validated for their correctness with appropriate machine learning metrics and parameters like accuracy, F1 score, etc. Our proposed model affords the accuracy of 98.96% for all the considered activities. The model can adapt itself even for a minimum sampling rate and subcarriers found in the test bed.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10614-021-10137-2,en,The Analysis of Credit Risks in Agricultural Supply Chain Finance Assessment Model Based on Genetic Algorithm and Backpropagation Neural Network,OriginalPaper,"The risk assessment methods of agricultural supply chain finance (SCF) are explored to reduce agricultural SCF’s credit risks. First, the genetic algorithm (GA) is utilized to adjust and determine the initial weights and thresholds of the backpropagation neural network (BPNN), which assesses the credit risks. Second, for the problem that many factors affect the credit risks and the difficulty in selecting the characteristics, the principle of assessment indicator selection is proposed; the characteristics of these indicators are selected by principal component analysis (PCA). Finally, the case analysis method is utilized to verify the proposed risk assessment method, and an optimal credit risk assessment method is established. The results show that GA-BPNN can accelerate the convergence speed of the BPNN and improve the disadvantage in easily falling into the local minimum of BPNN. The PCA method simplifies the complexity of assessment indicator selection, and the representative indicators in agricultural SCF credit risk assessment are successfully selected. Through verification, it is found that the GA-BPNN algorithm performs well in credit risk prediction of agricultural SCF, and its prediction accuracy and prediction speed are improved. Therefore, the used GA-BPNN has performed well in the credit risk prediction of agricultural SCF, which applies to financial credit risk assessment to reduce the credit risks in agricultural SCF.","['Economics', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Computer Appl. in Social and Behavioral Sciences', 'Operations Research/Decision Theory', 'Behavioral/Experimental Economics', 'Math Applications in Computer Science']"
doi:10.1007/s11081-022-09710-x,en,A Lagrangian dual method for two-stage robust optimization with binary uncertainties,"['OriginalPaper', 'Research Article']","This paper presents a new exact method to calculate worst-case parameter realizations in two-stage robust optimization problems with categorical or binary-valued uncertain data. Traditional exact algorithms for these problems, notably Benders decomposition and column-and-constraint generation, compute worst-case parameter realizations by solving mixed-integer bilinear optimization subproblems. However, their numerical solution can be computationally expensive not only due to their resulting large size after reformulating the bilinear terms, but also because decision-independent bounds on their variables are typically unknown. We propose an alternative Lagrangian dual method that circumvents these difficulties and is readily integrated in either algorithm. We specialize the method to problems where the binary parameters switch on or off constraints as these are commonly encountered in applications, and discuss extensions to problems that lack relatively complete recourse and to those with integer recourse. Numerical experiments provide evidence of significant computational improvements over existing methods.","['Mathematics', 'Optimization', 'Engineering, general', 'Systems Theory, Control', 'Environmental Management', 'Operations Research/Decision Theory', 'Financial Engineering']"
doi:10.1007/s11063-022-10844-3,en,Conditional Embedding Pre-Training Language Model for Image Captioning,OriginalPaper,"The pre-trained language model can not only learn language representations with different granularity from a large number of corpus, but also provide a good initialization for downstream tasks. Aggregation or alignment of text features and visual features as input of pre-training language model is the mainstream approach to deal with visual-language tasks. People can accurately describe an image by constantly referring to the visual information and key text information of the image. Inspired by this idea, we no longer follow mainstream approach, and propose to adjust the pre-training language model processing by using high-low visual features as conditional inputs. Specifically, conditional embedding layer normalization (CELN) we proposed is an effective mechanism for embedding visual features into pre-training language models for feature selection. We apply CELN to transformers in the unified pre-training language model (UNILM). This model parameter adjustment mechanism is an innovative attempt in pre-training language model. Extensive experiments on two challenging benchmarks (MSCOCO and Visual Genome datasets)demonstrate that this seminal work is effective. Code and models are publicly available at: https://github.com/lpfworld/CE-UNILM .","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s42757-021-0115-5,en,"CFD validation of condensation heat transfer in scaled-down small modular reactor applications, Part 1: Pure steam","['OriginalPaper', 'Research Article']","This study presented the state-of-the-art computational fluid dynamics (CFD) validation and scaling of the condensation heat transfer (CHT) models for passive containment cooling system (PCCS) of the small modular reactor (SMR). The STAR-CCM+ software with real 3D computational domains was used to validate the condensation models with a preliminary assessment of pure steam scaling performance. The boundary and appropriate physics conditions from the test data were applied. The condensation was modeled using the condensation-seed parameter as a source term for mass, momentum, and energy conservation equations. A small percentage of air (within 1%) was considered in the test section; hence, multi-component gas models were used. The implicit-unsteady numerical solver was applied to improve numerical stability. Mesh size, run time (duration), and time step sensitivity analyses were applied to obtain optimized simulation results. The test fluid parameters—temperature (at bulk steam-mixture, bulk coolant, inner and outer tube walls), condensation film thickness, mass fraction, and heat flux—were utilized to validate the CFD simulations. Finally, Nusselt number ( Nu ), as the dimensionless number heat transfer, was calculated for diameter scaled-up and scaled-down geometries. The heat transfer coefficient and Nu values were compared to evaluate the scalability performance of CHT models.","['Engineering', 'Engineering Fluid Dynamics', 'Fluid- and Aerodynamics', 'Environmental Engineering/Biotechnology']"
doi:10.1007/s10844-022-00765-x,en,Can recurrent neural networks learn process model structure?,OriginalPaper,"Various methods using machine and deep learning have been proposed to tackle different tasks in predictive process monitoring, forecasting for an ongoing case e.g. the most likely next event or suffix, its remaining time, or an outcome-related variable. Recurrent neural networks (RNNs), and more specifically long short-term memory nets (LSTMs), stand out in terms of popularity. In this work, we investigate the capabilities of such an LSTM to actually learn the underlying process model structure of an event log. We introduce an evaluation framework that combines variant-based resampling and custom metrics for fitness, precision and generalization. We evaluate 4 hypotheses concerning the learning capabilities of LSTMs, the effect of overfitting countermeasures, the level of incompleteness in the training set and the level of parallelism in the underlying process model. We confirm that LSTMs can struggle to learn process model structure, even with simplistic process data and in a very lenient setup. Taking the correct anti-overfitting measures can alleviate the problem. However these measures did not present themselves to be optimal when selecting hyperparameters purely on predicting accuracy. We also found that decreasing the amount of information seen by the LSTM during training, causes a sharp drop in generalization and precision scores. In our experiments, we could not identify a relationship between the extent of parallelism in the model and the generalization capability, but they do indicate that the process’ complexity might have impact.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1038/s41598-022-24886-z,en,Evaluation of roadside air quality using deep learning models after the application of the diesel vehicle policy (Euro 6),"['OriginalPaper', 'Article']","Euro 6 is the latest vehicle emission standards for pollutants such as CO, NO 2 and PM, that all new vehicles must comply, and it was introduced in September 2015 in South Korea. This study examined the effect of Euro 6 by comparing the measured pollutant concentrations after 2016 (Euro 6–era) to the estimated concentrations without Euro 6. The concentration without Euro 6 was estimated by first modeling the air quality using various environmental factors related to diesel vehicles, meteorological conditions, temporal information such as date and precursors in 2002–2015 (pre–Euro 6–era), and then applying the model to predict the concentration after 2016. In this study, we used both recurrent neural network (RNN) and random forest (RF) algorithms to model the air quality and showed that RNN can achieve higher R 2 (0.634 ~ 0.759 depending on pollutants) than RF, making it more suitable for air quality modeling. According to our results, the measured concentrations during 2016–2019 were lower than the concentrations predicted using RNN by − 1.2%, − 3.4%, and − 4.8% for CO, NO 2 and PM 10 . Such reduction can be attributed to the result of Euro 6.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11192-022-04380-z,en,Constructing a high-quality dataset for automated creation of summaries of fundamental contributions of research articles,OriginalPaper,"Research contributions, which indicate how a research paper contributes new knowledge or new understanding in contrast to prior research on the topic, are the most valuable type of information for researchers to understand the main content of a paper. However, there is little research using research contributions to identify and recommend valuable knowledge in academic literature for users. Instead, most existing studies mainly focus on the analysis of other elements in academic literature, such as keywords, citations, rhetorical structure, discourse, and others. This paper first introduces a fine-grained annotation scheme with six categories for research contributions in academic literature. To evaluate the reliability of our annotation scheme, we conduct annotation on 5024 sentences collected from Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL Anthology) and an academic journal Information Processing & Management (IP &M). We reach an inter-annotator agreement of Cohen’s kappa = 0.91 and Fleiss’ kappa = 0.91, demonstrating the high quality of the dataset. We then built two types of classifiers for automated research contribution identification based on the dataset: classic feature-based machine learning (ML) and transformer-based deep learning (DL). Our experimental results show that SCI-BERT, a pretrained language model for scientific text, achieves the best performance with an F1 score of 0.58, improving the best classic ML model (nouns + verbs + tf-idf + random forest) by 2%. This also indicates a comparable power of classic feature-based ML models to DL-based model like SCI-BERT on this dataset. The fine-grained annotation scheme can be applied for large-scale analysis for research contributions in academic literature. The automated research contribution classifiers built in this paper provide the basis for the automatic research contributions extraction and knowledge fragment recommendation. The high-quality research contribution dataset developed in this research is publicly available on Zenodo https://zenodo.org/record/6284137#.YhkZ7-iZO4Q . The code for the data analysis and experiments will be released at: https://github.com/HuyenNguyenHelen/Contribution-Sentence-Classification .","['Computer Science', 'Information Storage and Retrieval', 'Library Science']"
doi:10.1007/s11517-022-02680-3,en,Good view frames from ultrasonography (USG) video containing ONS diameter using state-of-the-art deep learning architectures,"['OriginalPaper', 'Original Article']","This paper presents an automated method for detection of the diagnostically prominent frames containing optic nerve sheath (ONS) from ocular ultrasonography video using deep learning; such frames are referred to as “Good View” frames in this paper. Vivid acquisition and measurement of diagnostic features during ultrasound imaging is a challenging task; it needs a highly skilled and experienced medical expert. Automated detection of the Good View frame and the subsequent automatic measurement of optic nerve sheath diameter (ONSD), predicting elevated intracranial pressure (ICP) status, will eliminate the need for frequent intervention of a medical expert for continuous monitoring and ICP status in traumatic patients. In the presented work, the proposed model automatically detects the appropriate frames containing ONS, from an ultrasound video, by using faster region-based CNN (Faster R-CNN) object detection model. The region proposal detection network finds the ONS by using bounding boxes. In addition, three CNN-based architectures are used for its feature extraction. Finally, SoftMax classifier classifies the ONS containing Good View frame. The Inceptionv2, ResNet50, and ResNet101 architectures are then compared by utilizing the optimized learning rate and epoch parameters for the CNN model so as to provide better detection of the Good View frame. The performance of the developed module has been analyzed by proposing a grading criterion of the Good View frame. Based on the detection score and mean opinion score, an USG frame is considered a Good View for a 95–99% detection score, and this Good View frame is used for measuring the ONSD value. It is found that Faster R-CNN ResNet101 (model 3) is an optimal model in terms of sensitivity and specificity for Good View frame detection at a learning rate of 0.0003. The sensitivity and specificity of this model are obtained as 90.41 and 91.45, respectively. Furthermore, the ONSD value is measured from Good View-detected frames using an automated algorithm involving image processing and computational methods. Considering the Good View frame (detection score 95–99), the algorithm-generated ONSD values are compared with the radiologist’s measured value of ONSD to validate the findings; a small percent root mean square difference (PRD) of 0.501 is found between these values, which is strong indicative of the accuracy of algorithm generated ONSD measurement using automatically detected Good View ocular USG frames. Graphical Abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s10898-022-01179-3,en,Discretization and global optimization for mixed integer bilinear programming,OriginalPaper,"We consider global optimization of mixed-integer bilinear programs (MIBLP) using discretization-based mixed-integer linear programming (MILP) relaxations. We start from the widely used radix-based discretization formulation (called R -formulation in this paper), where the base R may be any natural number, but we do not require the discretization level to be a power of R . We prove the conditions under which R -formulation is locally sharp, and then propose an $$R^+$$ R + -formulation that is always locally sharp. We also propose an H -formulation that allows multiple bases and prove that it is also always locally sharp. We develop a global optimization algorithm with adaptive discretization (GOAD) where the discretization level of each variable is determined according to the solution of previously solved MILP relaxations. The computational study shows the computational advantage of GOAD over general-purpose global solvers BARON and SCIP.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Real Functions', 'Computer Science, general']"
doi:10.1007/s11548-022-02718-9,en,SIG-Former: monocular surgical instruction generation with transformers,"['OriginalPaper', 'Original Article']","Purpose: Automatic surgical instruction generation is a crucial part for intra-operative surgical assistance. However, understanding and translating surgical activities into human-like sentences are particularly challenging due to the complexity of surgical environment and the modal gap between images and natural languages. To this end, we introduce SIG-Former , a transformer-backboned generation network to predict surgical instructions from monocular RGB images. Methods: Taking a surgical image as input, we first extract its visual attentive feature map with a fine-tuned ResNet-101 model, followed by transformer attention blocks to correspondingly model its visual representation, text embedding and visual–textual relational feature. To tackle the loss-metric inconsistency between training and inference in sequence generation, we additionally apply a self-critical reinforcement learning approach to directly optimize the CIDEr score after regular training. Results: We validate our proposed method on DAISI dataset, which contains 290 clinical procedures from diverse medical subjects. Extensive experiments demonstrate that our method outperforms the baselines and achieves promising performance on both quantitative and qualitative evaluations. Conclusion: Our experiments demonstrate that SIG-Former is capable of mapping dependencies between visual feature and textual information. Besides, surgical instruction generation is still at its preliminary stage. Future works include collecting large clinical dataset, annotating more reference instructions and preparing pre-trained models on medical images.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s10032-022-00417-4,en,A holistic approach for image-to-graph: application to optical music recognition,"['OriginalPaper', 'Special Issue Paper']","A number of applications would benefit from neural approaches that are capable of generating graphs from images in an end-to-end fashion. One of these fields is optical music recognition (OMR), which focuses on the computational reading of music notation from document images. Given that music notation can be expressed as a graph, the aforementioned approach represents a promising solution for OMR. In this work, we propose a new neural architecture that retrieves a certain representation of a graph—identified by a specific order of its vertices—in an end-to-end manner. This architecture works by means of a double output: It sequentially predicts the possible categories of the vertices, along with the edges between each of their pairs. The experiments carried out prove the effectiveness of our proposal as regards retrieving graph structures from excerpts of handwritten musical notation. Our results also show that certain design decisions, such as the choice of graph representations, play a fundamental role in the performance of this approach.","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11063-022-10853-2,en,Precise Correspondence Enhanced GAN for Person Image Generation,OriginalPaper,"To generate a realistic person image for pose-guided person image generation, especially for local body parts, is challenging. Two reasons account for it: (1) the difficulty for long-range relation modeling, (2) a deficiency in precise local correspondence capturing. We propose a Precise Correspondence Enhanced Generative Adversarial Network (PCE-GAN) to address these problems. PCE-GAN includes a global branch and a local branch. The former maintains the global consistency of the generated person image and the latter captures the precise local correspondence. More specifically, the long-range relation is well established via the spatial-channel Multi-layer Perceptrons module in the transformation blocks within both branches. The precise local correspondence is captured effectively by the local branch’s local-pair building and local-guiding modules. Finally, the outputs of each branch are combined for mutually improved benefits based on the enhanced correspondences. Experimental results show that, compared to previous state-of-the-art methods using the Market-1501 dataset, PCE-GAN performs quantitatively better, with a $$5.53\%$$ 5.53 % and $$7.74\%$$ 7.74 % improvement in SSIM and IS scores, respectively. Qualitative results for both Market-1501 and DeepFashion datasets are also provided herein to further validate the effectiveness of our method.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11325-022-02571-9,en,Detecting obstructive sleep apnea by craniofacial image–based deep learning,"['OriginalPaper', 'Sleep Breathing Physiology and Disorders • Original Article']","Study objectives This study aimed to develop a deep learning–based model to detect obstructive sleep apnea (OSA) using craniofacial photographs. Methods Participants referred for polysomnography (PSG) were recruited consecutively and randomly divided into the training, validation, and test groups for model development and evaluation. Craniofacial photographs were taken from five different angles (front, right 90° profile, left 90° profile, right 45° profile, and left 45° profile) and inputted to the convolutional neural networks. The neural networks extracted features from photographs and outputted the probabilities of the presence of the disease. Sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were calculated using PSG diagnosis as the reference standard. These analyses were repeated using two apnea–hypopnea index thresholds (≥ 5 and ≥ 15events/h). Results A total of 393 participants were enrolled. Using the operating point with maximum sum of sensitivity and specificity, the model of the photographs exhibited an AUC of 0.916 (95% confidence interval [CI], 0.847–0.960) with a sensitivity of 0.95 and a specificity of 0.80 at an AHI threshold of 5 events/h; an AUC of 0.812 (95% CI, 0.729–0.878) with a sensitivity of 0.91 and a specificity of 0.73 at an AHI threshold of 15 events/h. Conclusions The results suggest that combining craniofacial photographs and deep learning techniques can help detect OSA automatically. The model may have potential utility as a tool to assess OSA probability in clinics or screen for OSA in the community.","['Medicine & Public Health', 'Pneumology/Respiratory System', 'Otorhinolaryngology', 'Dentistry', 'Neurology', 'Internal Medicine', 'Pediatrics']"
doi:10.1007/s11356-022-21768-9,en,"Forecasting of non-accidental, cardiovascular, and respiratory mortality with environmental exposures adopting machine learning approaches","['OriginalPaper', 'Research Article']","Environmental exposure constantly changes with time and various interactions that can affect health outcomes. Machine learning (ML) or deep learning (DL) algorithms have been used to solve complex problems, such as multiple exposures and their interactions. This study developed predictive models for cause-specific mortality using ML and DL algorithms with the daily or hourly measured meteorological and air pollution data. The ML algorithm improved the performance compared to the conventional methods, even though the optimal algorithm depended on the adverse health outcomes. The best algorithms were extreme gradient boosting, ridge, and elastic net, respectively, for non-accidental, cardiovascular, and respiratory mortality with daily measurement; they were superior to the generalized additive model reducing a mean absolute error by 4.7%, 4.9%, and 16.8%, respectively. With hourly measurements, the ML model tended to outperform the conventional models, even though hourly data, instead of daily data, did not enhance the performance in some models. The proposed model allows a better understanding and development of robust predictive models for health outcomes using multiple environmental exposures.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10844-022-00727-3,en,An improved recommendation based on graph convolutional network,OriginalPaper,"Graph convolutional network is a recently developed artificial neural network method commonly used in recommendation system research. This paper points out three shortcomings of existing recommendation systems based on the graph convolutional network. 1. Existing models that take the one-hot encoding based on node ordinal numbers in the graph or encoding based on original entity attributes as input may not fully utilize the information carried by the attribute interactions. 2. Previous models update the node embeddings only by the first-order neighbors in the graph convolution layer, which is easily affected by noise. 3. Existing models do not take into account differences in user opinions. We propose an improved graph convolutional network-based collaborative filtering model to address these drawbacks. We identify inner and cross interaction between user attributes and item attributes, and then we take the vector representations of aggregated attributes graph as input. In the convolutional layer, we aggregate the second-order collaborative signals and incorporate the different user opinions. The experiments on three public datasets show that our model outperforms state-of-the-art models.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s00366-022-01648-z,en,SMA-Net: Deep learning-based identification and fitting of CAD models from point clouds,"['OriginalPaper', 'Original Article']","Identification and fitting is an important task in reverse engineering and virtual/augmented reality. Compared to the traditional approaches, carrying out such tasks with a deep learning-based method have much room to exploit. This paper presents SMA-Net (Spatial Merge Attention Network), a novel deep learning-based end-to-end bottom-up architecture, specifically focused on fast identification and fitting of CAD models from point clouds. The network is composed of three parts whose strengths are clearly highlighted: voxel-based multi-resolution feature extractor, spatial merge attention mechanism and multi-task head. It is trained with both virtually-generated point clouds and as-scanned ones created from multiple instances of CAD models, themselves obtained with randomly generated parameter values. Using this data generation pipeline, the proposed approach is validated on two different data sets that have been made publicly available: robot data set for Industry 4.0 applications, and furniture data set for virtual/augmented reality. Experiments show that this reconstruction strategy achieves compelling and accurate results in a very high speed, and that it is very robust on real data obtained for instance by laser scanner and Kinect.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s11276-022-03200-9,en,Load balanced cluster formation to avoid energy hole problem in WSN using fuzzy rule-based system,"['OriginalPaper', 'Original Paper']","In wireless sensor networks, the energy efficiency is a big challenge and this can be defeated using various techniques such as clustering and routing. In the clustering approach, clusters are made up of the sensor nodes. In most of the applications, the sensor nodes are battery-powered. The sensor nodes may carry an uneven load of data and consume an unequal amount of energy. This leads to the formation of an energy hole in the network. Therefore, load balancing in the network with respect to the distance of data transmission is important in WSN. Hence, this paper proposes an energy-efficient as well as Load Balanced Clustering algorithm using Fuzzy logic (LBCF). The values of load and distance between the sensor node and BS are associated using rule-based fuzzy logic. The output values of the fuzzy system decides the cluster heads (CHs) and the size of the cluster. The sensor nodes get assigned to the CHs with respect to the capacity of CH. The simulations are conducted under different scenarios and network parameters and the proposed LBCF shows out performance to the state-of-the-art algorithms.","['Engineering', 'Communications Engineering, Networks', 'Computer Communication Networks', 'Electrical Engineering', 'IT in Business']"
doi:10.1007/s12532-022-00222-4,en,Enhanced formulation for the Guillotine 2D Cutting Knapsack Problem,"['OriginalPaper', 'Full Length Paper ']","We advance the state of the art in Mixed-Integer Linear Programming formulations for Guillotine 2D Cutting Problems by (i) adapting a previously-known reduction to our preprocessing phase (plate-size normalization) and by (ii) enhancing a previous formulation (PP-G2KP from Furini et alli) by cutting down its size and symmetries. Our focus is the Guillotine 2D Knapsack Problem with orthogonal and unrestricted cuts, constrained demand, unlimited stages, and no rotation – however, the formulation may be adapted to many related problems. The code is available. Concerning the set of 59 instances used to benchmark the original formulation, the enhanced formulation takes about 4 hours to solve all instances while the original formulation takes 12 hours to solve 53 of them (the other six runs hit a three-hour time limit each). We integrate, to both formulations, a pricing framework proposed for the original formulation; the enhanced formulation keeps a significant advantage in this situation. Finally, in a recently proposed set of 80 harder instances, the enhanced formulation (with and without the pricing framework) found: 22 optimal solutions (5 already known, 17 new); better lower bounds for 25 instances; better upper bounds for 58 instances.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Theory of Computation', 'Mathematics of Computing']"
doi:10.1007/s10100-021-00784-z,en,Equivalent cyclic polygon of a euclidean travelling salesman problem tour and modified formulation,OriginalPaper,"We define a geometric transformation of Euclidean Travelling Salesman Problem (TSP) tours that leads to a new formulation of the TSP. For every Euclidean TSP n-city tour, it is possible to construct an inscribed n-polygon (Equivalent Cyclic Polygon, ECP) such that the lengths of the edges are equal to the corresponding TSP tour links and follow the same sequence order. The analysis of the ECP elicits the possibility of defining a new objective function in terms of angles instead of distances. This modification opens the way to identify characterizing geometric parameters of the TSP as well as to explore new heuristics based on the inclusion of additional constraints. The experimentation with a set of cases shows promising results compared to the traditional compact formulations. The behavior of the ECP-based TSP formulations is better when the nodes of the TSP are randomly or evenly distributed.","['Business and Management', 'Operations Research/Decision Theory']"
doi:10.1007/s10489-022-03350-5,en,Machine learning techniques for software vulnerability prediction: a comparative study,OriginalPaper,"Software vulnerabilities represent a major cause of security problems. Various vulnerability discovery models (VDMs) attempt to model the rate at which the vulnerabilities are discovered in a software. Although several VDMs have been proposed, not all of them are universally applicable. Also most of them seldom give accurate predictive results for every type of vulnerability dataset. The use of machine learning (ML) techniques has generally found success in a wide range of predictive tasks. Thus, in this paper, we conducted an empirical study on applying some well-known machine learning (ML) techniques as well as statistical techniques to predict the software vulnerabilities on a variety of datasets. The following ML techniques have been evaluated: cascade-forward back propagation neural network, feed-forward back propagation neural network, adaptive-neuro fuzzy inference system, multi-layer perceptron, support vector machine, bagging, M5Rrule, M5P and reduced error pruning tree. The following statistical techniques have been evaluated: Alhazmi-Malaiya model, linear regression and logistic regression model. The applicability of the techniques is examined using two separate approaches: goodness-of-fit to see how well the model tracks the data, and prediction capability using different criteria. It is observed that ML techniques show remarkable improvement in predicting the software vulnerabilities than the statistical vulnerability prediction models.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-12075-z,en,DDFN: a depth-differential fusion network for multi-focus image,OriginalPaper,"Dueto the limitations of digital image capturing equipment, it is usually difficult for the photographer to obtain a complete and clear image of a certain scene in the case of dual targets and multiple targets. This is because most digital imaging systems have a limited depth of field control range, so they can only focus on one or a few objects in the far or near distance, resulting in clear and blurred areas with clear boundaries, that is multi-focus image. This kind of image limits further image processing, such as target recognition, image segmentation, target tracking and so on. Often, two multi-focus images can basically integrate all scene information completely, and multiple multi-focus images can also be fused by cascading all images. Inspired by this, we propose a new image fusion method based on binocular depth estimation and binocular image difference, called depth-differential mapping fusion network (DDFN). In detail, DDFN is based on the idea of residual U-Net and the network structure. It takes two multi-focus images as input, extracts rich hierarchical features through the convolutional pooling pyramid, and learns the residuals between them and the corresponding groundtruth. In this process, DDFN will use their differential information to encode, merge the depth information, and finally perform the decoding process, so the features in the multi-focus image pair will be fully extracted. Finally a clear image without defocusing blur area is formed. We have conducted multiple ablation experiments and comparative experiments, furthermore, a large number of results fully demonstrate the effectiveness of our network structure.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10723-022-09634-x,en,K-AGRUED: A Container Autoscaling Technique for Cloud-based Web Applications in Kubernetes Using Attention-based GRU Encoder-Decoder,OriginalPaper,"Cloud service providers can operate several execution instances on a single physical server using virtualization technology, which improves resource utilization. In recent years, container-based virtualization has been developed as a remarkably lightweight alternative to virtual machines. Containers consume less memory than virtual machines, enabling faster setup and portability. Cloud-based applications require dynamic resource allocation in response to fluctuations in the number of incoming requests. Most articles on proactive autoscaling in cloud computing have shortcomings in two ways. 1) During feature extraction, the temporal patterns of the data are ignored, and the historical sequences are assigned equal weight. 2) Existing research omits cool down time (CDT) from the planning phase. 3) Scaling operations can be performed at any time depending only on the current input workload, resulting in a large number of contradicting scaling actions. In response to the above shortcomings, this paper presents a proactive autoscaling method for web applications in Kubernetes using an attention-based gated recurrent unit (GRU) encoder-decoder (K-AGRUED), which predicts the resource usage of several future steps based on CDT. The results demonstrate that the proposed method reduces prediction error by 2–25% compared to state of the art methods. Our approach significantly reduces scaling operations and under-provisioning compared to the standard horizontal pod autoscaler (HPA) of Kubernetes and two previous studies. The K-AGRUED increases the scaling speedup by a factor of up to five in a real environment.","['Computer Science', 'Processor Architectures', 'Management of Computing and Information Systems', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s12539-022-00521-3,en,Accurate Prediction of Anti-hypertensive Peptides Based on Convolutional Neural Network and Gated Recurrent unit,"['OriginalPaper', 'Original research article']","Hypertension (HT) is a general disease, and also one of the most ordinary and major causes of cardiovascular disease. Some diseases are caused by high blood pressure, including impairment of heart and kidney function, cerebral hemorrhage and myocardial infarction. Due to the limitations of laboratory methods, bioactive peptides for the treatment of HT need a long time to be identified. Therefore, it is of great immediate significance for the identification of anti-hypertensive peptides (AHTPs). With the prevalence of machine learning, it is suggested to use it as a supplementary method for AHTPs classification. Therefore, we develop a new model to identify AHTPs based on multiple features and deep learning. And the deep model is constructed by combining a convolutional neural network (CNN) and a gated recurrent unit (GRU). The unique convolution structure is used to reduce the feature dimension and running time. The data processed by CNN is input into the recurrent structure GRU, and important information is filtered out through the reset gate and update gate. Finally, the output layer adopts Sigmoid activation function. Firstly, we use Kmer, the deviation between the dipeptide frequency and the expected mean (DDE), encoding based on grouped weight (EBGW), enhanced grouped amino acid composition (EGAAC) and dipeptide binary profile and frequency (DBPF) to extract features. For Kmer, DDE, EBGW and EGAAC, it is widely used in the field of protein research. DBPF is a new feature representation method designed by us. It corresponds dipeptides to binary numbers, and finally obtains a binary coding file and a frequency file. Then these features are spliced together and input into our proposed model for prediction and analysis. After a tenfold cross-validation test, this model has a better competitive advantage than the previous methods, and the accuracy is 96.23% and 99.10%, respectively. From the results, compared with the previous methods, it has been greatly improved. It shows that the combination of convolution calculation and recurrent structure has a positive impact on the classification of AHTPs. The results show that this method is a feasible, efficient and competitive sequence analysis tool for AHTPs. Meanwhile, we design a friendly online prediction tool and it is freely accessible at http://ahtps.zhanglab.site/ . Graphical Abstract ","['Life Sciences', 'Computer Appl. in Life Sciences', 'Computational Biology/Bioinformatics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Theoretical and Computational Chemistry', 'Theoretical, Mathematical and Computational Physics', 'Computational Science and Engineering']"
doi:10.1007/s10483-022-2940-9,en,Active control of flow past an elliptic cylinder using an artificial neural network trained by deep reinforcement learning,OriginalPaper,"The active control of flow past an elliptical cylinder using the deep reinforcement learning (DRL) method is conducted. The axis ratio of the elliptical cylinder Γ varies from 1.2 to 2.0, and four angles of attack α = 0°, 15°, 30°, and 45° are taken into consideration for a fixed Reynolds number Re = 100. The mass flow rates of two synthetic jets imposed on different positions of the cylinder θ 1 and θ 2 are trained to control the flow. The optimal jet placement that achieves the highest drag reduction is determined for each case. For a low axis ratio ellipse, i.e., Γ = 1.2, the controlled results at α = 0° are similar to those for a circular cylinder with control jets applied at θ 1 = 90° and θ 2 = 270°. It is found that either applying the jets asymmetrically or increasing the angle of attack can achieve a higher drag reduction rate, which, however, is accompanied by increased fluctuation. The control jets elongate the vortex shedding, and reduce the pressure drop. Meanwhile, the flow topology is modified at a high angle of attack. For an ellipse with a relatively higher axis ratio, i.e., Γ ⩾ 1.6, the drag reduction is achieved for all the angles of attack studied. The larger the angle of attack is, the higher the drag reduction ratio is. The increased fluctuation in the drag coefficient under control is encountered, regardless of the position of the control jets. The control jets modify the flow topology by inducing an external vortex near the wall, causing the drag reduction. The results suggest that the DRL can learn an active control strategy for the present configuration.","['Mathematics', 'Applications of Mathematics', 'Classical Mechanics', 'Mathematical Modeling and Industrial Mathematics', 'Partial Differential Equations', 'Fluid- and Aerodynamics']"
doi:10.1007/s00170-022-10355-4,en,Prediction of micro-hardness in thread rolling of St37 by convolutional neural networks and transfer learning,"['OriginalPaper', 'ORIGINAL ARTICLE']","This study introduces a non-destructive method by applying convolutional neural networks (CNN) to predict the micro-hardness of the thread-rolled steel. Material microstructure images were collected for our research, and micro-hardness tests were conducted to label the extracted microstructure images. In recent years, researchers have used machine learning (ML) and deep learning (DL) models to predict material properties for forming, machining, additive manufacturing, and other processes. However, they encountered industrial limitations primarily because of the absence of historical information on new and unknown materials, which are necessary to predict material properties by DL models. These problems can be solved by employing CNN models. In our work, we used a CNN model with two convolutional layers and visual geometry group (VGG19) as transfer learning (TL). We predicted four classes of micro-hardness of the St37 rolled threads. The prediction results of the micro-hardness test images by our proposed CNN model and pre-trained VGG19 model are comparable. Our proposed model has produced the same precision and recall scores as VGG19 for class B and class C hardness. VGG19 performed slightly better than our model for precision in class A and recall in class D. We observed that the training time of our proposed model using the CPU (central processing unit) was approximately nine times faster than the VGG19 model. Our proposed CNN and VGG19 have direct applications in advanced manufacturing (AM). They can automatically predict the micro-hardness in the thread rolling of St37. Our proposed model requires less memory and computational power and can be deployed more efficiently than the VGG19 model.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s11625-022-01245-5,en,A multi-model approach to explore sustainable food and land use pathways for Argentina,"['Report', 'Special Feature: Technical Report']","In Argentina, current food and land-use systems are drivers of greenhouse gas emissions, biodiversity loss, nutrient outflows, chemical pollution and water stress, while they fail to produce sustainable livelihoods for farmers and herders. Argentina must transition toward more sustainable food and land-use systems to achieve the sustainable development goals (SDGs) and the objectives of the Paris Agreement. Here, we present mid-century food and land-use system pathways to achieve biodiversity, freshwater use, food production and greenhouse gas emission targets, co-developed with the government, research and civil society stakeholders. We used a multi-model approach, integrating outputs from a food system and land-use accounting tool (FABLE calculator), a land-use allocation model (Dinamica EGO) and a spatially explicit conservation prioritization approach (NatureMap) to construct a carbon neutral, actionable food system and land-use scenario that could also lead to the achievement of biodiversity, freshwater use, food production and carbon storage targets by 2050. Such integrated approaches are rare, despite their high value for helping cross-sectoral experts and policymakers cut through complexity to find pathways to achieve multiple sustainability objectives in tandem. This paper presents a nationally designed transferable methodology to: (1) construct a carbon neutral pathway toward 2050, (2) create spatially explicit land-use projections, (3) detect and assess trade-offs between sustainability goals, (4) modify this pathway to foster co-benefits and (5) work toward concurrent attainment of multiple SDGs. Preliminary results suggest Argentina is well suited to meet multiple SDGs, provided businesses, civil society and government agree to several key commitments, including completely halting deforestation, promoting afforestation and reforestation, and increasing agricultural productivity to spare natural lands.","['Environment', 'Environmental Management', 'Climate Change Management and Policy', 'Environmental Economics', 'Landscape Ecology', 'Sustainable Development', 'Public Health']"
doi:10.1007/s11042-022-13237-9,en,A novel method for single nighttime image haze removal based on gray space,OriginalPaper,"Nighttime haze images always suffer from non-uniform illumination from artificial light sources, and most of the current dehazing algorithms are more suitable for daytime image haze removal than nighttime. In this paper, we propose a novel method for nighttime image dehazing via gray space. Firstly, we mapped the haze image from RGB color space to gray space and adopted convolutional neural network to obtain the feature distribution map of the haze. We then fused the haze feature distribution map with original image to obtain the initial haze-free image. Finally, the value and chroma of the initial haze-free image were enhanced in HSV space by improved gamma function. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art algorithms for nighttime image haze removal, especially in terms of color consistency and artifacts reduction.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11119-022-09940-0,en,Leaf area index estimations by deep learning models using RGB images and data fusion in maize,OriginalPaper,"The leaf area index (LAI) is a biophysical crop parameter of great interest for agronomists and plant breeders. Direct methods for measuring LAI are normally destructive, while indirect methods are either costly or require long pre- and post-processing times. In this study, a novel deep learning-based (DL) model was developed using RGB nadir-view images taken from a high-throughput plant phenotyping platform for LAI estimation of maize. The study took place in a commercial maize breeding trial during two consecutive growing seasons. Ground-truth LAI values were obtained non-destructively using an allometric relationship that was derived to calculate the leaf area of individual leaves from their main leaf dimensions (length and maximum width). Three convolutional neural network (CNN)-based DL model approaches were proposed using RGB images as input. One of the models tested is a classification model trained with a set of RGB images tagged with previously measured LAI values (classes). The second model provides LAI estimates from CNN-based linear regression and the third one uses a combination of RGB images and numerical data as input of the CNN-based model (multi-input model). The results obtained from the three approaches were compared against ground-truth data and LAI estimations from a classic indirect method based on nadir-view image analysis and gap fraction theory. All DL approaches outperformed the classic indirect method. The multi-input_model showed the least error and explained the highest proportion of the observed LAI variance. This work represents a major advance for LAI estimation in maize breeding plots as compared to previous methods, in terms of processing time and equipment costs.","['Life Sciences', 'Agriculture', 'Soil Science & Conservation', 'Remote Sensing/Photogrammetry', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Atmospheric Sciences']"
doi:10.1007/s40313-022-00932-z,en,Multicriteria Analysis of Natural Gas Network Pipe Sizing Design Under Load-Evolution Uncertainty,OriginalPaper,"Natural gas has been increasingly used as a source of energy and presents itself as a strong trend for the future. In this context, regarding the high cost of installing pipelines, the design of gas networks requires highlight quality solutions, relating not only financial indicators but also reliability and security concerning demand. Thus, this paper proposes an approach for the design of natural gas networks under conditions of uncertainty of load evolution over a time horizon. A predefined network topology is assumed, where the pipe diameters define the design variables. We propose a Multiobjective Variable Neighborhood Search (MOVNS)-based algorithm, which is evaluated considering a set of test instances defined from the TSPLIB library data. The proposed methodology is also applied to a real case study being the results compared to those obtained by three engineers of a gas company with six years of experience on average. The solutions are investigated from a dominance analysis perspective, considering the criteria: installation cost, minimum gas pressure, feasibility rate, average cost of failure, and sensitivity. The results indicate solutions relatively different from those obtained by the engineers, presenting more robust and safe networks under conditions of uncertainties of load evolution.","['Engineering', 'Electrical Engineering', 'Control, Robotics, Mechatronics', 'Control and Systems Theory', 'Robotics and Automation']"
doi:10.1038/s41467-022-35094-8,en,A unified computational framework for single-cell data integration with optimal transport,"['OriginalPaper', 'Article']","Single-cell data integration can provide a comprehensive molecular view of cells. However, how to integrate heterogeneous single-cell multi-omics as well as spatially resolved transcriptomic data remains a major challenge. Here we introduce uniPort, a unified single-cell data integration framework that combines a coupled variational autoencoder (coupled-VAE) and minibatch unbalanced optimal transport (Minibatch-UOT). It leverages both highly variable common and dataset-specific genes for integration to handle the heterogeneity across datasets, and it is scalable to large-scale datasets. uniPort jointly embeds heterogeneous single-cell multi-omics datasets into a shared latent space. It can further construct a reference atlas for gene imputation across datasets. Meanwhile, uniPort provides a flexible label transfer framework to deconvolute heterogeneous spatial transcriptomic data using an optimal transport plan, instead of embedding latent space. We demonstrate the capability of uniPort by applying it to integrate a variety of datasets, including single-cell transcriptomics, chromatin accessibility, and spatially resolved transcriptomic data. Integrating heterogeneous single-cell multi-omics as well as spatially resolved transcriptomic data remains a major challenge. Here the authors report a unified single-cell data integration framework using an unbalanced optimal transport-based deep network.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00530-020-00661-w,en,D-BullyRumbler: a safety rumble strip to resolve online denigration bullying using a hybrid filter-wrapper approach,"['OriginalPaper', 'Special Issue Paper']","Denigration is a specialized form of cyberbullying which describes a recurrent, sustained and intentional attempt to damage the victim’s reputation or ruin the friendships that he or she has by spreading unfounded gossip or rumors online. It is the most common bullying tactic involving character assassination of public figures like celebrities and politicians. As a comprehensive approach to match to the scale of social media this research put forwards a D-BullyRumbler model for automatic detection and resolution of denigration cyberbullying in online textual content using a hybrid of lexicon-based and machine learning-based techniques. The model processes textual, content-based and user-based features to uncover denigration from two perspectives. Firstly, a direct explicit content analysis is done to look for denigration markers as features for model training and testing. Concurrently, potentially harmful messages, rumors, are identified as candidates and examined for target profile type to reveal the case of denigration. An additional OR operation is done to maintain the holistic framework. Another novelty of the work includes the use of hybrid filter-wrapper method, Chi-square filter and cuckoo search wrapper algorithm to improve the performance of reputation rumor classification module. Experimental results on social media datasets show the superior classification performance. The results validate the effectiveness of the proposed model which facilitates timely intervention by buzzing an alarm to the moderators and further forming a rumble safety strip to inhibit the production and dissemination of inappropriate content to protect the victims.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s12145-022-00879-4,en,A novel strategy for classifying spectral-spatial shallow and deep hyperspectral image features using 1D-EWT and 3D-CNN,"['OriginalPaper', 'Research']","Hyperspectral images (HIs) are used in diverse disciplines, such as resource handling, land cover analysis, food science, anomaly detection, and precision agriculture. Researchers have been working on a number of visual processing and machine intelligence algorithms to handle this type of data as efficiently as feasible. Deep learning approaches have advanced significantly in the field of machine vision, which is also having a big impact on the analysis of hyperspectral data. To increase its discriminative potential for HI classification, this work suggests a powerful 3D-CNNs (Convolutional Neural Networks) architecture, where the shallow features extracted using 1D-EWT (Empirical Wavelet Transform) are served as input, and the ultimate output of the CNN are projected class-related outcomes. The framework is known as PEC , where P stands for PCA, E for 1D-EWT, and C for 3D-CNN model. Prior to features extraction, the HI undergoes spectral dimension reduction via Principal Component Anaalysis (PCA). To forecast segmentation for a volumetric area of a 3D HI sample, 3D CNNs use 3D convolutional kernels. The usage of more parameters by these CNNs means that the ability to employ interslice context that can boost performance. The CNN model’s parameters are optimised using a limited training set. The newly proposed PEC framework achieves a considerable overall accuracy of 99.58% and 99.94% (percentage increase of 0.08% and 0.54%) and Kappa score of 99.51% and 99.92 (percentage increase of 1.5% and 0.8%) compared to similar approaches for two real-world data sets IP and PU respectively with 30% training samples.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1557/s43579-022-00288-0,en,A perspective on Bayesian methods applied to materials discovery and design,"['ReviewPaper', 'Computational Approaches for Materials Discovery and Development Prospective']","For more than two decades, there has been increasing interest in developing frameworks for the accelerated discovery and design of novel materials that could enable promising and transformative technologies. The Integrated Computational Materials Engineering (ICME) program called for integrating computational tools to establish linkages along process–structure–property–performance chains. The Materials Genome Initiative called for integrating experiments and computations within data science frameworks as a strategy to accelerate the materials development cycle. While these frameworks and paradigms have been quite influential, traditional ICME or data science-based approaches tend to have some limitations, mainly when querying the materials space is costly and very little information is available. Bayesian methods are more suitable in this context due to their efficiency gains. To this end, the materials discovery problem is framed as a Bayesian optimization (BO). Different examples in which BO has been applied to solve materials discovery problems are presented. The methods/examples discussed include BO under model uncertainty, multi-information source BO, multi-objective and multi-constraint BO, and batch BO. Bayesian Materials Discovery is a promising area of research that is likely to become more influential as more attention is put on autonomous materials discovery platforms. Therefore, a discussion is provided on the potential development of such methods to increase the ability of existing platforms in materials discovery. The ultimate goal is to pave the way to autonomous materials discovery. Graphical abstract ","['Materials Science', 'Materials Science, general', 'Materials Engineering', 'Nanotechnology', 'Characterization and Evaluation of Materials', 'Biomaterials', 'Polymer Sciences']"
doi:10.1007/s00521-021-06147-8,en,Towards end-to-end car license plate location and recognition in unconstrained scenarios,"['OriginalPaper', 'S.I. : NCACVIP']","Benefiting from the rapid development of convolutional neural networks, the performance of car license plate detection and recognition has been largely improved. Nonetheless, most existing methods solve detection and recognition problems separately, and focus on specific scenarios, which hinders the deployment for real-world applications. To overcome these challenges, we present an efficient and accurate framework to solve the license plate detection and recognition tasks simultaneously. It is a lightweight and unified deep neural network, that can be optimized end-to-end and work in real-time. Specifically, for unconstrained scenarios, an anchor-free method is adopted to efficiently detect the bounding box and four corners of a license plate, which are used to extract and rectify the target region features. Then, a novel convolutional neural network branch is designed to further extract features of characters without segmentation. Finally, the recognition task is treated as sequence labeling problems, which are solved by Connectionist Temporal Classification (CTC) directly. Several public datasets including images collected from different scenarios under various conditions are chosen for evaluation. Experimental results indicate that the proposed method significantly outperforms the previous state-of-the-art methods in both speed and precision.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12161-022-02362-8,en,Application of Pre-Trained Deep Convolutional Neural Networks for Coffee Beans Species Detection,OriginalPaper,"Coffee is an important export product of the tropical countries where it is grown. Therefore, the separation of coffee beans in the world in terms of the quality element and variety forgery is an important situation. Currently, the use of manual control methods leads to the fact that the parsing processes are inconsistent, time-consuming, and subjective. Automated systems are needed to eliminate such negative situations. The aim of this study is to classify 3 different coffee beans by using their images, through the transfer learning method by utilizing 4 different Convolutional Neural Networks-based models, which are SqueezeNet, Inception V3, VGG16, and VGG19. The dataset used in the models’ training was created specially for this study. A total of 1554 coffee bean images of Espresso, Kenya, and Starbucks Pike Place coffee types were collected with the created mechanism. Model training and model testing processes were carried out with the obtained images. In order to test the models, the cross-validation method was used. Classification success, Precision, Recall, and F-1 Score metrics were used for the detailed analysis of the models of performances. ROC curves were used for analyzing their distinctiveness. As a result of the tests, the average classification success of the models was determined as 87.3% for SqueezeNet, 81.4% for Inception V3, 78.2% for VGG16, and 72.5% for VGG19. These results demonstrate that the SqueezeNet is the most successful model. It is thought that this study may contribute to the subject of coffee beans of separation in the industry.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Microbiology', 'Analytical Chemistry']"
doi:10.1007/s10489-021-03074-y,en,An algorithm with harmonious blending of distributed swarm intelligence and geometric Brownian motion for greener heterogeneous scheduling,OriginalPaper,"This paper focuses on the design and implementation of green heterogeneous scheduling algorithm based on the theory and technology hotspot of energy saving and emission reduction in cloud computing, since intelligent scheduling algorithms often have defects such as insufficient dynamic optimization power or poor parallel framework design. Following that, a distributed swarm intelligence optimization algorithm for greener heterogeneous scheduling, is proposed, including ⓵ an optimal blending pattern of particle swarm intelligence and nonlinear geometric Brownian motion, based on the related physical sciences and stochastic mathematical analyses, and ⓶ a parallel fusion model that is suitable for the management server processor hybrid system, i.e., coarse-grained parallelism between nodes and CPU/GPU master-slave in a single node. Then, a large number of experimental results are given, whose evaluation indicators can be divided into two categories: static and dynamic optimization performance. Compared with most newly published scheduling algorithms, there are significant advantages of the proposed algorithm on the dynamic optimization performance for consistent or semiconsistent and large inconsistent scheduling instances, although with the lower improvement for the small inconsistent instances.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s12530-022-09423-7,en,Correlation-based modified long short-term memory network approach for software defect prediction,"['OriginalPaper', 'Original Paper']","Developing software applications has become more perplexing nowadays due to the huge usage of software applications. Under such circumstances, developing software without defects is a very challenging task. So, detecting defects in software modules is necessary for the developers to allocate appropriate sources for the project. Knowing the defects in advance increases the software quality at a low cost. This article aims to develop a correlation-based neural network model for identifying defects in software projects. A novel correlation-based modified long short-term memory neural network (CM-LSTM) is proposed to estimate the software defects in software modules with modeled data. Based on the positive correlation between the features and the target variable, target variables have been changed. The prepared data is fed to the LSTM model to overcome the imbalance issue in the software defect prediction data. The adequacy of the proposed method is tested with a JM1 software defect prediction dataset with various performance parameters. It is observed that the proposed correlation-based modified LSTM technique is effective in detecting defects in software projects. The proposed technique employs correlation-based feature selection for long-short term memory neural networks to identify defects in software projects, and it is found to be more efficient than other existing approaches such as correlation-based LSTM, K-nearest neighbor, Stochastic gradient descent, Random forest, Gaussian Naive Bays, Logistic regression, Decision trees, Linear discriminant analysis, Multi-layer perceptron.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1007/s40747-022-00699-5,en,Gradient-supervised person re-identification based on dense feature pyramid network,"['OriginalPaper', 'Original Article']","In the monitoring scene, parameters of different cameras are vary greatly, which makes Person re-identification (Re-ID) tasks extremely susceptible to factors such as scale, blur, and occlusion. To alleviate the these problems, this paper proposes a Dense Feature Pyramid Network (DFPN), which can converge to a better performance without pretraining. To be more specific, DFPN is composed of three main parts. First, a new Residual Convolutional Block (RCB) is designed by referring to the construction method of ResBlock. Taking RCB as a basic unit and combining it with the convolution layer structure of VGGNet, we construct the backbone RVNet (Residual VGGNet) to realize the rapid convergence of the network and solve the disappearance of the gradient. Second, based on Feature Pyramid Network, we design the Dense Pyramid Fusion Module by integrating the connection mode of DenseNet, which aims at the improvement of the richness and scale diversity of feature maps by taking semantic information and detail information into account. Finally, to increase the receptive field of the feature map, we introduce an improved retinal receptive field structure Improved RFB (IRFB) on the basis of Receptive Field Block (RFB), which can effectively solve the problem of pedestrian occlusion. In experiments on the public datasets Market1501, DukeMTMC-reID and Occluded-Duke, the Rank-1 accuracy can reach 94.12%, 87.25% and 51.72% with pretraining, respectively. A series of ablation experiments and comparative experiments have proved the effectiveness of our modules and overall scheme.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10722-022-01399-2,en,"Development and characterization of microsatellite markers, genetic diversity and population structure analysis in Sapota (Manilkara zapota (L.) P. Royen)","['OriginalPaper', 'Research Article']","Manilkara zapota (L.) P. Royen, a widely adapted and popular tree meant for its appetizing fruits in tropics with no genomic resources like microsatellite markers. In order to develop genomic markers primarily for Sapota, the study sequenced partial genomic DNA using next generation sequencing technology on Illumina HiSeq 2500 platform. 3.3 Gb data assembled into 6,396,224 contigs were analysed. From these contigs, 3591 simple sequence repeats were identified. Among these different types of repeats, mononucleotide repeats (59.1%) were predominant followed by dinucleotide (28.6%) and trinucleotide repeats (8.2%). Primers were designed for 1285 microsatellite regions from which randomly selected 30 primers were standardized and employed for amplification in 53 genotypes of Sapota. Six hundred and ninty-two alleles were observed from 30 loci with a polymorphic information content that ranged from 0.85 to 0.96 with a mean of 0.9118. The Probability of Identity ranged from 0.002 to 0.043 with a mean of 0.012. The genetic diversity assessed by neighbour-joining and STRUCTURE assignment tests showed an admixed population with three groups. Analysis of molecular variance revealed a significant F st value of 0.69659 indicating high genetic differentiation among the 53 genotypes. Thus, the developed microsatellites will be advantageous in assessing genetic diversity, developing linkage map, and also molecular characterization of genotypes.","['Life Sciences', 'Plant Sciences', 'Plant Systematics/Taxonomy/Biogeography', 'Plant Genetics and Genomics', 'Plant Physiology', 'Agriculture']"
doi:10.1007/s10115-022-01748-8,en,Risk-aware temporal cascade reconstruction to detect asymptomatic cases,"['OriginalPaper', 'Regular Paper']","This paper studies the problem of detecting asymptomatic cases in a temporal contact network in which multiple outbreaks have occurred. We show that the key to detecting asymptomatic cases well is taking into account both individual risk and the likelihood of disease-flow along edges. We consider both aspects by formulating the asymptomatic case detection problem as a directed prize-collecting Steiner tree ( Directed PCST ) problem. We present an approximation-preserving reduction from this problem to the directed Steiner tree problem and obtain scalable algorithms for the Directed PCST problem on instances with more than 1.5M edges obtained from both synthetic and fine-grained hospital data. On synthetic data, we demonstrate that our detection methods significantly outperform various baselines (with a gain of $$3.6 \times $$ 3.6 × ). We apply our method to the infectious disease prediction task by using an additional feature set that captures exposure to detected asymptomatic cases and show that our method outperforms all baselines. We further use our method to detect infection sources (“patient zero”) of outbreaks that outperform baselines. We also demonstrate that the solutions returned by our approach are clinically meaningful by presenting case studies.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s42600-022-00242-y,en,Covid-19 detection using chest X-rays: is lung segmentation important for generalization?,"['OriginalPaper', 'Original Article']","Purpose We evaluated the generalization capability of deep neural networks (DNNs) in the task of classifying chest X-rays as Covid-19, normal or pneumonia, when trained in a relatively small and mixed datasets. Methods We proposed a DNN to perform lung segmentation and classification, stacking a segmentation module (U-Net), an original intermediate module and a classification module (DenseNet201). To evaluate generalization capability, we tested the network with an external dataset (from distinct localities) and used Bayesian inference to estimate the probability distributions of performance metrics. Furthermore, we introduce a novel evaluation technique, which uses layer-wise relevance propagation (LRP) and Brixia scores to compare the DNN grounds for decision with radiologists. Results The proposed DNN achieved 0.917 AUC (area under the ROC curve) on the external test dataset, surpassing a DenseNet without segmentation, which showed 0.906 AUC. Bayesian inference indicated mean accuracy of 76.1% and [0.695, 0.826] 95% HDI (high-density interval, which concentrates 95% of the metric’s probability mass) with segmentation and, without segmentation, 71.7% and [0.646, 0.786]. Conclusion Employing an analysis based on LRP and Brixia scores, we discovered that areas where radiologists found strong Covid-19 symptoms are the most important for the stacked DNN classification. External validation showed smaller accuracies than internal, indicating difficulty in generalization, which is positively affected by lung segmentation. Finally, the performance on the external dataset and the analysis with LRP suggest that DNNs can successfully detect Covid-19 even when trained on small and mixed datasets.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Biomedical Engineering/Biotechnology']"
doi:10.1007/s00477-022-02249-4,en,"Predicting daily reference evapotranspiration rates in a humid region, comparison of seven various data-based predictor models","['OriginalPaper', 'Original Paper']","The reference crop evapotranspiration (ET 0 ) is one of the major components of the hydrological cycle, and its prediction is of great importance in agricultural operations, especially irrigation, of field and horticultural crops. The present study aims to evaluate the performances of two stochastic and machine learning models in predicting ET 0 for Mazandaran province, which is one of the most important centers of rice cultivation (as a high-water use plant) in Iran. The studied data belong to 5 synoptic stations in Mazandaran province. They include minimum, maximum, and mean air temperature, minimum, maximum, and mean relative humidity, wind speed, and sunshine duration. These data are received on a daily basis from the Iranian Meteorological Organization during the period 2003–2018. Then, these variables and the FAO-56 Penman-Monteith model are used to calculate daily ET 0 rates. Moreover, stochastic models including autoregressive (AR), moving average (MA), autoregressive moving average (ARMA), and autoregressive integrated moving average (ARIMA), and machine learning models including least square support vector machine (LSSVM), adaptive neuro-fuzzy inference system (ANFIS), and generalized regression neural network (GRNN) are used to predict ET 0 . Predictor inputs include ET 0 time lags selected by Autocorrelation Function (ACF) and partial ACF (PACF). The time series models of ARMA and ARIMA, and the machine learning model of LSSVM provide the most accurate predictions with the slight superiority of ARMA and ARIMA over LSSVM in most cases. As a result, it is found that stochastic models are superior to machine learning models due to their more accurate prediction and less complexity. The ARMA model (root mean square error = 0.623 $$\frac{mm}{day}$$ mm day , Wilmott index = 0.962, and R 2  = 86.22%) shows the highest prediction accuracy. The current approach can be applied to predict irrigation water requirements and has research value under similar or different climatic conditions.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s12206-022-1125-3,en,Wind power forecasting based on hourly wind speed data in South Korea using machine learning algorithms,"['OriginalPaper', 'Original Article']","Given that wind farms have high initial investment costs and are not easy to move after installation, the amount of energy that can be produced in the desired installation area needs to be predicted as accurately as possible before installation. Four machine learning algorithms are adopted to predict power production based on the daily wind speed average and standard deviation. The actual power output is calculated from the wind data generated by the numerical weather prediction, and its temporal resolution is 1 hour. The R-square (R 2 ) values of the models range from 0.97 to 0.98 while adopting the average value of daily wind speed as the input data, and it increases by −1 % with the additional input data of the standard deviation of wind speed. The power production is predicted based on the wind data at a relatively lower height of 10 m than the hub height, where the R 2 value ranges from 0.95 to 0.98. The results could provide the possibility of replacing the wind data measurement process at the hub height by that at a relatively lower height, reducing the cost of wind data measurement.","['Engineering', 'Mechanical Engineering', 'Vibration, Dynamical Systems, Control', 'Industrial and Production Engineering']"
doi:10.1007/s11277-022-09896-4,en,Linear 1 × 4 Microstrip Antenna Array Using Slotted Circular Patch for 5G Communication Applications,OriginalPaper,"The paper reports a high gain linear 1*4 antenna array using circular slotted patch for 5G communication applications. The proposed antenna has been designed for 28 GHz frequency and supports TM11 as a fundamental mode at resonance. To feed the antenna array, microstrip feed technique has been utilized here. The antenna has been designed on Rogers RT/Duroid 5880 substrate with dielectric constant of 2.2 and thickness of 0.254 mm. Further, a detailed electromagnetic analysis of antenna has been done in the paper to provide better understanding of the proposed concept using commercially available CST Microwave studio. To validate the concept, the prototype of the proposed antenna has been characterized using VNA and anechoic chamber. The proposed array antenna has been designed for 28 GHz center frequency with 16 dB return loss and having − 10 dB impedance bandwidth of 10% (24.6–27.24 GHz) in the millimeter-wave band. Measured and simulated results are in good agreement with each other.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s10032-022-00415-6,en,Benchmarking online sequence-to-sequence and character-based handwriting recognition from IMU-enhanced pens,"['OriginalPaper', 'Special Issue Paper']","Handwriting is one of the most frequently occurring patterns in everyday life and with it comes challenging applications such as handwriting recognition, writer identification and signature verification. In contrast to offline HWR that only uses spatial information (i.e., images), online HWR uses richer spatio-temporal information (i.e., trajectory data or inertial data). While there exist many offline HWR datasets, there are only little data available for the development of OnHWR methods on paper as it requires hardware-integrated pens. This paper presents data and benchmark models for real-time sequence-to-sequence learning and single character-based recognition. Our data are recorded by a sensor-enhanced ballpoint pen, yielding sensor data streams from triaxial accelerometers, a gyroscope, a magnetometer and a force sensor at 100 Hz. We propose a variety of datasets including equations and words for both the writer-dependent and writer-independent tasks. Our datasets allow a comparison between classical OnHWR on tablets and on paper with sensor-enhanced pens. We provide an evaluation benchmark for seq2seq and single character-based HWR using recurrent and temporal convolutional networks and transformers combined with a connectionist temporal classification (CTC) loss and cross-entropy (CE) losses. Our convolutional network combined with BiLSTMs outperforms transformer-based architectures, is on par with InceptionTime for sequence-based classification tasks and yields better results compared to 28 state-of-the-art techniques. Time-series augmentation methods improve the sequence-based task, and we show that CE variants can improve the single classification task. Our implementations together with the large benchmark of state-of-the-art techniques of novel OnHWR datasets serve as a baseline for future research in the area of OnHWR on paper.","['Computer Science', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11030-022-10382-z,en,Detection of polypharmacy side effects by integrating multiple data sources and convolutional neural networks,"['OriginalPaper', 'Original Article']","Abstract The consumption of drug combinations, named polypharmacy, is commonly used for treating patients with several diseases or those with complex conditions. However, the main drawback of polypharmacy is the increased probability of harmful side effects. The polypharmacy side effects are caused by an interaction between two medications. It means that the drug–drug interaction causes changes in their activities due to interfering in each other’s performance. Therefore, discovering these side effects is one of the most challenging and important aspects of drug production and consumption as it is associated with human health. In this paper, a method has been introduced for predicting the polypharmacy side effects, called PSECNN. It is a multi-label multi-class deep learning method that combines various basic features of drugs to predict the polypharmacy side effects. Firstly, PSECNN collects five basic features of drugs, such as individual drug’s side effects, drug–protein interactions, chemical substructures, targets, and enzymes in order to create a novel combination of drug features. A feature extraction module creates five feature vectors with the same dimension for each drug based on the Jaccard similarity index. Based on the feature vectors, a unique representative is then created for each drug. These representative vectors are given in pairs as input to the deep neural network to predict the occurrence probability of side effects. According to the experimental evaluations, PSECNN could outperform the state-of-the-art polypharmacy side effects prediction methods up to 74%. It has been found that PSECNN has better performance with polypharmacy side effects with a cause of molecular basis due to the novel combination of basic drug features. Graphical Abstract ","['Life Sciences', 'Biochemistry, general', 'Organic Chemistry', 'Polymer Sciences', 'Pharmacy']"
doi:10.1007/s10107-022-01910-8,en,The landscape of the proximal point method for nonconvex–nonconcave minimax optimization,"['OriginalPaper', 'Full Length Paper']","Minimax optimization has become a central tool in machine learning with applications in robust optimization, reinforcement learning, GANs, etc. These applications are often nonconvex–nonconcave, but the existing theory is unable to identify and deal with the fundamental difficulties this poses. In this paper, we study the classic proximal point method (PPM) applied to nonconvex–nonconcave minimax problems. We find that a classic generalization of the Moreau envelope by Attouch and Wets provides key insights. Critically, we show this envelope not only smooths the objective but can convexify and concavify it based on the level of interaction present between the minimizing and maximizing variables. From this, we identify three distinct regions of nonconvex–nonconcave problems. When interaction is sufficiently strong, we derive global linear convergence guarantees. Conversely when the interaction is fairly weak, we derive local linear convergence guarantees with a proper initialization. Between these two settings, we show that PPM may diverge or converge to a limit cycle.","['Mathematics', 'Calculus of Variations and Optimal Control; Optimization', 'Mathematics of Computing', 'Numerical Analysis', 'Combinatorics', 'Theoretical, Mathematical and Computational Physics', 'Mathematical Methods in Physics']"
doi:10.1007/s10489-022-03324-7,en,A time-dependent attention convolutional LSTM method for traffic flow prediction,OriginalPaper,"With traffic network becoming increasingly complicated, traffic flow prediction has important practical significance for the management of traffic roads and public safety. For example, an accurate taxi demand prediction can help to improve efficiency of vehicle scheduling and reduce traffic congestion. The main issue of flow prediction is how to extract the information of complex spatio-temporal dependencies and interactions between arrival and departure. To solve these problems, we develop a deep learning method based on time-dependent attention convolutional LSTM (TDAConvLSTM) in which a time-dependent attention mechanism is designed to learn similarities of historical traffic flows among different time intervals and a fusion mechanism is introduced to aggregate the feature information produced by convolutional LSTM and attention module. And then, the result of the feature aggregation is fed to a multi-layer deconvolutional network to gain the results of flow prediction. Experimental studies on two real-life datasets indicate that TDAConvLSTM achieves better results than the compared models. The source code of our proposed method is available at the URL 1 .","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10994-022-06272-y,en,Bayesian mixture variational autoencoders for multi-modal learning,OriginalPaper,"This paper provides an in-depth analysis on how to effectively acquire and generalize cross-modal knowledge for multi-modal learning. Mixture-of-Expert (MoE) and Product-of-Expert (PoE) are two popular directions in generalizing multi-modal information. Existing works based on MoE or PoE have shown notable improvement on data generation, while new challenges such as high training cost, overconfident experts, and encoding modal-specific features also emerge. In this work, we propose Bayesian mixture variational autoencoder (BMVAE) which learns to select or combine experts via Bayesian inference. We show that the proposed idea can naturally encourage models to learn modal-specific knowledge and avoid overconfident experts. Also, we show that the idea is compatible with both MoE and PoE frameworks. When being a MoE model, BMVAE can be optimized by a tight lower bound and is efficient to train. The PoE BMVAE has the same advantages and a theoretical connection to existing works. In the experiments, we show that BMVAE achieves state-of-the-art performance.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s12555-021-0929-8,en,RANET: A Grasp Generative Residual Attention Network for Robotic Grasping Detection,"['OriginalPaper', 'Regular Papers']","This paper presents a novel grasp generative residual attention network (RANET) for generating antipodal robotic grasp from multi-modal images with the pixel-wise method. To strengthen the generalization ability of unknown objects, this paper proposed a new structure that differs from the previous grasp generative network in that it additionally integrates a coordinate attention mechanism and a symmetrical skip connection, respectively. Using the coordinate attention module to emphasize meaningful information of the feature map and the symmetrical skip connection to remain more fine-grained details of feature. Moreover, a multi atrous convolution module is included in the structure to capture more high-level information, while a hypercolumn feature fusion method is incorporated for getting the best from the complementation of different layers’ features. Through evaluation on public datasets, the result demonstrates that we achieve 98.9% accuracy on the Cornell dataset which is the state-of-the-art performance with real-time speed(∼ 17 ms), meanwhile, we represent a 93.9% accuracy performance on the Jacquard dataset.","['Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11045-022-00835-x,en,TransRA: transformer and residual attention fusion for single remote sensing image dehazing,OriginalPaper,"Haze seriously reduces the quality of optical remote sensing images, resulting in poor performance in many applications, such as remote sensing image change detection and classification. In recent years, deep learning models have achieved convincing performance in image dehazing, which has attracted more and more attention in haze removal of remote sensing images. However, the existing deep learning-based methods are less able to recover the fine details of remote sensing images that suffered from haze, especially the cases of nonhomogeneous haze. In this paper, we propose a two-branch neural network fused with Transformer and residual attention to dehaze a single remote sensing image. Specifically, our upper branch is a U-shaped encoder–decoder architecture, using an efficient multi-head self-attention Transformer for capturing long-range dependencies. The lower branch is an attention stack of residual channels to enhance fitting capability of models and complement fine-detailed features for upper branch. Finally, the features of the two branches are stacked and mapped to the haze-free remote sensing image by fusion block. Extensive experiments demonstrate that our TransRA achieves superior performance against other dehazing competitors both qualitatively and quantitatively.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing', 'Artificial Intelligence']"
doi:10.1007/s10845-021-01784-1,en,A novel hypergraph convolution network-based approach for predicting the material removal rate in chemical mechanical planarization,OriginalPaper,"The material removal rate (MRR) plays a critical role in the chemical mechanical planarization (CMP) process in the semiconductor industry. Many physics-based and data-driven approaches have been proposed to-date to predict the MRR. Nevertheless, most of them neglect the underlying equipment structure containing essential interaction mechanisms among different components. To fill the gap, this paper proposes a novel hypergraph convolution network (HGCN) based approach for predicting MRR in the CMP process. The main contributions include: (1) a generic hypergraph model to represent the interrelationships of complex equipment; and (2) a temporal-based prediction approach to learn the complex data correlation and high-order representation based on the hypergraph. To validate the effectiveness of the proposed approach, a case study is conducted by comparing with other cutting-edge models, of which it outperforms in several metrics. It is envisioned that this research can also bring insightful knowledge to similar scenarios in the manufacturing process.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11063-022-10829-2,en,A Hybrid Optimized Deep Learning Framework to Enhance Question Answering System,OriginalPaper,"One of the challenging tasks in big data machine learning is the Question-Answering (QA) system. The QA system datasets have several question types: multiple-choice questions, yes or no queries, Wh questions, short questions, factoid questions, etc. Henceforth, training the data and classification of questions and answers is a difficult task. To address these issues, the current research work has focused on constructing a novel Recurrent Hybrid Ant Colony and African Buffalo Model (RHAC-ABM) for the QA classification and answer selection process. Initially, the specific QA dataset was trained to the system, then the trained datasets were tokenized, and training flaws are removed systematically. Moreover, the proposed model was designed by upgrading the hybrid Ant and African buffalo fitness to a dense layer. Also, the hybrid function in the recurrent classification layer can afford a high accuracy rate for query specification and answer selection process. Subsequently, the proficient measure of the designed approach was validated with other related existing models by comparing the chief metrics. Henceforth, the developed RHAC-ABM has gained 99.6% accuracy, F-measure, 99.51%, recall 98.5%, precision 98.5% and low error rate as 1.4% for question classification and answer selection process. Moreover, the achieved results are quite better than other models, it has proved the robustness of the proposed system.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11600-022-00796-6,en,Sustainable optimized LSTM-based intelligent system for air quality prediction in Chennai,"['OriginalPaper', 'Research Article - Special Issue']","Nowadays, air quality prediction is the most essential process taken by an Indian government. Due to poor quality of air, unhealthy lifestyle and premature deaths of humans have arisen in India, especially in Delhi. Not only has a human’s health, but the air pollution also made a huge impact on several areas like economy, agriculture and road accidents, etc. In recent times, deep learning (DL) technologies are influenced every application rapidly even in air pollution prediction. In this work, the novel optimised DL algorithms are proposed for the efficient prediction of air quality particularly focussing on Chennai, Tamil Nadu. To provide higher accuracy in air quality prediction, the novel optimised DL algorithms are proposed which is combined several models like ARIMA and CNN-LSTM and Tuna Optimization Algorithm, respectively. Initially, CNN and LSTM are combined to provide hybrid architecture. Next, the metaheuristics-based tuna swarm optimization model is applied for fine-tuning the hyperparameters of the CNN-LSTM model which is known as the Tuna Optimised CNN-LSTM (TOCL) method. Finally, the novel TOCL is applied to the residuals of the ARIMA model to form an ARIMA- TOCL (ARTOCL) model. As a result, the novel ARTOCL is learned and performed with an optimal air quality prediction. The metrics of the Hybrid ARTOCL model are evaluated as a better mean absolute error (MAE), root mean squared error (RMSE), R2 score and the normalized RMSE (nRMSE) with higher accuracy than the previous models. The results show that the proposed prediction model has 22.6% R2 improvement, 14.6% MAE reductions, 22% RMSE reductions and 16.45% nRMSE reductions than the existing models.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s12243-022-00910-1,en,Investigating the practicality of adversarial evasion attacks on network intrusion detection,OriginalPaper,"As machine learning models are increasingly integrated into critical cybersecurity tools, their security issues become a priority. Particularly after the rise of adversarial examples, original data to which a small and well-computed perturbation is added to influence the prediction of the model. Applied to cybersecurity tools, like network intrusion detection systems, they could allow attackers to evade detection mechanisms that rely on machine learning. However, if the perturbation does not consider the constraints of network traffic, the adversarial examples may be inconsistent, thus making the attack invalid. These inconsistencies are a major obstacle to the implementation of end-to-end network attacks. In this article, we study the practicality of adversarial attacks for the purpose of evading network intrusion detection models. We evaluate the impact of state-of-the-art attacks on three different datasets. Through a fine-grained analysis of the generated adversarial examples, we introduce and discuss four key criteria that are necessary for the validity of network traffic, namely value ranges, binary values, multiple category membership, and semantic relations.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Signal,Image and Speech Processing', 'Computer Communication Networks', 'Information and Communication, Circuits', 'R & D/Technology Policy']"
doi:10.3758/s13428-021-01734-y,en,SpaVerb-WN—A megastudy of naming times for 4562 Spanish verbs: Effects of psycholinguistic and motor content variables,OriginalPaper,"Several studies have been carried out in various languages to explore the role of the main psycholinguistic variables in word naming, mainly in nouns. However, reading of verbs has not been explored to the same extent, despite the differences that have been found between the processing of nouns and verbs. To reduce this research gap, we present here SpaVerb-WN, a megastudy of word naming in Spanish, with response times (RT) for 4562 verbs. RT were obtained from at least 20 healthy adult participants in a reading-aloud task. Several research questions on the role of syllable frequency, word length, neighbourhood, frequency, age of acquisition (AoA), and the novel variable ‘motor content’ in verb naming were also examined. Linear mixed-effects model analyses indicated that (1) RT increase in with increasing word length and with decreasing neighbourhood size, (2) syllable frequency does not show a significant effect on RT, (3) AoA mediates the effect of motor content, with a positive slope of motor content at low AoA scores and a negative slope at high AoA scores, and (4) there is an interaction between word frequency and AoA, in which the AoA effect for low-frequency verbs gradually decreases as frequency increases. The results are discussed in relation to existing evidence and in the context of the consistency of the spelling–sound mappings in Spanish.","['Psychology', 'Cognitive Psychology']"
doi:10.1007/s10844-022-00708-6,en,A contextual-bandit approach for multifaceted reciprocal recommendations in online dating,OriginalPaper,"Recommender Systems (RS) provide an effective way to deal with the problem of information overload by suggesting relevant items to users that the users may prefer. However, many online social platforms such as online dating and online recruitment recommend users to each other where both the users have preferences that should be considered for generating successful recommendations. Reciprocal Recommender Systems (RRS) are user-to-user Recommender Systems that recommend a list of users to a user by considering the preferences of both the parties involved. Generating successful recommendations inherently face the exploitation-exploration dilemma which requires predicting the best recommendation from the current information or gathering more information about the environment. To address this, we formulate reciprocal recommendation generation task as a contextual bandit problem which is a principled approach where the agent chooses an action from a set of actions based on contextual information and receives a reward for the chosen action. We propose SiameseNN-UCB algorithm: a deep neural network-based strategy that follows Siamese architecture to transform raw features and learn reward for the chosen action. Upper confidence bound type exploration is used to solve exploitation-exploration trade-off. In this algorithm, we attempt to generate reciprocal recommendations by utilizing multiple aspects such as multi-criteria ratings of a user, popularity-awareness, demographic information, and availability of users. Experimental studies conducted with speed dating data set demonstrate the effectiveness of the proposed approach.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s00530-022-00966-y,en,"Combating multimodal fake news on social media: methods, datasets, and future perspective","['OriginalPaper', 'Regular Paper']","The growth in the use of social media platforms such as Facebook and Twitter over the past decade has significantly facilitated and improved the way people communicate with each other. However, the information that is available and shared online is not always credible. These platforms provide a fertile ground for the rapid propagation of breaking news along with other misleading information. The enormous amounts of fake news present online have the potential to trigger serious problems at an individual level and in society at large. Detecting whether the given information is fake or not is a challenging problem and the traits of social media makes the task even more complicated as it eases the generation and spread of content to the masses leading to an enormous volume of content to analyze. The multimedia nature of fake news on online platforms has not been explored fully. This survey presents a comprehensive overview of the state-of-the-art techniques for combating fake news on online media with the prime focus on deep learning (DL) techniques keeping multimodality under consideration. Apart from this, various DL frameworks, pre-trained models, and transfer learning approaches are also underlined. As till date, there are only limited multimodal datasets that are available for this task, the paper highlights various data collection strategies that can be used along with a comparative analysis of available multimodal fake news datasets. The paper also highlights and discusses various open areas and challenges in this direction.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00521-022-07621-7,en,Sliding space-disparity transformer for stereo matching,"['OriginalPaper', 'Original Article']","Transformers have achieved impressive performance in natural language processing and computer vision, including text translation, semantic segmentation, etc. However, due to excessive self-attention computation and memory occupation, the stereo matching task does not share its success. To promote this technology in stereo matching, especially with limited hardware resources, we propose a sliding space-disparity transformer named SSD-former. According to matching modeling, we simplify transformer for achieving faster speed, memory-friendly, and competitive performance. First, we employ the sliding window scheme to limit the self-attention operations in the cost volume for adapting to different resolutions, bringing efficiency and flexibility. Second, our space-disparity transformer remarkably reduces memory occupation and computation, only computing the current patch’s self-attention with two parts: (1) all patches of current disparity level at the whole spatial location and (2) the patches of different disparity levels at the exact spatial location. The experiments demonstrate that: (1) different from the standard transformer, SSD-former is faster and memory-friendly; (2) compared with 3D convolution methods, SSD-former has a larger receptive field and provides an impressive speed, showing great potential in stereo matching; and (3) our model obtains state-of-the-art performance and a faster speed on the multiple popular datasets, achieving the best speed–accuracy trade-off.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00500-021-06291-2,en,Enhancement of single channel speech quality and intelligibility in multiple noise conditions using wiener filter and deep CNN,"['OriginalPaper', 'Focus']","Nowadays, deep neural network has become the prime approach for enhancing speech signals as it yields good results compared to the traditional methods. This paper describes the transformation in the enhanced speech signal by applying the deep convolutional neural network (Deep CNN), which can model nonlinear relationships and compare it with the Wiener filtering method, which is the best technique for speech enhancement among the traditional methods. Denoising is performed in the frequency domain and converted back to the time domain to analyze performance metrics such as speech quality and speech intelligibility. The speech quality is analyzed based on the signal to noise ratio (SNR) and perceptual evaluation of speech quality (PESQ). Speech intelligibility is analyzed by short-time objective intelligibility (STOI). Both the methods evaluated the denoised speech, and the analysis made on the results shows that the SNR of the conventional Wiener filtering method is much improved when compared with Deep CNN. However, the PESQ and STOI of Deep CNN-based enhanced speech outperform the Wiener filtering method. The performance metrics indicate that Deep CNN achieves better results than the conventional technique.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00530-022-01018-1,en,A multi-level feature weight fusion model for salient object detection,"['OriginalPaper', 'Regular Paper']","Although the Fully Convolutional Neural Networks (FCNs) has achieved good performance in salient object detection, there are problems, such as fuzzy boundary and unsatisfactory performance in complex scenes. Hence, how to better integrate multi-level convolution feature requires further investigation. This paper proposes a salient object detection algorithm, which uses Gram matrix and its F norm to weigh the importance of each multi-level feature map and uses weight to fuse multi-level prediction results recursively, finally generate the final saliency map. The algorithm evaluates the importance of different depth multi-level feature maps by calculating the Gram matrix's F norm of feature tensor slices. The multi-level feature maps are fused effectively according to the weight. It reduces the loss of multi-level prediction results during fusion, and preserves the spatial details. Besides, to achieve a more accurate boundary, a deep supervision is used to optimize salient feature maps’ results. Pixel-level supervision information from ground truth will guide each layer’s prediction. Experiments on five benchmark data sets demonstrate that the proposed method performs well in various scenes, especially in complex scenes.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s12293-022-00375-8,en,A novelty-search-based evolutionary reinforcement learning algorithm for continuous optimization problems,"['OriginalPaper', 'Regular research paper']","The evolutionary reinforcement learning (ERL) algorithm is a hybrid algorithm which combines evolutionary computation and reinforcement learning. By exchanging information between the population and the agent, the ERL algorithm can perfectly handle a range of challenging control tasks. However, for some complex reward structure problems, both deep reinforcement learning and ERL algorithms easily get stuck in local optima because of the deception of reward function. To address this problem, we integrate a novelty search in the framework of the ERL algorithm, and it guides the agent or population to visit state space where it has rarely or never visited. Five robot locomotion continuous optimization problems were employed as benchmarks. Simulation results show our proposed algorithm outperformed its competitors in most tested environments.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Complex Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s00500-022-07499-6,en,Deep learning models for detecting respiratory pathologies from raw lung auscultation sounds,"['OriginalPaper', 'Data analytics and machine learning']","In recent years deep learning models improve the diagnosis performance of many diseases especially respiratory diseases. This paper will propose an evaluation for the performance of different deep learning models associated with the raw lung auscultation sounds in detecting respiratory pathologies to help in providing diagnostic of respiratory pathologies in digital recorded respiratory sounds. Also, we will find out the best deep learning model for this task. In this paper, three different deep learning models have been evaluated on non-augmented and augmented datasets, where two different datasets have been utilized to generate four different sub-datasets. The results show that all the proposed deep learning methods were successful and achieved high performance in classifying the raw lung sounds, the methods were applied on different datasets and used either augmentation or non-augmentation. Among all proposed deep learning models, the CNN–LSTM model was the best model in all datasets for both augmentation and non-augmentation cases. The accuracy of CNN–LSTM model using non-augmentation was 99.6%, 99.8%, 82.4%, and 99.4% for datasets 1, 2, 3, and 4, respectively, and using augmentation was 100%, 99.8%, 98.0%, and 99.5% for datasets 1, 2, 3, and 4, respectively. While the augmentation process successfully helps the deep learning models in enhancing their performance on the testing datasets with a notable value. Moreover, the hybrid model that combines both CNN and LSTM techniques performed better than models that are based only on one of these techniques, this mainly refers to the use of CNN for automatic deep features extraction from lung sound while LSTM is used for classification.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s13369-022-06927-x,en,Energy Harvesting from a Cantilever Beam with Geometric Nonlinearity Subjected to a Moving Mass,"['OriginalPaper', 'Research Article-Mechanical Engineering']","With onset of the twenty-first century, the earth is converted into a smart world in no time due to advancements in technology like the internet of things, wireless sensor network, and microelectromechanical system. Mobile energy source is highly essential to power these devices. However, the vibration energy harvester extracts the electrical energy from the mechanical vibratory system to fulfill the need for power requirements. The present study assesses the effect of geometrical nonlinearity of beam due to the occurrence of large deformation under the excitation of moving load. As an endeavor, the nonlinear analysis of the cantilever beam bonded with piezo-patch in the field of energy harvesting under the desired condition is examined. The desired basic formulation is based on the finite element approach. Hamilton’s principle is incorporated to derive the equation of motion of the system. Further, Newmark’s integration method is adopted to study the behavior of moving load. To solve the nonlinear set of equations Newton–Raphson method is successfully implemented. It is concluded from the analysis that with low speed and high moving force the power output is more. A decreasing trend is observed in displacement due to geometric nonlinearity which results in low voltage and power generation as compared to linear energy harvester. Later real coded genetic algorithm is successfully implemented to obtain the desired design parameter values for optimal power output.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s11063-022-10847-0,en,An Efficient Galactic Swarm Optimization Based Fractal Neural Network Model with DWT for Malignant Melanoma Prediction,OriginalPaper,"The most aggressive and malignant type of skin cancer is melanoma. When input data is in the form of images, image processing plays a vital role to detect and classifying cancer in the human body. Existing research discovered many weaknesses in complex data models, such as higher feature dimensionality, which required more data for training, resulting in lower detection accuracy, higher computational difficulties, portability, and processing time. Hence, we introduced fractal neural network-based galactic swarm optimization (FNN–GSO) algorithm for the detection of malignant melanoma such as superficial spreading, nodular, and lentigo malignant melanoma. The main aim of this work is to apply a deep learning technique to classify skin lesions for effective treatment and prognostication instead of the gold standard excision biopsy, which is currently used to diagnose this condition. Expert analysis, time consumption, and expensive processing associated with malignant melanoma classification and prediction are minimized in this manner. The proposed work involves four major components such as pre-processing, segmentation, feature extraction, and classification. The raw input images are pre-processed thereby the noise removal and contrast level enhancement are carried out. An adaptive watershed segmentation algorithm performs malignant melanoma segmentation. Following that, image features such as are extracted correctly using the DWT–GLCM feature extraction model. Finally, the malignant melanoma classifications are performed using the FNN–GSO algorithm. The proposed method's performance is evaluated using MATLAB software and different evaluation parameters. The proposed FNN–GSO algorithm demonstrates better classification results than other existing methods.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s10237-022-01611-3,en,Experiments and hyperelastic modeling of porcine meniscus show heterogeneity at high strains,"['OriginalPaper', 'Original Paper']","Constitutive modeling of the meniscus is critical in areas like knee surgery and tissue engineering. At low strain rates, the meniscus can be described using a hyperelastic model. Calibration of hyperelastic material models of the meniscus is challenging on many fronts due to material variability and friction. In this study, we present a framework to determine the hyperelastic material parameters of porcine meniscus (and similar soft tissues) using no-slip uniaxial compression experiments. Because of the nonhomogeneous deformation in the specimens, a finite element solution is required at each step of the iterative calibration process. We employ a Bayesian calibration approach to account for the inherent material variability and a Bayesian optimization approach to minimize the resulting cost function in the material parameter space. Cylindrical specimens of porcine meniscus from the anterior, middle and posterior regions are tested up to 30% compressive strain and the Yeoh form of hyperelastic strain energy density function is used to describe the material response. The results show that the Yeoh form is able to accurately describe the compressive response of porcine meniscus and that the Bayesian calibration and optimization approaches are able to calibrate the model in a computationally efficient manner while taking into account the inherent material variability. The results also show that the shear modulus or the initial stiffness is roughly uniform across the different areas of the meniscus, but there is significant spatial heterogeneity in the response at high strains. In particular, the middle region is considerably stiffer at high strains. This heterogeneity is important to consider in modeling the response of the meniscus for clinical applications.","['Engineering', 'Theoretical and Applied Mechanics', 'Biomedical Engineering and Bioengineering', 'Biological and Medical Physics, Biophysics']"
doi:10.1007/s00484-022-02364-5,en,Machine Learning approach to Predict net radiation over crop surfaces from global solar radiation and canopy temperature data,"['OriginalPaper', 'Original Paper']","As the ground-based instruments for measuring net radiation are costly and need to be handled skillfully, the net radiation data at spatial and temporal scales over Indian subcontinent are scanty. Sometimes, it is necessary to use other meteorological parameters to estimate the value of net radiation, although the prediction may vary based on season, ground cover and estimation method. In this context, artificial intelligence can be used as a powerful tool for predicting the data considering past observed data. This paper proposes a novel method to predict the net radiation for five crop surfaces using global solar radiation and canopy temperature. This contribution includes the generation of real-time data for five crops grown in West Bengal state of India. After manual analysis and data preprocessing, data normalization has been done before applying machine learning approaches for training a robust model. We have presented the comparison in various machine learning algorithm such as ridge and spline regression, random forest, ensemble and deep neural networks. The result shows that the gradient boosting regression and ridge regression are outperforming other ML approaches. The estimated predictors enable to reduce the number of resources in terms of time, cost and manpower for proper net radiation estimation. Thus, the problem of predicting net radiation over various crop surfaces can be sorted out through ML algorithm.","['Environment', 'Environment, general', 'Biological and Medical Physics, Biophysics', 'Meteorology', 'Animal Physiology', 'Plant Physiology', 'Environmental Health']"
doi:10.1007/s11042-022-14121-2,en,A CNN-transformer hybrid approach for an intrusion detection system in advanced metering infrastructure,OriginalPaper,"Bi-directional communication networks are the foundation of advanced metering infrastructure (AMI), but they also expose smart grids to serious intrusion risks. While previous studies have proposed various intrusion detection systems (IDS) for AMI, most have not comprehensively considered the impact of different factors on intrusions. To ensure the security of the bi-directional communication network of AMI, this paper proposes an IDS based on deep learning theory. First, the invalid features are eliminated according to the feature screening strategy based on eXtreme Gradient Boosting (XGBoost), after which the data distribution is balanced by the adaptive synthetic (ADASYN) sampling technique. Next, multi-space feature subsets based on the convolutional neural network (CNN) are constructed to enrich the spatial distribution of samples. Finally, the Transformer is used to construct feature associations and extract crucial traits, such as the temporal and fine-grained characteristics of features, to complete the identification of intrusion behaviors. The proposed IDS is tested on the KDDCup99, NSL-KDD, and CICIDS-2017 datasets, and the results show that it has high performance with accuracy of 97.85 % , 91.04 % , and 91.06 % respectively.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s42401-022-00162-w,en,SAFuseNet: integration of fusion and detection for infrared and visible images,"['OriginalPaper', 'Original Paper']","Utilizing the complementary characteristics of infrared and visible images can be helpful for object detection. Detection after image-level fusion is a general kind of two-step strategy for multi-source images, which separates two tasks in both training and testing phases. These methods reduce detector’s accuracy greatly due to the information loss of fusion, meanwhile increasing algorithm complexity. In this paper, SAFuseNet (Self-Attention Fusion Net) is proposed to solve above issues. Based on feature-level fusion, it integrates the processes of two tasks, to build end-to-end structure and preserve the information of images. It also utilizes self-attention mechanism to further improve performance. Our model includes three phases. Extraction phase obtains multi-grain information of infrared and visible images, fusion phase then fuses these extracted features, and feeds the results into detection phase to make accurate predictions. Experiments show that our method performs better than other two-step ones in remote sensing dataset, proposed by Sebastien Razakarivonyb et al.","['Engineering', 'Aerospace Technology and Astronautics', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)']"
doi:10.1007/s10489-022-04255-z,en,Multimodal fusion for alzheimer’s disease recognition,OriginalPaper,"Alzheimer’s disease (AD) is the most prevalent form of progressive degenerative dementia, which has a great impact on social economics throughout the world. In the vast majority of cases, AD patients are diagnosed by biochemical analysis, lumbar puncture and advanced imaging examination, which cannot play a preventive role in early stage of Alzheimer’s disease. Speech signals contain abundant personal information, especially AD patients always accompany with speech disorder, which provides a potential to utilize speech information to distinguish AD patients from healthy persons. The work presented in this paper aims to develop new approach for early detection of AD by noninvasive methods. We propose to make utilization of multimodal features with speech acoustic and linguistic features for the speech recognition of Alzheimer’s disease. Three different kinds of features, IS10_paraling features, deep acoustic using fine-tuned Wav2Vec2.0 model and deep linguistic features extracted using fine-tuned BERT, are adopted for AD classification by SVM classifier. By conducting experiments on two publicly available datasets of NCMMSC2021 and ADReSSo, the experimental results show that our model achieves state-of-the-art (SOTA) performance with satisfactory recognition effect. Our best-performing model obtains the accuracy of 89.1 % and 84.0 % in the long and short-audio of NCMMSC2021, and 83.7 % in ADReSSo, which is promising for the early diagnosis and classification of AD patients.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1038/s41593-022-01189-0,en,A deep learning framework for inference of single-trial neural population dynamics from calcium imaging with subframe temporal resolution,"['OriginalPaper', 'Technical Report']","In many areas of the brain, neural populations act as a coordinated network whose state is tied to behavior on a millisecond timescale. Two-photon (2p) calcium imaging is a powerful tool to probe such network-scale phenomena. However, estimating the network state and dynamics from 2p measurements has proven challenging because of noise, inherent nonlinearities and limitations on temporal resolution. Here we describe Recurrent Autoencoder for Discovering Imaged Calcium Latents (RADICaL), a deep learning method to overcome these limitations at the population level. RADICaL extends methods that exploit dynamics in spiking activity for application to deconvolved calcium signals, whose statistics and temporal dynamics are quite distinct from electrophysiologically recorded spikes. It incorporates a new network training strategy that capitalizes on the timing of 2p sampling to recover network dynamics with high temporal precision. In synthetic tests, RADICaL infers the network state more accurately than previous methods, particularly for high-frequency components. In 2p recordings from sensorimotor areas in mice performing a forelimb reach task, RADICaL infers network state with close correspondence to single-trial variations in behavior and maintains high-quality inference even when neuronal populations are substantially reduced. Zhu et al. develop a deep learning method to precisely infer single-trial neural dynamics from calcium imaging with subframe temporal resolution, which shows improvement over the state-of-the-art methods in capturing high-frequency dynamics and predicting behavior.","['Biomedicine', 'Biomedicine, general', 'Neurosciences', 'Behavioral Sciences', 'Biological Techniques', 'Neurobiology', 'Animal Genetics and Genomics']"
doi:10.1007/s11042-022-13476-w,en,A deep data augmentation framework based on generative adversarial networks,"['OriginalPaper', '1221: Deep Learning for Image/Video Compression and Visual Quality Assessment']","In the process of training convolutional neural networks, the training data is often insufficient to obtain ideal performance and encounters the overfitting problem. To address this issue, traditional data augmentation (DA) techniques, which are designed manually based on empirical results, are often adopted in supervised learning. Essentially, traditional DA techniques are in the implicit form of feature engineering. The augmentation strategies should be designed carefully, for example, the distribution of augmented samples should be close to the original data distribution. Otherwise, it will reduce the performance on the test set. Instead of designing augmentation strategies manually, we propose to learn the data distribution directly. New samples can then be generated from the estimated data distribution. Specifically, a deep DA framework is proposed which consists of two neural networks. One is a generative adversarial network, which is used to learn the data distribution, and the other one is a convolutional neural network classifier. We evaluate the proposed model on a handwritten Chinese character dataset and a digit dataset, and the experimental results show it outperforms baseline methods including one manually well-designed DA method and two state-of-the-art DA methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11548-022-02697-x,en,Feature pyramid self-attention network for respiratory motion prediction in ultrasound image guided surgery,"['OriginalPaper', 'Original Article']","Purpose The robot-assisted automated puncture system under ultrasound guidance can well improve the puncture accuracy in ablation surgery. The automated puncture system requires advanced definition of the puncture location, while the displacement of thoracic-abdominal tumors caused by respiratory motion makes it difficult for the system to locate the best puncture position. Predicting tumor motion is an effective way to help the automated puncture system output a more accurate puncture position. Methods In this paper, we propose a self-attention-based feature pyramid algorithm FPSANet for time-series forecasting, which can extract both linear and nonlinear dependencies of time series. Firstly, we use the temporal convolutional network as the backbone to extract different scale time-series features, and the self-attention module is followed to weigh more significant features to improve nonlinear prediction. Secondly, we use autoregressive models to perform linear prediction. Finally, we directly combine the above two kinds of predictions as the final prediction. Results FPSANet is trained and tested on our private datasets captured from clinical individuals, and we predict the target position after 50 ms, 150 ms, 300 ms and 400 ms. The result shows the evaluation criteria of the MAE is less than 1 mm at 50 ms and 150 ms, and less than 2 mm at 300 ms. Compared with the AR model, bidirectional LSTM and RVM, our method not only outperforms both models in accuracy (AR: ~ 7.7%; bidirectional LSTM: ~ 75.9%; RVM: ~ 76.5%) but is also more stable on different types of respiratory curves. Conclusion Respiratory motion in the liver in actual clinical practice vary widely from person to person, while sometimes having less distinct periodic patterns. Under these conditions, our algorithm has the advantage of excellent stability for prediction on various sequences, and its running time of performing single sequence prediction can meet clinical requirements.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s42421-022-00061-8,en,Mobile Sensing for Multipurpose Applications in Transportation,"['OriginalPaper', 'Original Paper']","Routine and consistent data collection is required to address contemporary transportation issues. The cost of data collection increases significantly when sophisticated machines are used to collect data. Due to this constraint, State Departments of Transportation struggle to collect consistent data for analyzing and resolving transportation problems in a timely manner. Recent advancements in sensors integrated into smartphones have resulted in a more affordable method of data collection. The primary objective of this study is to develop and implement a smartphone-based application for transportation-related data collection. The app consists of three major modules: a frontend graphical user interface (GUI), a sensor module, and a backend module. While the frontend GUI enables interaction with the app, the sensor modules collect relevant data such as video, gyroscope, motion and accelerometer readings while the app is in use. The backend leverages a real-time database to stream and store data from sensors, together with providing the computational resources needed to support the application. In comparison to other developed apps for transportation data collection, this app is not overly reliant on the internet enabling the app to be used in internet-restricted areas. Additionally, the app is designed for multipurpose applications in transportation. The collected data were analyzed for a variety of purposes, including calculating the International Roughness Index (IRI), identifying pavement distresses, and understanding driver’s behaviors and environment. From the sensor data, we detected turning movements, lane changes and estimated IRI values. In addition, several pavement distresses were identified from the video data with machine learning.","['Engineering', 'Transportation Technology and Traffic Engineering', 'Computational Intelligence', 'Data Mining and Knowledge Discovery']"
doi:10.1007/s00521-022-07550-5,en,Semantic segmentation of chemical plumes from airborne multispectral infrared images using U-Net,"['OriginalPaper', 'Original Article']","The United States Environmental Protection Agency Airborne Spectral Photometric Environmental Collection Technology program provides infrared (IR) remote sensing capabilities from an aircraft platform to assist first responders in managing chemical releases into the atmosphere. One of the instruments used is a downward-looking eight-band multispectral imaging system that receives the upwelling IR radiance from the ground and atmosphere below the aircraft. Volatile organic compounds absorb and emit IR radiation at characteristic wavelengths and produce unique signatures in the imaging data. To automate the detection of chemical plumes, this research applied a deep learning-based semantic segmentation model on multispectral images collected during controlled releases of methanol. A U-Net model was developed for this application, and multiple experiments were conducted to optimize the model. Issues studied included the use of a temperature and emissivity separation algorithm to suppress temperature effects, a custom normalization method to reduce scene composition variance, experiments to shrink the network architecture, and evaluation of the utility of data augmentation. The optimized U-Net model was able to detect the plume area in images collected across different temperatures and locations while achieving false detection rates < 0.02%. The U-Net model exhibited an improved ability to discriminate methanol plumes from other scene elements such as buildings and roads when compared to the performance of shallow neural networks studied in previous work.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11063-022-10865-y,en,Sequential Enhancement for Compressed Video Using Deep Convolutional Generative Adversarial Network,OriginalPaper,"Compression artifacts cause negative visual perception and are tough to reduce because of the balance between compressibility and fidelity. Despite extensive research on traditional methods, they take insufficient effect on quality enhancement. Researches concerning the problem turn to concentrate on quality elevation of single frame using CNNs but ignore the continuity, which is called inter-frame correlation that is critical for video enhancement. There are some CNN-based approaches pursuing good effects, however, sacrificing efficiency. Considering the demand for video quality enhancement and the feature of consecutive frames, this paper proposes a bi-frame generative adversarial network. It takes advantage of inter-frame correlation for bi-frame motion compensation, producing accurate compensated frames. Then, a multi-scale convolutional layer with dilated filters, which constrains parameters and overcomes block effects, is proposed to promote efficiency. Subsequently, a multi-layer deep fusion section is employed to avoid gradients vanishing and realize deep compression artifacts reduction. The ability of discrimination is enhanced with the engagement of a devised relativistic average discriminator which optimizes the whole network. As experiment results demonstrated, bi-frame generative adversarial network shows its effectiveness in terms of various indices. It also presents satisfactory visual performance with comparative test speed compared to listed approaches.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s11134-022-09736-z,en,A general “power-of-d” dispatching framework for heterogeneous systems,OriginalPaper,"Intelligent dispatching is crucial to obtaining low response times in large-scale systems. One common scalable dispatching paradigm is the “power-of- d ,” in which the dispatcher queries d servers at random and assigns the job to a server based only on the state of the queried servers. The bulk of power-of- d policies studied in the literature assume that the system is homogeneous, meaning that all servers have the same speed; meanwhile, real-world systems often exhibit server speed heterogeneity. This paper introduces a general framework for describing and analyzing heterogeneity-aware power-of- d policies. The key idea behind our framework is that dispatching policies can make use of server speed information at two decision points: when choosing which d servers to query and when assigning a job to one of those servers. Our framework explicitly separates the dispatching policy into a querying rule and an assignment rule; we consider general families of both rule types. While the strongest assignment rules incorporate both detailed queue-length information and server speed information, these rules typically are difficult to analyze. We overcome this difficulty by focusing on heterogeneity-aware assignment rules that ignore queue length information beyond idleness status. In this setting, we analyze mean response time and formulate novel optimization problems for the joint optimization of querying and assignment. We build upon our optimized policies to develop heuristic queue length-aware dispatching policies. Our heuristic policies perform well in simulation, relative to policies that have appeared in the literature.","['Business and Management', 'Operations Research/Decision Theory', 'Computer Communication Networks', 'Probability Theory and Stochastic Processes', 'Supply Chain Management', 'Systems Theory, Control']"
doi:10.1007/s40747-022-00728-3,en,Self-attention-guided scale-refined detector for pedestrian detection,"['OriginalPaper', 'Original Article']","Pedestrian detection has been researched for decades. Recently, an anchor-free method CSP is proposed to generate the pedestrian bounding box directly. When the predicted center deviates from the ground truth in the testing phase, the CSP model generates deviated pedestrian bounding box, which leads to false detection in occlusion situations. To handle this problem, we refine the scale regression branch of the CSP model to generate a more accurate prediction. The new scale regression branch outputs the distances between the center and the four edges of the pedestrian bounding box. Even if the predicted center deviates from the ground truth, an accurate bounding box can still be obtained. Moreover, we integrate a self-attention module into our model to take full advantage of the features in different depth layers. Our proposed model achieves better performance than the state-of-the-art detectors in comparison experiments on the two datasets, i.e., Citypersons and Caltech.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s12555-021-0749-x,en,Learning-based Extended Dynamic Mode Decomposition for Addressing Path-following Problem of Underactuated Ships with Unknown Dynamics,"['OriginalPaper', 'Regular Papers']","Path-following techniques of ships have received a lot of attention in recent years, to promote future autonomous ships and develop advanced autopilots. This paper deals with the path-following problem of underactuated ships without having prior knowledge regarding the hydrodynamic coefficients and ship parameters. A novel data-driven control strategy that combines Koopman operator theory and extended dynamic mode decomposition (EDMD) method and integrates with a model predictive control (MPC) framework is proposed. It makes use of data collected from experiments to learn the Koopman eigenfunctions of unknown ship dynamics via supervised learning, which are utilized as the lifting functions in the EDMD method to build a linear, lifted state-space model. The identified linear model acts as the predictor in the designed MPC controller, and a line-of-sight (LOS) algorithm is introduced as the guidance law for path-following. Simulation results show that the prediction model could provide sufficient prediction accuracy, and that it can be combined with MPC to achieve good path-following performance in a computationally efficient way.","['Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11356-022-21277-9,en,A daily carbon emission prediction model combining two-stage feature selection and optimized extreme learning machine,"['OriginalPaper', 'Research Article']","Global warming caused by increased carbon emissions is a common challenge for all mankind. Facing the unprecedented pressure of carbon emission reduction, it is particularly important to grasp the dynamics of carbon emission in time and accurately. This paper proposes a novel daily carbon emission forecasting model. Firstly, the daily carbon emission data is decomposed into a series of completely noise-free mode functions by improved complete ensemble empirical mode decomposition method with adaptive noise (ICEEMDAN). Then, a two-stage feature selection method composed of partial autocorrelation function (PACF) and ReliefF is applied to select appropriate input variables for the next prediction process. Finally, the extreme learning machine optimized by improved sparrow search algorithm (ISSA-ELM) is used to predict. The empirical results show that the proposed two-stage feature selection method can further improve the prediction accuracy. After two-stage feature selection, the values of R 2 , MAPE, and RMSE were improved by 0.55%, 30.23%, and 28.46%, respectively. It can also be found that ISSA has good optimization performance. By combining with ISSA, R 2 , MAPE, and RMSE improved by 7.60%, 31.97%, and 44.79%, respectively. Therefore, the proposed model can provide a valuable reference for the formulation of carbon emission reduction policies and future carbon emission prediction research.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10994-022-06217-5,en,Learning with risks based on M-location,OriginalPaper,"In this work, we study a new class of risks defined in terms of the location and deviation of the loss distribution, generalizing far beyond classical mean-variance risk functions. The class is easily implemented as a wrapper around any smooth loss, it admits finite-sample stationarity guarantees for stochastic gradient methods, it is straightforward to interpret and adjust, with close links to M-estimators of the loss location, and has a salient effect on the test loss distribution, giving us control over symmetry and deviations that are not possible under naive ERM.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s00521-022-07714-3,en,A multi-neighborhood-based multi-objective memetic algorithm for the energy-efficient distributed flexible flow shop scheduling problem,"['OriginalPaper', 'Original Article']","This paper focuses on an energy-efficient distributed flexible flow shop scheduling problem (EEDFFSP) with variable machine speed. The EEDFFSP needs to solve four sub-problems: factory assignment, determination of the job sequence at each stage, machine selection, and the speed selection for each job on a machine. A multi-neighborhood-based multi-objective memetic algorithm (MMMA) is proposed to optimize total weighted tardiness and energy consumption. The MMMA employs a two-level encoding scheme including a job permutation and a speed matrix. A highly-efficient decoding strategy is utilized to reduce the search space of the sub-problems. In the initial phase, a weighted NEH (Nawaz-Enscore-Ham) based-initial method is developed to generate an initial population. Two genetic global search operators are designed to perform exploration evolution. Then, several multiple neighborhoods including several permutation adjustment operations within or between factories, an energy-saving strategy, and a speed adjustment strategy are integrated to enhance exploitation ability. The comprehensive experiments on extensive instances are performed to test the contribution of the main components and the performance of the MMMA. The average values of Hypervolume and Unary Epsilon indicators obtained by the variants of the MMMA without the initialization method, genetic global search, local search, and energy-saving strategy are worse than the complete MMMA, which demonstrates a significant contribution of these components to the MMMA. The MMMA obtains the best values of indicators among all the compared algorithms within a limited run time, which demonstrates the MMMA is an effective and efficient algorithm for solving the EEDFFSP.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10479-021-04422-4,en,Bi-objective facility location under uncertainty with an application in last-mile disaster relief,"['OriginalPaper', 'Original Research']","Multiple and usually conflicting objectives subject to data uncertainty are main features in many real-world problems. Consequently, in practice, decision-makers need to understand the trade-off between the objectives, considering different levels of uncertainty in order to choose a suitable solution. In this paper, we consider a two-stage bi-objective single source capacitated model as a base formulation for designing a last-mile network in disaster relief where one of the objectives is subject to demand uncertainty. We analyze scenario-based two-stage risk-neutral stochastic programming, adaptive (two-stage) robust optimization, and a two-stage risk-averse stochastic approach using conditional value-at-risk (CVaR). To cope with the bi-objective nature of the problem, we embed these concepts into two criterion space search frameworks, the $$\epsilon $$ ϵ -constraint method and the balanced box method, to determine the Pareto frontier. Additionally, a matheuristic technique is developed to obtain high-quality approximations of the Pareto frontier for large-size instances. In an extensive computational experiment, we evaluate and compare the performance of the applied approaches based on real-world data from a Thies drought case, Senegal.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s10973-022-11635-7,en,Cross-heating-rate prediction of thermogravimetry of PVC and XLPE cable insulation material: a novel artificial neural network framework,OriginalPaper,"The analysis of thermogravimetric data of material at multiple heating rates is very labor-intensive and time-consuming. To provide an accurate and effective prediction of the thermogravimetric (TG) curves at various heating rates, this work presents a novel artificial neural network (ANN) framework for cross-heating-rate prediction on the TG curves of commonly used cable insulation materials. The proposed ANN framework consists of data transformation and division techniques that differ from previous studies. By comparing the actual test results and predicted TG results of polyvinyl chloride (PVC), the effectiveness of the proposed ANN framework in the cross-heating-rate prediction of TG curves is validated. By which, the relationship between heating rates and conversion rates can be reliably captured, demonstrating the capability of the proposed ANN framework in interpreting cross-heating-rate TG data. In addition to PVC, the proposed ANN framework has been extended to analyze the TG curves of XLPE.","['Chemistry', 'Physical Chemistry', 'Analytical Chemistry', 'Polymer Sciences', 'Inorganic Chemistry', 'Measurement Science and Instrumentation']"
doi:10.1007/s12065-021-00640-8,en,Transmission expansion planning using composite teaching learning based optimisation algorithm,"['OriginalPaper', 'Research Paper']","With the ever increasing demand and stressed operating conditions, resource expansion is the only way to have sustainable electric grid. Transmission system expansion is one of the important aspects in this regard. In the recent years, expansion problem has been addressed by several researchers. Meta-heuristic techniques have been applied to solve expansion problems. In this paper, a new variant of Teaching Learning Based Optimization (TLBO) Algorithm is proposed by adding a sine function based diversity in the teaching phase. The proposed variant is named as Composite TLBO (C-TLBO). The efficacy of the proposed variant has been evaluated on standard benchmark functions and then it is evaluated on two standard electrical networks with cases of inclusion of uncertainty and demand burst. The results obtained from optimization processes have been evaluated with the help of several analytical and statistical tests. Results affirm that the proposed modification enhances the performance of the algorithm in a substantial manner.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1007/s10470-022-02086-z,en,A novel expeditious switching circuit design for non volatile combinational circuit,OriginalPaper,"Magnetic Tunnel Junction (MTJ) is an important device to store the non-volatile. This MTJ device overcomes the disadvantages of CMOS technology by reducing the leakage power. Also, it stores the data in magnet domain. Non Volatile combinational circuits are nonvolatile memory elements, which distributes over a logic-circuit plane and expected to realize both the ultra-low-power and reduced interconnection delay. The existing Spin Orbit Torque- Magnetic Tunnel Junction method is used to store data with higher density, cost benefits, endurance and Non-Volatility. But it has some limitations, such as high current density needed for writing can infrequently damage the MTJ barrier, achieving reliable reading without making a difference remains a challenge. More constraints limit the designer to attain higher speed with reliable MRAM architectures, because the operations of writing and reading share the same path (via the junction). To overcome these limitations, an innovative nonvolatile combinational circuit depending on Spin Transfer Torque (STT) with perpendicular Magnetic Tunnel Junction (STT-PMTJ) is proposed in this manuscript for fast data storage in real time applications. Here, the nonvolatile combinational circuits are Spin Transfer Torque and Spin Orbit Torque. The proposed method gives higher reliability and the lower resistive writing path acts high-speed with energy-efficient WRITE operation with the help of Read and Write Parallel Switching (RWPS). The RWPS circuit is designed using Spin Orbit Torque (SOT) logic. The RWPS-PMTJ device used to maximize the robustness of entire structure and the noise is apparently reduced. The performance of the proposed method is compared with other existing methods, such as STT-MTJ, SOT-MRAM. The experimental results shows that the proposed RWPS-PMTJ method is efficiently reduced write delay by 64% compared to the STT switching, it also exhibits 61% faster read access with full swing eventually eliminating the setup time requirement. It enables the design of a new era of In-memory computing circuits to meet the challenges in the design of memory-based computation logic circuits. The AND logic gate and Full Adder (FA) circuits are implemented using Cadence virtuoso 45 nm technology.","['Engineering', 'Circuits and Systems', 'Electrical Engineering', 'Signal,Image and Speech Processing']"
doi:10.1007/s12630-022-02334-w,en,Novel use of transesophageal echocardiography to optimize hemodynamics and patient positioning during prone scoliosis surgery and safety considerations in the setting of intraoperative neuromonitoring: a case report,"['OriginalPaper', 'Case Reports / Case Series']","Purpose The prone position can lead to anatomical compression of the thoracic cavity resulting in reduced cardiac output, especially in the context of chest wall deformities commonly present in patients with scoliosis. There are no protocols for using transesophageal echocardiography (TEE) to optimize prone positioning and for safe use of TEE during cases requiring neuromonitoring. Clinical features We present a case of a 23-yr-old male with Cornelia de Lange syndrome undergoing elective posterior spinal fusion for syndromic scoliosis who developed severe refractory hypotension and cardiac arrest in the prone position. After hemodynamic stabilization in the intensive care unit, the patient returned to the operating room on postoperative day 2 for completion of his spinal fusion. Transesophageal echocardiography determined the optimal position of longitudinal bolster placements associated with minimal left ventricular compression in the supine position. The patient was then proned and intraoperative hemodynamics during the second surgery remained stable. Owing to the special considerations of using TEE in the prone position with neuromonitoring, we describe technical aspects to consider to protect the equipment and patient. Conclusion Patients with compliant chest walls or thoracic deformities are at risk of hemodynamic instability in the prone position. Intraoperative TEE can be used in the supine patient prior to proning to determine optimal longitudinal bolster positioning to minimize cardiac compression. Transesophageal echocardiography used during spine surgery in the prone position with neuromonitoring and motor-evoked potentials requires special considerations for patient safety. Objectif La position ventrale peut entraîner une compression anatomique de la cavité thoracique provoquant une réduction du débit cardiaque, en particulier dans le contexte de déformations de la paroi thoracique, fréquentes chez les patients atteints de scoliose. Il n’existe aucun protocole guidant l’utilisation de l’échocardiographie transœsophagienne (ETO) pour optimiser le positionnement ventral et pour favoriser l’utilisation sécuritaire de l’ETO dans les cas nécessitant un neuro-monitorage. Caractéristiques cliniques Nous présentons le cas d’un homme de 23 ans atteint d’un syndrome de Cornelia de Lange bénéficiant d’une fusion spinale postérieure non urgente pour traiter une scoliose syndromique; le patient a manifesté une hypotension réfractaire sévère et un arrêt cardiaque en position ventrale. Après stabilisation hémodynamique à l’unité de soins intensifs, le patient est retourné en salle d’opération au jour postopératoire 2 pour terminer sa fusion spinale. L’échocardiographie transœsophagienne a permis de déterminer la position optimale des traversins longitudinaux qui était associée à une compression ventriculaire gauche minimale en décubitus dorsal. Le patient a ensuite été positionné sur le ventre, et les valeurs hémodynamiques peropératoires sont restées stables au cours de la deuxième chirurgie. En raison des considérations particulières de l’utilisation de l’ETO en position ventrale avec neuro-monitorage, nous décrivons les aspects techniques à prendre en compte pour protéger l’équipement et le patient. Conclusion Les patients présentant des parois thoraciques compliantes ou des déformations thoraciques sont à risque d’instabilité hémodynamique en position ventrale. L’ETO peropératoire peut être utilisée chez le patient en décubitus dorsal avant le positionnement ventral pour déterminer le positionnement optimal des traversins longitudinaux afin de minimiser la compression cardiaque. L’utilisation de l’échocardiographie transœsophagienne lors d’une chirurgie du rachis en position ventrale avec neuro-monitorage et potentiels évoqués moteurs nécessite des considérations particulières en ce qui a trait à la sécurité des patients.","['Medicine & Public Health', 'Anesthesiology', 'Pain Medicine', 'Intensive / Critical Care Medicine', 'Pneumology/Respiratory System', 'Cardiology', 'Pediatrics']"
doi:10.1007/s00220-022-04480-0,en,Duality for Optimal Couplings in Free Probability,OriginalPaper,"We study the free probabilistic analog of optimal couplings for the quadratic cost, where classical probability spaces are replaced by tracial von Neumann algebras, and probability measures on $${\mathbb {R}}^m$$ R m are replaced by non-commutative laws of m -tuples. We prove an analog of the Monge–Kantorovich duality which characterizes optimal couplings of non-commutative laws with respect to Biane and Voiculescu’s non-commutative $$L^2$$ L 2 -Wasserstein distance using a new type of convex functions. As a consequence, we show that if ( X ,  Y ) is a pair of optimally coupled m -tuples of non-commutative random variables in a tracial $$\mathrm {W}^*$$ W ∗ -algebra $$\mathcal {A}$$ A , then $$\mathrm {W}^*((1 - t)X + tY) = \mathrm {W}^*(X,Y)$$ W ∗ ( ( 1 - t ) X + t Y ) = W ∗ ( X , Y ) for all $$t \in (0,1)$$ t ∈ ( 0 , 1 ) . Finally, we illustrate the subtleties of non-commutative optimal couplings through connections with results in quantum information theory and operator algebras. For instance, two non-commutative laws that can be realized in finite-dimensional algebras may still require an infinite-dimensional algebra to optimally couple. Moreover, the space of non-commutative laws of m -tuples is not separable with respect to the Wasserstein distance for $$m > 1$$ m > 1 .","['Physics', 'Theoretical, Mathematical and Computational Physics', 'Mathematical Physics', 'Quantum Physics', 'Complex Systems', 'Classical and Quantum Gravitation, Relativity Theory']"
doi:10.1007/s10479-021-04462-w,en,Decision space robustness for multi-objective integer linear programming,"['OriginalPaper', 'Original Research']","In this article we introduce robustness measures in the context of multi-objective integer linear programming problems. The proposed measures are in line with the concept of decision robustness, which considers the uncertainty with respect to the implementation of a specific solution. An efficient solution is considered to be decision robust if many solutions in its neighborhood are efficient as well. This rather new area of research differs from robustness concepts dealing with imperfect knowledge of data parameters. Our approach implies a two-phase procedure, where in the first phase the set of all efficient solutions is computed, and in the second phase the neighborhood of each one of the solutions is determined. The indicators we propose are based on the knowledge of these neighborhoods. We discuss consistency properties for the indicators, present some numerical evaluations for specific problem classes and show potential fields of application.","['Business and Management', 'Operations Research/Decision Theory', 'Combinatorics', 'Theory of Computation']"
doi:10.1007/s40095-022-00478-5,en,Performance improvement of variable speed wind turbine by uncertainty estimator-based robust control design,"['OriginalPaper', 'Original Research']","The development of a robust control system for a variable speed wind turbine (VSWT) is presented in this paper. The proposed controller is designed for torque and pitch control of VSWT operation based on the uncertainty estimator and output feedback for the extraction of utmost power from the wind. A suitable reference model has been obtained for smooth tracking of rotor speed and estimating the uncertainty. The simulation study has been made to show the efficacy of the robust controller designed for VSWT. The performance of the proposed controller has been analyzed by a comparative evaluation with the standard controllers of the wind turbines in terms of the degree of rotor speed tracking, elimination of the effect of uncertainties, maximum power extraction, etc. It is found that the performance of the VSWT operation using the proposed control scheme has been improved significantly in comparison to a few existing control schemes.","['Energy', 'Renewable and Green Energy']"
doi:10.1007/s11761-022-00351-7,en,Recent progress and perspectives on quantum computing for finance,"['EditorialNotes', 'Editorial']",,"['Computer Science', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'e-Commerce/e-business', 'Computer Appl. in Administrative Data Processing', 'Management of Computing and Information Systems', 'IT in Business']"
doi:10.1007/s11759-022-09461-2,en,Gamification of Digital Heritage as an Approach to Improving Museum and Art Gallery Engagement for Blind and Partially Sighted Visitors,"['OriginalPaper', 'Research']","Digitization of heritage in art gallery and museum contexts raises ethical concerns around ownership, consent, and use. It also highlights fundamental issues of access and engagement for blind and partially sighted (BPS) visitors, especially elders. Gamification, which refers to the use of game elements and game design techniques, such as user feedback and additive levels of progress in non-game contexts, has been used to improve heritage pedagogy, accessibility for and engagement with museum and art gallery visitors. This paper examines collaborative efforts in digital heritage that engage with BPS visitors from historically excluded communities, thereby addressing their traditional exclusion from experiential learning in museum and art gallery settings. In this ethical framework, we use 3D printed models to demonstrate how gamification can play an essential role in providing BPS visitors in museum and art galleries an incentive to engage with the digital and physical archives, guiding them in experiential learning, and enabling new insights into their heritage. Fulsome implementation of 3D models as gamified objects can improve viewership, sharing, learning, and open discussion on redress for BPS members of historically excluded groups when it comes to their heritage. Gamification of digital heritage can enable a more diverse group of visitors to fully participate in the museum and art gallery experience. La numérisation du patrimoine dans des contextes de galerie d'art et de musée soulève des préoccupations d'ordre éthique relatives à la propriété, au consentement et à l'utilization. Elle met aussi en exergue des questions fondamentales d'accès et de participation pour les visiteurs non-voyants et malvoyants, en particulier les plus âgés. La ludification ( gamification en anglais) qui renvoie à l'utilization d'éléments de jeu et de techniques de conception de jeu, comme les commentaires d'utilisateur et les niveaux ajoutés de progression dans les contextes hors jeu, a été utilisée pour optimiser la pédagogie, l'accessibilité et la participation en faveur du patrimoine auprès des visiteurs de musées et de galeries d'art. Cet article examine les efforts collaboratifs en matière de patrimoine numérique destinés aux visiteurs malvoyants issus de communautés historiquement exclues, s'attachant ainsi à traiter de leur exclusion traditionnelle de l'apprentissage par l'expérience dans les environnements de musée et de galerie d'art. Nous nous appuyons dans ce cadre éthique sur des modèles imprimés en 3D pour démontrer comment la ludification peut jouer un rôle essentiel afin de susciter chez les visiteurs malvoyants des musées et galeries d'art un désir d'explorer les archives numériques et physiques, en les guidant au cours d'un apprentissage par l'expérience et en leur permettant d'acquérir des perceptions nouvelles de leur patrimoine. La mise en œuvre généralisée des modèles en 3D à titre d'objets au service du jeu, est susceptible d'optimiser l'audience, le partage, l'apprentissage et la discussion ouverte sur la réparation en faveur des membres malvoyants de groupes historiquement exclus pour ce qui relève de leur patrimoine. La ludification du patrimoine numérique peut permettre à un groupe plus diversifié de visiteurs de participer pleinement à l'expérience vécue au sein d'un musée ou d'une galerie d'art. La digitalización del patrimonio en los contextos de galerías de arte y museos plantea preocupaciones éticas en torno a la propiedad, el consentimiento y el uso. También pone de relieve cuestiones fundamentales de acceso y compromiso para los visitantes ciegos y discapacitados visuales, especialmente los ancianos. La ludificación, que se refiere al uso de los elementos de juegos y las técnicas de diseño de juegos, como la retroalimentación de los usuarios y niveles adicionales de progreso en contextos que no son de juegos, se ha utilizado para mejorar la pedagogía del patrimonio, la accesibilidad y el compromiso con los visitantes a museos y galerías de arte. Este artículo examina los esfuerzos de colaboración en el patrimonio digital que involucran a los visitantes ciegos y discapacitados visuales de comunidades históricamente excluidas, abordando así su exclusión tradicional del aprendizaje experiencial en entornos de museos y galerías de arte. En este marco ético, utilizamos modelos impresos en 3D para demostrar cómo la ludificación puede desempeñar un papel esencial para proporcionar a los visitantes ciegos y discapacitados visuales en museos y galerías de arte un incentivo para interactuar con los archivos físicos y digitales, guiándolos en el aprendizaje experiencial y permitiendo nuevos conocimientos sobre su patrimonio. La implementación completa de modelos 3D como objetos ludificados puede mejorar la audiencia, el intercambio, el aprendizaje y la discusión abierta sobre la reparación para los miembros ciegos y discapacitados visuales de grupos históricamente excluidos en lo que respecta a su herencia. La ludificación del patrimonio digital puede permitir que un grupo más diverso de visitantes participe plenamente en la experiencia del museo y la galería de arte.","['Social Sciences', 'Archaeology', 'Anthropology', 'Cultural Heritage']"
doi:10.1007/s11063-022-10840-7,en,Online Pedestrian Multiple-Object Tracking with Prediction Refinement and Track Classification,OriginalPaper,"The performance of pedestrian multiple object tracking (MOT), which is based on the tracking-by-detection framework, is exceedingly susceptible to the quality of detection, especially suffering from detection missing or inaccuracy caused by occlusion. Several studies aimed at alleviating the problem continue to perform poorly in scenarios with frequent heavy occlusions. In this study, a novel online pedestrian MOT method is proposed for targets with severe occlusion. First, a regression network is employed to refine the predicted position of the target to obtain a precise bounding box and visibility score. Considering the visibility score and the overlap between these refined bounding boxes globally, the targets that are heavily occluded are categorised into the following two types: (1) targets occluded by a non-pedestrian object and (2) targets occluded by other pedestrians. Then, these occluded targets are handled in different ways, which reduces the number of false negatives (FNs) and false positives (FPs). Finally, to enhance the precision of the prediction, a motion model that combines the Kalman filter and camera motion compensation is developed. The tracking results applied to three widely used pedestrian MOT benchmark datasets demonstrates the state-of-the-art performance of the proposed method.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s10586-022-03613-3,en,Energy-aware scientific workflow scheduling in cloud environment,OriginalPaper,"Cloud computing represents a significant shift in computer capability acquisition from the former ownership model to the current subscription approach. In cloud computing, services are provisioned and released in a distributed environment and encourage researchers to further investigate the benefits of cloud resources for executing scientific applications such as workflows. Workflow is composed by a number of fine-grained and coarse-grained tasks. The runtime of fine-grained tasks may be shorter than the duration of system overheads. These overheads can be reduced by merging the multiple fine-grained tasks into a single job which is called task clustering. Clustering of the task is itself a big challenge because workflow tasks are dependent on each other either by data or control dependency. Further, workflow scheduling is also critical issues which aimed to successfully complete the execution of workflow without compromising the agreed Quality of Service parameters such as deadline, cost, etc. Energy efficiency is another challenging issues and energy-aware scheduling is a promising way to achieve the energy-efficient cloud environment. Traditional research in workflow scheduling mainly focuses on the optimization constrained by time or cost without paying attention to provide complete framework for workflow scheduling. The main contribution of this study is to propose a novel scheduling framework that provide a step by step solution for workflow execution while considering the mentioned issues. In order to minimize energy consumption and total execution cost, power-aware dynamic scheduling algorithms are designed and developed that try to execute scientific applications within the user-defined deadline. We implement the task clustering and partial critical path algorithm which helps to forms the jobs of fine-grained tasks and recursively assign the sub-deadlines to the task which are on the partial critical path. Further, to improve the energy efficiency, we implement Dynamic Voltage and Frequency Scaling (DVFS) technique on computing nodes to dynamically adjust voltage and frequency of the processor. Simulation is performed on Montage, CyberShake, SIPHT, LIGO Inspiral Analysis scientific applications and it is observed that the proposed framework deal with the mentioned issues. From the analysis of results it is observed that using clustering and DVFS technique transmission cost and energy consumption is reduced at considerable level.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s00477-022-02261-8,en,Hybrids of machine learning techniques and wavelet regression for estimation of daily solar radiation,"['OriginalPaper', 'Original Paper']","As a primary input in meteorology, the accuracy of solar radiation simulations affects hydrological, climatological, and agricultural studies and sustainable development practices and plans. With the advent of machine learning models and their proven capabilities in modelling the hydro-meteorological phenomena, it is necessary to find the best model suitable for each phenomenon. Models performance depends upon their structure and the input data set. Therefore, some well-known and newest machine learning models with different inputs are tested here for solar radiation simulation in Illinois, USA. The data mining models of Support Vector Machine (SVM), Gene Expression Programming (GEP), Long Short-Term Memory (LSTM), and their combination with the wavelet transformation building a total of six model structures are applied to five data sets to examine their suitability for solar radiation simulation. The five input data sets (SCN_1 to SCN_5) are based on five readily accessible parameters, namely extraterrestrial radiation (R a ), maximum and minimum air temperature (T min , T max ), corrected clear-sky solar irradiation (ICSKY), and Day of Year (DOY). The LSTM outperformed other models, consulting the performance measures of RMSE, SI, MAE, SS RMSE , and SS MAE . Of the different input data sets, in general, SCN_4 was the best input scenario for predicting global daily solar radiation using Ra, Tmax, Tmin, and DOY variables. Overall, six machine learning based models showed acceptable performances for estimating solar radiation, with the LSTM machine learning technique being the most recommended.","['Environment', 'Math. Appl. in Environmental Science', 'Earth Sciences, general', 'Probability Theory and Stochastic Processes', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Computational Intelligence', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s11042-022-13494-8,en,Efficient abnormal event detection in video using deep attention based bidirectional lstm with a mayfly optimization,"['OriginalPaper', '1220: Visual and Sensory Data Processing for Real Time Intelligent Surveillance System']","Abnormal event detection is a challenging issue in video surveillance, and it is quite necessary for detecting suspicious behaviour in the normal video data. Detecting abnormalities in video is very crucial and the application ranges from automatic control of quality to visual surveillance data. This paper presented efficient abnormal event detection in video utilizing deep attention based bidirectional LSTM (Long Short Term Memory) with a Mayfly optimization. Initially, the key frames of input video are selected utilizing threshold based discrete wavelet transform. In the second stage, Kernel Entropy Component Analysis (KECA) is used for decreasing the dimensionality. In the third stage, optimal weighted bilateral filtering is utilized for removing the unnecessary noises. In the next stage, a hybrid dual tree Gabor transform is utilized for the effective feature extraction. Afterwards, the Farne back optical methodology is incorporated to estimate the motion in the video sequence. In the final stage, deep attention based bidirectional LSTM with a mayfly optimized model effectively detects the abnormal events. This presented methodology effectively detects normal and abnormal events and it is implemented in PYTHON platform. The performance of the proposed approach is tested on QMUL and UCF datasets. The experimental outcomes of the presented methodology proved that the presented work is significantly better in terms of various effective performance measures like accuracy, AUC (Area Under Curve), execution time and ROC (Receiver Operating Characteristics) measures. The proposed approach achieved the improved outcomes in terms of accuracy as (94.19%) for QMUL dataset, and (93.60%) for UCF dataset.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10878-022-00875-w,en,Private non-monotone submodular maximization,OriginalPaper,"We propose a private algorithm for the problem of maximizing a submodular but not necessary monotone set function over a down-closed family of sets. The constraint is very general since it includes some important and typical constraints such as knapsack and matroid constraints. Our algorithm Differentially Private Measure Continuous Greedy is proved to be $${\mathcal {O}}(\epsilon )$$ O ( ϵ ) -differential private. For the multilinear relaxation of the above problem, it yields $$\left( Te^{-T}-o(1)\right) $$ T e - T - o ( 1 ) -approximation guarantee with additive error $${\mathcal {O}}\left( \frac{2\varDelta }{\epsilon n^4}\right) $$ O 2 Δ ϵ n 4 , where $$T\in [0,1]$$ T ∈ [ 0 , 1 ] is the stopping time of the algorithm, $$\varDelta $$ Δ is the defined sensitivity of the objective function, which is associated to a sensitive dataset, and n is the size of the given ground set. For a specific matroid constraint, we could obtain a discrete solution with near 1/ e -approximation guarantee and same additive error by lossless rounding technique. Besides, our algorithm can be also applied in monotone case. The approximation guarantee is $$\left( 1-e^{-T}-o(1)\right) $$ 1 - e - T - o ( 1 ) when the submodular set function is monotone. Furthermore, we give a conclusion in terms of the density of the relaxation constraint, which is always at least as good as the tight bound $$(1-1/e)$$ ( 1 - 1 / e ) .","['Mathematics', 'Combinatorics', 'Convex and Discrete Geometry', 'Mathematical Modeling and Industrial Mathematics', 'Theory of Computation', 'Optimization', 'Operations Research/Decision Theory']"
doi:10.1007/s00521-022-07651-1,en,SHAPE: a dataset for hand gesture recognition,"['OriginalPaper', 'Original Article']","Hand gestures are becoming an important part of the communication method between humans and machines in the era of fast-paced urbanization. This paper introduces a new standard dataset for hand gesture recognition, Static HAnd PosturE (SHAPE), with adequate side, variation, and practicality. Compared with the previous datasets, our dataset has more classes, subjects, or scenes than other datasets. In addition, the SHAPE dataset is also one of the first datasets to focus on Asian subjects with Asian hand gestures. The SHAPE dataset contains more than 34,000 images collected from 20 distinct subjects with different clothes and backgrounds. A recognition architecture is also presented to investigate the proposed dataset. The architecture consists of two phases that are the hand detection phase for preprocessing and the classification phase by customized state-of-the-art deep neural network models. This paper investigates not only the high accuracy, but also the lightweight hand gesture recognition models that are suitable for resource-constrained devices such as portable edge devices. The promising application of this study is to create a human–machine interface that solves the problem of insufficient space for a keyboard or a mouse in small devices. Our experiments showed that the proposed architecture could obtain high accuracy with the self-built dataset. Details of our dataset can be seen online at https://users.soict.hust.edu.vn/linhdt/dataset/","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s12630-022-02372-4,en,Nurse knowledge and attitudes towards organ donation and deemed consent: the Human Organ and Tissue Donation Act in Nova Scotia,"['OriginalPaper', 'Reports of Original Investigations']","Purpose In April 2019, the Human Organ and Tissue Donation Act (HOTDA) in Nova Scotia was modified to incorporate a deemed consent model. In this study, we sought to understand intensive care unit (ICU) and emergency department (ED) nurses’ knowledge of and confidence around organ donation and transplantation, experiences with organ donors and recipients, attitudes toward organ donation and deemed consent, and perceived opportunities and barriers to a deemed consent approach in view of the legislative change. Methods We sent an electronic, self-administered survey to all ICU and ED nurses in Nova Scotia. The survey queried respondents on their knowledge of, experience with, and attitudes around organ donation and HOTDA, and opportunities and barriers to the implementation of HOTDA in clinical practice. Survey results were analyzed using descriptive statistics. Results One-hundred and ninety-four nurses responded to the survey. Nearly all (98%) supported organ donation, with 86% having signed an organ donor card to donate organs and/or tissues after death. A considerable majority (89%) also supported the new legislation. Nevertheless, a minority of respondents (13%) believed that deemed consent legislation would be considered a violation of the general principles of freedom and autonomy. The three most identified topics for ongoing training were coordination of the donation process (70%), clinical management of donors (70%), and family issues in decision-making (70%). Conclusion Intensive care unit and ED nurses had positive attitudes toward organ donation, including deemed consent model. The findings should inform educational initiatives in Nova Scotia and beyond to optimize organ donation processes and outcomes. Objectif En avril 2019, la Loi sur le don d’organes et de tissus humains ( Human Organ and Tissue Donation Act – HOTDA) de la Nouvelle-Écosse a été modifiée pour intégrer un modèle de consentement présumé. Dans cette étude, nous avons cherché à comprendre les connaissances et l’aisance du personnel infirmier des unités de soins intensifs (USI) et des services d’urgence en matière de don et de transplantation d’organes, leurs expériences avec les donneurs et les receveurs d’organes, leurs attitudes à l’égard du don d’organes et du consentement présumé, ainsi que les occasions et les obstacles perçus à une approche de consentement présumé compte tenu de la modification législative. Méthode Nous avons envoyé un sondage électronique auto-administré à tout le personnel infirmier des soins intensifs et des urgences de Nouvelle-Écosse. Le sondage a interrogé les répondant.e.s sur leurs connaissances, leur expérience et leurs attitudes à l’égard du don d’organes et de la HOTDA, ainsi que sur les occasions et les obstacles à la mise en œuvre de l’HOTDA dans la pratique clinique. Les réponses au sondage ont été analysées à l’aide de statistiques descriptives. Résultats Cent-quatre-vingt-quatorze infirmières et infirmiers ont répondu au questionnaire. Presque toutes les personnes ayant répondu (98 %) appuient le don d’organes, 86 % ayant signé une carte de don d’organes pour donner des organes et/ou des tissus après leur décès. Une majorité considérable (89 %) soutient également la nouvelle législation. Néanmoins, une minorité de répondant.e.s (13 %) estime que la législation sur la présomption de consentement serait considérée comme une violation des principes généraux de liberté et d’autonomie. Les trois sujets de formation continue les plus fréquemment mentionnés étaient la coordination du processus de don (70 %), la prise en charge clinique des donneurs et donneuses (70 %) et les questions familiales dans la prise de décision (70 %). Conclusion Le personnel infirmier des soins intensifs et des urgences avait une attitude positive à l’égard du don d’organes, y compris du modèle de consentement présumé. Ces résultats devraient éclairer les initiatives éducatives en Nouvelle-Écosse et ailleurs afin d’optimiser les processus et les issues du don d’organes.","['Medicine & Public Health', 'Anesthesiology', 'Pain Medicine', 'Intensive / Critical Care Medicine', 'Pneumology/Respiratory System', 'Cardiology', 'Pediatrics']"
doi:10.1007/s00170-022-10406-w,en,Estimation of surface roughness in selective laser sintering using computational models,"['OriginalPaper', 'ORIGINAL ARTICLE']","This study presents a comprehensive experimental dataset and a novel classification model based on Deep Neural Networks to estimate surface roughness for additive manufacturing. Many problems exist due to the very complex nature of the production process. Some focus on the production planning phase, including the nesting problem under many constraints. However, it is not possible to solve the main function without a clear understanding of the nature of the constraints. The purpose of this research is to present a method to automate the surface roughness estimation process in the production planning phase. The significance of this study is to implement a data-driven model for one of the most critical decision constraints in the nesting process. Solving this problem will automate a key decision constraint, and it might be implemented as an automated constraint module in solving the nesting problem. The proposed model focused on selective laser sintering (SLS) technology based on polyamide 12 powder applications. A comprehensive dataset is designed to simulate the behaviour of an industrial SLS manufacturing process based on a 3D positioning strategy. A set of samples with random positions are also created to test present the model’s robustness. The proposed classification model is based on Deep Neural Networks (DNN) with hyper-parameters designed for the problem. The dataset and the model provide a new user interface to estimate the surface roughness depending on the coordinates of a given product surface in an SLS production chamber and the production parameters employed in the production planning phase. The results show that the model can classify sample surfaces as “rough” or “smooth” with a very high percentage (95.8%) for the training set and with 100% for the test set. Benchmark results also show that the model outperforms other machine learning methods in classifying the surface roughness successfully on the test set.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s10660-021-09473-0,en,Exploitative or explorative innovation? An event study of cloud computing business value,OriginalPaper,"Firms may invest in cloud computing for exploitative or explorative innovation. Exploitative innovation oriented cloud computing investments impact the firms' core business limitedly while the investments for explorative innovation bring the firms both more opportunities and risks such as complex integration with multiple participants. Different enterprises should invest in cloud computing for different orientations according to their situations so as to fully realize the business value. This study examines how the business value of cloud computing is contingent on the investment orientations and the contextual factors of firm characteristics. Taking 183 Chinese listed companies adopted cloud computing as the research sample, we conduct event study methodology to investigate the business value of cloud computing regarding the focal firms' objectives and characteristics including organizational inertia, innovation capability and industry types. The results show that the business value of cloud investment for explorative innovation is more positive than that for exploitative innovation especially for the innovative firms with weak organizational inertia in non-service industries. Meanwhile, achievement of cloud computing business value is complementary to overcoming organizational inertia and promoting innovation capability. This study extends the boundary of business value research of cloud computing and provides references for cloud investment decision making.","['Business and Management', 'IT in Business', 'Data Structures and Information Theory', 'Operations Research/Decision Theory', 'Computer Communication Networks', 'Business and Management, general', 'e-Commerce/e-business']"
doi:10.1007/s00521-022-07622-6,en,Automatic reconstruction of irregular shape defects in pulsed thermography using deep learning neural network,"['OriginalPaper', 'Original Article']","Quantitative defect and damage reconstruction play a critical role in industrial quality management. Accurate defect characterisation in Infrared Thermography (IRT), as one of the widely used Non-Destructive Testing (NDT) techniques, always demands adequate pre-knowledge which poses a challenge to automatic decision-making in maintenance. This paper presents an automatic and accurate defect profile reconstruction method, taking advantage of deep learning Neural Networks (NN). Initially, a fast Finite Element Modelling (FEM) simulation of IRT is introduced for defective specimen simulation. Mask Region-based Convolution NN (Mask-RCNN) is proposed to detect and segment the defect using a single thermal frame. A dataset with a single-type-shape defect is tested to validate the feasibility. Then, a dataset with three mixed shapes of defect is inspected to evaluate the method’s capability on the defect profile reconstruction, where an accuracy over 90% on Intersection over Union (IoU) is achieved. The results are compared with several state-of-the-art of post-processing methods in IRT to demonstrate the superiority at detailed defect corners and edges. This research lays solid evidence that AI deep learning algorithms can be utilised to provide accurate defect profile reconstruction in thermography NDT, which will contribute to the research community in material degradation analysis and structural health monitoring.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00449-022-02797-7,en,Control of parallelized bioreactors II: probabilistic quantification of carboxylic acid reductase activity for bioprocess optimization,"['OriginalPaper', 'Research Paper']","Autonomously operated parallelized mL-scale bioreactors are considered the key to reduce bioprocess development cost and time. However, their application is often limited to products with very simple analytics. In this study, we investigated enhanced protein expression conditions of a carboxyl reductase from Nocardia otitidiscaviarum in E. coli . Cells were produced with exponential feeding in a L-scale bioreactor. After the desired cell density for protein expression was reached, the cells were automatically transferred to 48 mL-scale bioreactors operated by a liquid handling station where protein expression studies were conducted. During protein expression, the feed rate and the inducer concentration was varied. At the end of the protein expression phase, the enzymatic activity was estimated by performing automated whole-cell biotransformations in a deep-well-plate. The results were analyzed with hierarchical Bayesian modelling methods to account for the biomass growth during the biotransformation, biomass interference on the subsequent product assay, and to predict absolute and specific enzyme activities at optimal expression conditions. Lower feed rates seemed to be beneficial for high specific and absolute activities. At the optimal investigated expression conditions an activity of $$1153\ U\ mL^{-1}$$ 1153 U m L - 1 was estimated with a $$90\%$$ 90 % credible interval of $$[992, 1321]\ U\ mL^{-1}$$ [ 992 , 1321 ] U m L - 1 . This is about 40-fold higher than the highest published data for the enzyme under investigation. With the proposed setup, 192 protein expression conditions were studied during four experimental runs with minimal manual workload, showing the reliability and potential of automated and digitalized bioreactor systems.","['Chemistry', 'Biotechnology', 'Industrial and Production Engineering', 'Environmental Engineering/Biotechnology', 'Industrial Chemistry/Chemical Engineering', 'Food Science']"
doi:10.1007/s00607-022-01103-3,en,OPHAencoder: An unsupervised approach to identify groups in group recommendations,"['OriginalPaper', 'Regular Paper']","Recommender systems recommend items to users that would suit the users’ preferences. Suggesting personalized items in the context of a group of users is a non-trivial task. The increasing popularity of group recommender systems in recent years attracted researchers to compute the consensus among the group members more accurately. A recommendation is possible by aggregating the user preferences of the group. The composition of a group plays a significant role in group recommendation. As grouping is an unsupervised task, it becomes essential to form groups from the available information where each group member shares some common characteristics. In this paper, we have blended one permutation hashing and autoencoder techniques to auto-detect the groups. We use both methods very effectively to form the groups. We establish the efficacy of the proposed model in the order and flexible size preference models. We conducted experiments on real-world datasets and found that the proposed method is an efficient and robust approach to form a group automatically.","['Computer Science', 'Computer Science, general', 'Information Systems Applications (incl.Internet)', 'Computer Communication Networks', 'Software Engineering', 'Artificial Intelligence', 'Computer Appl. in Administrative Data Processing']"
doi:10.1007/s12561-022-09343-9,en,Targeted Search for Individualized Clinical Decision Rules to Optimize Clinical Outcomes,OriginalPaper,"Novel biomarkers, in combination with currently available clinical information, have been sought to enhance clinical decision making in many branches of medicine, including screening, surveillance and prognosis. An individualized clinical decision rule (ICDR) is a decision rule that matches subgroups of patients with tailored medical regimen based on patient characteristics. We proposed new approaches to identify ICDRs by directly optimizing a risk-adjusted clinical benefit function that acknowledges the trade-off between detecting disease and over-treating patients with benign conditions. In particular, we developed a novel plug-in algorithm to optimize the risk-adjusted clinical benefit function, which leads to the construction of both nonparametric and linear parametric ICDRs. In addition, we proposed a novel approach based on the direct optimization of a smoothed ramp loss function to further enhance the robustness of a linear ICDR. We studied the asymptotic theories of the proposed estimators. Simulation results demonstrated good finite sample performance for the proposed estimators and improved clinical utilities when compared to standard approaches. The methods were applied to a prostate cancer biomarker study.","['Statistics', 'Statistics for Life Sciences, Medicine, Health Sciences', 'Biostatistics', 'Theoretical Ecology/Statistics']"
doi:10.1007/s43538-022-00127-9,en,Real time railway track crack analysis using multi-level classification,"['OriginalPaper', 'Research Paper']","Rail transportation system remains one of the most cost effective and suited means of passenger and goods transportation for both long distance and suburban travel. Hence it is a critical task to ensure regular railway maintenance. This work explores the scope of detecting cracks and also pedestrians in the track by making use of deep learning and image recognition system. This task becomes very challenging as the cracks in the track are varying and the pedestrian can take wide range of poses. This system make use of a conventional Convolutional Neural Network (CNN) along with Support Vector Machine for recognition of crack defective from the dataset. SVM is the binary classifier used, while CNN was used as an automated feature extractor. The dataset was used to train and test the algorithm that was implemented in the proposed model. CNN’s receptive field aids in extracting the most recognizable aspects from these images automatically. The experimental findings show that the suggested framework is successful, with a recognition accuracy of 78% throughout the dataset and a template based lane detection method is used. Lane tracking is done with edge detection and the position of a region of interest in vanishing point in tack image is used for identifying the land departure. This detector on real time video with the region of interest selection and achieve in 23 fps.","['Materials Science', 'Materials Science, general']"
doi:10.1007/s11042-022-13222-2,en,An efficient image dahazing using Googlenet based convolution neural networks,OriginalPaper,"The dehazing is a significant colour image-processing technique for attaining a high quality of images from haze images. Now a day’s digital cameras are playing an important key role in many applications, such as scanning, HD image generation, traffic user, tourists, especially in hilly areas satellite and radar applications. The dehazing is a complex function for digital cameras since it converts a bayers mosaic image into a final color image and then estimates the output image. The full colour image cannot be reconstructed from incomplete samples due to haze problem, and hence appropriate dehazing models are implemented to overcome this problem. In this work, a dehazing algorithm is proposed with GoogleNet deep learning mechanism for getting the improved quality of the image. In this investigation, GoogleNet deep learning model is used to reconstruct the full color image without degrading the sensitivity and resolution. In this proposed work, the deep learning based convolutional networks are realized using demosaicking for pre-processing to reproduce the dehaged full color images from the incomplete samples. In this demosaicking task, the first step is demosaicking to produce a rough image consists of unwanted color artifacts. Second step is the refining step, in which, the deep residual estimation is used to decrese the color artifacts along with the multi model fusion concept to produce good quality output images. The performance measures, viz., Peak-Signal to Noise Ratio (PSNR), Structural similarity Index (SSIM), and Mean Square Error (MSE) are evaluated and compared with existing models. The PSNR is 32.78, SSIM is 0.9412, MSE is 0.098, F1-score is 0.989, sensitivity is 0.972, and CC is 0.978 have been attained by this optimized algorithm. The GoogleNet technology outperforms the existing methods. This deep learning mechanism does process the input hazy images and decomposes the smoothness hazy free elements from texture elements.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40593-021-00281-z,en,Keep Me in the Loop: Real-Time Feedback with Multimodal Data,"['OriginalPaper', 'Article']","This paper describes the CPR Tutor, a real-time multimodal feedback system for cardiopulmonary resuscitation (CPR) training. The CPR Tutor detects training mistakes using recurrent neural networks. The CPR Tutor automatically recognises and assesses the quality of the chest compressions according to five CPR performance indicators. It detects training mistakes in real-time by analysing a multimodal data stream consisting of kinematic and electromyographic data. Based on this assessment, the CPR Tutor provides audio feedback to correct the most critical mistakes and improve the CPR performance. The mistake detection models of the CPR Tutor were trained using a dataset from 10 experts. Hence, we tested the validity of the CPR Tutor and the impact of its feedback functionality in a user study involving additional 10 participants. The CPR Tutor pushes forward the current state of the art of real-time multimodal tutors by providing: (1) an architecture design, (2) a methodological approach for delivering real-time feedback using multimodal data and (3) a field study on real-time feedback for CPR training. This paper details the results of a field study by quantitatively measuring the impact of the CPR Tutor feedback on the performance indicators and qualitatively analysing the participants’ questionnaire answers.","['Computer Science', 'Artificial Intelligence', 'Educational Technology', 'User Interfaces and Human Computer Interaction', 'Computers and Education']"
doi:10.1007/s11042-021-11759-2,en,Sentiment analysis: a convolutional neural networks perspective,OriginalPaper,"With the dramatic growth of various social media platforms, sentiment analysis (SA) of and emotion detection (ED) in various social network posts, blogs, and conversations are very useful and effective for mining the true opinions on different issues, entities, or aspects. During the last decade, many statistical and probabilistic models based on lexical and machine learning approaches have been employed for these tasks. Majority of the relevant literature has emphasized on improving the contemporary SA determination and emotion extraction techniques. With the recent advancements in deep neural networks, various deep learning models have been heavily used to enhance the accuracy of SA. Convolutional neural networks (CNN), a deep neural network model formerly adopted for visual data processing only, has recently gained acceptance for textual inputs as well. As the inputs for SA may be textual, visual, or any combination of these, CNN seems to be a powerful tool. Capturing spatial and contextual information in an incremental fashion respectively from visual and textual inputs proves CNN as an effective model for SA. In this paper, we present an extensive survey that covers the applicability, challenges, and issues for textual, visual, and multimodal SA using CNNs. A detailed discussion and analysis for SA using a CNN model is summarized. For both of the unimodal inputs i.e. , textual and visual , we present an optimized algorithmic approach for SA determination using CNN.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40750-022-00194-8,en,Gender and Context-Specific Effects of Vocal Dominance and Trustworthiness on Leadership Decisions,"['OriginalPaper', 'Original Article']","Objective The evolutionary-contingency hypothesis, which suggests that preferences for leaders are context-dependent, has found relatively consistent support from research investigating leadership decisions based on facial pictures. Here, we test whether these results transfer to leadership decisions based on voice recordings. We examined how dominance and trustworthiness perceptions relate to leadership decisions in wartime and peacetime contexts and whether effects differ by a speaker’s gender. Further, we investigate two cues that might be related to leadership decisions, as well as dominance and trustworthiness perceptions: voice pitch and strength of regional accent. Methods We conducted a preregistered online study with 125 raters and recordings of 120 speakers (61 men, 59 women) from different parts in Germany. Raters were randomly distributed into four rating conditions: dominance, trustworthiness, hypothetical vote (wartime) and hypothetical vote (peacetime). Results We find that dominant speakers were more likely to be voted for in a wartime context while trustworthy speakers were more likely to be voted for in a peacetime context. Voice pitch functions as a main cue for dominance perceptions, while strength of regional accent functions as a main cue for trustworthiness perceptions. Conclusions This study adds to a stream of research that suggests that (a) people’s voices contain important information based on which we form social impressions and (b) we prefer different types of leaders across different contexts. Future research should disentangle effects of gender bias in leadership decisions and investigate underlying mechanisms that influence how people’s voices contribute to achieving social status.","['Social Sciences', 'Anthropology', 'Biological Psychology', 'Human Physiology', 'Evolutionary Biology', 'Neurosciences', 'Behavioral Sciences']"
doi:10.1007/s40435-022-00943-3,en,Yaw stability control of single-trailer truck using steerable wheel at middle axle: hardware-in-the-loop simulation,OriginalPaper,"A single-trailer truck has several advantages in transportation due to its simple loading and unloading process as well as its ability to transport a large and heavy load with low delivery cost. Despite more advantages in transportation, this vehicle often involved in road accident caused by reasons such as skidding, jack-knifing, trailer swing and trailer oscillation due to the unwanted yaw and lateral motions while manoeuvring. This elevates much concerns on the stability of the single-trailer truck vehicle, especially while travelling at high speed. This study proposed a yaw rejection control of single-trailer truck using a steerable wheel located at the middle axle of the truck. This study covers the development and verification of 15-DOF model of single-trailer truck to simulate vehicle responses in longitudinal plane. Then, yaw rejection controller is developed and examined using two conditions of manoeuvrings, namely single and double lane change tests. The yaw rejection controller results managed to reduce up to 34.66% lateral acceleration and 22.92% yaw rate compared to the passive vehicle configuration. Lastly, the yaw rejection controller was implemented on a small-scale prototype of single-trailer truck vehicle system using Hardware-in-the-loop simulation. The HIL simulation results showed that the yaw rejection controller managed to stabilise the vehicle during manoeuvring with realistic steering wheel actions deliverable by the actuator.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1007/s00259-022-05947-x,en,An EANM position paper on the application of artificial intelligence in nuclear medicine,"['OriginalPaper', 'Original Article']","Artificial intelligence (AI) is coming into the field of nuclear medicine, and it is likely here to stay. As a society, EANM can and must play a central role in the use of AI in nuclear medicine. In this position paper, the EANM explains the preconditions for the implementation of AI in NM and takes position.","['Medicine & Public Health', 'Nuclear Medicine', 'Imaging / Radiology', 'Orthopedics', 'Cardiology', 'Oncology']"
doi:10.1007/s13198-021-01511-2,en,Application of convolutional neural network under nonlinear excitation function in the construction of employee incentive and constraint model,"['OriginalPaper', 'Original article']","It is aimed to explore the relationship between the incentive constraint model and corporate performance, and expand the application of neural networks in the incentive mechanism, thereby providing a direction for the innovation development of the enterprise to a certain extent. Based on the convolutional neural network (CNN), the construction and practice of the employee incentive constraint model are discussed. First, fully combining the excellent performance of the nonlinear excitation function in CNN, a CNN-based PReLUs-Sigmoid (P-S) nonlinear excitation function is proposed and compared with several excitation functions. Second, the P-S nonlinear excitation function is integrated. Based on the law of diminishing marginal returns, the construction of the employee incentive constraint model is completed. Finally, companies with and without equity constraint mechanisms are selected as the research sample to analyze the relationship between the implementation of the incentive constraint mechanism and the performance level of the company. The results show that the P-S nonlinear excitation function based on CNN has both sparse expression ability and smooth nonlinear mapping correction ability. Also, it has applicability in the optimal solution. When the employee’s work effort is $$x = 2.5743$$ x = 2.5743 and excitation coefficient is $$\beta^{*} = 0.8285$$ β ∗ = 0.8285 , the optimal returns can be obtained between the enterprise organization and employees under this incentive constraint model. Before and after the implementation of the equity incentive constraint mechanism, there are significant differences in the performance level of enterprises. The implementation of the incentive constraint mechanism is beneficial to the improvement of enterprise performance level. The employee incentive constraint model constructed expands the application of CNN in the incentive mechanism and provides a direction for the development of enterprise performance.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s13042-022-01623-6,en,A hyper-heuristic guided by a probabilistic graphical model for single-objective real-parameter optimization,"['OriginalPaper', 'Original Article']","Metaheuristics algorithms are designed to find approximate solutions for challenging optimization problems. The success of the algorithm over a given optimization task relies on the suitability of its search heuristics for the problem-domain. Thus, the design of custom metaheuristic algorithms leads to more accurate solutions. Hyper-heuristics (HH) are important tools commonly used to select low-level heuristics (LLHs) to solve a specific problem. HH are able to acquire knowledge from the problems where they are used. However, as other artificial intelligence tools it is necessary to identify how the knowledge affects the performance of the algorithm. One way to generate such knowledge is to capture interactions between variables using probabilistic graphical models such as Bayesian networks (BN) in conjunction with estimation of distribution algorithms (EDA). This article presents a method based on that used an EDA based on BN as a high-level selection mechanism for HH called Hyper-heuristic approach based on Bayesian learning and evolutionary operators (HHBNO). Here the knowledge is extracted form BN to evolve the sequences of LLHs in an online learning process by exploring the inter-dependencies among the LLHs. The proposes approach is tested over CEC’17 set of benchmark function of single-objective real-parameter optimization. Statical tests verifies that the HHBNO  presents competitive results in comparison with other metaheuristic algorithms with high performance in terms of convergence. The generated BN is further visually investigated to display the acquired knowledge during the evolutionary process, and it is constructed with the probabilities of each LLHs.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s10278-022-00738-0,en,Contrast Enhancement of RGB Retinal Fundus Images for Improved Segmentation of Blood Vessels Using Convolutional Neural Networks,"['OriginalPaper', 'Original Paper']","Retinal fundus images are non-invasively acquired and faced with low contrast, noise, and uneven illumination. The low-contrast problem makes objects in the retinal fundus image indistinguishable and the segmentation of blood vessels very challenging. Retinal blood vessels are significant because of their diagnostic importance in ophthalmologic diseases. This paper proposes improved retinal fundus images for optimal segmentation of blood vessels using convolutional neural networks (CNNs). This study explores some robust contrast enhancement tools on the RGB and the green channel of the retinal fundus images. The improved images undergo quality evaluation using mean square error (MSE), peak signal to noise ratio (PSNR), Similar Structure Index Matrix (SSIM), histogram, correlation, and intersection distance measures for histogram comparison before segmentation in the CNN-based model. The simulation results analysis reveals that the improved RGB quality outperforms the improved green channel. This revelation implies that the choice of RGB to the green channel for contrast enhancement is adequate and effectively improves the quality of the fundus images. This improved contrast will, in turn, boost the predictive accuracy of the CNN-based model during the segmentation process. The evaluation of the proposed method on the DRIVE dataset achieves an accuracy of 94.47, sensitivity of 70.92, specificity of 98.20, and AUC (ROC) of 97.56.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s11063-022-10831-8,en,Implicit Relation Inference with Deep Path Extraction for Commonsense Question Answering,OriginalPaper,"Natural language inference plays an essential role in Commonsense Question Answering. Conventional models usually adopt keywords in questions and choices as queries to retrieve static and explicit evidence that is used to obtain final answers, where dynamic interaction between different keywords and implicit relations inference of deeper information are often neglected. In this paper, we propose a novel joint model, the Graph Relation retrieval Reasoning Network (GRRN), to explicitly introduce the dynamic interaction among different keywords and generate informative features that contribute to representation updating. In addition, to pursue in-depth relations between different keywords, we develop an optimised Path Evidence Fusion in the GRRN to obtain evidence based on deep paths and implicit relations with comprehensive knowledge by making full use of the original paths in external knowledge graphs. The experimental results show that compared with the baselines, our approach achieves remarkable improvement of 1.74 $$\%$$ % for precision on the CommonsenseQA dataset, thereby demonstrating the superiority of our state-of-the-art approach on implicit relation inference with deep paths.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.17269/s41997-022-00651-7,en,A comparison of the COVID-19 response for urban underserved patients experiencing healthcare transitions in three Canadian cities,"['OriginalPaper', 'Special Section on Equity and the COVID-19 Response in Canada: Qualitative Research']","Objectives The COVID-19 pandemic and response has highlighted existing strengths within the system of care for urban underserved populations, but also many fault lines, in particular during care transitions. The objectives of this study were to describe COVID-19 response policies for urban underserved populations in three Canadian cities; examine how these policies impact continuity of care for urban underserved populations; determine whether and how urban underserved community members were engaged in policy processes; and develop policy and operational recommendations for optimizing continuity of care for urban underserved populations during public health crises. Methods Using Walt & Gilson’s Policy Triangle framework as a conceptual guide, 237 policy and media documents were retrieved. Five complementary virtual group interview sessions were held with 22 front-line and lived-experience key informants to capture less well-documented policy responses and experiences. Documents and interview transcripts were analyzed inductively for policy content, context, actors, and processes involved in the pandemic response. Results Available documents suggest little focus on care continuity for urban underserved populations during the pandemic, despite public health measures having disproportionately negative impacts on their care. Policy responses were largely reactive and temporary, and community members were rarely involved. However, a number of community-based initiatives were developed in response to policy gaps. Promising practices emerged, including examples of new multi-level and multi-sector collaboration. Conclusion The pandemic response has exposed inequities for urban underserved populations experiencing care transitions; however, it has also exposed system strengths and opportunities for improvement to inform future policy direction. Objectifs La pandémie et la riposte au COVID-19 ont mis en évidence les forces existantes au sein du système de soins pour les populations urbaines mal desservies, mais aussi de nombreuses faillites, en particulier lors des transitions de soins. Les objectifs de cette étude étaient de décrire les politiques de réponse au COVID-19 pour les populations urbaines mal desservies dans trois villes canadiennes; examiner l’impact de ces politiques sur la continuité des soins pour les populations urbaines mal desservies; déterminer si et comment les membres de la communauté urbaine mal desservie ont été impliqués dans les processus politiques; et développer des recommandations politiques et opérationnelles pour optimiser la continuité des soins pour les populations urbaines mal desservies pendant les crises de santé publique. Méthodes Utilisant le cadre Policy Triangle de Walt et Gilson comme guide conceptuel, 237 documents politiques et des médias ont été récupérés. Cinq séances d’entrevues de groupe virtuelles complémentaires ont été organisées avec 22 informateurs clés de première ligne et d’expérience vécue pour saisir des réponses et des expériences politiques moins bien documentées. Les documents et les transcriptions des entrevues ont été analysés de manière inductive pour le contenu politique, le contexte, les acteurs et les processus impliqués dans la riposte à la pandémie. Résultats Les documents disponibles suggèrent que l’accent est peu mis sur la continuité des soins pour les populations urbaines mal desservies pendant la pandémie, malgré les mesures de santé publique ayant des impacts négatifs disproportionnés sur leurs soins. Les réponses politiques étaient en grande partie réactives et temporaires, et les membres de la communauté étaient rarement impliqués. Cependant, un certain nombre d’initiatives communautaires ont été élaborées en réponse aux lacunes des politiques. Des pratiques prometteuses ont émergé, y compris des exemples de nouvelles collaborations multiniveaux et multisectorielles. Conclusion La réponse à la pandémie a révélé des inégalités pour les populations urbaines mal desservies qui subissent des transitions de soins, mais elle a également exposé les forces du système et les possibilités d’amélioration pour éclairer l’orientation future des politiques.","['Medicine & Public Health', 'Public Health']"
doi:10.1007/s10619-021-07361-y,en,FIGS-DEAF: an novel implementation of hybrid deep learning algorithm to predict autism spectrum disorders using facial fused gait features,OriginalPaper,"Autism spectrum disorder (A.S.D.) is considered a heterogeneous mental disorder, which is notoriously difficult to identify for a better diagnosis, especially among children. The current diagnosis methodology is purely based on the behavioural observation of symptoms prone to misdiagnosis. Several hybrid methods were explored, which also needs its improvisation in better prediction and diagnosis to move this field towards intelligent and accurate diagnosis. The main objective of this research paper was to develop the new diagnosis software which integrates the novel fuzzy hybrid deep convolutional neural networks and fusion of facial expressions and human gaits based on input video sequences. The algorithm has been trained and validated with the different video datasets such as Kaggle FER2013 and Karolinska Directed Emotional Faces (KDEF) datasets with real-time test scenarios, and various parameters such as accuracy, recall and F1-score were evaluated. Our proposed deep learning model outperforms another state-of-the-art method with an increase in prediction accuracy up to 30% with maximum accuracy of 95%. The model presented in this paper yields more advantages in terms of prediction time and accuracy also. However, the speech therapists, teachers, caretakers, and parents can use the software as a technological tool when working with children with A.S.D.","['Computer Science', 'Database Management', 'Data Structures', 'Information Systems Applications (incl.Internet)', 'Operating Systems', 'Memory Structures']"
doi:10.1007/s11759-022-09466-x,en,Connecting Past to Present: Enacting Indigenous Data Governance Principles in Westbank First Nation’s Archaeology and Digital Heritage,"['OriginalPaper', 'Research']","In this paper, we describe a collaboration between the Westbank First Nation Archaeology Office and UBC Okanagan that aims to create digital maps to enable engagement with syilx digital heritage and build capacity in digital tools and technologies. We examine what data governance frameworks mean for digital heritage and how they articulate with the United Nations Declaration on the Rights of Indigenous Peoples (2007) and the First Nations Information Governance Centre’s OCAP® principles. We propose digital tools such as open-source and mobile-ready storymaps to showcase digital heritage that is appropriate for public sharing, practices that can promote and enhance community decision-making, and create training opportunities in digital methods in Westbank First Nation. Opening a conversation around digital tools is one way that archaeologists can begin to enact Indigenous data governance as a step towards dismantling colonial structures and practice in archaeology and digital heritage. Nous décrivons dans cet article une collaboration entre le Westbank First Nation Archaeology Office et UBC Okanagan ayant pour objectif de créer des cartes numériques afin de permettre une participation au patrimoine numérique syilx et de construire une capacité en matière d'outils et de technologies numériques. Nous examinons ce que les cadres de gouvernance de données impliquent pour le patrimoine numérique et comment ils s'articulent avec la Déclaration des Nations Unies sur les droits des peuples autochtones (2007) et les principes OCAP® du Centre de gouvernance de l'information des Premières Nations. Nous proposons des outils numériques tels que des cartes narratives en open source et consultables sur un appareil mobile afin de présenter un patrimoine numérique approprié aux fins d'un partage public, de pratiques susceptibles de promouvoir et d'optimiser la prise de décision communautaire et de créer des opportunités de formation s'appuyant sur des méthodes numériques au sein de la Westbank First Nation. Initier une conversation sur les outils numériques est un moyen grâce auquel les archéologues peuvent commencer à mettre en œuvre une gouvernance des données autochtones à titre d'avancée vers le démantèlement de la pratique et des structures coloniales en matière d'archéologie et de patrimoine numérique En este documento, describimos una colaboración entre la oficina arqueológica Westbank First Nation Archaeology Office y UBC Okanagan que tiene como objetivo crear mapas digitales para permitir el compromiso con el patrimonio digital syilx y desarrollar capacidades en herramientas y tecnologías digitales. Examinamos el significado de los marcos de gobernanza de datos para el patrimonio digital y cómo se articulan con la Declaración de las Naciones Unidas sobre los Derechos de los Pueblos Indígenas (2007) y los principios OCAP® del Centro de Gobernanza de la Información de las Primeras Naciones. Proponemos herramientas digitales como esquemas narrativos (storymaps) de código abierto y listos para dispositivos móviles para mostrar el patrimonio digital que es apropiado para el intercambio público, prácticas que pueden promover y mejorar la toma de decisiones de la comunidad y crear oportunidades de capacitación en métodos digitales en Westbank First Nation. Abrir una conversación sobre herramientas digitales es una forma en que los arqueólogos pueden comenzar a promulgar la gobernanza de datos indígenas como un paso hacia el desmantelamiento de las estructuras y prácticas coloniales en la arqueología y el patrimonio digital.","['Social Sciences', 'Archaeology', 'Anthropology', 'Cultural Heritage']"
doi:10.1007/s13349-022-00565-5,en,Domain-adapted Gaussian mixture models for population-based structural health monitoring,"['OriginalPaper', 'Original Paper']","Transfer learning, in the form of domain adaptation, seeks to overcome challenges associated with a lack of available health-state data for a structure, which severely limits the effectiveness of conventional machine learning approaches to structural health monitoring (SHM). These technologies utilise labelled information across a population of structures (and physics-based models), such that inferences are improved, either for the complete population, or for particular target structures — enabling a population-based view of SHM. The aim of these methods is to infer a mapping between each member of the population’s feature space (called a domain) in which a classifier trained on one member of the population will generalise to the remaining structures. This paper introduces the domain-adapted Gaussian mixture model (DA-GMM) for population-based SHM (PBSHM) scenarios. The DA-GMM, infers a linear mapping that transforms target data from one structure onto a Gaussian mixture model that has been inferred from source data (from another structure). The proposed model is solved via an expectation maximisation technique. The method is demonstrated on three case studies: an artificial dataset demonstrating the approach’s effectiveness when the target domain differs by two-dimensional rotations; a population of two numerical shear-building structures; and a heterogeneous population of two bridges, the Z24 and KW51 bridges. In each case study, the method is shown to provide informative results, outperforming other conventional forms of GMM (where no target labelled data are assumed available), and provide mappings that allow the effective exchange of labelled information from source to target datasets.","['Engineering', 'Civil Engineering', 'Measurement Science and Instrumentation', 'Vibration, Dynamical Systems, Control']"
doi:10.1007/s12145-022-00881-w,en,Estimation of the amplification properties of soil through HVSR inversion based on an elitist genetic algorithm,"['OriginalPaper', 'Research']","The horizontal-to-vertical spectral ratio method is one of the most commonly utilized techniques to estimate the site response. This method is frequently preferred for practical calculation of the dynamic properties of soil layers. Recently, the popularity of this technique has been increasing thanks to the methods developed to obtain the shear wave velocity profile from the horizontal-to-vertical spectral ratio. In this study, a MATLAB-based graphical user interface has been developed for inversion and forward calculation of the horizontal-to-vertical spectral ratio. This code uses the equivalent linear approach based on the viscoelastic Kelvin-Voigt model to compute the theoretical site response of the horizontally stratified soil layers. Furthermore, the developed graphical user interface can easily estimate the dynamic parameters such as thickness, shear wave velocity, density and damping ratio of the soil layers through an elitist genetic algorithm, and thereby obtain the shear wave velocity profiles. The reliability of the developed algorithm has been tested using the synthetic and real datasets. The results from the real data examples have been compared with those from previous studies and the satisfactory results have been obtained.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s11242-022-01864-7,en,Transport of Organic Volatiles through Paper: Physics-Informed Neural Networks for Solving Inverse and Forward Problems,OriginalPaper,"Transport of volatile organic compounds (VOCs) through porous media with active surfaces takes place in many important applications, such as in cellulose-based materials for packaging. Generally, it is a complex process that combines diffusion with sorption at any time. To date, the data needed to use and validate the mathematical models proposed in literature to describe the mentioned processes are scarce and have not been systematically compiled. As an extension of the model of Ramarao et al. (Dry Technol 21(10):2007–2056, 2003) for the water vapor transport through paper, we propose to describe the transport of VOCs by a nonlinear Fisher–Kolmogorov–Petrovsky–Piskunov equation coupled to a partial differential equation (PDE) for the sorption process. The proposed PDE system contains specific material parameters such as diffusion coefficients and adsorption rates as multiplication factors. Although these parameters are essential for solving the PDEs at a given time scale, not all of the required parameters can be directly deduced from experiments, particularly diffusion coefficients and sorption constants. Therefore, we propose to use experimental concentration data, obtained for the migration of dimethyl sulfoxide (DMSO) through a stack of paper sheets, to infer the sorption constant. These concentrations are considered as the outcome of a model prediction and are inserted into an inverse boundary problem. We employ Physics-Informed Neural Networks (PINNs) to find the underlying sorption constant of DMSO on paper from this inverse problem. We illustrate how to practically combine PINN-based calculations with experimental data to obtain trustworthy transport-related material parameters. Finally we verify the obtained parameter by solving the forward migration problem via PINNs and finite element methods on the relevant time scale and show the satisfactory correspondence between the simulation and experimental results. A mathematical model to describe transport of polar volatile organics through paper is proposed. Based on experimental data, the deep learning method of physics-informed neural networks (PINNs) is used to solve the inverse problem of finding the sorption time constant. Solutions for the forward problem are obtained by the standard finite element method (FEM) and PINN methods. These solutions are compared with each other as well as with the experimental data to verify the model.","['Earth Sciences', 'Geotechnical Engineering & Applied Earth Sciences', 'Industrial Chemistry/Chemical Engineering', 'Hydrology/Water Resources', 'Civil Engineering', 'Hydrogeology', 'Classical and Continuum Physics']"
doi:10.1007/s42235-022-00298-7,en,Crisscross Harris Hawks Optimizer for Global Tasks and Feature Selection,"['OriginalPaper', 'Research Article']","Harris Hawks Optimizer (HHO) is a recent well-established optimizer based on the hunting characteristics of Harris hawks, which shows excellent efficiency in solving a variety of optimization issues. However, it undergoes weak global search capability because of the levy distribution in its optimization process. In this paper, a variant of HHO is proposed using Crisscross Optimization Algorithm (CSO) to compensate for the shortcomings of original HHO. The novel developed optimizer called Crisscross Harris Hawks Optimizer (CCHHO), which can effectively achieve high-quality solutions with accelerated convergence on a variety of optimization tasks. In the proposed algorithm, the vertical crossover strategy of CSO is used for adjusting the exploitative ability adaptively to alleviate the local optimum; the horizontal crossover strategy of CSO is considered as an operator for boosting explorative trend; and the competitive operator is adopted to accelerate the convergence rate. The effectiveness of the proposed optimizer is evaluated using 4 kinds of benchmark functions, 3 constrained engineering optimization issues and feature selection problems on 13 datasets from the UCI repository. Comparing with nine conventional intelligence algorithms and 9 state-of-the-art algorithms, the statistical results reveal that the proposed CCHHO is significantly more effective than HHO, CSO, CCNMHHO and other competitors, and its advantage is not influenced by the increase of problems’ dimensions. Additionally, experimental results also illustrate that the proposed CCHHO outperforms some existing optimizers in working out engineering design optimization; for feature selection problems, it is superior to other feature selection methods including CCNMHHO in terms of fitness, error rate and length of selected features.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s10586-022-03813-x,en,Security provisions in smart edge computing devices using blockchain and machine learning algorithms: a novel approach,OriginalPaper,"It is difficult to manage massive amounts of data in an overlying environment with a single server. Therefore, it is necessary to comprehend the security provisions for erratic data in a dynamic environment. The authors are concerned about the security risk of vulnerable data in a Mobile Edge based distributive environment. As a result, edge computing appears to be an excellent perspective in which training can be done in an Edge-based environment. The combination of Edge computing and consensus approach of Blockchain in conjunction with machine learning techniques can further improve data security, mitigate the possibility of exposed data, and it reduces the risk of a data breach. As a result, the concept of federated learning provides a path for training the shared data. A dataset was collected that contained several vulnerable, exposed, recovered, and secured data and data security was precepted under the surveillance of two-factor authentication. This paper discusses the evolution of data and security flaws and their corresponding solutions in smart edge computing devices. The proposed model incorporates data security using consensus approach of Blockchain and machine learning techniques that include several classifiers and optimization techniques. Further, the authors applied the proposed algorithms in an edge computing environment by distributing several batches of data to different clients. As a result, the client privacy was maintained by using Blockchain servers. Furthermore, the authors segregated the client data into batches that were trained using the federated learning technique. The results obtained in this paper demonstrate the implementation of a Blockchain-based training model in an edge-based computing environment.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s10844-022-00764-y,en,An image and text-based multimodal model for detecting fake news in OSN’s,OriginalPaper,"Digital Mass Media has become the new paradigm of communication that revolves around online social networks. The increase in the utilization of online social networks (OSNs) as the primary source of information and the increase of online social platforms providing such news has increased the scope of spreading fake news. People spread fake news in multimedia formats like images, audio, and video. Visual-based news is prone to have a psychological impact on the users and is often misleading. Therefore, Multimodal frameworks for detecting fake posts have gained demand in recent times. This paper proposes a framework that flags fake posts with Visual data embedded with text. The proposed framework works on data derived from the Fakeddit dataset, with over 1 million samples containing text, image, metadata, and comments data gathered from a wide range of sources, and tries to exploit the unique features of fake and legitimate images. The proposed framework has different architectures to learn visual and linguistic models from the post individually. Image polarity datasets, derived from Flickr, are also considered for analysis, and the features extracted from these visual and text-based data helped in flagging news. The proposed fusion model has achieved an overall accuracy of 91.94%, Precision of 93.43%, Recall of 93.07%, and F1-score of 93%. The experimental results show that the proposed Multimodality model with Image and Text achieves better results than other state-of-art models working on a similar dataset.","['Computer Science', 'Information Storage and Retrieval', 'Data Structures and Information Theory', 'Artificial Intelligence', 'IT in Business', 'Natural Language Processing (NLP)']"
doi:10.1007/s41660-022-00301-y,en,Towards Sustainable Integration of STATCOM and DGs Based Radial Distribution Systems Using Dynamic Adaptive Aquila Optimizer,"['OriginalPaper', 'Original Research Paper']","This paper focuses on the application of a new metaheuristic method, namely, the Dynamic Aquila Optimizer Algorithm (DAAOA), to improve the performances of large radial distribution system (RDS). The proposed power management strategy aims to optimize the reactive and active power flow in the RDS by integrating and controlling multi-shunt compensators-based STATCOM devices and multi-distributed generation (DG) units. To relieve the major drawbacks of the standard AOA, a new variant namely DAAOA is proposed. The first modification introduced in the proposed variant is associated with the expanded exploration; however, the second modification introduced is related to narrowed exploitation by creating dynamic adjustment of the two coefficients (G1 and G2) using “sine” and “cosine” functions. The dynamic aspect of these two proposed modifications allows creating adequate diversity in the search space and improving the intensification process, this will led to improve greatly the convergence behavior of the standard algorithm and reduce the number of trials required in particular in solving large-scale problems. In the first scenario, the particularity of the proposed variant against the standard AOA and other recent methods has been validated on thirteen benchmark functions with unimodal and multimodal forms. In the second scenario, the proposed DAAOA has been validated on two electric test systems, 33-Bus and the large 85-Bus RDS. Firstly, and to reduce the size of the search space, two sensitivity indices are introduced to find the most suitable locations to integrate STATCOM devices and DGs. Secondly, the proposed DAAOA is applied to select the optimal locations and to optimize dynamically the reactive powers of multi STATCOM devices and the active power of DGs units to be exchanged with the RDS. Two objective functions such as the total power loss (TPL) and the loading margin (LM) are optimized. Achieved results compared to other recent techniques found in the recent literature prove the efficiency of the proposed DAAOA in solving large-scale optimization problems related to RDS.","['Engineering', 'Industrial and Production Engineering', 'Sustainable Development', 'Industrial Chemistry/Chemical Engineering', 'Energy Policy, Economics and Management', 'Waste Management/Waste Technology']"
doi:10.1007/s12667-022-00541-4,en,Implementation of a novel hybrid optimizer for energy management of fuel cell/battery/supercapacitor system,"['ReviewPaper', 'Review Article']","In this paper, a fuel cell (FC) hybrid emergency system is presented. It contains fuel cells, batteries, and supercapacitors which are felicitous for different conveyance applications. Abbreviating the hydrogen consumption and incrementing the lifetime of potency sources are the key factors of each energy management strategy (EMS). An incipient proposed EMS depending on a conventional proportional-integral (PI) controller is presented considering FC efficiency to ascertain the operating of the FC stack within its maximum efficiency region. Regarding tunning the controller gains ( $${K}_{p}$$ K p , $${K}_{i}$$ K i ), A proposed hybrid metaheuristic optimization technique denominated JSPSOBAT is presented which coalesces jellyfish (JS) optimizer, particle swarm optimizer (PSO), and BAT optimizer. To validate the overall execution of the JSPSOBAT technique, a comparative study with other single and hybrid metaheuristic optimization techniques is presented. The proposed EMS is compared with other EMSs such as state machine control, classical PI control, equivalent consumption minimization, and external energy maximization. The results show that the proposed technique produces the best performance.","['Energy', 'Energy Policy, Economics and Management', 'Operations Research/Decision Theory', 'Optimization', 'Energy Systems']"
doi:10.1007/s00158-022-03459-8,en,Single-objective aerodynamic optimization of a streamlined bridge deck subjected to shape modification using a polynomial emulator and genetic algorithm,"['OriginalPaper', 'Research Paper']","Traditional approach based on wind tunnel tests used in bridge wind-resistant design is no more efficient since long-span bridges require an iterative process involving multiple design variables and processes. To ease the design process, scholars have proposed aerodynamic shape optimization based on the Kriging surrogate model. Despite the prowess of the Kriging model, it is accurate only when using high-fidelity data (experimental or Large Eddy Simulation data) and is not suitable for noisy datasets, making this model costly and hindering its practical use. To tackle this issue, the present study proposed a polynomial surrogate combined with a genetic algorithm to determine the optimal shape of a streamlined bridge deck cross-section. First, a uniform sampling plan considering 57 different geometries was generated. Then, the polynomial surrogate was used to predict the aerodynamic coefficients and the flutter velocity based on the dataset obtained from the unsteady Reynolds-average Navier–Stokes computational fluid dynamics simulations. The accuracy of the surrogate model is evaluated using error metrics, such as the sum-squared error, R -squared, and the root mean squared error. The results obtained from the statistical metrics demonstrate that the proposed polynomial surrogate provides an accurate prediction of the force coefficients and their derivatives, as well as the critical flutter velocity. A benchmark example is used to demonstrate the efficiency of the proposed model. Finally, a genetic algorithm was introduced to determine an optimal shape, and the results demonstrate that the proposed optimization framework can improve the design process remarkably.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1186/s13059-022-02815-7,en,MoDLE: high-performance stochastic modeling of DNA loop extrusion interactions,"['OriginalPaper', 'Software']","DNA loop extrusion emerges as a key process establishing genome structure and function. We introduce MoDLE, a computational tool for fast, stochastic modeling of molecular contacts from DNA loop extrusion capable of simulating realistic contact patterns genome wide in a few minutes. MoDLE accurately simulates contact maps in concordance with existing molecular dynamics approaches and with Micro-C data and does so orders of magnitude faster than existing approaches. MoDLE runs efficiently on machines ranging from laptops to high performance computing clusters and opens up for exploratory and predictive modeling of 3D genome structure in a wide range of settings.","['Life Sciences', 'Animal Genetics and Genomics', 'Human Genetics', 'Plant Genetics and Genomics', 'Microbial Genetics and Genomics', 'Bioinformatics', 'Evolutionary Biology']"
doi:10.1007/s00170-022-10525-4,en,Multi-channel sensor fusion for real-time bearing fault diagnosis by frequency-domain multilinear principal component analysis,"['OriginalPaper', 'ORIGINAL ARTICLE']","Real-time health condition monitoring of bearings plays a significant role in the functionality of the rotary machinery. Multi-channel sensor fusion can be more robust for identifying diverse bearing fault diagnosis scenarios. However, the high-dimensional data and complex fault scenarios that can occur in the system pose significant challenges for effective fault diagnosis. State-of-the-art artificial intelligence-based bearing fault diagnosis system involves multi-channel sensor fusion, which usually leverages time–frequency analysis, feature extraction, and supervised learning. Nevertheless, those methods usually require a large training dataset for the machine learning model development. This paper proposes a new multi-channel sensor fusion methodology, named frequency-domain multilinear principal component analysis (FDMPCA), by integrating acoustics and vibration signals with different sampling rates and limited training data. Frequency analysis is firstly leveraged to transform the original signals from time to frequency domain, and the frequency responses of heterogeneous channels form a tensor structure named the frequency-domain (FD) tensor. Subsequently, the FD tensor is decomposed by multilinear principal component analysis (MPCA), resulting in low-dimensional process features for fault diagnosis. Finally, the extracted features can be used to train a Neural Network (NN) model for fault diagnosis. To validate the effectiveness of the proposed method, the bearing fault experiments were conducted on a machinery fault simulator while multiple vibration and acoustic signals were collected. Experimental results demonstrated that the proposed approach can effectively identify the machine fault conditions and outperform the benchmark methods given the limited training data.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s12204-022-2530-6,en,Ship Pipe Layout Optimization Based on Improved Particle Swarm Optimization,OriginalPaper,"Ship pipe layout optimization is one of the difficulties and hot spots in ship intelligent production design. A high-dimensional vector coding is proposed based on the research of related pipe coding and ship pipe route features in this paper. The advantages of this coding method are concise structure, strong compatibility, and independence from the gridding space. Based on the proposed coding, the particle swarm optimization algorithm is implemented, and the algorithm is improved by the pre-selected path strategy and the branch-pipe processing strategy. Finally, two simulation results reveal that the proposed coding and algorithm have feasibility and engineering practicability.","['Engineering', 'Electrical Engineering', 'Materials Science, general', 'Computer Science, general', 'Architecture, general', 'Life Sciences, general']"
doi:10.1007/s10994-022-06273-x,en,"Model-free inverse reinforcement learning with multi-intention, unlabeled, and overlapping demonstrations",OriginalPaper,"In this paper, we define a novel inverse reinforcement learning (IRL) problem where the demonstrations are multi-intention, i.e., collected from multi-intention experts, unlabeled, i.e., without intention labels, and partially overlapping, i.e., shared between multiple intentions. In the presence of overlapping demonstrations, current IRL methods, developed to handle multi-intention and unlabeled demonstrations, cannot successfully learn the underlying reward functions. To solve this limitation, we propose a novel clustering-based approach to disentangle the observed demonstrations and experimentally validate its advantages. Traditional clustering-based approaches to multi-intention IRL, which are developed on the basis of model-based Reinforcement Learning (RL), formulate the problem using parametric density estimation. However, in high-dimensional environments and unknown system dynamics, i.e., model-free RL, the solution of parametric density estimation is only tractable up to the density normalization constant. To solve this, we formulate the problem as a mixture of logistic regressions to directly handle the unnormalized density. To research the challenges faced by overlapping demonstrations, we introduce the concepts of shared pair , which is a state-action pair that is shared in more than one intention, and separability , which resembles how well the multiple intentions can be separated in the joint state-action space. We provide theoretical analyses under the global optimality condition and the existence of shared pairs. Furthermore, we conduct extensive experiments on four simulated robotics tasks, extended to accept different intentions with specific levels of separability, and a synthetic driver task developed to directly control the separability. We evaluate the existing baselines on our defined problem and demonstrate, theoretically and experimentally, the advantages of our clustering-based solution, especially when the separability of the demonstrations decreases.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s40430-022-03924-x,en,"Design, aerodynamic analysis and optimization of a next-generation commercial airliner","['OriginalPaper', 'Technical Paper']","Unconventional configurations and innovative propulsion technologies have been continuously developed for reducing both fuel-burn and global net carbon emissions. This article describes an advanced civil transport aircraft designed from the combination of a Box-Wing configuration with a Boundary Layer Ingestion (BLI) propulsion system. A conceptual-level Multidisciplinary Design Optimization strategy provided the main aerodynamic and performance characteristics of the aircraft, based on appropriate design requirements, variables and constraints. For direct performance comparison against a conventional aircraft, a single-point objective function based on minimum block fuel was evaluated by means of low-fidelity aircraft models. Subsequently, a back-to-back Computational Fluid Dynamics assessment of non-BLI and BLI versions of the aircraft was performed. Two major analyses comprised the aerodynamic evaluation: (i) quantification of the BLI benefit using the power balance method, (ii) performance evaluation of the propulsor inlet in terms of the total pressure recovery and the distortion index. The conceptual design results showed the box-wing configuration provided major fuel-burn savings compared to its conventional counterpart. On the other hand, the BLI version reduced engine power requirements at cruise in comparison with the non-BLI version, but decreased the total pressure recovery, resulting in more distortion at the aerodynamic interface plane. The main contribution of this study lies on the potential benefits of such an original unconventional configuration, whose technologies increased aerodynamic performance, which reduced fuel consumption and hence carbon emissions.","['Engineering', 'Mechanical Engineering']"
doi:10.1007/s11063-022-11092-1,en,Moving Object Detection in Video Sequences Based on a Two-Frame Temporal Information CNN,OriginalPaper,"Moving object detection methods, MOD, must solve complex situations found in video scenarios related to bootstrapping, illumination changes, bad weather, PTZ, intermittent objects, color camouflage, camera jittering, low camera frame rate, noisy videos, shadows, thermal videos, night videos, etc. Some of the most promising MOD methods are based on convolutional neural networks, which are among the best-ranked algorithms in the CDnet14 dataset. Therefore, this paper presents a novel CNN to detect moving objects called Two-Frame CNN, 2FraCNN. Unlike best-ranked algorithms in CDnet14, 2FraCNN is a non-transfer learning model and employs temporal information to estimate the motion of moving objects. The architecture of 2FraCNN is inspired by how the optical flow helps to estimate motion, and its core is the FlowNet architecture. 2FraCNN processes temporal information through the concatenation of two consecutive frames and an Encoder-Decoder architecture. 2FraCNN includes a novel training scheme to deal with unbalanced pixel classes background/foreground. 2FraCNN was evaluated using three different schemes: the CDnet14 benchmark for a state-of-the-art comparison; against human performance metric intervals for a realistic evaluation; and for practical purposes with the performance instrument PVADN that considers the quantitative criteria of performance, speed, auto-adaptability, documentation, and novelty. Findings show that 2FraCNN has a performance comparable to the top ten algorithms in CDnet14 and is one of the best twelve in the PVADN evaluation. Also, 2FraCNN demonstrated that can solve many video challenges categories with human-like performance, such as dynamic backgrounds, jittering, shadow, bad weather, and thermal cameras, among others. Based on these findings, it can be concluded that 2FraCNN is a robust algorithm solving different video conditions with competent performance regarding state-of-the-art algorithms.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1007/s13563-022-00357-9,en,"Steel price index forecasting through neural networks: the composite index, long products, flat products, and rolled products","['OriginalPaper', 'Original Paper']","Forecasting commodity prices is a vital issue to a wide spectrum of market participants and policy makers in the metal sector. In this work, the forecast problem is investigated by focusing on the Chinese market, with the daily steel price indices of the composite index, long products, flat products, and rolled products spanning a 10-year period from June 15, 2011, to April 15, 2021. The non-linear auto-regressive neural network is employed as the forecasting model and model performance corresponding to a variety of settings is explored over algorithms for model estimations, numbers of hidden neurons and delays, and ratios for splitting the data for each of the four price indices. Models that are relatively simple and generate forecasts of high accuracy and stabilities are arrived at. Particularly, relative root mean square errors (RRMSEs) of 0.49%/0.49%/0.52%/0.55%, 0.51%/0.44%/0.51%/0.49%, and 0.52%/0.53%/0.56%/0.53%, respectively, for training, validation, and testing, and the overall RRMSE of 0.50%/0.49%/0.53%/0.54% are reached for the composite index/long products/rolled products/flat products. The results could, on the one hand, serve as standalone technical price forecasts. They could, on the other hand, be combined with other (fundamental) forecast results for forming perspectives of price trends and carrying out policy analysis.","['Economics', 'Industrial Organization', 'Mineral Resources', 'Innovation/Technology Management', 'Environmental Economics', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1038/s41598-022-24417-w,en,Decoding the cognitive states of attention and distraction in a real-life setting using EEG,"['OriginalPaper', 'Article']","Lapses in attention can have serious consequences in situations such as driving a car, hence there is considerable interest in tracking it using neural measures. However, as most of these studies have been done in highly controlled and artificial laboratory settings, we want to explore whether it is also possible to determine attention and distraction using electroencephalogram (EEG) data collected in a natural setting using machine/deep learning. 24 participants volunteered for the study. Data were collected from pairs of participants simultaneously while they engaged in Tibetan Monastic debate, a practice that is interesting because it is a real-life situation that generates substantial variability in attention states. We found that attention was on average associated with increased left frontal alpha, increased left parietal theta, and decreased central delta compared to distraction. In an attempt to predict attention and distraction, we found that a Long Short Term Memory model classified attention and distraction with maximum accuracy of 95.86% and 95.4% corresponding to delta and theta waves respectively. This study demonstrates that EEG data collected in a real-life setting can be used to predict attention states in participants with good accuracy, opening doors for developing Brain-Computer Interfaces that track attention in real-time using data extracted in daily life settings, rendering them much more usable.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s13399-022-03532-8,en,Statistical optimization of the fermentation factors for increased biorefinery of corn-steep water effluent to lactic acid by Enterococcus faecium strain WH51-1,"['OriginalPaper', 'Original Article']","Organic waste bioprocessing for production of useful products are increasing due to fossil fuel depletion, problems with waste management, and the requirement for biodegradable plastics. Lactic acid (LA) is important in foods, polymer beverages, cosmetics, and medicine applications. However, the main challenge in the large-scale processing of LA is the enhanced production and productivity using low-cost resources. In this study, Enterococcus faecium WH51-1, a newly obtained thermo-alkali tolerant LA bacterium, was used for LA production from corn-steep water (CSW) as raw material. The optimal culture conditions and medium components were established. Different concentrations of yeast extract (YE) and pH values were investigated with highest LA production of 48.2 ± 2.7 g/L at a yield of 0.89 ± 0.04 g/g-consumed sugars and a productivity of 0.50 ± 0.03 g/L.h at pH 8.5 without YE supplementation. A central composite design (CCD) was utilized to determine the high LA concentrations at optimal values for the process variables (sugar concentrations, temperature, inocula sizes, pH value, and YE concentration). The model was realized with an acceptable fit. The optimization of the factors was carried out by normal probability plots, interaction-plots, incorporating effect plots, analysis of variance (ANOVA), surface plots, Pareto charts, and contour plots. All the parameters were influenced at a 5% significance level. Additionally, some of the possible interactions between these parameters also influenced the production process. A regression model was suggested and fitted the experimental data very well. The results of this work investigated that LA synthesized of the optimized-fermentation conditions was 10% greater than classical optimization methods. This study presented a green “free nutrient” and cost-effective utilization of corn steep effluent as a cheap alternative substrate for biorefinery to lactic acid. Graphical Abstract ","['Energy', 'Renewable and Green Energy', 'Renewable and Green Energy', 'Biotechnology']"
doi:10.1038/s41598-022-24887-y,en,A multi-scale feature extraction fusion model for human activity recognition,"['OriginalPaper', 'Article']","Human Activity Recognition (HAR) is an important research area in human–computer interaction and pervasive computing. In recent years, many deep learning (DL) methods have been widely used for HAR, and due to their powerful automatic feature extraction capabilities, they achieve better recognition performance than traditional methods and are applicable to more general scenarios. However, the problem is that DL methods increase the computational cost of the system and take up more system resources while achieving higher recognition accuracy, which is more challenging for its operation in small memory terminal devices such as smartphones. So, we need to reduce the model size as much as possible while taking into account the recognition accuracy. To address this problem, we propose a multi-scale feature extraction fusion model combining Convolutional Neural Network (CNN) and Gated Recurrent Unit (GRU). The model uses different convolutional kernel sizes combined with GRU to accomplish the automatic extraction of different local features and long-term dependencies of the original data to obtain a richer feature representation. In addition, the proposed model uses separable convolution instead of classical convolution to meet the requirement of reducing model parameters while improving recognition accuracy. The accuracy of the proposed model is 97.18%, 96.71%, and 96.28% on the WISDM, UCI-HAR, and PAMAP2 datasets respectively. The experimental results show that the proposed model not only obtains higher recognition accuracy but also costs lower computational resources compared with other methods.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11004-022-10037-7,en,Ensemble and Self-supervised Learning for Improved Classification of Seismic Signals from the Åknes Rockslope,"['OriginalPaper', 'Special Issue']","A case study with seismic geophone data from the unstable Åknes rock slope in Norway is considered. This rock slope is monitored because there is a risk of severe flooding if the massive-size rock falls into the fjord. The geophone data is highly valuable because it provides 1000 Hz sampling rates data which are streamed to a web resource for real-time analysis. The focus here is on building a classifier for these data to distinguish different types of microseismic events which are in turn indicative of the various processes occurring on the slope. There are 24 time series from eight 3-component geophone data for about 3500 events in total, and each of the event time series has a length of 16 s. For the classification task, novel machine learning methods such as deep convolutional neural networks are leveraged. Ensemble prediction is used to extract information from all time series, and this is seen to give large improvements compared with doing immediate aggregation of the data. Further, self-supervised learning is evaluated to give added value here, in particular for the case with very limited training data.","['Earth Sciences', 'Earth Sciences, general', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Geotechnical Engineering & Applied Earth Sciences', 'Hydrogeology']"
doi:10.1186/s12938-022-01048-w,en,Global–local multi-stage temporal convolutional network for cataract surgery phase recognition,"['OriginalPaper', 'Research']","Background Surgical video phase recognition is an essential technique in computer-assisted surgical systems for monitoring surgical procedures, which can assist surgeons in standardizing procedures and enhancing postsurgical assessment and indexing. However, the high similarity between the phases and temporal variations of cataract videos still poses the greatest challenge for video phase recognition. Methods In this paper, we introduce a global–local multi-stage temporal convolutional network (GL-MSTCN) to explore the subtle differences between high similarity surgical phases and mitigate the temporal variations of surgical videos. The presented work consists of a triple-stream network (i.e., pupil stream, instrument stream, and video frame stream) and a multi-stage temporal convolutional network. The triple-stream network first detects the pupil and surgical instruments regions in the frame separately and then obtains the fine-grained semantic features of the video frames. The proposed multi-stage temporal convolutional network improves the surgical phase recognition performance by capturing longer time series features through dilated convolutional layers with varying receptive fields. Results Our method is thoroughly validated on the CSVideo dataset with 32 cataract surgery videos and the public Cataract101 dataset with 101 cataract surgery videos, outperforming state-of-the-art approaches with 95.8% and 96.5% accuracy, respectively. Conclusions The experimental results show that the use of global and local feature information can effectively enhance the model to explore fine-grained features and mitigate temporal and spatial variations, thus improving the surgical phase recognition performance of the proposed GL-MSTCN.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Biotechnology', 'Biomedical Engineering/Biotechnology']"
doi:10.1007/s11277-022-10087-4,en,Multi-Faceted Clustering with Enhanced Multi-channel Allocation for Optimal Path Selection in Wireless Sensor Networks,OriginalPaper,"In today’s modern era, heterogeneous wireless sensor networks (HWSN) have a wide range of applications, notably in monitoring and detection of events without user intervention. Sensor nodes in close vicinity to base station experience a severe barrier leading to congestion and packet loss in dense network. Achieving high throughput and packet delivery ratio is an important criterion for researchers. Various protocols in HWSN applies multi-channel communication and multi-radio approach. In this paper, we propose a Hybrid diffusion Clustering Scheme for CH selection and Jellyfish routing based on Enhanced Multi-Channel Allocation (EMCA) for efficient path selection. Enhanced multi-channel allocation uses both Time division multiple access (TDMA) and Frequency division multiple access (FDMA) and dual radio compared to existing state- of- art techniques, the proposed approach assures less energy consumption, delay in complex network.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1186/s43074-022-00076-y,en,Multi-focus light-field microscopy for high-speed large-volume imaging,"['OriginalPaper', 'Research']","High-speed visualization of three-dimensional (3D) processes across a large field of view with cellular resolution is essential for understanding living systems. Light-field microscopy (LFM) has emerged as a powerful tool for fast volumetric imaging. However, one inherent limitation of LFM is that the achievable lateral resolution degrades rapidly with the increase of the distance from the focal plane, which hinders the applications in observing thick samples. Here, we propose Spherical-Aberration-assisted scanning LFM (SAsLFM), a hardware-modification-free method that modulates the phase-space point-spread-functions (PSFs) to extend the effective high-resolution range along the z-axis by ~ 3 times. By transferring the foci to different depths, we take full advantage of the redundant light-field data to preserve finer details over an extended depth range and reduce artifacts near the original focal plane. Experiments on a USAF-resolution chart and zebrafish vasculatures were conducted to verify the effectiveness of the method. We further investigated the capability of SAsLFM in dynamic samples by imaging large-scale calcium transients in the mouse brain, tracking freely-moving jellyfish, and recording the development of Drosophila embryos. In addition, combined with deep-learning approaches, we accelerated the three-dimensional reconstruction of SAsLFM by three orders of magnitude. Our method is compatible with various phase-space imaging techniques without increasing system complexity and can facilitate high-speed large-scale volumetric imaging in thick samples.","['Engineering', 'Microwaves, RF and Optical Engineering']"
doi:10.1007/s13272-022-00628-9,en,A framework for the bi-level optimization of a generic transport aircraft fuselage using aeroelastic loads,"['OriginalPaper', 'Original Paper']","The aeroelastic loads and design processes at the German Aerospace Center, Institute of Aeroelasticity in the framework of multi-disciplinary optimization are constantly evolving. New developments have been made in the in-house model generation tool ModGen, which allow us to create detailed fuselage models for preliminary design. As a part of the subsequent developments to integrate the fuselage structure in our aeroelastic design process, a new framework for optimizing the fuselage structure has been developed. The process is based on a bi-level optimization approach which follows a global–local optimization methodology to simplify a large optimization problem. A sub-structuring procedure is used to define stiffened panels as independent structures for local optimization. The panels are sized with stress and buckling constraints with consideration of several aeroelastic load cases. Furthermore, in this paper, we present a physical sub-structure grouping process which enables reduced number of panel optimizations and saves considerable computational effort with little compromise in the solution accuracy.","['Engineering', 'Aerospace Technology and Astronautics']"
doi:10.1186/s41747-022-00311-y,en,Generation of synthetic ground glass nodules using generative adversarial networks (GANs),"['OriginalPaper', 'Original article']","Background Data shortage is a common challenge in developing computer-aided diagnosis systems. We developed a generative adversarial network (GAN) model to generate synthetic lung lesions mimicking ground glass nodules (GGNs). Methods We used 216 computed tomography images with 340 GGNs from the Lung Image Database Consortium and Image Database Resource Initiative database. A GAN model retrieving information from the whole image and the GGN region was built. The generated samples were evaluated with visual Turing test performed by four experienced radiologists or pulmonologists. Radiomic features were compared between real and synthetic nodules. Performances were evaluated by area under the curve (AUC) at receiver operating characteristic analysis. In addition, we trained a classification model (ResNet) to investigate whether the synthetic GGNs can improve the performances algorithm and how performances changed as a function of labelled data used in training. Results Of 51 synthetic GGNs, 19 (37%) were classified as real by clinicians. Of 93 radiomic features, 58 (62.4%) showed no significant difference between synthetic and real GGNs ( p  ≥ 0.052). The discrimination performances of physicians (AUC 0.68) and radiomics (AUC 0.66) were similar, with no-significantly different ( p  = 0.23), but clinicians achieved a better accuracy (AUC 0.74) than radiomics (AUC 0.62) ( p  < 0.001). The classification model trained on datasets with synthetic data performed better than models without the addition of synthetic data. Conclusions GAN has promising potential for generating GGNs. Through similar AUC, clinicians achieved better ability to diagnose whether the data is synthetic than radiomics.","['Medicine & Public Health', 'Imaging / Radiology', 'Diagnostic Radiology', 'Interventional Radiology', 'Neuroradiology', 'Ultrasound', 'Internal Medicine']"
doi:10.1007/s40747-022-00916-1,en,Scene text recognition via context modeling for low-quality image in logistics industry,"['OriginalPaper', 'Original Article']","Text recognition has been applied in many fields recently, such as robot vision, video retrieval, and scene understanding. However, minimal research has been conducted in the field of logistics wherein images of express sheets captured by cameras are mostly curved, distorted, and have low resolution. In this study, a new method is proposed to address the aforementioned research gap while simultaneously considering irregular and low-resolution English letters. The entire approach comprises a rectification module, a convolutional neural network (CNN) extractor, a semantic context module (SCM), a global context module (GCM), and a lightweight transformer decoder that can exhibit improved training speed. In particular, we propose the idea of context modeling in our proposed method. (1) The proposed SCM is introduced to capture full-image dependencies and generates rich semantic context information. (2) We propose the GCM, which not only enhances long-range dependencies from the output of SCM but also outputs abundant pixel information to the self-attention decoder. (3) To solve the low-resolution text recognition problem in a large number of express sheet scenes, we propose Chinese datasets for improving intelligent logistics. Experiments conducted on six public benchmarks demonstrate that the developed method achieves better robustness to low-resolution and irregular text images.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10115-022-01799-x,en,Identifying significant textual features in titles of Google play store applications and their influence on user review rating,"['OriginalPaper', 'Regular Paper']","User review rating of mobile applications is a crucial factor related to downloads and it greatly impacts the customer’s decisions to prefer the applications with the highest (most positive) ratings. Whereas, titles are among the first information displayed to users when they search for any particular application and a compelling title can be a leading cause for an application’s success. Hence, developer companies fashion (optimize) their application titles strategically, in such a way, that they are highly eye-catching and descriptive about application functionalities in an attempt to lure users to download and positively rate their applications. However, traditional literature may lack the scientific approaches which investigate what (specific) kind of textual features in application titles actually have a positive (or negative) effect on the review rating. Therefore, aim of this research work is to perform two separate kinds of scientific analyses to determine the impacts of unconscious (aspects usually not observed by users) and conscious (keyterms which are observed by users) features of Google-play store application titles on the user review rating. At first, for the investigation of unconscious aspects various machine learning algorithms are employed and secondly, for the conscious features another keyterms analysis is carried out. Overall, according to the results, certain unconscious aspects can lead towards the elevated review ratings in both cases of Applications and Games. Albeit, conscious aspects tend to have a positive impact only on the review ratings of Games.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s13202-022-01586-y,en,"Discrete fracture modeling by integrating image logs, seismic attributes, and production data: a case study from Ilam and Sarvak Formations, Danan Oilfield, southwest of Iran","['OriginalPaper', 'Original Paper - Production Geology ']","Understanding the fracture patterns of hydrocarbon reservoirs is vital in the Zagros area of southwest of Iran as they are strongly affected by the collision of the Arabian and Iranian plates. It is essential to evaluate both primary and secondary (fracture) porosity and permeability to understand the fluid dynamics of the reservoirs. In this study, we adopted an integrated workflow to assess the influence of various fracture sets on the heterogeneous carbonate reservoir rocks of the Cenomanian–Santonian Bangestan group, including Ilam and upper Sarvak Formations. For this purpose, a combination of field data was used including seismic data, core data, open-hole well-logs, petrophysical interpretations, and reservoir dynamic data. FMI interpretation revealed that a substantial amount of secondary porosity exists in the Ilam and Sarvak Formations. The upper interval of Sarvak 1-2 (3491 m to 3510 m), Sarvak 1-3 (3530 m to 3550 m), and the base of Sarvak 2-1 are the most fractured intervals in the formation. The dominant stress regime in the study area is a combination of compressional and strike-slip system featuring reverse faults with a NW–SE orientation. From the depositional setting point of view, mid-ramp and inner-ramp show a higher concentration of fractures compared to open marine environment. Fracture permeability was modeled iteratively to establish a realistic match with production log data. The results indicate that secondary permeability has a significant influence on the productivity of wells in the study area.","['Earth Sciences', 'Geology', 'Industrial and Production Engineering', 'Energy Systems', 'Offshore Engineering', 'Industrial Chemistry/Chemical Engineering', 'Monitoring/Environmental Analysis']"
doi:10.1007/s10462-022-10308-z,en,An IoT based smart menstrual cup using optimized adaptive CNN model for effective menstrual hygiene management,OriginalPaper,"Poor feminine hygiene leads to various infections such as hepatitis B, fungal infections, reproductive tract infections, urinary infections, etc. The main concern associated with the menstrual cup is the problem of disposal and hygiene management which is still a major concern in the whole world. To overcome this problem, this paper proposed a novel technique integrating the Internet of Things (IoT) and machine learning methodology. The chaos game (CG) optimized adaptive convolutional neural network (ACNN) architecture is used to detect the menstrual flow when it exceeds a specific mark (17 ml) in the menstrual cup. When the menstrual cup is prone to leakage the participant is alerted via a text message. The efficiency of the proposed methodology is evaluated by using the voltage data obtained from the different IoT devices of participants. The efficiency of the CG optimized ACNN architecture is evaluated using different performance metrics such as accuracy, Root means Square Error, sensitivity, specificity, and recall. The proposed methodology is both capable of identifying the abnormal menstrual flow and alerting the user for disposal. Our experimental findings illustrate the efficiency of this methodology in improving women’s menstrual hygiene in real time by analyzing the flow.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10586-022-03817-7,en,Edge resource slicing approaches for latency optimization in AI-edge orchestration,OriginalPaper,"Edge service computing is an emerging paradigm for computing, storage, and communication services to optimize edge framework latency and cost based on mobile edge computing (MEC) devices. The devices are battery-enabled and have limited communication and computation resources. X consolidation is a major issue in distributed heterogeneous MEC orchestrations, where X represents the task scheduling/device selection/channel selection/offloading strategy . The network entities need to enhance network performance under uncertain circumstances for such orchestrations. Haphazard X consolidation leads to abnormal resource and energy usage, quality of service (QoS) and latency of the edge framework. However, this study concentrates on analysing the impact of reinforcement learning-based edge resource consolidation models. The models are classified according to functionality, including device resource management, service request allocation, device selection, and offloading types. Finally, the article discusses and highlights some unresolved challenges for further study on MEC orchestration to enhance offloading strategy and resource management, as well as device and channel selection efficiency.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s11227-022-04977-4,en,Real-time traffic light violations using distributed streaming,OriginalPaper,"Vehicles controlled by intelligent technologies, whose goal is to reduce human error and ease congestion, do not solely rely on human resources. Cities worldwide use camera systems to monitor the traffic, which collects the images and processes them through different computer vision algorithms. It is challenging for traffic monitoring systems to maintain their accuracy during the day and night lighting conditions, camera location relative to objects, video quality, traffic light position relative to the crossing line, and object angle from the surveillance camera. In this paper, we propose an improved traffic light violation detection method that concurrently streams videos through Apache Kafka and processes them with Apache Spark. It continues to operate for long periods without human intervention and adjusts automatically to changes in the environment. The violation detection algorithm utilizes a modified YOLOv5 and Hough space to efficiently capture the violation. YOLOv5 is a lightweight, fast, and efficient algorithm for real-time object detection. The improved YOLOv5 retrieves the object coordinates relative to the traffic lights, and Hough space analysis is employed to determine the violation region during the red traffic light. Hough space considers the object’s location and angle relative to the traffic lights. The model performs well in various situations of the input video datasets, as validated by performance metrics. The outcomes of extensive experiments show that the approach is well suited for deployment in real-time traffic violation detections. The outcomes are compared to a number of performance measures for object identification and traffic violations. In terms of traffic light violations, the model had 88.24% accuracy. The model is scalable enough, and it can deal effectively with real-world traffic video data at large scales.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1038/s41598-022-24774-6,en,"Improving deep learning performance for predicting large-scale geological 
              
                
              
              $${{CO}_{2}}$$
              
                
                  
                    CO
                  
                  2
                
              
             sequestration modeling through feature coarsening","['OriginalPaper', 'Article']","Physics-based reservoir simulation for fluid flow in porous media is a numerical simulation method to predict the temporal-spatial patterns of state variables (e.g. pressure p ) in porous media, and usually requires prohibitively high computational expense due to its non-linearity and the large number of degrees of freedom (DoF). This work describes a deep learning (DL) workflow to predict the pressure evolution as fluid flows in large-scale 3-dimensional(3D) heterogeneous porous media. In particular, we develop an efficient feature coarsening technique to extract the most representative information and perform the training and prediction of DL at the coarse scale, and further recover the resolution at the fine scale by spatial interpolation. We validate the DL approach to predict pressure field against physics-based simulation data for a field-scale 3D geologic $$CO_2$$ C O 2 sequestration reservoir model. We evaluate the impact of feature coarsening on DL performance, and observe that the feature coarsening not only decreases the training time by $$>74\%$$ > 74 % and reduces the memory consumption by $$>75\%$$ > 75 % , but also maintains temporal error $$0.63\%$$ 0.63 % on average. Besides, the DL workflow provides predictive efficiency with 1406 times speedup compared to physics-based numerical simulation. The key findings from this research significantly improve the training and prediction efficiency of deep learning model to deal with large-scale heterogeneous reservoir models, and thus it can also be further applied to accelerate workflows of history matching and reservoir optimization for close-loop reservoir management.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s12667-022-00547-y,en,Hierarchical control strategy of electric vehicles with demand response in retail electricity markets,"['OriginalPaper', 'Original Paper']","Uncontrolled charging of numerous electric vehicles (EV) can cause problems on distribution transformer and feeder voltage. Demand response (DR) can reduce the loading by incentivizing EV owners to change their charging schedules. DR is composed of two mechanisms, price-based and incentive-based. For the price-based DR, EV owners manage their charging according to price signals to save their charging costs. However, the accumulated loads of the distribution network may unexpectedly increase. This paper develops a combination approach of price-based and incentive-based demand response in a hierarchical manner to alleviate the distribution network problems from passenger EV charging in retail electricity markets. On the bottom level, the EV owners optimize their charging schedules according to the day-ahead real-time pricing (DA-RTP). In case that the accumulated demand is larger than the regulated value, the top-level centralized control will be initiated with incentive direct load control (DLC). To manage on dynamic characteristics of the distribution loads, heuristic algorithm is applied to reduce the run times. The developed approach is simulated with two scenarios regarding structures of retail electricity markets. Both scenarios consist of agents that interact with residential customers in providing the electrical energy and balancing supplies and loads in the distribution networks. The simulation results show that the approach can relieve the problems. Uncertainties due to traffic condition, charging and driving behavior are included in the simulations. Financial aspects of stakeholders are also analyzed.","['Energy', 'Energy Policy, Economics and Management', 'Operations Research/Decision Theory', 'Optimization', 'Energy Systems']"
doi:10.1007/s00500-022-07678-5,en,Transformer fast gradient method with relative positional embedding: a mutual translation model between English and Chinese,"['OriginalPaper', 'Focus']","Machine translation uses computers to transform one natural language into another. Text-like neural machine translation tasks cannot fully identify the sequence order of texts or the long-term dependence between words, and they suffer from excessive translation and mistranslation. To improve the naturalness, fluency, and accuracy of translation, this study proposes a new training strategy, the transformer fast gradient method with relative positional embedding (TF-RPE), which includes the fast gradient method (FGM) of adversarial training and relative positional embedding. The input sequence is founded on the transformer model, and after the word embedding matrix converts a word vector in the word embedding layer, the positional encoding can be embedded in it through relative positional embedding, helping the word vector to better save the linguistic information of the word (meaning, semantics). The addition of FGM adversarial training to the multi-head attention encoder mechanism strengthens the training of word vectors and reduces the phenomenon of miss-or-error translation, enabling significant improvement of the overall computational efficiency and accuracy of the model. TF-RPE can also provide satisfactory high-quality translations for the low-resource corpus. Extensive ablation studies and comparative analyses validate the effectiveness of the scheme, and TF-RPE achieves an improvement of average 3+ Bilingual evaluation understudy scores compared with the SOTA methods.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10278-022-00734-4,en,"3D CT-Inclusive Deep-Learning Model to Predict Mortality, ICU Admittance, and Intubation in COVID-19 Patients",OriginalPaper,"Abstract Chest CT is a useful initial exam in patients with coronavirus disease 2019 (COVID-19) for assessing lung damage. AI-powered predictive models could be useful to better allocate resources in the midst of the pandemic. Our aim was to build a deep-learning (DL) model for COVID-19 outcome prediction inclusive of 3D chest CT images acquired at hospital admission. This retrospective multicentric study included 1051 patients (mean age 69, SD = 15) who presented to the emergency department of three different institutions between 20th March 2020 and 20th January 2021 with COVID-19 confirmed by real-time reverse transcriptase polymerase chain reaction (RT-PCR). Chest CT at hospital admission were evaluated by a 3D residual neural network algorithm. Training, internal validation, and external validation groups included 608, 153, and 290 patients, respectively. Images, clinical, and laboratory data were fed into different customizations of a dense neural network to choose the best performing architecture for the prediction of mortality, intubation, and intensive care unit (ICU) admission. The AI model tested on CT and clinical features displayed accuracy, sensitivity, specificity, and ROC-AUC, respectively, of 91.7%, 90.5%, 92.4%, and 95% for the prediction of patient’s mortality; 91.3%, 91.5%, 89.8%, and 95% for intubation; and 89.6%, 90.2%, 86.5%, and 94% for ICU admission (internal validation) in the testing cohort. The performance was lower in the validation cohort for mortality (71.7%, 55.6%, 74.8%, 72%), intubation (72.6%, 74.7%, 45.7%, 64%), and ICU admission (74.7%, 77%, 46%, 70%) prediction. The addition of the available laboratory data led to an increase in sensitivity for patient’s mortality (66%) and specificity for intubation and ICU admission (50%, 52%, respectively), while the other metrics maintained similar performance results. We present a deep-learning model to predict mortality, ICU admittance, and intubation in COVID-19 patients. Key Points • 3D CT-based deep learning model predicted the internal validation set with high accuracy, sensibility and specificity (> 90%) mortality, ICU admittance, and intubation in COVID-19 patients. • The model slightly increased prediction results when laboratory data were added to the analysis, despite data imbalance. However, the model accuracy dropped when CT images were not considered in the analysis, implying an important role of CT in predicting outcomes.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1186/s12915-022-01453-6,en,Synthetic Micrographs of Bacteria (SyMBac) allows accurate segmentation of bacterial cells using deep neural networks,"['OriginalPaper', 'Methodology article']","Background Deep-learning–based image segmentation models are required for accurate processing of high-throughput timelapse imaging data of bacterial cells. However, the performance of any such model strictly depends on the quality and quantity of training data, which is difficult to generate for bacterial cell images. Here, we present a novel method of bacterial image segmentation using machine learning models trained with Synthetic Micrographs of Bacteria (SyMBac). Results We have developed SyMBac, a tool that allows for rapid, automatic creation of arbitrary amounts of training data, combining detailed models of cell growth, physical interactions, and microscope optics to create synthetic images which closely resemble real micrographs, and is capable of training accurate image segmentation models. The major advantages of our approach are as follows: (1) synthetic training data can be generated virtually instantly and on demand; (2) these synthetic images are accompanied by perfect ground truth positions of cells, meaning no data curation is required; (3) different biological conditions, imaging platforms, and imaging modalities can be rapidly simulated, meaning any change in one’s experimental setup no longer requires the laborious process of manually generating new training data for each change. Deep-learning models trained with SyMBac data are capable of analysing data from various imaging platforms and are robust to drastic changes in cell size and morphology. Our benchmarking results demonstrate that models trained on SyMBac data generate more accurate cell identifications and precise cell masks than those trained on human-annotated data, because the model learns the true position of the cell irrespective of imaging artefacts. We illustrate the approach by analysing the growth and size regulation of bacterial cells during entry and exit from dormancy, which revealed novel insights about the physiological dynamics of cells under various growth conditions. Conclusions The SyMBac approach will help to adapt and improve the performance of deep-learning–based image segmentation models for accurate processing of high-throughput timelapse image data.","['Life Sciences', 'Life Sciences, general']"
doi:10.1007/s00500-022-07655-y,en,Improving diversity and quality of adversarial examples in adversarial transformation network,"['OriginalPaper', 'Data analytics and machine learning']","This paper proposes PatternAttack to mitigate two major issues of Adversarial Transformation Network (ATN) including the low diversity and the low quality of adversarial examples. In order to deal with the first issue, this research proposes a stacked convolutional autoencoder based on patterns to generalize ATN. This proposed autoencoder could support different patterns such as all-pixel pattern , object boundary pattern , and class model map pattern . In order to deal with the second issue, this paper presents an algorithm to improve the quality of adversarial examples in terms of $$L_0$$ L 0 -norm and $$L_2$$ L 2 -norm. This algorithm employs adversarial pixel ranking heuristics such as JSMA and COI to prioritize adversarial pixels. To demonstrate the advantages of the proposed method, comprehensive experiments have been conducted on the MNIST dataset and the CIFAR-10 dataset. For the first issue, the proposed autoencoder generates diverse adversarial examples. For the second issue, the proposed algorithm significantly improves the quality of adversarial examples. In terms of $$L_0$$ L 0 -norm, the proposed algorithm decreases from hundreds of adversarial pixels to one adversarial pixel. In terms of $$L_2$$ L 2 -norm, the proposed algorithm reduces the average distance considerably. These results show that the proposed method can generate high-quality and diverse adversarial examples in practice.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s40747-022-00910-7,en,Variable surrogate model-based particle swarm optimization for high-dimensional expensive problems,"['OriginalPaper', 'Original Article']","Many industrial applications require time-consuming and resource-intensive evaluations of suitable solutions within very limited time frames. Therefore, many surrogate-assisted evaluation algorithms (SAEAs) have been widely used to optimize expensive problems. However, due to the curse of dimensionality and its implications, scaling SAEAs to high-dimensional expensive problems is still challenging. This paper proposes a variable surrogate model-based particle swarm optimization (called VSMPSO) to meet this challenge and extends it to solve 200-dimensional problems. Specifically, a single surrogate model constructed by simple random sampling is taken to explore different promising areas in different iterations. Moreover, a variable model management strategy is used to better utilize the current global model and accelerate the convergence rate of the optimizer. In addition, the strategy can be applied to any SAEA irrespective of the surrogate model used. To control the trade-off between optimization results and optimization time consumption of SAEAs, we consider fitness value and running time as a bi-objective problem. Applying the proposed approach to a benchmark test suite of dimensions ranging from 30 to 200 and comparisons with four state-of-the-art algorithms show that the proposed VSMPSO achieves high-quality solutions and computational efficiency for high-dimensional problems.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s11042-022-14229-5,en,Improved exponential cuckoo search method for sentiment analysis,OriginalPaper,"Sentiment analysis is a type of contextual text mining that determines how people feel about emotional issues that are frequently discussed on social media. The sentiments of emotive data are analyzed using a variety of sentiment analysis approaches, including lexicon-based, machine learning-based, and hybrid methods. Unsupervised approaches, particularly clustering methods are preferred over other methods since they can be applied directly to unlabeled datasets. Therefore, a clustering method based on an improved exponential cuckoo search has been proposed in this study for sentiment analysis. The proposed clustering method finds the optimal cluster centers from emotive datasets, which are then utilized to determine the sentiment polarity of emotive contents. The proposed improved exponential cuckoo search is first tested on standard and CEC-2013 benchmark functions before being utilized to determine the best cluster centroids from sentimental datasets. To assess the efficiency of the proposed method, it has been compared with K-means, cuckoo search, grey wolf optimizer, grey wolf optimizer with simulated annealing, hybrid step size-based cuckoo search, and spiral cuckoo search on nine sentimental datasets. The Experimental results and statistical analysis have proven the efficacy of the proposed method.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s42835-022-01306-w,en,Hierarchical Optimal Reactive Power Dispatch for Active Distribution Network with Multi-microgrids,"['OriginalPaper', 'Original Article']","The interconnection of active distribution network and multi-microgrids leads to the increase of variable dimension of optimal reactive power dispatch. The overall reactive power dispatch will face the problems of high dimension, slow convergence, and reduced accuracy. Meanwhile, the decomposition dispatch requires a large number of coordination iterations. Hence, this paper proposes a hierarchical optimal reactive power dispatch method for active distribution network with multi-microgrids based on differential evolution algorithm and the network loss sensitivity. In this method, the upper layer takes the minimum loss of the distribution network as the goal, regards each microgrid as a generator node, and formulates the interactive power between each microgrid and the distribution network. The lower layer aims to minimize the network loss of the microgrid, regards the point of common coupling as the balance node, and make the power of the lower layer meet the constraints of the upper layer in the form of penalty function. To minimize the network loss of the whole network, the sensitivity is calculated to correct the output of reactive power equipment for further optimization. The improved IEEE-33 bus distribution network system is used to verify the method, and the results are compared with the global optimization results. Due to the reduction of the network scale, the number of iterations of each system of hierarchical optimization is 1000, which is greatly reduced compared with 10,000 of the overall optimization. The results show that the proposed method improves the economy of active distribution network system with multi-microgrids and significantly shortens the running time.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/s41965-022-00111-8,en,Membrane computing with harmony search algorithm for gene selection from expression and methylation data,"['OriginalPaper', 'Regular Paper']","Selecting disease-causing genes from gene expression and methylation data with hundreds of thousands of loci is of great benefit for cancer diagnosis and treatment, but it also faces tremendous technical challenges due to its small sample size and ultrahigh-dimensional genetic markers. To enhance the search speed, this paper proposes a new gene selection algorithm, called the Membrane Computing with Harmony Search Algorithm (MC-HSA), based on the theory of membrane computing to quickly select a subset of potential disease-causing genes. In the MC-HSA, an active membrane dissolving P system is designed to obtain a trade-off between global exploration and local exploitation ability for detecting gene combinations that have a strong association with disease status. The harmony search algorithm is embedded in the P system to comprehensively detect gene subsets in both gene expression and DNA methylation data. An enhanced classifier consisting of four general classifiers is employed to improve classification accuracy (CA) and avoid overfitting, while a penalty function is developed to screen out redundant genes. Experiments on six real datasets indicate that our method is very competitive compared with ten excellent optimization algorithms (HybridGA, QSFS, RMA, WOA-CM, ME-BPSO, CDNC, ABCD, HAMS, mRMR, and ImRMR). Taking the gene expression and DNA methylation data of prostate cancer as an example, the experimental results show that our method finds a smaller number of genes with high CA (> 99%) than four state-of-the-art algorithms and maintains stable performance. Finally, we specifically analyzed the representative genes and comprehensively validated them in terms of Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways and Gene Ontologies (GO).","['Computer Science', 'Theory of Computation', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Optimization', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11042-022-14165-4,en,Detection of Diabetic Retinopathy using Convolutional Neural Networks for Feature Extraction and Classification (DRFEC),"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Diabetic Retinopathy (DR) is caused as a result of Diabetes Mellitus which causes development of various retinal abrasions in the human retina. These lesions cause hindrance in vision and in severe cases, DR can lead to blindness. DR is observed amongst 80% of patients who have been diagnosed from prolonged diabetes for a period of 10–15 years. The manual process of periodic DR diagnosis and detection for necessary treatment, is time consuming and unreliable due to unavailability of resources and expert opinion. Therefore, computerized diagnostic systems which use Deep Learning (DL) Convolutional Neural Network (CNN) architectures, are proposed to learn DR patterns from fundus images and identify the severity of the disease. This paper proposes a comprehensive model using 26 state-of-the-art DL networks to assess and evaluate their performance, and which contribute for deep feature extraction and image classification of DR fundus images. In the proposed model, ResNet50 has shown highest overfitting in comparison to Inception V3, which has shown lowest overfitting when trained using the Kaggle’s EyePACS fundus image dataset. EfficientNetB4 is the most optimal, efficient and reliable DL algorithm in detection of DR, followed by InceptionResNetV2, NasNetLarge and DenseNet169. EfficientNetB4 has achieved a training accuracy of 99.37% and the highest validation accuracy of 79.11%. DenseNet201 has achieved the highest training accuracy of 99.58% and a validation accuracy of 76.80% which is less than the top-4 best performing models.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10614-022-10344-5,en,Uncertainty Optimization Based Feature Selection Model for Stock Marketing,OriginalPaper,"Market analyzers use different parameters as features in the market data to analyze the market trends. The feature’s values act as a signal to market fluctuations. Many studies have examined these features to predict market movement more effectively. However, the method to minimize the uncertainties associated with the features is not available in the literature. This exploratory study introduces the uncertainty optimization based feature selection method for stock marketing. We introduce a notion of certainty region of the feature as the set of feature values, which signify particular happening with certainty. We use rough set theory to find the feature’s certainty region and uncertainty region and measure each feature’s significance. The feature whose certainty region is the maximum is the most significant in the feature space. Hence we group the features by minimizing the uncertainty region of the most informative features to get feature subsets for feature selection. We propose an algorithm based on uncertainty optimization to find subsets of the feature set for effectiveness and performance enhancement in the feature selection. We obtain the decision rules with comprehensive coverage and excellent support using the selected features. The accuracy of classification using the chosen parameters is up to 85.91%, which is higher than 79.54% of the complete feature set. The study provides an uncertainty optimization model for more efficient market movement prediction.","['Economics', 'Economic Theory/Quantitative Economics/Mathematical Methods', 'Computer Appl. in Social and Behavioral Sciences', 'Operations Research/Decision Theory', 'Behavioral/Experimental Economics', 'Math Applications in Computer Science']"
doi:10.1007/s00521-022-08034-2,en,An approximate randomization-based neural network with dedicated digital architecture for energy-constrained devices,"['OriginalPaper', 'Original Article']","Variable energy constraints affect the implementations of neural networks on battery-operated embedded systems. This paper describes a learning algorithm for randomization-based neural networks with hard-limit activation functions. The approach adopts a novel cost function that balances accuracy and network complexity during training. From an energy-specific perspective, the new learning strategy allows to adjust, dynamically and in real time, the number of operations during the network’s forward phase. The proposed learning scheme leads to efficient predictors supported by digital architectures. The resulting digital architecture can switch to approximate computing at run time, in compliance with the available energy budget. Experiments on 10 real-world prediction testbeds confirmed the effectiveness of the learning scheme. Additional tests on limited-resource devices supported the implementation efficiency of the overall design approach.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s42417-022-00793-5,en,Fault Diagnosis of Wind Turbine Bearings Based on CNN and SSA–ELM,"['ReviewPaper', 'Review']","Purpose As a critical component of the wind turbine drive train, the bearings are easy to fail under the complex environment of variable working conditions and loads in long-term operation. So it is essential to carry out a study targeting at fault diagnosis on it to improve the safety and reliability of the whole wind turbine operating. Methods This paper presents a kind of bearing fault diagnosis method for wind turbines based on convolutional neural network (CNN) and sparrow search algorithm (SSA) optimized extreme learning machine (ELM). First, the wavelet time-frequency diagram (WTD) is constructed by using the continuous wavelet transform (CWT) to the original vibrational signal of the wind turbine bearing. Then, the WTD is input into deep learning CNN for extracting features. Finally, the SSA-ELM classifier is constructed by searching the optimal parameters of ELM with SSA, and the extracted features are put into SSA-ELM to identify different fault types. Results The proposed CWT-CNN-SSA- ELM method is experimentally validated by two bearing datasets and compared with other methods. The result shows that the method has better diagnosis capability. Conclusion In this paper, a wind turbine bearing fault diagnosis method based on CNN and SSA-ELM is proposed. The approach is able to well extract fault features and classify and identify the bearing data under variable working conditions and time-varying speed with good generalization ability.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Engineering Acoustics', 'Acoustics']"
doi:10.1186/s12911-022-02052-9,en,Medication adherence prediction through temporal modelling in cardiovascular disease management,"['OriginalPaper', 'Research']","Background Chronic conditions place a considerable burden on modern healthcare systems. Within New Zealand and worldwide cardiovascular disease (CVD) affects a significant proportion of the population and it is the leading cause of death. Like other chronic diseases, the course of cardiovascular disease is usually prolonged and its management necessarily long-term. Despite being highly effective in reducing CVD risk, non-adherence to long-term medication continues to be a longstanding challenge in healthcare delivery. The study investigates the benefits of integrating patient history and assesses the contribution of explicitly temporal models to medication adherence prediction in the context of lipid-lowering therapy. Methods Data from a CVD risk assessment tool is linked to routinely collected national and regional data sets including pharmaceutical dispensing, hospitalisation, lab test results and deaths. The study extracts a sub-cohort from 564,180 patients who had primary CVD risk assessment for analysis. Based on community pharmaceutical dispensing record, proportion of days covered (PDC) $$\ge$$ ≥  80 is used as the threshold for adherence. Two years (8 quarters) of patient history before their CVD risk assessment is used as the observation window to predict patient adherence in the subsequent 5 years (20 quarters). The predictive performance of temporal deep learning models long short-term memory (LSTM) and simple recurrent neural networks (Simple RNN) are compared against non-temporal models multilayer perceptron (MLP), ridge classifier (RC) and logistic regression (LR). Further, the study investigates the effect of lengthening the observation window on the task of adherence prediction. Results Temporal models that use sequential data outperform non-temporal models, with LSTM producing the best predictive performance achieving a ROC AUC of 0.805. A performance gap is observed between models that can discover non-linear interactions between predictor variables and their linear counter parts, with neural network (NN) based models significantly outperforming linear models. Additionally, the predictive advantage of temporal models become more pronounced when the length of the observation window is increased. Conclusion The findings of the study provide evidence that using deep temporal models to integrate patient history in adherence prediction is advantageous. In particular, the RNN architecture LSTM significantly outperforms all other model comparators.","['Medicine & Public Health', 'Health Informatics', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1038/s41534-022-00652-x,en,Equivalence checking of quantum circuits by nonlocality,"['OriginalPaper', 'Article']","Suppose two quantum circuit chips are located at different places, for which we do not have any prior knowledge, and cannot see the internal structures either. In such a situation, a realistic and fundamental problem is to find out whether they have the same functions or not with certainty. In this paper, we show that this problem can be solved completely from the viewpoint of quantum nonlocality. Specifically, we design an elegant protocol that examines underlying quantum nonlocality, where the strongest nonlocality can be observed if and only if two quantum circuits are equivalent to each other. We show that the protocol also works approximately, where the distance between two quantum circuits can be calculated accurately by observing quantum nonlocality in an analytical manner. Furthermore, it turns out that the computational cost of our protocol is independent of the size of compared quantum circuits. Lastly, we also discuss the possibility to generalize the protocol to multipartite cases, i.e., if we do equivalence checking for multiple quantum circuits, we try to solve the problem in one go.","['Physics', 'Physics, general', 'Quantum Physics', 'Quantum Information Technology, Spintronics', 'Quantum Computing', 'Quantum Field Theories, String Theory', 'Classical and Quantum Gravitation, Relativity Theory']"
doi:10.1007/s00521-022-07793-2,en,Designing self attention-based ResNet architecture for rice leaf disease classification,"['OriginalPaper', 'Original Article']","In India, rice crops are very significant. Rice cultivation comprises several phases, and it is crucial to keep an eye on the crop's development to avoid any leaf diseases and to provide a good yield. To avoid yield loss, crop diseases need to be determined at the initial stage. Deep learning-based pre-trained CNN architecture is used in this study to identify rice leaf diseases. This paper discusses four different CNN architectures to classify and identify healthy and diseased leaves such as Brown spot, Hispa, and Leaf Blast. Initially, to avoid vanishing gradient problems that degrade the performance of the Network, ResNet34 and ResNet50 are used. Even though the CNN model performs the feature extraction, Self-attention with ResNet18 and ResNet34 architecture is utilized to improve the feature selection process. As a result of enhanced feature extraction, the accuracy of rice leaf disease identification and classification has improved. Finally, high accuracy of 98.54% is achieved with the proposed ResNet34 with self-attention architecture when compared to other CNN models used in this paper. In terms of multiclass classification, the proposed model offers improved outcomes when compared to state-of-the-art techniques.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00530-022-01024-3,en,Attribute-aware style adaptation for person re-identification,"['OriginalPaper', 'Special Issue Paper']","Person re-identification (re-ID) aims to address a unique challenge in cross-camera pedestrian retrieval, especially in the case of incomplete attribute annotation. In recent years, a robust algorithm based on a generative model has been proposed that can achieve rapid convergence by extending the training data. However, these pipelines are developed separately from re-ID learning and ignore the fine-grained extension to adapt the camera style. To solve this problem, a joint learning framework is proposed in this work to implement end-to-end optimization and ultimately achieve high-quality images and impressive performance for person re-ID. In this work, an attribute-aware style adaptation based on CamStyle, called AA-CamStyle, is designed to combine fine-grained style adaptation and discriminative person re-ID. The AA-CamStyle model integrates the critical attributes into the generative learning to smooth the differences in camera style while maintaining the fine-grained information through joint representation learning of multiple styles, including attribute-aware and camera-aware. Attribute-aware (AA) strategy is applied to recommend the transmission of appropriate attributes of each pedestrian, resulting in AA-CamStyle’s tremendous quality of translated images compared to existing models. We empirically demonstrate the effectiveness of the proposed approach on person re-ID tasks.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1007/s00521-022-08047-x,en,Sound source localization for auditory perception of a humanoid robot using deep neural networks,"['OriginalPaper', 'Original Article']","This paper presents an estimation of the sound source location using deep neural networks in order to provide auditory perception of a humanoid robot. Estimation of a moving sound source is crucial for a humanoid robot to improve functionality in some environments where the robot’s camera cannot operate. It plays an important role, especially in a recovery scenario with no visual contact. In this study, the data of the sound source around the robot were recorded by four microphones placed on the humanoid robot’s head. A wheeled robot was used to obtain the sound source with odometry. Recorded sound dataset and collected odometry dataset were used as input data and target data, respectively. The discrete wavelet transform (DWT) was applied for pre-processing of the input data. After pre-processing, the obtained matrices were applied as inputs of the proposed convolutional neural network (CNN), long short-term memory (LSTM), bidirectional long-short-term memory (biLSTM), and multilayer perceptron (MLP) networks to estimate the sound source location around the humanoid robot. As a result of all tests for the estimation models created by proposed networks, the $$R^2$$ R 2 metrics of the biLSTM structure were obtained as approximately 0.97. This study showed experimentally that humanoid robots can sense the position of sound source in the environment with sufficient accuracy like many living creatures.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10489-022-04288-4,en,Semi-supervised adversarial discriminative domain adaptation,OriginalPaper,"Domain adaptation is a potential method to train a powerful deep neural network across various datasets. More precisely, domain adaptation methods train the model on training data and test that model on a completely separate dataset. The adversarial-based adaptation method became popular among other domain adaptation methods. Relying on the idea of GAN, the adversarial-based domain adaptation tries to minimize the distribution between the training and testing dataset based on the adversarial learning process. We observe that the semi-supervised learning approach can combine with the adversarial-based method to solve the domain adaptation problem. In this paper, we propose an improved adversarial domain adaptation method called Semi-Supervised Adversarial Discriminative Domain Adaptation (SADDA), which can outperform other prior domain adaptation methods. We also show that SADDA has a wide range of applications and illustrate the promise of our method for image classification and sentiment classification problems.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00500-022-07662-z,en,Blind color image watermarking incorporating a residual network for watermark denoising and super-resolution reconstruction,"['OriginalPaper', 'Mathematical Methods in Data Science']","Watermarking is a technique for hiding secret information in various types of multimedia data to protect intellectual property rights. The integration of deep learning technology with image watermarking is currently reshaping the application and promotion of relevant techniques developed so far. This paper presents a novel type of blind color image watermarking method that embeds a downsized color image into a host color image. Watermarking implementation involves partitioning the host image into non-overlapping blocks of 8 × 8 pixels, performing discrete cosine transform (DCT) for each block of every channel, and then manipulating the magnitudes of three designated DCT coefficients subject to a minimization constraint. The experimental results confirmed that the proposed image watermarking method outperformed six other methods in terms of zero-normalized cross-correlation (ZNCC). Moreover, watermark imperceptibility, as reflected by the measured peak signal-to-noise ratio and mean structural similarity metrics, remained satisfactory. In addition to this new style of color image watermarking, we employed a deep residual network to reduce noise and increase the resolution of the retrieved watermarks. Overall, the residual network achieved a satisfactory ZNCC level (> 0.88) when the watermark images were super-resolved by a factor of sixteen.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10489-022-04324-3,en,Probabilistic autoencoder with multi-scale feature extraction for multivariate time series anomaly detection,OriginalPaper,"Effectively detecting anomalies for multivariate time series is of great importance for the modern industrial system. Recently, reconstruction-based deep learning methods have been widely used in time series anomaly detection. However, the rich local and global characteristics of time series may not be well captured by methods that compress and reconstruct time series through a single-scale neural network. In addition, under the influence of a complex environment, small fluctuations occur during the normal operation of the system, which will also bring challenges for those methods to reconstruct exact values. In this paper, we propose an unsupervised multivariate time series anomaly detection method based on a probabilistic autoencoder with multi-scale feature extraction (PAMFE). The multiple parallel dilated convolutions with different dilation factors and feature fusion module enable PAMFE to capture overall and detailed information of time series to identify various types of anomalies better. Furthermore, considering the normal fluctuation of data, we reconstruct the expected distribution of input and calculate the anomaly score based on the probability that the input belongs to the distribution. Extensive experiments on four publicly real-world datasets demonstrate that PAMFE outperforms state-of-the-art methods in F1-Score. Moreover, we investigate the contributions of the major components of PAMFE, and the experimental results show that they all contribute to performance improvement.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10618-022-00888-3,en,HARPA: hierarchical attention with relation paths for knowledge graph embedding adversarial learning,OriginalPaper,"Knowledge graph embedding (KGE) aims to map the knowledge graph into a low-dimensional continuous vector space and provide a unified underlying representation for downstream tasks. Recently, graph neural network (GNN) has been widely used in knowledge graph embedding because of its powerful feature extraction ability, and most KGE models based on GNN use aggregation operations to extract potential information from the triples. Unfortunately, they only emphasize entity embedding and use shallow operations to update relations. As a result, the learning of relation embedding is relatively simple. And they ignore the rich inference information contained in the multi-hop paths. In addition, their complex network structure lacks regularization constraint, which is prone to the over-fitting problem. Therefore, this paper proposes a novel hierarchical attention with relation paths model for knowledge graph embedding adversarial learning (HARPA). HARPA constructs a two-layer attention encoder to learn the information of triples and neighborhoods at the triples-level and further utilizes the rich inference information of paths to deeply learn relation embedding at the paths-level. Besides, HARPA proposes an improved generative adversarial network (GAN) named I-GAN as the regularization term of the model, which imposes constraints on the process of learning embedding and enables the model to learn high-quality and robust embedding. The link prediction experiments on four general knowledge graphs show that the HARPA model outperforms state-of-the-art methods.","['Computer Science', 'Data Mining and Knowledge Discovery', 'Artificial Intelligence', 'Information Storage and Retrieval', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences']"
doi:10.1007/s10115-022-01801-6,en,Automated urban planning aware spatial hierarchies and human instructions,"['OriginalPaper', 'Original Paper']","Traditional urban planning demands urban experts to spend much time producing an optimal urban plan under many architectural constraints. The remarkable imaginative ability of deep generative learning provides hope for renovating this domain. Existing works are constrained by: (1) neglecting human requirements; (2) omitting spatial hierarchies, and (3) lacking urban plan samples. We propose a novel, deep human-instructed urban planner to fill these gaps and implement two practical frameworks. In the preliminary version, we formulate the task into an encoder–decoder paradigm. The encoder is to learn the information distribution of surrounding contexts, human instructions, and land-use configuration. The decoder is to reconstruct the land-use configuration and the associated urban functional zones. Although it has achieved good results, the generation performance is still unstable due to the complex optimization directions of the decoder. Thus, we propose a cascading deep generative adversarial network (GAN) in this paper, inspired by the workflow of urban experts. The first GAN is to build urban functional zones based on human instructions and surrounding contexts. The second GAN will produce the land-use configuration by considering the built urban functional zones. Finally, we conducted extensive experiments and case studies to validate the effectiveness and superiority of our work.","['Computer Science', 'Information Systems and Communication Service', 'Database Management', 'Data Mining and Knowledge Discovery', 'Information Storage and Retrieval', 'Information Systems Applications (incl.Internet)', 'IT in Business']"
doi:10.1007/s40747-022-00919-y,en,Evolutionary convolutional neural network for image classification based on multi-objective genetic programming with leader–follower mechanism,"['OriginalPaper', 'Original Article']","As a popular research in the field of artificial intelligence in the last 2 years, evolutionary neural architecture search (ENAS) compensates the disadvantage that the construction of convolutional neural network (CNN) relies heavily on the prior knowledge of designers. Since its inception, a great deal of researches have been devoted to improving its associated theories, giving rise to many related algorithms with pretty good results. Considering that there are still some limitations in the existing algorithms, such as the fixed depth or width of the network, the pursuit of accuracy at the expense of computational resources, and the tendency to fall into local optimization. In this article, a multi-objective genetic programming algorithm with a leader–follower evolution mechanism (LF-MOGP) is proposed, where a flexible encoding strategy with variable length and width based on Cartesian genetic programming is designed to represent the topology of CNNs. Furthermore, the leader–follower evolution mechanism is proposed to guide the evolution of the algorithm, with the external archive set composed of non-dominated solutions acting as the leader and an elite population updated followed by the external archive acting as the follower. Which increases the speed of population convergence, guarantees the diversity of individuals, and greatly reduces the computational resources. The proposed LF-MOGP algorithm is evaluated on eight widely used image classification tasks and a real industrial task. Experimental results show that the proposed LF-MOGP is comparative with or even superior to 35 existing algorithms (including some state-of-the-art algorithms) in terms of classification error and number of parameters.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s10489-022-04246-0,en,PhraseTransformer: an incorporation of local context information into sequence-to-sequence semantic parsing,OriginalPaper,"Semantic parsing is a challenging task mapping a natural language utterance to machine-understandable information representation. Recently, approaches using neural machine translation (NMT) have achieved many promising results, especially the Transformer. However, the typical drawback of adapting the vanilla Transformer to semantic parsing is that it does not consider the phrase in expressing the information of sentences while phrases play an important role in constructing the sentence meaning. Therefore, we propose an architecture, PhraseTransformer, that is capable of a more detailed meaning representation by learning the phrase dependencies in the sentence. The main idea is to incorporate Long Short-Term Memory into the Self-Attention mechanism of the original Transformer to capture the local context of a word. Experimental results show that our proposed model performs better than the original Transformer in terms of understanding sentences structure as well as logical representation and raises the model local context-awareness without any support from external tree information. Besides, although the recurrent architecture is integrated, the number of sequential operations of the PhraseTransformer is still O ( 1 ) $\boldsymbol {\mathcal {O}}\mathbf {(1)}$ similar to the original Transformer. Our proposed model achieves strong competitive performance on Geo and MSParS datasets, and leads to SOTA performance on the Atis dataset for methods using neural networks. In addition, to prove the generalization of our proposed model, we also conduct extensive experiments on three translation datasets IWLST14 German-English, IWSLT15 Vietnamese-English, WMT14 English-German, and show significant improvement. Our code is available at https://github.com/phuongnm94/PhraseTransformer.git .","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s13042-022-01716-2,en,Near-infrared fusion for deep lightness enhancement,"['OriginalPaper', 'Original Article']","Lightness enhancement is a long-standing research topic in computer vision. Existing deep learning-based approaches usually extract features from the low-light image to model the enlightening process, which may fall short of robustness since low-light features can be unreliable in heavily dark regions. Inspired by the fact that infrared imaging is immune to illumination variation, we propose to exploit an extra infrared image to help brighten the low-light one. Specifically, we design a deep convolutional neural network to jointly extract the infrared and low-light features and produce a normal-light image under the supervision of multi-scale loss functions, including a discriminator loss that enforces the network output image to mimic a real one. Moreover, a contextual attention module is proposed to reconstruct reliable low-light features in heavily dark regions by exploring feature correlation consistency among low-light and infrared features. Extensive experiments on two composited and one real-world datasets demonstrate the superiority of the proposed approach over existing methods qualitatively and quantitatively.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s10015-022-00833-z,en,Generating collective wall-jumping behavior for a robotic swarm with self-teaching automatic curriculum learning,"['OriginalPaper', 'Original Article']","Swarm robotics (SR) is a research field about how to design a large number of robots so that they can generate meaningful collective behaviors. One of the promising approaches in designing a control policy is reinforcement learning (RL). However, it is well known that the sparse reward problem may arise, especially in cases of solving highly complex problems. Curriculum learning (CL) can be one of the effective approaches to overcoming this difficulty. In this paper, we propose a novel method called Self-Teaching Automatic Curriculum Learning (STACL). The training progress of different lessons is compared by agents to determine which lesson should be trained in the next episode. The collective wall-jumping task, in which the robots have to generate collective wall-jumping behavior to jump over the high wall and reach the goal as soon as possible, is employed to illustrate the effects. Simulation results show that the proposed approach has the fastest convergence speed and the most stable performance. In addition, we also conducted experiments to examine the flexibility of the developed controllers.","['Computer Science', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10639-022-11483-w,en,A typology for digital leadership in higher education: the case of a large-scale mobile technology initiative (using tablets),OriginalPaper,"This study investigated a digital reform initiative, rated excellent by the government, of one higher education institution (HEI) in an Arab State in the Gulf. The focus of the study was to develop a digital typology, while exploring the leadership attributes that characterized the core leadership team, as they accomplished the migration towards a digital culture in one year, within a context where faculty members showed resistance against digitalization. The study was conducted immediately after the implementation of the initiative that took place over the course of one year, just before the Covid-19 pandemic. Semi-structured interviews were conducted with all the six key individuals in the leadership team who led this initiative. Data was analyzed using content-based analysis. Findings of the study were used to synthesize a 5D typology for digital leadership attributes: (1) Digital competence; (2) Digital culture; (3) Digital Differentiation; (4) Digital governance; and (5) Digital advocacy. The paper provides in depth discussion how these attributes supported the adaptive ability of a Higher Education Institution towards accepting digitalization.","['Computer Science', 'Computers and Education', 'Educational Technology', 'User Interfaces and Human Computer Interaction', 'Education, general', 'Information Systems Applications (incl.Internet)', 'Computer Appl. in Social and Behavioral Sciences']"
doi:10.1186/s12911-022-02063-6,en,Electrocardiogram lead conversion from single-lead blindly-segmented signals,"['ReviewPaper', 'BMC Supplements Reviewed']","Background The standard configuration’s set of twelve electrocardiogram (ECG) leads is optimal for the medical diagnosis of diverse cardiac conditions. However, it requires ten electrodes on the patient’s limbs and chest, which is uncomfortable and cumbersome. Interlead conversion methods can reconstruct missing leads and enable more comfortable acquisitions, including in wearable devices, while still allowing for adequate diagnoses. Currently, methodologies for interlead ECG conversion either require multiple reference (input) leads and/or require input signals to be temporally aligned considering the ECG landmarks. Methods Unlike the methods in the literature, this paper studies the possibility of converting ECG signals into all twelve standard configuration leads using signal segments from only one reference lead, without temporal alignment (blindly-segmented). The proposed methodology is based on a deep learning encoder-decoder U-Net architecture, which is compared with adaptations based on convolutional autoencoders and label refinement networks. Moreover, the method is explored for conversion with one single shared encoder or multiple individual encoders for each lead. Results Despite the more challenging settings, the proposed methodology was able to attain state-of-the-art level performance in multiple target leads, and both lead I and lead II seem especially suitable to convert certain sets of leads. In cross-database tests, the methodology offered promising results despite acquisition setup differences. Furthermore, results show that the presence of medical conditions does not have a considerable effect on the method’s performance. Conclusions This study shows the feasibility of converting ECG signals using single-lead blindly-segmented inputs. Although the results are promising, further efforts should be devoted towards the improvement of the methodologies, especially the robustness to diverse acquisition setups, in order to be applicable to cardiac health monitoring in wearable devices and less obtrusive clinical scenarios.","['Medicine & Public Health', 'Health Informatics', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/s10659-022-09954-9,en,A Hint on the Localization of the Buckling Deformation at Vanishing Curvature Points on Thin Elliptic Shells,OriginalPaper,"The general theory of slender structure buckling by Grabovsky and Truskinovsky (Cont. Mech. Thermodyn. 19(3–4):211-243, 2007 ), (later extended in J. Nonlinear Sci. 26(1):83–119, 2016 by Grabovsky and the author), predicts that the critical buckling load of a thin shell under dead loading is closely related to the Korn’s constant (in Korn’s first inequality) of the shell under the Dirichlet boundary conditions resulting from the loading program. It is known that under zero Dirichlet boundary conditions on the thin part of the boundary of positive, negative, and zero (one principal curvature vanishing, and one apart from zero) Gaussian curvature shells, the optimal Korn constant in Korn’s first inequality scales like the thickness to the power of −1, − 4 / 3 $-4/3$ , and − 3 / 2 $-3/2$ respectively. In this work we analyse the scaling of the optimal constant in Korn’s first inequality for elliptic shells that contain a finite number of points where both principal curvatures vanish. We prove that the presence of at least one such point on the shell leads to the scaling drop from the thickness to the power of −1 to the thickness to the power of − 3 / 2 $-3/2$ . To our best knowledge, this is the first result in the direction for constant-sign curvature shells, that do not contain a developable region. In addition, under the assumption that a suitable trivial branch exists, we prove that in fact the buckling deformation of such shells under dead loading, should be localized at the vanishing curvature points, as the shell thickness h $h$ goes to zero.","['Engineering', 'Theoretical and Applied Mechanics', 'Classical and Continuum Physics', 'Mathematical Applications in the Physical Sciences', 'Classical Mechanics', 'Materials Science, general', 'Biomechanics']"
doi:10.1007/s13369-022-07494-x,en,A Voltage PID Controller Synthesis Based on a New Small-Signal Linear Model to Enhance the Performance of the Standard P &O Algorithm Employed in Photovoltaic Panels,"['OriginalPaper', 'Research Article-Electrical Engineering ']","This paper presents a new linear model design including the KC200GT photovoltaic generator GPV panel, DC-DC boost converter, and resistive load. The small-signal principle is indeed applied to the configuration connecting these three devices that are previously developed graphically using the $$Matlab^{\circledR }/Simulink$$ M a t l a b ® / S i m u l i n k software package. This new model design is established through a set of proposed steps, providing the first contribution of this paper. A new voltage proportional-integral-derivative ( PID ) controller is synthesized based on the previous linear small-signal model. The controller parameters are computed by proposing a reference open-loop system and using the frequency identification approach, providing the second contribution of this paper. The resulting PID controller is combined with the classical perturb and observe ( P &O ) algorithm where its fill factor ( FF ) and its reference tracking dynamic are significantly improved. Improving these two performances for a sudden change of both climatic conditions, i.e., absolute temperature and solar irradiance, constitutes another contribution of this paper. Given improving the previous two performances, the maximum power point tracking ( MPPT ) scheme with oscillation free tracking of the desired MPP is established for conventional and improved P &O algorithms using $$Simpower^{\circledR }$$ S i m p o w e r ® system libraries of the $$Matlab^{\circledR }/Simulink$$ M a t l a b ® / S i m u l i n k environment. The given simulation tests confirm the effectiveness of the proposed P &O-MPPT control strategy in terms of reference tracking accuracy, rise and settling times, and solving the ripple issue that occurred in the DC-DC boost converter output current.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s10462-022-10333-y,en,RG-NBEO: a ReliefF guided novel binary equilibrium optimizer with opposition-based S-shaped and V-shaped transfer functions for feature selection,OriginalPaper,"In most data mining tasks, feature selection (FS) is a necessary preprocessing step that can reduce the dimensionality of the dataset while ensuring adequate classification accuracy. In this paper, a ReliefF-guided novel binary equilibrium optimizer (RG-NBEO) is proposed for feature selection. Based on the binary equilibrium optimizer, two novel mechanisms are employed to improve the evolution performance. First, two novel transfer functions (SSr and VVr) based on the concept of opposition learning are proposed to transform the continuous search space into a binary search space and achieve a good balance between exploration and exploitation. Second, a ReliefF bootstrapping strategy is proposed to add and remove features directionally in the iterative process according to the feature weights. The simulation experiments are first based on the equilibrium optimizer (EO) variants constructed from the classical S- and V-shaped transfer functions. The variant EO with the best performance is selected and compared with five superior swarm intelligence optimization algorithms and six classical filter feature selection algorithms. The performance of the proposed method was tested on 18 standard datasets, and the results of the different algorithms were statistically evaluated using the Wilcoxon rank sum test and the Freidman rank sum test. The results show that this method can effectively improve the classification accuracy in most cases.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s42235-022-00297-8,en,Boosting Whale Optimizer with Quasi-Oppositional Learning and Gaussian Barebone for Feature Selection and COVID-19 Image Segmentation,"['OriginalPaper', 'Research Article']","Whale optimization algorithm (WOA) tends to fall into the local optimum and fails to converge quickly in solving complex problems. To address the shortcomings, an improved WOA (QGBWOA) is proposed in this work. First, quasi-opposition-based learning is introduced to enhance the ability of WOA to search for optimal solutions. Second, a Gaussian barebone mechanism is embedded to promote diversity and expand the scope of the solution space in WOA. To verify the advantages of QGBWOA, comparison experiments between QGBWOA and its comparison peers were carried out on CEC 2014 with dimensions 10, 30, 50, and 100 and on CEC 2020 test with dimension 30. Furthermore, the performance results were tested using Wilcoxon signed-rank (WS), Friedman test, and post hoc statistical tests for statistical analysis. Convergence accuracy and speed are remarkably improved, as shown by experimental results. Finally, feature selection and multi-threshold image segmentation applications are demonstrated to validate the ability of QGBWOA to solve complex real-world problems. QGBWOA proves its superiority over compared algorithms in feature selection and multi-threshold image segmentation by performing several evaluation metrics.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1038/s41598-022-24840-z,en,Optimization of complex engineering problems using modified sine cosine algorithm,"['OriginalPaper', 'Article']","In this article, a modified version of the Sine Cosine algorithm (MSCA) is proposed to solve the optimization problem. Based on the Sine Cosine algorithm (SCA), the position update formula of SCA is redefined to increase the convergence speed, then the Levy random walk mutation strategy is adopted to improve the population diversity. In order to verify the performance of MSCA, 24 well-known classical benchmark problems and IEEE CEC2017 test suites were introduced, and by comparing MSCA with several popular methods, it is demonstrated that MSCA has good convergence and robustness. Finally, MSCA is used to address six complex engineering design problems, demonstrating the engineering utility of the algorithm.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s42235-022-00292-z,en,Identification of Pulmonary Hypertension Animal Models Using a New Evolutionary Machine Learning Framework Based on Blood Routine Indicators,"['OriginalPaper', 'Research Article']","Pulmonary Hypertension (PH) is a global health problem that affects about 1% of the global population. Animal models of PH play a vital role in unraveling the pathophysiological mechanisms of the disease. The present study proposes a Kernel Extreme Learning Machine (KELM) model based on an improved Whale Optimization Algorithm (WOA) for predicting PH mouse models. The experimental results showed that the selected blood indicators, including Haemoglobin (HGB), Hematocrit (HCT), Mean, Platelet Volume (MPV), Platelet distribution width (PDW), and Platelet–Large Cell Ratio (P-LCR), were essential for identifying PH mouse models using the feature selection method proposed in this paper. Remarkably, the method achieved 100.0% accuracy and 100.0% specificity in classification, demonstrating that our method has great potential to be used for evaluating and identifying mouse PH models.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s11071-022-08083-1,en,Study on multi-clearance nonlinear dynamic characteristics of herringbone gear transmission system under optimal 3d modification,"['OriginalPaper', 'Original Paper']","The tooth surface 3d modification technology is the most effective method to realize the reduction of vibration and noise of gear system (GS) by comprehensively considering tooth profile modification and axial modification. Moreover, the system nonlinear factors such as backlash and bearing clearance will make the motion of GS have complex variability. Therefore, we will combine 3d modification technology with contact performance and dynamic characteristics to optimize the meshing characteristics of herringbone gear pair through tooth contact analysis technology, loaded tooth contact analysis technology and antlion optimizer with error minimum amplitude as optimization objective, determine three nonlinear dynamic model of herringbone gear for numerical solution. Finally, this paper studies the influence of different system parameters on the system nonlinear dynamic characteristics under 3d modification and non-modification from both local vibration characteristics on the diagrams of time history, phase, spectrum and Poincare map and global vibration characteristics on the diagrams of system bifurcation and maximum Lyapunov exponent. The results show: After optimizing modification, the system vibration is obviously reduced. With the increase of input power, input speed, backlash and static transmission error (STE), the system has jump, while there is no jump with the change of bearing clearance and damping ratio. Compared with other parameters, the changes of input speed and STE make system have complex bifurcation characteristics. The 3d modification can eliminate the jump and make system motion more regular.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s00521-022-08053-z,en,Multiclass skin lesion classification in dermoscopic images using swin transformer model,"['OriginalPaper', 'Original Article']","Automatic skin lesion classification in dermoscopic images is a very challenging task due to the huge intraclass variation, the high degree of interclass visual similarity, low contrast between skin lesion and surrounding normal skin, and the existence of extraneous and intrinsic artifacts. However, existing algorithms for skin lesion classification are developed by leveraging convolutional neural networks (CNNs), and the effectiveness of these algorithms is mostly validated for binary classification of skin lesions. In addition, the relatively low diagnostic sensitivity achieved by these studies demonstrates the uncertainty involved in skin lesion classification. In order to overcome these difficulties, a swin transformer model for multiclass skin lesion classification is proposed by taking advantage of both transformer and CNNs that are based on end-to-end mapping and do not require prior knowledge. Furthermore, the problem of class imbalance is addressed through a weighted cross entropy loss. Moreover, key components of the proposed approach are explored in detail in order to ensure efficient and effective learning process with multiclass data in the skin lesion classification. The proposed method is extensively evaluated on International Skin Imaging Collaboration (ISIC) 2019 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset and achieves a sensitivity, specificity, accuracy, and balanced accuracy value of $$82.3\%$$ 82.3 % , $$97.9\%$$ 97.9 % , $$97.2\%$$ 97.2 % , and $$82.3\%$$ 82.3 % , respectively. Experimental results demonstrate that the proposed method has the highest balanced accuracy value and outperforms most of the other state-of-the-art methods in multiclass skin lesion classification.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1038/s41598-022-24461-6,en,Septal myocardial scar burden predicts the response to cardiac contractility modulation in patients with heart failure,"['OriginalPaper', 'Article']","We hypothesized that myocardial septal scarring, assessed by cardiac magnetic resonance (CMR) using late gadolinium enhancement (LGE), at the site of cardiac contractility modulation (CCM) lead placement may predict treatment response. Eligible heart failure (HF) patients underwent LGE CMR imaging before CCM device implantation. The response to CCM therapy at follow-up was determined by a change in NYHA class and echocardiographic left ventricular ejection fraction (LVEF) assessment. Patients were classified as responders, if they showed an improvement in either NYHA class or improvement of LVEF by ≥ 5%. 58 patients were included. 67% of patients were classified as responders according to improved NYHA; 55% according to LVEF improvement. 74% of patients were responders if either NYHA class or LVEF improvement was observed. 90% of responders (according to NYHA class) showed septal LGE < 25% at septal position of the leads, while 44% of non-responders showed septal LGE > 25% (p < 0.01). In patients treated with CCM, an improvement of NYHA class was observed when leads were placed at myocardial segments with a CMR- LGE burden less than 25%.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40998-022-00572-8,en,A Hybrid Approach for Text Summarization Using Social Mimic Optimization Algorithm,"['OriginalPaper', 'Research Paper']","Every day, millions of Internet users share a lot of information on the web. In this digital era, the exponential growth of data on the web causes difficulties in getting the needed information quickly. Text summarization plays a crucial role in getting the needed information quickly. This work introduces a new extractive single-document summarization technique using the hybrid social mimic optimization algorithm. The objective function of the proposed work maximizes the summary sentences informative score and the sentence coherence factor. In this study, we used the three popular benchmark datasets (DUC2002, BBC News, and CNN) for the experimental work. In this study, we used ROUGE score as a performance evaluation measure and compared with five state-of-the-art single-document summarization techniques. The performance comparison analysis shows that the proposed summarization technique outperforms the other competitor approaches.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s12517-022-10982-x,en,Comparative study of homogeneous ensemble methods with conventional ML classifiers in litho-facies detection using real-time drilling data,"['OriginalPaper', 'Original Paper']","The drilling operation is known to be influenced by the formation’s lithology. Real-time prediction of formation parameters is essential to steer the well and make proper completion decisions—litho-facies aid in quantifying formation qualities, allowing drilling parameters to be optimized. Traditional machine learning (ML) algorithms such as logistic regression (LR) and support vector machine (SVM) are compared to new homogeneous ensemble algorithms such as random forest (RF), adaptive boosting (AdaBoost), and XGBoost for predicting the litho-facies of any formation within the research area in real time. Field data from four wells with over 31,000 data points were used to identify litho-facies in real time. The developed model was trained with fourteen different drilling parameters as independent variables and litho-facies as the dependent variable from the Eagleford shale region of the USA. Drilling parameters such as rotation per minute, rate of penetration, differential pressure, surface torque, gamma-ray correlation, and others are used in the model. The suggested study employs a fivefold cross-validation strategy for developing the models. The F1 and accuracy scores were used to assess the model’s efficiency. When the various machine learning algorithms were examined, it was clear that XGBoost outperformed all other algorithms, with an accuracy of nearly 90%. The proposed approach is unique in the industry since it can predict complicated lithology in real time for vertical/inclined/horizontal wellbores without considering survey parameters.","['Earth Sciences', 'Earth Sciences, general']"
doi:10.1007/s10489-022-04333-2,en,Multiscale laplacian learning,OriginalPaper,"Machine learning has greatly influenced a variety of fields, including science. However, despite tremendous accomplishments of machine learning, one of the key limitations of most existing machine learning approaches is their reliance on large labeled sets, and thus, data with limited labeled samples remains an important challenge. Moreover, the performance of machine learning methods is often severely hindered in case of diverse data, which is usually associated with smaller data sets or data associated with areas of study where the size of the data sets is constrained by high experimental cost and/or ethics. These challenges call for innovative strategies for dealing with these types of data. In this work, the aforementioned challenges are addressed by integrating graph-based frameworks, semi-supervised techniques, multiscale structures, and modified and adapted optimization procedures. This results in two innovative multiscale Laplacian learning (MLL) approaches for machine learning tasks, such as data classification, and for tackling data with limited samples, diverse data, and small data sets. The first approach, multikernel manifold learning (MML), integrates manifold learning with multikernel information and incorporates a warped kernel regularizer using multiscale graph Laplacians. The second approach, the multiscale MBO (MMBO) method, introduces multiscale Laplacians to the modification of the famous classical Merriman-Bence-Osher (MBO) scheme, and makes use of fast solvers. We demonstrate the performance of our algorithms experimentally on a variety of benchmark data sets, and compare them favorably to the state-of-art approaches.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-14095-1,en,Skin lesion classification on dermatoscopic images using effective data augmentation and pre-trained deep learning approach,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Skin cancer is a severe disease that is common and causes death if left untreated. When skin cancer is detected early through dermatoscopic imaging, the possibility of definitive treatment is very high. Although melanoma is one of the fatal types of skin cancer, early detection dramatically increases the chances of survival. There is a low morbidity rate and limited actual data to study this deadly disease. This is a significant handicap in the application of machine learning techniques. Accurate diagnosis is essential because of the similarity of some types of lesions. The accuracy of the diagnosis is related to the professional experience of the specialist. The development of rapid and successful computerized diagnostic systems for the diagnosis and classification of skin cancer has become increasingly important. Deep learning-based applications are especially new trend in the detection of diseases from medical images. In this study, an effective data augmentation and a pre-trained deep learning approach are proposed for skin lesion classification. A hybrid network model called the Inception-Resnet-v2 is proposed to classify skin cancer images. The main aim of this study is to increase the number of images in the dataset by applying the affine transformation technique (data augmentation) and analyzing its effect on the skin cancer classification system. The highest reported accuracy in this study with an augmented dataset is 95.09% for the Inception-Resnet-v2 model while the same model achieved 83.59% with the original dataset.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00170-022-10404-y,en,Intelligent recognition of tool wear in milling based on a single sensor signal,"['OriginalPaper', 'ORIGINAL ARTICLE']","A major problem in the high-speed cutting process of machine tools is tool wear. Tool wear directly affects the surface quality and machining accuracy of the workpiece. However, the limits of fusing multiple sensing signals to indirectly monitor tool wear are rarely concerned in real manufacturing environments. In this paper, a tool wear identification method based on a single sensor signal is proposed. To solve the limits of less obtained information and poor anti-interference ability of single sensor, multi-domain feature fusion strategy is established. By establishing a hybrid model of deep convolutional neural network and stacked long short-term memory network, the complex mapping relationship between fusion features and tool wear is constructed. Specifically, the spatial features of the input data set are extracted by the convolution kernel of the deep convolutional neural network. Then, a stacked double-layer long short-term memory neural network is established to capture sequence features with long-term dependence, thereby identifying tool wear. Finally, the superiority of the developed method is verified by tool wear experiments. The results show that the method can be effectively applied to tool wear identification from single sensor signals, and the mean RMSE and MAE of the identification results are 9.43 and 7.15, respectively. Compared with four other traditional multiple regression methods, RMSE and MAE are reduced by 73.0% and 78.7% on average. This study provides a reference value for the industrial implementation of tool wear monitoring system.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s10489-022-04263-z,en,Seformer: a long sequence time-series forecasting model based on binary position encoding and information transfer regularization,OriginalPaper,"Long sequence time-series forecasting (LSTF) problems, such as weather forecasting, stock market forecasting, and power resource management, are widespread in the real world. The LSTF problem requires a model with high prediction accuracy. Recent studies have shown that the transformer model architecture is the most promising model structure for LSTF problems compared with other model architectures. The transformer model has the property of permutation equivalence, which leads to the importance of sequence position encoding, an essential process in model training. Currently, the continuous dynamics models constructed for position encoding using the neural differential equations (neural ODEs) method can model sequence position information well. However, we have found that there are some limitations when neural ODEs are applied to the LSTF problem, including the time cost problem, the baseline drift problem, and the information loss problem; thus, neural ODEs cannot be directly applied to the LSTF problem. To address this problem, we design a binary position encoding-based regularization model for long sequence time-series prediction, named Seformer, which has the following structure: 1) The binary position encoding mechanism, including intrablock and interblock position encoding. For intrablock position encoding, we design a simple ODE method by discretizing the continuum dynamics model, which reduces the time cost required to compute neural ODEs while maintaining their dynamics properties to the maximum extent. In interblock position encoding, a chunked recursive form is adopted to alleviate the baseline drift problem caused by eigenvalue explosion. 2) Information transfer regularization mechanism: By regularizing the model intermediate hidden variables as well as the encoder-decoder connection variables, we can reduce information loss during the model training process while ensuring the smoothness of the position information. Extensive experimental results obtained on six large-scale datasets show a consistent improvement in our approach over the baselines.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s40808-022-01609-x,en,Deep learning model for temperature prediction: an empirical study,"['OriginalPaper', 'Original Article']","Planning the daily routines of human life depends heavily on the weather. Knowing the weather ahead of time substantially aids in better planning for aviation, agriculture, tourism, and other operations, which avoids financial loss and casualties. People in today's technological era heavily rely on weather forecasts. Researchers and computer scientists are paying special attention to machine learning (ML) techniques in an effort to develop and adopt a different approach to the conventional way of weather prediction. Predicting the weather is difficult owing to the non-linear link between input data and output conditions. Multivariate polynomial regression (MPR) and Deep neural networks (DNN)-based models are an alternative of costly and complex traditional systems. To predict maximum temperature, deep neural network-based weather forecasting models are quite simple and can be designed with less effort and cost in comparison to a traditional forecasting system. This research work objective is to investigate and predict New Delhi’s temperature in 6-h intervals for the upcoming year using the time series dataset, using input features which include date and time, temperature, atmospheric pressure, humidity, dew point, and conditions like fog, heavy fog, drizzle, etc. In this study, ML models (MPR and DNN) are designed and implemented for temperature prediction. To evaluate the efficiency of the predictions, a comparison of the predicted temperature and the actual recorded temperature is done, and the performance and accuracy of the models are examined. The DNN model (DNNM-3) outperformed the other models with an accuracy rate of 96.4%.","['Earth Sciences', 'Earth System Sciences', 'Math. Appl. in Environmental Science', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Applications in the Physical Sciences', 'Ecosystems', 'Environment, general']"
doi:10.1007/s11356-022-24321-w,en,Forecasting of solar radiation for a cleaner environment using robust machine learning techniques,"['OriginalPaper', 'Research Article']","Intensified research is going on worldwide to increase renewable energy sources like solar and wind to reduce emissions and achieve worldwide targets and also to address the depleting fossil fuels resources and meet the increasing energy demand of the population. Solar radiation (SR) is intermittent, so forecasting solar radiation is a must. The objective of this research is to use modern machine techniques for different climatic conditions to forecast SR with higher accuracy. The required dataset is collected from National Solar Radiation Database having features such as temperature, pressure, relative humidity, dew point, solar zenith angle, wind speed, and direction, concerning the y -parameter Global Horizontal Irradiance (GHI) (W/m 2 ). The collected data is first split based on different types of climatic conditions. Each climatic model was trained on various machine learning (ML) algorithms like multiple linear regression (MLR), support vector regression (SVR), decision tree regression (DTR), random forest regression (RFR), gradient boosting regression (GBR), lasso and ridge regression, and deep learning algorithm especially long-short-term memory (LSTM) using Google Colab Platform. From the analysis, LSTM has the least error approximation of 0.0040 loss at the 100 th epoch and of all ML models, gradient boosting and RFR top high, when it comes to the Hot weather season—gradient boosting leads 2% than RFR, and similarly for cold weather, autumn and monsoon climate—RFR has 1% higher accuracy than gradient boosting. This high-accuracy model is deployed in a user interface (UI) that will be more useful for real-time solar prediction, load operators for maintenance scheduling, stock commitment, and load dispatch centres for engineers to decide on setting up solar panels, for household clients and future researchers.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s40031-022-00801-y,en,A Self-Adaptive Differential Evolution Using a New Adaption Based Operator for Software Cost Estimation,"['OriginalPaper', 'Original Contribution']","Today, predicting software parameters accurately during the initial software development stage is one of the biggest challenges facing most companies. In this article, it was discussed how meta-heuristic algorithms are used to solve multiple optimization problems that arise in mathematical and software models. The proposed method for solving optimization problems employs new adaptive mutation operators by incorporating a new syndrome adaptive mutation operator, which provides more diversity among candidate solutions. Further, by comparing the proposed mutation operator method with standard meta-heuristic algorithms, these were able to select better mutation results for 24 benchmark functions. Furthermore, the proposed method is useful for solving software engineering issues, including estimating software costs, which accurately predicts software parameters by optimizing the effort and errors for the constructive cost model. In comparison with other standard optimization algorithms, the proposed algorithm has a better ability to predict costs.","['Engineering', 'Communications Engineering, Networks']"
doi:10.1007/s11517-022-02696-9,en,TwinEDA: a sustainable deep-learning approach for limb-position estimation in preterm infants’ depth images,"['OriginalPaper', 'Original Article']","Early diagnosis of neurodevelopmental impairments in preterm infants is currently based on the visual analysis of newborns’ motion patterns by trained operators. To help automatize this time-consuming and qualitative procedure, we propose a sustainable deep-learning algorithm for accurate limb-pose estimation from depth images. The algorithm consists of a convolutional neural network (TwinEDA) relying on architectural blocks that require limited computation while ensuring high performance in prediction. To ascertain its low computational costs and assess its application in on-the-edge computing, TwinEDA was additionally deployed on a cost-effective single-board computer. The network was validated on a dataset of 27,000 depth video frames collected during the actual clinical practice from 27 preterm infants. When compared to the main state-of-the-art competitor, TwinEDA is twice as fast to predict a single depth frame and four times as light in terms of memory, while performing similarly in terms of Dice similarity coefficient (0.88). This result suggests that the pursuit of efficiency does not imply the detriment of performance. This work is among the first to propose an automatic and sustainable limb-position estimation approach for preterm infants. This represents a significant step towards the development of broadly accessible clinical monitoring applications. Graphical abstract ","['Biomedicine', 'Human Physiology', 'Biomedical Engineering and Bioengineering', 'Imaging / Radiology', 'Computer Applications']"
doi:10.1007/s41315-022-00265-9,en,Grasp space exploration method for an underactuated gripper using human initiated primitive grasps,"['OriginalPaper', 'Regular Paper']","Grasp planning and most specifically the grasp space exploration when considering adaptive and underactuated multifingered grippers is still an open issue in robotics. This article describes an efficient procedure for exploring the grasp space of such grippers that aims at generating reliable grasps given a known object pose. This article also assesses its performances, and compares it to more commonly used grasp space exploration methods. This method relies on a limited dataset of human specified expert grasps, and uses variational autoencoders to learn grasp intrinsic features together with an analytic grasp quality metric in a compact way from a computational point of view. It is evaluated both in simulation and on a real setup for the specific and complex case of adaptive and underactuated multifingered grasping. Using the proposed grasp planner, it reaches a grasp success rate of 99.54% on 7000 simulated trials, and successfully plans stable and reliable grasps on the real setup, with no failed grasp reported on around 30 trials. It also shows a significantly higher grasp success rate than other grasps space exploration methods.","['Computer Science', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'User Interfaces and Human Computer Interaction', 'Manufacturing, Machines, Tools, Processes', 'Electronics and Microelectronics, Instrumentation']"
doi:10.1007/s10489-022-04301-w,en,Making attention mechanisms more robust and interpretable with virtual adversarial training,OriginalPaper,"Although attention mechanisms have become fundamental components of deep learning models, they are vulnerable to perturbations, which may degrade the prediction performance and model interpretability. Adversarial training (AT) for attention mechanisms has successfully reduced such drawbacks by considering adversarial perturbations. However, this technique requires label information, and thus, its use is limited to supervised settings. In this study, we explore the concept of incorporating virtual AT (VAT) into the attention mechanisms, by which adversarial perturbations can be computed even from unlabeled data. To realize this approach, we propose two general training techniques, namely VAT for attention mechanisms (Attention VAT) and “interpretable” VAT for attention mechanisms (Attention iVAT), which extend AT for attention mechanisms to a semi-supervised setting. In particular, Attention iVAT focuses on the differences in attention; thus, it can efficiently learn clearer attention and improve model interpretability, even with unlabeled data. Empirical experiments based on six public datasets revealed that our techniques provide better prediction performance than conventional AT-based as well as VAT-based techniques, and stronger agreement with evidence that is provided by humans in detecting important words in sentences. Moreover, our proposal offers these advantages without needing to add the careful selection of unlabeled data. That is, even if the model using our VAT-based technique is trained on unlabeled data from a source other than the target task, both the prediction performance and model interpretability can be improved.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00339-022-06212-6,en,On the strong coupling of polarization and charge trapping in HfO2/Si-based ferroelectric field-effect transistors: overview of device operation and reliability,"['OriginalPaper', 'S.I. : 50th Anniversary of Applied Physics']","Ferroelectric field-effect transistors (FeFETs) have become an attractive technology for memory and emerging applications on a silicon electronic platform after the discovery of the ferroelectric phase in silicon-friendly hafnium oxide insulators. In this tutorial, we review one nonideal physical phenomenon that determines the device operation of practical FeFETs based on ferroelectric hafnium oxide (FE-HfO 2 ) insulators and silicon channels: polarization-induced electron trapping. The ferroelectric polarization in FE-HfO 2 induces an enormous amount of trapped electron density of an order of 10 14  cm −2 near the interface between the FE-HfO 2 and interfacial layer, which in turn screens the electric flux from polarization. We examine how electron trapping affects the device operation particularly the polarization switching mechanism, retention characteristics, endurance characteristics, and read-after-write delay. The asymmetric behavior of electron and hole trapping in FeFETs and its impact on the device operation are also discussed. We review several approaches based on different operations, device structure modification, and material engineering to mitigate anomalous electron trapping and improve the device characteristics of FeFETs.","['Physics', 'Condensed Matter Physics', 'Optical and Electronic Materials', 'Nanotechnology', 'Characterization and Evaluation of Materials', 'Surfaces and Interfaces, Thin Films', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11042-022-14268-y,en,Detection and classification of breast cancer availing deep canid optimization based deep CNN,OriginalPaper,"Breast cancer is one of the substantial diseases that affect millions of females each year also the velocity of affected individuals is rising every year. Timely recognition of the illness is the only feasible solution to reduce its influence of the disease. Numerous techniques are invented by researchers in support of the determination of breast cancer and the usage of histopathology descriptions provided the auspicious solution. As an enhancement, in this research, a Deer-Canid based deep CNN is implemented by means of the histopathology images used for the detection of breast cancer through the taxonomy of benign, malignant, and normal regions. The segmentation of the histopathology images is performed using the V-net architecture that segments the image without losing its originality. The primary involvement of the research relies on the Deer-Canid optimization that helps in attaining the global best solution and effectively minimizes the time taken for the classification. The superiority of the research is proved by measuring the values of accuracy, precision, recall, and f1 measure, and the proposed Deer Canid optimization-based deep CNN attained the values of 92.967%, 94.342%, 93.454%, 92.896%, which is more efficient.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00466-022-02252-0,en,A physics-informed neural network technique based on a modified loss function for computational 2D and 3D solid mechanics,"['OriginalPaper', 'Original Paper']","Despite its rapid development, Physics-Informed Neural Network (PINN)-based computational solid mechanics is still in its infancy. In PINN, the loss function plays a critical role that significantly influences the performance of the predictions. In this paper, by using the Least Squares Weighted Residual (LSWR) method, we proposed a modified loss function, namely the LSWR loss function, which is tailored to a dimensionless form with only one manually determined parameter. Based on the LSWR loss function, an advanced PINN technique is developed for computational 2D and 3D solid mechanics. The performance of the proposed PINN technique with the LSWR loss function is tested through 2D and 3D (geometrically nonlinear) problems. Thoroughly studies and comparisons are conducted between the two existing loss functions, the energy-based loss function and the collocation loss function, and the proposed LSWR loss function. Through numerical experiments, we show that the PINN based on the LSWR loss function is effective, robust, and accurate for predicting both the displacement and stress fields. The source codes for the numerical examples in this work are available at https://github.com/JinshuaiBai/LSWR_loss_function_PINN/ .","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Science and Engineering', 'Classical and Continuum Physics']"
doi:10.1007/s00158-022-03460-1,en,Multi-material topology optimization using Wachspress interpolations for designing a 3-phase electrical machine stator,"['OriginalPaper', 'Research Paper']","This work uses multi-material topology optimization (MMTO) to maximize the average torque of a 3-phase permanent magnet synchronous machine (PMSM). Eight materials are considered in the stator: air, soft magnetic steel, three electric phases, and their three returns. To address the challenge of designing a 3-phase PMSM stator, a generalized density-based framework is used. The proposed methodology places the prescribed material candidates on the vertices of a convex polytope, interpolates material properties using Wachspress shape functions, and defines Cartesian coordinates inside polytopes as design variables. A rational function is used as penalization to ensure convergence towards meaningful structures, without the use of a filtering process. The influences of different polytopes and penalization parameters are investigated. The results indicate that a hexagonal-based diamond polytope is a better choice than the classical orthogonal domains for this MMTO problem. In addition, the proposed methodology yields high-performance designs for 3-phase PMSM stators by implementing a continuation method on the electric load angle.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1007/s00158-022-03425-4,en,A comprehensive review of digital twin — part 1: modeling and twinning enabling technologies,"['ReviewPaper', 'Review Paper']","As an emerging technology in the era of Industry 4.0, digital twin is gaining unprecedented attention because of its promise to further optimize process design, quality control, health monitoring, decision and policy making, and more, by comprehensively modeling the physical world as a group of interconnected digital models. In a two-part series of papers, we examine the fundamental role of different modeling techniques, twinning enabling technologies, and uncertainty quantification and optimization methods commonly used in digital twins. This first paper presents a thorough literature review of digital twin trends across many disciplines currently pursuing this area of research. Then, digital twin modeling and twinning enabling technologies are further analyzed by classifying them into two main categories: physical-to-virtual, and virtual-to-physical, based on the direction in which data flows. Finally, this paper provides perspectives on the trajectory of digital twin technology over the next decade, and introduces a few emerging areas of research which will likely be of great use in future digital twin research. In part two of this review, the role of uncertainty quantification and optimization are discussed, a battery digital twin is demonstrated, and more perspectives on the future of digital twin are shared. Code and preprocessed data for generating all the results and figures presented in the battery digital twin case study in part 2 of this review are available on Github .","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1186/s12968-022-00899-5,en,Cardiovascular magnetic resonance images with susceptibility artifacts: artificial intelligence with spatial-attention for ventricular volumes and mass assessment,"['OriginalPaper', 'Research']","Background Segmentation of cardiovascular magnetic resonance (CMR) images is an essential step for evaluating dimensional and functional ventricular parameters as ejection fraction (EF) but may be limited by artifacts, which represent the major challenge to automatically derive clinical information. The aim of this study is to investigate the accuracy of a deep learning (DL) approach for automatic segmentation of cardiac structures from CMR images characterized by magnetic susceptibility artifact in patient with cardiac implanted electronic devices (CIED). Methods In this retrospective study, 230 patients (100 with CIED) who underwent clinically indicated CMR were used to developed and test a DL model. A novel convolutional neural network was proposed to extract the left ventricle (LV) and right (RV) ventricle endocardium and LV epicardium. In order to perform a successful segmentation, it is important the network learns to identify salient image regions even during local magnetic field inhomogeneities. The proposed network takes advantage from a spatial attention module to selectively process the most relevant information and focus on the structures of interest. To improve segmentation, especially for images with artifacts, multiple loss functions were minimized in unison. Segmentation results were assessed against manual tracings and commercial CMR analysis software cvi 42 (Circle Cardiovascular Imaging, Calgary, Alberta, Canada). An external dataset of 56 patients with CIED was used to assess model generalizability. Results In the internal datasets, on image with artifacts, the median Dice coefficients for end-diastolic LV cavity, LV myocardium and RV cavity, were 0.93, 0.77 and 0.87 and 0.91, 0.82, and 0.83 in end-systole, respectively. The proposed method reached higher segmentation accuracy than commercial software, with performance comparable to expert inter-observer variability (bias ± 95%LoA): LVEF 1 ± 8% vs 3 ± 9%, RVEF − 2 ± 15% vs 3 ± 21%. In the external cohort, EF well correlated with manual tracing (intraclass correlation coefficient: LVEF 0.98, RVEF 0.93). The automatic approach was significant faster than manual segmentation in providing cardiac parameters (approximately 1.5 s vs 450 s). Conclusions Experimental results show that the proposed method reached promising performance in cardiac segmentation from CMR images with susceptibility artifacts and alleviates time consuming expert physician contour segmentation.","['Medicine & Public Health', 'Imaging / Radiology', 'Angiology', 'Cardiology']"
doi:10.1007/s11042-022-14227-7,en,"Arrhythmia detection using TQWT, CEEMD and deep CNN-LSTM neural networks with ECG signals","['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Cardiac arrhythmia is a typically clinical manifestation of cardiovascular disease which leads to serious health problem. Detection of arrhythmia is traditionally relying on manual interpretation of electrocardiography (ECG) signals by cardiologists, which is time consuming and subjective. Therefore, development of an automated arrhythmia detection system with high accuracy becomes urgent in clinical applications. In the present study, we propose an effective deep learning model with one-lead ECG signals to automatically detect different types of arrhythmias based upon tunable Q-factor wavelet transform (TQWT) and complete ensemble empirical mode decomposition (CEEMD). First, TQWT decomposes the ECG signal into different frequency bands by using the input parameters (Q, R, and J) without any segmentation, which are used to extract the main subband with majority of the ECG signal’s energy. Second, CEEMD is used to decompose the main subband of ECG signals into different intrinsic modes. It captures most part of the main subband’s information, preserving important waveform features as a slightly asymmetry. It is employed to measure the variability of ECG signals. There is no need for the preprocessing of QRS detection. Then, they are selected as features and fed to combined neural networks consisting of one-dimensional (1D) convolutional neural networks (CNN) and long short-term memory (LSTM) networks for multi-class classification of cardiac arrhythmias. Finally, experiments are carried out on the well-known and publicly available MIT-BIH arrhythmia database to evaluate the performance of the proposed method, in which 744 ECG signal fragments for one lead (MLII) of seventeen classes of heart beats from 29 persons were extracted. By using 10-fold cross-validation style, the achieved average classification accuracy is reported to be 97.20 % , 96.85 % , 96.48 % and 96.13 % , respectively, for five-class, thirteen-class, fifteen-class and seventeen-class classification. Compared with other state-of-the-art methods, the results demonstrate superior performance and the proposed method has the potential to serve as a candidate for the automatic detection of myocardial dysfunction in the clinical ECG examination.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11356-022-24395-6,en,Comparative evaluation of optimal Weibull parameters for wind power predictions using numerical and metaheuristic optimization methods for different Indian terrains,"['OriginalPaper', 'Research Article']","The accurate selection of the wind speed distributions is crucial for a better utilisation of wind energy. The Weibull distribution is most commonly used distribution and hence its parameters need to be optimized. In this study five numerical methods, namely, maximum likelihood method (MLM), graphical method (GM), empirical method of Justus (EMJ), modified maximum likelihood method (MMLM) and wind atlas analysis and application program (WAsP) and three metaheuristic optimization algorithms, namely, social spider optimization (SSO), particle swarm optimization (PSO) and genetic algorithm (GA) are applied for estimating Weibull distribution parameters at three different locations (onshore—Kayathar, nearshore—Jafrabad and offshore—Gulf of Khambhat (GOK) in India and also comparison of numerical and optimization methods are employed to tune the optimal parameters. The accuracy of the methods was evaluated using three different statistical analysis techniques. As per the results, GOK has the maximum wind power density of 450.2 W/m 2 compared to Jafrabad and Kayathar. It was observed that among the five methods used for Weibull parameters estimation, WAsP method presented a better curve fit with the histogram of the wind speed. The results shows that SSO and PSO presents a comparably better performance than GA in the term of accuracy on the basis of closeness to converged solution.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s11042-022-14245-5,en,Image inpainting based on cross-hierarchy global and local aware network,OriginalPaper,"Recent image inpainting approaches based on deep convolutional neural networks have achieved promising results. However, existing methods fail to consider the global and local consistency of the pixels to be recovered. We designed a cross-hierarchy global and local aware network (CGLANet) to solve these problems. This model refines the high-level feature maps from deeper layers with global pixel attention (GPA), then fuses hierarchical feature maps with proposed local-consistent attention (LCA). The GPA is an image-level block that aims to improve the structural consistency between the restored images and those around the uncorrupted images. The LCA is a patch-level block that guarantees the consistency of local textural details by modeling the semantic relevance between the pixels within the local patches of images. Experiments on benchmark datasets demonstrated the superiority of our method over state-of-the-art image inpainting models.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11067-022-09581-w,en,The Impact of Seat Resource Fragmentation on Railway Network Revenue Management,OriginalPaper,"The previous literature of railway revenue management (RM) ignores the negative impact of the problem of fragmented seat resources (PFSR) on passenger transport income. A single train is characterized by continuous transport of multiple segments. Under the condition that a given seat number is assigned to each random arriving customer during the pre-sale period, the remaining seat resource of each rail leg of the train may be distributed on different seats in a fragmented way. When a customer wants to purchase a long-distance transport product, because the remaining seat resource of each rail leg may be not in the same seat, the train cannot provide a service to the customer. This will result in lost customer demand and wasted seat resources. This paper mainly studies the impact of PFSR on railway RM, and a new seat control method is proposed to avoid the revenue loss caused by PFSR. Based on the case study of a real high speed railway (HSR) network, PFSR causes an average revenue loss of 3.95% for passenger transport. The influence of the number of train segments, the size of customer demand and passenger refund rate on PFSR is studied.","['Economics', 'Regional/Spatial Science', 'Civil Engineering', 'Operations Research/Decision Theory']"
doi:10.1007/s00784-022-04801-6,en,Assessment of YOLOv3 for caries detection in bitewing radiographs based on the ICCMS™ radiographic scoring system,"['OriginalPaper', 'Research']","Objectives To assess the feasibility of the YOLOv3 model under the intersection over union (IoU) thresholds of 0.5 (IoU 50 ) and 0.75 (IoU 75 ) for caries detection in bitewing radiographs based on the International Caries Classification and Management System (ICCMS™). Materials and methods We trained the YOLOv3 model by feeding 994 annotated radiographs with the IoU 50 and IoU 75 thresholds. The testing procedure ( n  = 175) was subsequently conducted to evaluate the model’s prediction metrics on caries classification based on the ICCMS™ radiographic scoring system. Results Regarding the 4-class classification representing caries severity, YOLOv3 could accurately detect and classify enamel caries and initial dentin caries (class RA) (IoU 50 vs IoU 75 : precision, 0.75 vs 0.71; recall, 0.67 vs 0.64). Concerning the 7-class classification signifying specific caries depth (class 0, healthy tooth; classes RA1-3, initial caries affecting outer half, inner half of enamel, and the outer 1/3 of dentin; class RB4, caries extending to the middle 1/3 of dentin; classes RC5-6, extensively cavitated caries affecting the inner 1/3 of dentin and involving the pulp chamber), YOLOv3 could accurately detect and classify caries with pulpal exposure (class RC6) (IoU 50 vs IoU 75 : precision, 0.77 vs 0.73; recall, 0.61 vs 0.57) but it failed to predict the outer half of enamel caries (class RA1) (IoU 50 vs IoU 75 : precision, 0.35 vs 0.32; recall, 0.23 vs 0.21). Conclusions YOLOv3 yielded acceptable performances in both IoU 50 and IoU 75 . Although the performance metrics decreased in the 7-class detection, the two thresholds revealed comparable results. However, the model could not consistently detect initial-stage caries affecting the outermost surface of the enamel. Clinical relevance YOLOv3 could be implemented to detect and classify dental caries according to the ICCMS™ classification with acceptable performances to assist dentists in making treatment decisions.","['Dentistry', 'Dentistry']"
doi:10.1007/s11947-022-02939-5,en,Automated Detection of Mechanical Damage in Flaxseeds Using Radiographic Imaging and Machine Learning,"['OriginalPaper', 'Research']","The growing demand for flaxseed as a source of healthy edible oil mandates the need for adopting novel strategies for preserving its quantity and quality. Mechanical damage during harvest and handling is one of the important threats that can adversely affect the quality and viability of flaxseeds. Currently, mechanical damage assessment in grains is mainly performed by human visual inspection, which is a subjective and time-consuming procedure. In this study, the authors propose to utilize radiographic imaging with the machine and deep learning tools to characterize the mechanical damage in flaxseeds intelligently. Images were acquired under four levels of mechanical damage, and two strategies were used to discriminate seeds’ damage: pattern recognition and convolutional neural network (CNN). In the former case, 69 morphological, color, and texture features were extracted. Various classifiers, namely, linear discriminant analysis (LDA), K-nearest neighbors (KNN), support vector machines (SVM), and decision trees were used for the analysis. SVM provided the best performance with a classification accuracy of 87.4%. Furthermore, the analysis of variance (ANOVA) F-test feature selection algorithm was utilized, and the 17 most effective features were selected to be used with an SVM classifier to classify seeds with 88.4% accuracy. In the case of CNN-based classifiers, six state-of-the-art architectures were employed including EfficientNet-B0, VGG19, Resnet18, MobileNet-v2, Inception-v3, and Xception. Among them, EfficientNet-B0 provided superior performance with a classification accuracy of 91.0%. The developed models’ high accuracy confirms the capabilities of radiographic imaging and artificial intelligence tools for rapid, reliable, and automated assessments of mechanical damage in flaxseeds.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology']"
doi:10.1038/s41598-022-24660-1,en,Comparison of discriminant methods and deep learning analysis in plant taxonomy: a case study of Elatine,"['OriginalPaper', 'Article']","Elatine is a genus in which, flower and seed characteristics are the most important diagnostic features; i.e. seed shape and the structure of its cover found to be the most reliable identification character. We used a combination of classic discriminant methods by combining with deep learning techniques to analyze seed morphometric data within 28 populations of six Elatine species from 11 countries throughout the Northern Hemisphere to compare the obtained results and then check their taxonomic classification. Our findings indicate that among the discriminant methods, Quadratic Discriminant Analysis (QDA) had the highest percentage of correct matching (mean fit—91.23%); only the deep machine learning method based on Convolutional Neural Network (CNN) was characterized by a higher match (mean fit—93.40%). The QDA method recognized the seeds of E . brochonii and E . orthosperma with 99% accuracy, and the CNN method with 100%. Other taxa, such as E . alsinastrum , E . trianda , E . californica and E . hungarica were matched with an accuracy of at least 95% (CNN). Our results indicate that the CNN obtains remarkably more accurate classifications than classic discriminant methods, and better recognizes the entire taxa pool analyzed. The least recognized species are E . macropoda and E . hexandra (88% and 78% match).","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00366-022-01765-9,en,A deep learning-based multi-fidelity optimization method for the design of acoustic metasurface,"['OriginalPaper', 'Original Article']","A desirable acoustic metasurface requires the scattered acoustic field distribution uniform. Neural networks are effective substitutions to mimic the expensive FE simulations in most research. However, the computational cost required to construct a model with only single high-fidelity (HF) simulation data is still unacceptable. This paper presents a deep learning-based multi-fidelity optimization framework to improve the uniformity of the scattered acoustic field distribution. First, a multi-fidelity composite convolutional neural network (MF-CCNN) method is proposed to predict the high-dimensional scattered acoustic field at a lower data cost. The developed MF-CCNN consists of four convolutional subnets. The first part predicts a low-fidelity (LF) output, whose features are then extracted by the second part and concatenated with the inputs to predict the HF result. Two parallel branches are utilized to map the LF features to the HF output. Then, the physical parameters’ optimization neural network is proposed to minimize the objective under the prediction of MF-CCNN. The proposed method is compared with other state-of-the-art multi-fidelity networks, and the results demonstrate that MF-CCNN reaches the highest accuracy and the mean absolute error is improved by at least 20%. The variance of the obtained scattered acoustic field after optimization is reduced by 3.62%, and the time cost is only 8% of the genetic algorithm (GA), proving the efficiency and accuracy of the proposed framework.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s42452-022-05160-3,en,Comparing autoencoder-based approaches for anomaly detection in highway driving scenario images,"['OriginalPaper', 'Research Article']","Autoencoder-based anomaly detection approaches can be used for precluding scope compliance failures of the automotive perception. However, the applicability of these approaches for the automotive domain should be thoroughly investigated. We study the capability of two autoencoder-based approaches using reconstruction errors and bottleneck-values for detecting semantic anomalies in automotive images. As a use-case, we consider a specific highway driving scenario identifying if there are any vehicles in the field of view of a front-looking camera. We conduct a series of experiments with two simulated driving scenario datasets and measure anomaly detection performance for different cases. We systematically test different autoencoders and training parameters, as well as the influence of image colors. We show that the autoencoder-based approaches demonstrate promising results for detecting semantic anomalies in highway driving scenario images in some cases. However, we also observe the variability of anomaly detection performance between different experiments. The autoencoder-based approaches are capable of detecting semantic anomalies in highway driving scenario images to some extent. However, further research with other use-cases and real datasets is needed before they can be safely applied in the automotive domain.","['Engineering', 'Engineering, general', 'Materials Science, general', 'Earth Sciences, general', 'Applied and Technical Physics', 'Chemistry/Food Science, general', 'Environment, general']"
doi:10.1007/s42835-022-01321-x,en,Proxy Data Generation for Fast and Efficient Neural Architecture Search,"['OriginalPaper', 'Original Article']","Neural architecture search (NAS) gains popularity in designing best networks for various tasks. Although it shows promising results on many benchmarks and competitions, NAS still suffers from its demanding computation cost for searching in high dimensional architectural design space, and this problem becomes worse when we want to use large-scale datasets. In this paper, we propose a systematic approach to measuring the importance of each training sample on NAS process and making a reliable proxy data, which is a small subset of the original data and thus reduces the computational cost. The idea behind proxy data comes from our observation that each sample has a different impact on NAS process and most of the examples are redundant when we compare the relative accuracy of possible network configurations. Our experimental results show that we can preserve almost the same accuracy ranking among all possible network configurations with proxy data consisting of 5–10% of the original dataset. To the best of our knowledge, our proposed method is the first attempt to make a reliable proxy data for NAS in a systematic manner.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/s11146-022-09929-6,en,Spatial Prediction of Apartment Rent using Regression-Based and Machine Learning-Based Approaches with a Large Dataset,OriginalPaper,"Employing a large dataset (at most, the order of n  = 10 6 ), this study attempts enhance the literature on the comparison between regression and machine learning-based rent price prediction models by adding new empirical evidence and considering the spatial dependence of the observations. The regression-based approach incorporates the nearest neighbor Gaussian processes (NNGP) model, enabling the application of kriging to large datasets. In contrast, the machine learning-based approach utilizes typical models: extreme gradient boosting (XGBoost), random forest (RF), and deep neural network (DNN). The out-of-sample prediction accuracy of these models was compared using Japanese apartment rent data, with a varying order of sample sizes (i.e., n  = 10 4 , 10 5 , 10 6 ). The results showed that, as the sample size increased, XGBoost and RF outperformed NNGP with higher out-of-sample prediction accuracy. XGBoost achieved the highest prediction accuracy for all sample sizes and error measures in both logarithmic and real scales and for all price bands if the distribution of rents is similar in training and test data. A comparison of several methods to account for the spatial dependence in RF showed that simply adding spatial coordinates to the explanatory variables may be sufficient.","['Economics', 'Regional/Spatial Science', 'Financial Services']"
doi:10.1007/s00449-022-02819-4,en,Assessment of artificial neural networks to predict red colorant production by Talaromyces amestolkiae,"['OriginalPaper', 'Research Paper']","Consumer choice is typically influenced by color, leading to an increase in the use of artificial colorants by industry. However, several artificial colorants have been banned due to their harmful effects on human health and the environment, leading to increased interest in colorants from natural sources. Natural colorants can be found in plants, insects, and microorganisms. The importance of evaluating the technical and cost feasibility for the production of natural colorants are important factors for the replacement of artificial counterpart. Therefore, it is highly beneficial to predict the productivity of microbial colorants. The use of statistical methods that generate polynomial models through multiple regressions can provide information of interest about a bioprocess. However, modeling and control of biological processes require complex systems models, because they are nonlinear and non-deterministic systems. In this regard, artificial neural networks are suitable for estimating bioprocess variables with systems modeling. In this work, two different strategies were developed to predict the production of red colorants by Talaromyces amestolkiae , namely simulation by artificial neural networks (ANN) and response surface methodology (RSM). The results showed that the colorant concentration predicted by ANN is closer to the experimental data than that predicted by polynomial models fitted by multiple regression. Thus, this work suggests that the use of ANN can identify the initial conditions of the culture parameters that have the greatest influence on colorant production and can be a tool to be employed to improve the production of biotechnological products, such as microbial colorants.","['Chemistry', 'Biotechnology', 'Industrial and Production Engineering', 'Environmental Engineering/Biotechnology', 'Industrial Chemistry/Chemical Engineering', 'Food Science']"
doi:10.1007/s00500-022-07664-x,en,A response generator with response-aware encoder for generating specific and relevant responses,"['OriginalPaper', 'Data analytics and machine learning']","The dialogue data usually consist of the pairs of a query and its response, but no previous response generators have exploited the responses explicitly in their training while a response provides significant information about the meaning of a query. Therefore, this paper proposes a sequence-to-sequence response generator with a response-aware encoder. The proposed generator exploits golden responses by reflecting them into query representation. For this purpose, the response-aware encoder adds a relevancy scorer layer to the transformer encoder that calculates the relevancy of query tokens to a response. However, golden responses are available only during training of the response generator and unavailable at inference time. As a solution to this problem, the joint learning of a teacher and a student relevancy scorer is adopted. That is, at the training time, both the teacher and the student relevancy scorers are optimized but the decoder generates a response using only the relevancy of the teacher scorer. However, at the inference time, the decoder uses that of the student scorer. Since the student scorer is trained to minimize the difference from the teacher scorer, it can be used to compute the relevancy of a prospective response. The proposed model is the first attempt to use a golden response directly for generating a query representation, whereas previous studies used the responses for its implicit and indirect reflection. As a result, it achieved higher dialogue evaluation score than the current state-of-the-art model for Reddit, Persona-Chat, and DailyDialog data sets.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10278-022-00736-2,en,Spleen Tissue Segmentation Algorithm for Cryo-Imaging Data,"['OriginalPaper', 'Original Paper']","Spleen tissue segmentation is an essential process for analyzing various immunological diseases as observed in the cryo-imaging data. Because manual labeling of the spleen tissue by human experts is not efficient, an automatic segmentation algorithm is needed. In this study, we developed a novel algorithm for automatically segmenting spleen substructures including white pulp and red pulp for the first time. The algorithm is designed for datasets created by a cryo-imaging system. This unique technology can effectively enable cellular tracking anywhere in the whole mouse with single-cell sensitivity. The proposed algorithm consists of four components: initial spleen mask creation, feature extraction, Supervised Patch-based Fuzzy c-Mean (spFCM) classification, and post-processing. The algorithm accurately and efficiently labeled spleen tissues in all experiment settings. The algorithm also improved the spleen segmentation throughput by 90 folds as compared to the manual segmentation. Moreover, we show that our novel spFCM algorithm outperformed traditional fast-learning classifiers as well as the U-Net deep-learning model in many aspects. Two major contributions of this paper are (1) an explainable algorithm for segmenting spleen tissues in cryo-images for the first time and (2) an spFCM algorithm as a new classifier. We also discussed that our work can be beneficial to researchers who work not only in the fields of graft-versus-host disease (GVHD) mouse models, but also in that of other immunological disease models where spleen analysis is essential. Future work building upon our research may lay the foundations for biomedical studies that utilize cryo-imaging technology.","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1186/s12859-022-04964-9,en,Deep learning and multi-omics approach to predict drug responses in cancer,"['OriginalPaper', 'Research']","Background Cancers are genetically heterogeneous, so anticancer drugs show varying degrees of effectiveness on patients due to their differing genetic profiles. Knowing patient’s responses to numerous cancer drugs are needed for personalized treatment for cancer. By using molecular profiles of cancer cell lines available from Cancer Cell Line Encyclopedia (CCLE) and anticancer drug responses available in the Genomics of Drug Sensitivity in Cancer (GDSC), we will build computational models to predict anticancer drug responses from molecular features. Results We propose a novel deep neural network model that integrates multi-omics data available as gene expressions, copy number variations, gene mutations, reverse phase protein array expressions, and metabolomics expressions, in order to predict cellular responses to known anti-cancer drugs. We employ a novel graph embedding layer that incorporates interactome data as prior information for prediction. Moreover, we propose a novel attention layer that effectively combines different omics features, taking their interactions into account. The network outperformed feedforward neural networks and reported 0.90 for $$R^2$$ R 2 values for prediction of drug responses from cancer cell lines data available in CCLE and GDSC. Conclusion The outstanding results of our experiments demonstrate that the proposed method is capable of capturing the interactions of genes and proteins, and integrating multi-omics features effectively. Furthermore, both the results of ablation studies and the investigations of the attention layer imply that gene mutation has a greater influence on the prediction of drug responses than other omics data types. Therefore, we conclude that our approach can not only predict the anti-cancer drug response precisely but also provides insights into reaction mechanisms of cancer cell lines and drugs as well.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1038/s41598-022-24893-0,en,Perturb and optimize users’ location privacy using geo-indistinguishability and location semantics,"['OriginalPaper', 'Article']","Location-based services (LBS) are capable of providing location-based information retrieval, traffic navigation, entertainment services, emergency rescues, and several similar services primarily on the premise of the geographic location of users or mobile devices. However, in the process of introducing a new user experience, it is also easy to expose users’ specific location which can result in more private information leakage. Hence, the protection of location privacy remains one of the critical issues of the location-based services. Moreover, the areas where humans work and live have different location semantics and sensitivities according to their different social functions. Although the privacy protection of a user’s real location can be achieved by the perturbation algorithm, the attackers may employ the semantics information of the perturbed location to infer a user’s real location semantics in an attempt to spy on a user’s privacy to certain extent. In order to mitigate the above semantics inference attack, and further improve the quality of the location-based services, this paper hereby proposes a user side location perturbation and optimization algorithm based on geo-indistinguishability and location semantics. The perturbation area satisfying geo-indistinguishability is thus generated according to the planar Laplace mechanism and optimized by combining the semantics information and time characteristics of the location. The optimum perturbed location that is able to satisfy the minimum loss of location-based service quality is selected via a linear programming method, and can be employed to replace the real location of the user so as to prevent the leakage of the privacy. Experimental comparison of the actual road network and location semantics dataset manifests that the proposed method reduces approximately 37% perturbation distance in contrast to the other state-of-the-art methods, maintains considerably lower similarity of location semantics, and improves region counting query accuracy by a margin of around 40%.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00773-022-00914-5,en,"Machine learning for naval architecture, ocean and marine engineering","['ReviewPaper', 'Review article']","Machine learning (ML)-based techniques have found significant impact in many fields of engineering and sciences, where data-sets are available from experiments and high-fidelity numerical simulations. Those data-sets are generally utilised in a machine learning model to extract information about the underlying physics and derive functional relationships mapping input variables to target quantities of interest. Commonplace machine learning algorithms utilised in scientific machine learning (SciML) include neural networks, support vector machines, regression trees, random forests, etc. The focus of this article is to review the applications of ML in naval architecture, ocean and marine engineering problems; and identify priority directions of research. We discuss the applications of machine learning algorithms for different problems such as wave height prediction, calculation of wind loads on ships, damage detection of offshore platforms, calculation of ship-added resistance and various other applications in coastal and marine environments. The details of the data-sets including the source of data-sets utilised in the ML model development are included. The features used as the inputs to the ML models are presented in detail and finally, the methods employed in optimisation of the ML models were also discussed. Based on this comprehensive analysis, we point out future directions of research that may be fruitful for the application of ML to ocean and marine engineering problems.","['Engineering', 'Automotive Engineering', 'Engineering Fluid Dynamics', 'Engineering Design', 'Offshore Engineering', 'Mechanical Engineering']"
doi:10.1007/s11831-022-09853-1,en,A Systematic Review on Metaheuristic Optimization Techniques for Feature Selections in Disease Diagnosis: Open Issues and Challenges,"['ReviewPaper', 'Review Article']","There is a need for some techniques to solve various problems in today’s computing world. Metaheuristic algorithms are one of the techniques which are capable of providing practical solutions to such issues. Due to their efficiency, metaheuristic algorithms are now used in healthcare data to diagnose diseases practically and with better results than traditional methods. In this study, an efficient search has been performed where 173 papers from different research databases such as Scopus, Web of Science, PubMed, PsycINFO, and others have been considered impactful in diagnosing the diseases using metaheuristic techniques. Ten metaheuristic techniques have been studied, which include spider monkey, shuffled frog leaping algorithm, cuckoo search algorithm, ant lion technique of optimization, lion optimization technique, moth flame technique, bat-inspired algorithm, grey wolf algorithm, whale optimization, and dragonfly technique of optimization for selecting and optimizing the features to predict heart disease, Alzheimer's disease, brain disorder, diabetes, chronic disease features, liver disease, covid-19, etc. Besides, the framework has also been shown to provide information on various phases behind the execution of metaheuristic techniques to predict diseases. The study’s primary goal is to present the contribution of the researchers by demonstrating their methodology to predict diseases using the metaheuristic techniques mentioned above. Later, their work has also been compared and evaluated using accuracy, precision, F1 score, error rate, sensitivity, specificity, an area under a curve, etc., to help the researchers to choose the right field and methods for predicting the diseases in the future.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s11227-022-04959-6,en,Dung beetle optimizer: a new meta-heuristic algorithm for global optimization,OriginalPaper,"In this paper, a novel population-based technique called dung beetle optimizer (DBO) algorithm is presented, which is inspired by the ball-rolling, dancing, foraging, stealing, and reproduction behaviors of dung beetles. The newly proposed DBO algorithm takes into account both the global exploration and the local exploitation, thereby having the characteristics of the fast convergence rate and the satisfactory solution accuracy. A series of well-known mathematical test functions (including both 23 benchmark functions and 29 CEC-BC-2017 test functions) are employed to evaluate the search capability of the DBO algorithm. From the simulation results, it is observed that the DBO algorithm presents substantially competitive performance with the state-of-the-art optimization approaches in terms of the convergence rate, solution accuracy, and stability. In addition, the Wilcoxon signed-rank test and the Friedman test are used to evaluate the experimental results of the algorithms, which proves the superiority of the DBO algorithm against other currently popular optimization techniques. In order to further illustrate the practical application potential, the DBO algorithm is successfully applied in three engineering design problems. The experimental results demonstrate that the proposed DBO algorithm can effectively deal with real-world application problems.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s42107-022-00540-x,en,Output-only identification of a simplified onshore wind turbine model using a modified harmony search algorithm,"['OriginalPaper', 'Research']","Structural identification plays an important part in the monitoring of sensitive structures such as onshore wind turbines. Due to the nature of wind, such structures are subjected to turbulent wind fields that cause aerodynamic excitations to the structure. Such turbulent wind fields are hard to be measured, thus output-only identification is required. Output-only identification is a hard category of structural identification methods that is used whenever input excitations are unknown or hard to be measured. With their ability to estimate and reproduce the excitation histories, output-only has established itself as an important method in structural health monitoring. This contribution utilizes a previously developed simplified model to perform an output-only identification to onshore wind turbine towers. The studied wind turbine is an idling 5-MW reference turbine developed by the National Renewable Energy Laboratory (NREL). The effect of the number of sensors distributed on the tower was studied, and obtained results were satisfying in both identification accuracy and force estimation, encouraging to improve the identification scheme by coupling it with a noise filtering method to deal with noisy vibration responses.","['Engineering', 'Civil Engineering', 'Building Materials', 'Sustainable Architecture/Green Buildings']"
doi:10.1007/s11548-022-02799-6,en,Robust co-teaching learning with consistency-based noisy label correction for medical image classification,"['OriginalPaper', 'Original Article']","Purpose Deep neural networks (DNNs) have made great achievements in computer-aided diagnostic systems, but the success highly depends on massive data with high-quality labels. However, for many medical image datasets, a considerable number of noisy labels are introduced by inter- and intra-observer variability, thus hampering DNNs’ performance. To address this problem, a robust noisy label correction method with the co-teaching learning paradigm is proposed. Methods The proposed method aims to reduce the effect of noisy labels by correcting or removing them. It consists of two modules. An adaptive noise rate estimation module is employed to calculate the dataset’s noise rate, which is helpful to detect noisy labels but is usually unavailable in clinical applications. A consistency-based noisy label correction module aims to detect noisy labels and correct them to reduce the disturbance from noisy labels and exploit useful information in data. Results Experiments are conducted on the public skin lesion dataset ISIC-2017, ISIC-2019, and our constructed thyroid ultrasound image dataset. The results demonstrate that the proposed method outperforms other noisy label learning methods in medical image classification tasks. It is also evaluated on the natural image dataset CIFAR-10 to show its generalization. Conclusion This paper proposes a noisy label correction method to handle noisy labels in medical image datasets. Experimental results show that it can self-adapt to different datasets and efficiently correct noisy labels, which is suitable for medical image classification.","['Medicine & Public Health', 'Imaging / Radiology', 'Surgery', 'Health Informatics', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Computer Science, general']"
doi:10.1007/s10845-022-02051-7,en,Cross-scale fusion and domain adversarial network for generalizable rail surface defect segmentation on unseen datasets,OriginalPaper,"Surface quality control is a crucial part of rail manufacturing. Deep neural networks have shown impressive accuracy in rail surface defect segmentation under the assumption that the test images have the same distribution as the training images. However, in practice detection, the rail images exhibit variations in appearance and scale for different rail types and production conditions. Directly deploying the deep neural network on unseen images shows a performance degradation due to the distribution discrepancies of training images. To this end, we propose a cross-scale fusion and domain adversarial network (CFDANet) to improve the generalization ability of deep neural networks on unseen datasets. To alleviate the domain shift caused by defect scale differences, we design a dual-encoder to extract multi-scale features from images of different resolutions. Then, those features are adaptively fused through a cross-scale fusion module. For the domain shift caused by inconsistent rail appearance, we introduce transferable-aware domain adversarial learning to extract domain invariant features from different datasets. Moreover, we further propose a transferable curriculum to suppress the negative impact of images with low transferability. Experimental results show that our CFDANet can accurately segment defects in unseen datasets and surpass other state-of-the-art domain generalization methods in all five target domain settings. The source code is released at https://github.com/dotaball/railseg_dg .","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s13198-022-01797-w,en,Optimization solution of congestion problem with FACTS devices using symbiotic organism search algorithm,"['OriginalPaper', 'Original Article']","Present paper deals with installation and framing of proper control strategy of flexible AC transmission systems (FACTs) devices by utilizing evolutionary optimization technique for reactive power control aiming at alleviating congestion of power systems under deregulated regime. This study projects the symbiotic organism search (SOS) algorithm for optimal FACTs devices planning in power system under numerous loading conditions. The study has been implemented on IEEE 57- and 118-bus test power systems with various objectives such as minimization of either operating cost or active power loss and improvement of voltage stability profile with the application of FACTs devices. The attained results are compared to those offered by some other evolutionary optimization techniques appeared in the recent state-of-the-art literature. It has been witnessed that by proper placement of FACTs devices along with the optimal control of generators voltage setting and transformer tap arrangements; system operating cost, line loss, and congestion in the transmission lines are reduced significantly by the proposed SOS algorithm. The simulation results showed that the total system loss has been reduced to 0.2171 MW from 0.2799 MW for IEEE-57 bus system while 1.0455 MW from 1.3286 MW for IEEE-118 bus system. Thereby, simulation outcomes authenticate efficiency of the proposed methodology in optimizing the overall system cost function that consists of operating cost and the investment costs associated with FACTs devices while tackling congestion in power networks.","['Engineering', 'Quality Control, Reliability, Safety and Risk', 'Engineering Economics, Organization, Logistics, Marketing']"
doi:10.1007/s13369-022-07474-1,en,MPPT Performance and Power Quality Improvement by Using Fractional-Order Adaptive Backstepping Control of a DFIG-Based Wind Turbine with Disturbance and Uncertain Parameters,"['OriginalPaper', 'Research Article-Electrical Engineering']","This paper proposes a fractional-order adaptive backstepping control (FOABC) with disturbance and uncertainty terms compensation for improving the MPPT (maximum power point tracking) performance and output power quality of a doubly fed induction generator (DFIG)-based wind turbine. In the proposed high-efficacy controller, disturbance and uncertainty terms are estimated in real time using an adaptive estimator designed using a recursion design process based on the nonlinear backstepping control. Meanwhile, a robust compensator is schemed and incorporated into the backstepping control algorithm so that it suppresses the effects of external disturbances and system uncertainties and, as a result, ensures the maximum wind energy extraction as much as possible, which is usually well known as MPPT. Furthermore, the fractional-order control approach was deployed in the proposed adaptive backstepping controller to provide a smooth control signal for enhancing the quality of the power injected into the grid. The stability analysis of the overall closed-loop system was performed using Lyapunov's stability theory. The high effectiveness of the proposed control method was assessed through simulation studies carried out in MATLAB/Simulink of a DFIG-wind turbine operating in various conditions.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1007/s10462-022-10325-y,en,Data clustering: application and trends,OriginalPaper,"Clustering has primarily been used as an analytical technique to group unlabeled data for extracting meaningful information. The fact that no clustering algorithm can solve all clustering problems has resulted in the development of several clustering algorithms with diverse applications. We review data clustering, intending to underscore recent applications in selected industrial sectors and other notable concepts. In this paper, we begin by highlighting clustering components and discussing classification terminologies. Furthermore, specific, and general applications of clustering are discussed. Notable concepts on clustering algorithms, emerging variants, measures of similarities/dissimilarities, issues surrounding clustering optimization, validation and data types are outlined. Suggestions are made to emphasize the continued interest in clustering techniques both by scholars and Industry practitioners. Key findings in this review show the size of data as a classification criterion and as data sizes for clustering become larger and varied, the determination of the optimal number of clusters will require new feature extracting methods, validation indices and clustering techniques. In addition, clustering techniques have found growing use in key industry sectors linked to the sustainable development goals such as manufacturing, transportation and logistics, energy, and healthcare, where the use of clustering is more integrated with other analytical techniques than a stand-alone clustering technique.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s13042-022-01725-1,en,SCMP-IL: an incremental learning method with super constraints on model parameters,"['OriginalPaper', 'Original Article']","Deep learning technology has played an important role in our life. Since deep learning technology relies on the neural network model, it is still plagued by the catastrophic forgetting problem, which refers to the neural network model will forget what it has learned after learning new knowledge. The neural network model learns knowledge through labeled samples, and its knowledge is stored in its parameters. Therefore, many methods try to solve this problem from the perspective of constraint parameters and stored samples. There are few ways to solve this problem from the perspective of constraining features output of neural network models. This paper proposes an incremental learning method with super constraints on model parameters. This method not only calculates the parameter similarity loss of the old and new models, but also calculates the layer output feature similarity loss of the old and new models, and finally suppresses the change of model parameters from two directions. In addition, we also propose a new strategy for selecting representative samples from dataset and tackling the imbalance between stored samples and new task samples. Finally, we utilize the neural kernel mapping support vector machine theory to increase the interpretability of the model. In order to better meet the actual situation, five sample sets with different categories and amounts were employed in experiments. Experiments show the effectiveness of our method. For example, after learning the last task, our method is at least 1.930% and 0.562% higher than other methods on the training set and test set, respectively.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s11831-022-09852-2,en,Adaptive Computational Solutions to Energy Efficiency in Cloud Computing Environment Using VM Consolidation,"['ReviewPaper', 'Review Article']","Cloud Computing has emerged as a computing paradigm where services are provided through the internet in recent years. Offering on-demand services has transformed the IT companies' working environment, leading to a linearly increasing trend of its usage. The provisioning of the Computing infrastructure is achieved with the help of virtual machines. A great figure of physical devices is required to satisfy the users' resource requirements. To meet the requirements of the submitted workloads that are usually dynamic, the cloud data centers cause the over-provisioning of cloud resources. The result of this over-provisioning is the resource wastage with an increase in the levels of energy consumption, causing a raised operational cost. High CO 2 emissions result from this huge energy consumption by data centers, posing a threat to environmental stability. The environmental concern demands for the controlled energy consumption, which can be attained by optimal usage of resources to achieve in the server load, by minimizing the number of active nodes, and by minimizing the frequency of switching between active and de-active server mode in the data center. Motivated by these actualities, we discuss numerous statistical, deterministic, probabilistic, machine learning and optimization based computational solutions for the cloud computing environment. A comparative analysis of the computational methods, on the basis of architecture, consolidation step involved, objectives achieved, simulators involved and resources utilized, has also been presented. A taxonomy for virtual machine (VM) consolidation has also been derived in this research article followed by emerging challenges and research gaps in the field of VM consolidation in cloud computing environment.","['Engineering', 'Mathematical and Computational Engineering']"
doi:10.1007/s00521-022-08038-y,en,A Novel original feature fusion network for joint diabetic retinopathy and diabetic Macular edema grading,"['OriginalPaper', 'Original Article']","Diabetic retinopathy (DR) and its complication diabetic macular edema (DME) are the leading cause of permanent blindness in the working-age population worldwide. Automated grading of DR and DME enables ophthalmologists to carry out tailored treatments to patients in early stages of their diseases. However, most of the current works only focus on the grading of a single disease, ignoring the relationship between DR and DME, and the traditional convolutional architectures face the problem that they can not capture long-distance dependencies despite of the effectiveness of extracting image features. To this end, we propose an original feature fusion network (OFFNet) for joint DR and DME grading based on the idea of key-value query, which consists of a specific feature extraction module (SFEM) based on self-attention and an original feature fusion module (OFFM) based on cross-attention. The proposed OFFNet enjoys several merits. First, to the best of our knowledge, this is the first joint grading effort based on the idea of key-value query. Second, OFFNet only needs image-level supervision, which can facilitate the acquisition of training data, rather than patch-level or pixel-level supervision. Third, OFFNet has obvious advantages in capturing long-distance dependencies. Extensive experiments on two public datasets Messidor and 2018 IDRiD challenge show that our method outperforms other joint grading methods on joint grading accuracy and the ability of capturing long-distance dependencies.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11669-022-01011-1,en,Development of a Diffusion Mobility Database for Co-Based Superalloys,OriginalPaper,"To facilitate the development of high-temperature Co-based γ-γ′ superalloys, a Co-Ni based diffusion mobility database is developed for the eight component FCC (Face Centered Cubic) system of Co-Al-W-Ni-Cr-Ti-Ta-Re. A CALPHAD approach is used to represent the temperature and composition dependency of the multicomponent system. The mobility descriptions were based on previous assessment work for the Ni-based superalloys, published experimental and computational data, and established diffusion correlations. The initial mobility descriptions were then refined using additional diffusion couple experimental data, particularly for the Co-Cr, Co-Ta, and Ni-Ta systems. After re-optimizing the descriptions with the new experimental data, the mobility descriptions were validated using a collection of published diffusion couple composition profiles, which were not included in the initial assessment process.","['Physics', 'Crystallography and Scattering Methods', 'Thermodynamics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Ceramics, Glass, Composites, Natural Materials', 'Metallic Materials']"
doi:10.1007/s10845-022-02053-5,en,A novel automatic classification approach for micro-flaws on the large-aperture optics surface based on multi-light source fusion and integrated deep learning architecture,OriginalPaper,"Micron-level flaws on component surface can seriously affect the optical and mechanical properties of optics, so it is significant to accurately classify and repair surface flaws. However, the small scale, multiple categories, and complex characteristics of surface flaws make the accurate classification a great challenge in the field of precision optical engineering. In this work, a novel automated classification approach is proposed for multi-classification of flaws. Images of flaws illuminated by multi-light sources are automatically acquired by classification equipment to enrich the characteristics of flaws, and a series of automated workflows are set up to accomplish flaw detection, classification, and mitigation. SingleCNN-MergeRGB, a feature extraction structure based on a single convolutional neural network (CNN) and multi-light source fusion, is constructed to extract the integrated features of flaws under different illumination. The experimental results show that SingleCNN-MergeRGB has stronger capability of feature description compared with single light source and other feature extraction networks. The integrated deep learning architecture SingleCNN-MergeRGB + SVM and prediction probability threshold adjustment strategy are proposed to improve the accuracy of flaw classification and correct failure cases. Validation results on the test set show that the proposed flaw classification method obtains an accuracy of 96.34% and a recall rate of 100%. The experiments demonstrate the effectiveness of the automatic classification equipment and methods in this work in the classification of surface flaws in large-aperture optical components.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11069-022-05709-w,en,Development of environmental contours from rainfall intensity and duration data for slopes,"['OriginalPaper', 'Original Paper']","Rainfall is an important environmental factor affecting the stability of natural slopes. In this study, an environmental contour method is developed by incorporating the infiltration model proposed by Green and Ampt to evaluate the stability of slopes exposed to rainfall. Several environmental variables—such as rainfall intensity and rainfall duration, cohesion, and internal friction angle of soil parameters—are treated as random variables. The influence of these variables, especially their joint characteristics, on the reliability of slopes is discussed. The environmental contour is determined by specifying a target reliability index or a return period. In addition, the reliability index is derived by using the environmental contour method on the basis of the limit state function. Based on the data set pertaining to long-term rainfall observations for the Peloritani Mountains regions of Italy, the illustrative reliability analysis is performed for the hill slopes. The environmental contour in the case of multiple variables is demonstrated, and the reliability index of slope stability against sliding is obtained. The extreme slope responses are derived for the points on the environmental contour, which can be helpful for probabilistic slope stability assessment and design.","['Earth Sciences', 'Natural Hazards', 'Hydrogeology', 'Geophysics/Geodesy', 'Geotechnical Engineering & Applied Earth Sciences', 'Civil Engineering', 'Environmental Management']"
doi:10.1007/s11192-022-04547-8,en,"A review of scientific impact prediction: tasks, features and methods",OriginalPaper,"With the rapid evolution of scientific research, there are a huge volume of papers published every year and the number of scholars is also growing fast. How to effectively predict the scientific impact has become an important research problem, attracting the attention of researchers in various fields, and it is of great significance in improving research efficiency and assisting in decision-making and scientific evaluation. In this paper, we propose a new framework to perform a systematical survey of scientific impact prediction research. Specifically, we take the four common academic entities into account: papers, scholars, venues and institutions. We reviewed all the prediction tasks reported in the literature in detail; the input features are divided into six groups: paper-related, author-related, venue-related, institution-related, network-related and altmetrics-related. Moreover, we classify the forecasting methods into mathematical statistics-based, traditional machine learning-based, deep learning-based and graph-based, and subdivide each category according to the characteristics. Finally, we discuss open issues and existing challenges, and provide potential research directions.","['Computer Science', 'Information Storage and Retrieval', 'Library Science']"
doi:10.1186/s41935-022-00314-1,en,Fully automated method for dental age estimation using the ACF detector and deep learning,"['OriginalPaper', 'Original Article']","Background Dental age estimation plays an important role in identifying an unknown person. In forensic science, estimating age with high accuracy depends on the experience of the practitioner. Previous studies proposed classification of tooth development of the mandibular third molar by following Demirjian’s method, which is useful for dental age estimation. Although stage of tooth growth is very helpful in assessing age estimation, it must be performed manually. The drawback of this procedure is its need for skilled observers to carry out the tasks precisely and reproducibly because it is quite detailed. Therefore, this research aimed to apply computer-aid methods for reducing time and subjectivity in dental age estimation by using dental panoramic images based on Demirjian’s method. Dental panoramic images were collected from persons aged 15 to 23 years old. In accordance with Demirjian’s method, this study focused only on stages D to H of tooth development, which were discovered in the 15- to 23-year age range. The aggregate channel features detector was applied automatically to localize and crop only the lower left mandibular third molar in panoramic images. Then, the convolutional neural network model was applied to classify cropped images into D to H stages. Finally, the classified stages were used to estimate dental age. Results Experimental results showed that the proposed method in this study can localize the lower left mandibular third molar automatically with 99.5% accuracy, and training in the convolutional neural network model can achieve 83.25% classification accuracy using the transfer learning strategy with the Resnet50 network. Conclusion In this work, the aggregate channel features detector and convolutional neural network model were applied to localize a specific tooth in a panoramic image and identify the developmental stages automatically in order to estimate the age of the subjects. The proposed method can be applied in clinical practice as a tool that helps clinicians to reduce the time and subjectivity for dental age estimation.","['Medicine & Public Health', 'Pathology', 'Forensic Medicine']"
doi:10.1007/s11356-022-24202-2,en,Dimensionality reduction strategies for land use land cover classification based on airborne hyperspectral imagery: a survey,"['ReviewPaper', 'Review Article']","Hyperspectral image (HSI) contains hundreds of adjacent spectral bands, which can effectively differentiate the region of interest. Nevertheless, many irrelevant and highly correlated spectral bands lead to the Hughes phenomenon. Consequently, hyperspectral image dimensionality reduction is necessary to select the most informative and significant spectral band and eliminate the redundant spectral band. To this end, this paper represents an extensive and systematic survey of hyperspectral dimensionality reduction approaches for land use land cover (LULC) classification. Moreover, this paper reviewed the following important points: (1) hyperspectral imaging data acquisition methods, (2) the difference between hyperspectral and multispectral images, (3) hyperspectral image dimensionality reduction based on machine learning (ML) and deep learning (DL) techniques, (4) the popular benchmark hyperspectral datasets with the performance metrics for LULC classification, and (5) the significant challenges with the future trends for hyperspectral dimensionality reduction.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s12205-022-0113-6,en,Law and Early Warning of Vertical Sluice Cluster Displacements in Soft Coastal Soil,"['OriginalPaper', 'Structural Engineering']","The vertical displacement, which is the product of natural sources and human activities, is the key factor affecting the sluice safety. This study provides a systematic approach used for analyzing the law and early warning of sluice cluster vertical displacements in coastal soft soil locations. Two important methods, including probability analysis and principal component analysis (PCA), are used to obtain the necessary information in this study. Among them, PCA is mainly used to identify the risk indices during vertical deformations of sluice cluster. As case studies, 27 sluices in a cluster in Northern Jiangsu Province’s coastal area in China are chosen and 14 variables related to sluice uplift, settlement and differential settlement deformations are used. The PCA and additional evidence from the sluice deformation law are used to identify three variables as risk indices, including maximum differential settlement ( MMDS ), maximum cumulative vertical settlement ( MCVS ) and maximum cumulative vertical uplift ( MCVU ). This study divides the risk levels into five grades (i.e., Level 1 to Level 5) based on the selected risk indices and determines their risk thresholds based on the in-situ deformation data from 2010 to 2020. In general, the results demonstrate that the newly proposed approach exhibits an acceptable performance. However, the influence of epistemic and aleatory uncertainties on this study is worthy of further discussion in the future.","['Engineering', 'Civil Engineering', 'Industrial Pollution Prevention', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s11227-022-04936-z,en,A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models,OriginalPaper,"Human-robot collaboration has gained a notable prominence in Industry 4.0, as the use of collaborative robots increases efficiency and productivity in the automation process. However, it is necessary to consider the use of mechanisms that increase security in these environments, as the literature reports that risk situations may exist in the context of human-robot collaboration. One of the strategies that can be adopted is the visual recognition of the collaboration environment using machine learning techniques, which can automatically identify what is happening in the scene and what may happen in the future. In this work, we are proposing a new framework that is capable of detecting robotic arm keypoints commonly used in Industry 4.0. In addition to detecting, the proposed framework is able to predict the future movement of these robotic arms, thus providing relevant information that can be considered in the recognition of the human-robot collaboration scenario. The proposed framework has two main modules. The first one contains a convolutional neural network based on self-calibrated convolutions enabling better discriminative feature extraction and the support of extreme learning machine neural networks with different kernels for predicting robotic arm keypoints. The second module is composed of deep recurrent learning models, such as long short-term memory and gated recurrent unit. These models are able to predict future robotic arm keypoints. All experiments were evaluated using the mean squared error metric. Results show that the proposed framework is capable of detecting and predicting with low error, contributing to the mitigation of risks in human-robot collaboration. In addition, it was possible to verify that the use of convolutional neural networks in conjunction with extreme learning machines can offer a lower detection error in a regression task (e.g., keypoint detection), something that, as far as the authors are aware of, is not yet known, nor had been evaluated previously in the literature.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1038/s41377-022-01013-1,en,Large-scale coherent Ising machine based on optoelectronic parametric oscillator,"['OriginalPaper', 'Article']","Ising machines based on analog systems have the potential to accelerate the solution of ubiquitous combinatorial optimization problems. Although some artificial spins to support large-scale Ising machines have been reported, e.g., superconducting qubits in quantum annealers and short optical pulses in coherent Ising machines, the spin stability is fragile due to the ultra-low equivalent temperature or optical phase sensitivity. In this paper, we propose to use short microwave pulses generated from an optoelectronic parametric oscillator as the spins to implement a large-scale Ising machine with high stability. The proposed machine supports 25,600 spins and can operate continuously and stably for hours. Moreover, the proposed Ising machine is highly compatible with high-speed electronic devices for programmability, paving a low-cost, accurate, and easy-to-implement way toward solving real-world optimization problems.","['Physics', 'Optics, Lasers, Photonics, Optical Devices', 'Microwaves, RF and Optical Engineering', 'Optical and Electronic Materials']"
doi:10.1007/s12046-022-02011-0,en,Priority fractional rationing (PFR) policy and a hybrid metaheuristic for managing stock in divergent supply chains,OriginalPaper,"A distributor catering to demands of multiple retailers is considered in this paper and stock-management in this divergent supply chain is achieved through the deployment of periodic review base-stock (i.e. ( R , S ) policy) policy at every member. In the model of the supply chain considered in this study, in every time-period, an attempt is made by the distributor to first transport backlogged-demands from the downstream members till the distributor’s previous instance-of-review even before considering demands from downstream members in recent time-periods. This practice of the distributor attempting first to satisfy backlogged-demands till its last instance of review will ensure that the shipment will reach the retailer, contemplating whom the order was made by the distributor. A Mixed Integer Linear Programming (MILP)-based mathematical formulation of the supply chain (with the objective of minimizing the Total Supply Chain Cost (TSCC)), to obtain optimum policy ( R , S ) parameters at each member of the supply chain, and inherently performing the allocation and rationing of stock over a finite planning horizon, is proposed through this paper. A new heuristic allocation and rationing mechanism for the distributor to distribute stock among retailers during the occurrence of shortage named as Priority Fractional Rationing (PFR) policy is also introduced in this study. A heuristic methodology which is a hybrid of Genetic Algorithm and Particle Swarm Optimization algorithm ( HGA-PSO ) combined with PFR policy is proposed through this study after analyzing the computational difficulty encountered and lack of tractability of stock-allocation and stock-rationing mechanism while the MILP-based mathematical formulation is solved to optimality. A local search technique as part of the Particle Swarm Optimization (PSO) algorithm, similar to mutation operation in Genetic Algorithm (GA) is introduced due to the observation of inferior results during pilot studies. The performance evaluation studies of the HGA-PSO with that of stand-alone GA, stand-alone PSO with new local search and the exact solution obtained by solving the MILP-based mathematical formulation is presented. The results indicate the superior performance of the hybrid algorithm in comparison with stand-alone GA and PSO.","['Engineering', 'Engineering, general']"
doi:10.1007/s11222-022-10168-1,en,"Constructing two-level 
              
                
              
              $$Q_B$$
              
                
                  Q
                  B
                
              
            -optimal screening designs using mixed-integer programming and heuristic algorithms","['OriginalPaper', 'Original Paper']","Two-level screening designs are widely applied in manufacturing industry to identify influential factors of a system. These designs have each factor at two levels and are traditionally constructed using standard algorithms, which rely on a pre-specified linear model. Since the assumed model may depart from the truth, two-level $$Q_B$$ Q B -optimal designs have been developed to provide efficient parameter estimates for several potential models. These designs also have an overarching goal that models that are more likely to be the best for explaining the data are estimated more efficiently than the rest. However, there is no effective algorithm for constructing them. This article proposes two methods: a mixed-integer programming algorithm that guarantees convergence to the two-level $$Q_B$$ Q B -optimal designs; and, a heuristic algorithm that employs a novel formula to find good designs in short computing times. Using numerical experiments, we show that our mixed-integer programming algorithm is attractive to find small optimal designs, and our heuristic algorithm is the most computationally-effective approach to construct both small and large designs, when compared to benchmark heuristic algorithms.","['Computer Science', 'Artificial Intelligence', 'Statistics and Computing/Statistics Programs', 'Statistical Theory and Methods', 'Probability and Statistics in Computer Science']"
doi:10.1038/s41598-022-24900-4,en,Lung_PAYNet: a pyramidal attention based deep learning network for lung nodule segmentation,"['OriginalPaper', 'Article']","Accurate and reliable lung nodule segmentation in computed tomography (CT) images is required for early diagnosis of lung cancer. Some of the difficulties in detecting lung nodules include the various types and shapes of lung nodules, lung nodules near other lung structures, and similar visual aspects. This study proposes a new model named Lung_PAYNet, a pyramidal attention-based architecture, for improved lung nodule segmentation in low-dose CT images. In this architecture, the encoder and decoder are designed using an inverted residual block and swish activation function. It also employs a feature pyramid attention network between the encoder and decoder to extract exact dense features for pixel classification. The proposed architecture was compared to the existing UNet architecture, and the proposed methodology yielded significant results. The proposed model was comprehensively trained and validated using the LIDC-IDRI dataset available in the public domain. The experimental results revealed that the Lung_PAYNet delivered remarkable segmentation with a Dice similarity coefficient of 95.7%, mIOU of 91.75%, sensitivity of 92.57%, and precision of 96.75%.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41597-022-01833-1,en,"TractoInferno - A large-scale, open-source, multi-site database for machine learning dMRI tractography","['OriginalPaper', 'Data Descriptor']","TractoInferno is the world’s largest open-source multi-site tractography database, including both research- and clinical-like human acquisitions, aimed specifically at machine learning tractography approaches and related ML algorithms. It provides 284 samples acquired from 3 T scanners across 6 different sites. Available data includes T1-weighted images, single-shell diffusion MRI (dMRI) acquisitions, spherical harmonics fitted to the dMRI signal, fiber ODFs, and reference streamlines for 30 delineated bundles generated using 4 tractography algorithms, as well as masks needed to run tractography algorithms. Manual quality control was additionally performed at multiple steps of the pipeline. We showcase TractoInferno by benchmarking the learn2track algorithm and 5 variations of the same recurrent neural network architecture. Creating the TractoInferno database required approximately 20,000 CPU-hours of processing power, 200 man-hours of manual QC, 3,000 GPU-hours of training baseline models, and 4 Tb of storage, to produce a final database of 350 Gb. By providing a standardized training dataset and evaluation protocol, TractoInferno is an excellent tool to address common issues in machine learning tractography. Measurement(s) Diffusion Weighted Imaging • Magnetic Resonance Imaging of the Brain without Contrast • Diffusion Tensor Imaging Technology Type(s) 3 T MRI scanner Factor Type(s) Age • Gender Sample Characteristic - Organism Homo sapiens","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s12859-022-05063-5,en,A graph neural network framework for mapping histological topology in oral mucosal tissue,"['OriginalPaper', 'Research']","Background Histological feature representation is advantageous for computer aided diagnosis (CAD) and disease classification when using predictive techniques based on machine learning. Explicit feature representations in computer tissue models can assist explainability of machine learning predictions. Different approaches to feature representation within digital tissue images have been proposed. Cell-graphs have been demonstrated to provide precise and general constructs that can model both low- and high-level features. The basement membrane is high-level tissue architecture, and interactions across the basement membrane are involved in multiple disease processes. Thus, the basement membrane is an important histological feature to study from a cell-graph and machine learning perspective. Results We present a two stage machine learning pipeline for generating a cell-graph from a digital H &E stained tissue image. Using a combination of convolutional neural networks for visual analysis and graph neural networks exploiting node and edge labels for topological analysis, the pipeline is shown to predict both low- and high-level histological features in oral mucosal tissue with good accuracy. Conclusions Convolutional and graph neural networks are complementary technologies for learning, representing and predicting local and global histological features employing node and edge labels. Their combination is potentially widely applicable in histopathology image analysis and can enhance explainability in CAD tools for disease prediction.","['Life Sciences', 'Bioinformatics', 'Microarrays', 'Computational Biology/Bioinformatics', 'Computer Appl. in Life Sciences', 'Algorithms']"
doi:10.1007/s11356-022-24021-5,en,Multi-objective prediction for denitration systems in cement: an approach combining process analysis and bi-directional long short-term memory network,"['OriginalPaper', 'Research Article']","Selective Non-Catalytic Reduction (SNCR) can improve the denitration process and reduce NOx emissions by accurizing prediction of NOx concentration and ammonia escape. However, there are inevitable time delays and nonlinearity problems in the prediction of NOx emission. To reduce NOx concentration quickly in SNCR, excessive ammonia spraying often causes a large amount of ammonia to escape, resulting in secondary pollution. Therefore, it is particularly important to monitor ammonia escape. To solve the above problems, this paper proposes a framework by specifically analyzing the cement denitration process and combining a multi-objective time series bi-directional long short-term memory network (MT-BiLSTM). Among them, the model achieves multi-objective prediction of NOx emission concentration and ammonia escape simultaneously. In addition, time series containing delay information are introduced in the input layer to eliminate the influence of delay. Based on the bi-directional LSTM model, the dropout strategy is adopted to improve the generalization of the model and the Adam optimizer is applied to improve the network performance. Besides, through the multi-step prediction of NOx emission at 3 time points, the dynamic nature of the data is preserved, which provides dynamic information support for realizing the automation of denitration system. The prediction performance of the MT-BiLSTM model is experimentally validated, and the results demonstrate that it can reliably predict both NOx and ammonia escape. The model achieves more accurate and reliable results for the prediction of flue gas concentrations compared with other methods such as SVR, DTR and LSTM. Therefore, the MT-BiLSTM model provides a basis for achieving NOx emission reduction and accurate ammonia injection.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s41965-022-00113-6,en,Chinese dialect tone’s recognition using gated spiking neural P systems,"['OriginalPaper', 'Regular Paper']","Tone is the changing trend of pitch with time. In Chinese, tone plays an essential role for distinguishing meaning. Chinese dialect’s tone is more complex with Mandarin. In the field of Chinese dialect phonetics research, using human earing to recognize the types of tones is still the main method. So batch processing is not possible. In this paper, we construct a GSNP (gated spiking neural P) model with 2 layers which can process time series data to recognize the tones of Chinese dialects. The average accuracy rate of seven cities’ speech is more than 97%. Even in the case of small training samples, compared with other methods, the GSNP model has simpler structure, higher accuracy and more efficiency. It can not only improve the work efficiency of Chinese dialect field investigation, but also help researchers to screen the sounds with special sounds.","['Computer Science', 'Theory of Computation', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Optimization', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00330-022-09272-7,en,Predicting muscle invasion in bladder cancer by deep learning analysis of MRI: comparison with vesical imaging–reporting and data system,"['OriginalPaper', 'Imaging Informatics and Artificial Intelligence']","Objectives To compare the diagnostic performance of a novel deep learning (DL) method based on T2-weighted imaging with the vesical imaging–reporting and data system (VI-RADS) in predicting muscle invasion in bladder cancer (MIBC). Methods A total of 215 tumours (129 for training and 31 for internal validation, centre 1; 55 for external validation, centre 2) were included. MIBC was confirmed by pathological examination. VI-RADS scores were provided by two groups of radiologists (readers 1 and readers 2) independently. A deep convolutional neural network was constructed in the training set, and validation was conducted on the internal and external validation sets. ROC analysis was performed to evaluate the performance for MIBC diagnosis. Results The AUCs of the DL model, readers 1, and readers 2 were as follows: in the internal validation set, 0.963, 0.843, and 0.852, respectively; in the external validation set, 0.861, 0.808, and 0.876, respectively. The accuracy of the DL model in the tumours scored VI-RADS 2 or 3 was higher than that of radiologists in the external validation set: for readers 1, 0.886 vs. 0.600, p = 0.006; for readers 2, 0.879 vs. 0.636, p = 0.021. The average processing time (38 s and 43 s in two validation sets) of the DL method was much shorter than the readers, with a reduction of over 100 s in both validation sets. Conclusions Compared to radiologists using VI-RADS, the DL method had a better diagnostic performance, shorter processing time, and robust generalisability, indicating good potential for diagnosing MIBC. Key Points • The DL model shows robust performance for MIBC diagnosis in both internal and external validation. • The diagnostic performance of the DL model in the tumours scored VI-RADS 2 or 3 is better than that obtained by radiologists using VI-RADS. • The DL method shows potential in the preoperative assessment of MIBC.","['Medicine & Public Health', 'Imaging / Radiology', 'Diagnostic Radiology', 'Interventional Radiology', 'Neuroradiology', 'Ultrasound', 'Internal Medicine']"
doi:10.1007/s11227-022-04963-w,en,Union-net: lightweight deep neural network model suitable for small data sets,OriginalPaper,"Traditional deep learning models prefer large data sets, and in reality small data sets are easier to obtain. It is more practical to build models suitable for small data sets. Based on CNN, this paper proposes the concept of union convolution to build a deep learning model Union-net that is suitable for small data sets. The Union-net has small model size and superior performance. In this paper, the model is tested based on multiple commonly used data sets. The experimental results show that Union-net outperforms most models when dealing with small datasets, and Union-net outperforms other models when dealing with complex classification tasks or dealing with few-shot datasets. The codes for this paper have been uploaded to https://github.com/yeaso/union-net .","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s11227-022-04930-5,en,Improved team learning-based grey wolf optimizer for optimization tasks and engineering problems,OriginalPaper,"Optimization refers to finding the optimal solution to minimize or maximize the objective function. In the field of engineering, this plays an important role in designing parameters and reducing manufacturing costs. Meta-heuristics such as the grey wolf optimizer (GWO) are efficient ways to solve optimization problems. However, the GWO suffers from premature convergence or low accuracy. In this study, a team learning-based grey wolf optimizer (TLGWO), which consists of two strategies, is proposed to overcome these shortcomings. The neighbor learning strategy introduces the influence of neighbors to improve the local search ability, whereas the random learning strategy provides new search directions to enhance global exploration. Four engineering problems with constraints and 21 benchmark functions were employed to verify the competitiveness of the TLGWO. The test results were compared with three derivatives of the GWO and nine other state-of-the-art algorithms. Furthermore, the experimental results were analyzed using the Friedman and mean absolute error statistical tests. The results show that the proposed TLGWO can provide superior solutions to the compared algorithms on most optimization tasks and solve engineering problems with constraints.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s00521-022-08055-x,en,An interior search algorithm based on chaotic and crossover strategies for parameter extraction of polyphase induction machines,"['OriginalPaper', 'Original Article']","The accuracy of the extracted parameters is important for studying the polyphase induction motor performance and/or the motor control schemes. An investigated and improved interior search algorithm (IISA) is presented in this study for extracting the optimal values of estimated parameters of six-phase and three-phase induction motors. This investigation was carried out on two polyphase induction motors as experimental research cases, utilizing features of manufacturer's operation. The estimated parameters show the high capability regarding the performance of the desired IISA optimizer. The performance of the proposed IISA is compared with different modern optimization algorithms including the basic ISA, and other state-of-the-art approaches. Experimental verifications are validated on two polyphase induction motors, called six-phase and three-phase induction motors. The obtained results show that the proposed method is very competitive in extracting the unknown parameters of different induction motor models with a high degree of closeness to the experimental records. Moreover, various statistical tests, such as the Wilcoxon rank test, stability analysis, and convergence analysis, have been conducted to justify the performance of the proposed IISA. From all the analyses, it has been revealed that the proposed IISA is a competitive method compared to other popular state-of-the-art competitors and ISA variant with accurately identified parameters.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1038/s41598-022-24485-y,en,Contrast phase recognition in liver computer tomography using deep learning,"['OriginalPaper', 'Article']","Hepatocellular carcinoma (HCC) has become the 4th leading cause of cancer-related deaths, with high social, economical and health implications. Imaging techniques such as multiphase computed tomography (CT) have been successfully used for diagnosis of liver tumors such as HCC in a feasible and accurate way and its interpretation relies mainly on comparing the appearance of the lesions in the different contrast phases of the exam. Recently, some researchers have been dedicated to the development of tools based on machine learning (ML) algorithms, especially by deep learning techniques, to improve the diagnosis of liver lesions in imaging exams. However, the lack of standardization in the naming of the CT contrast phases in the DICOM metadata is a problem for real-life deployment of machine learning tools. Therefore, it is important to correctly identify the exam phase based only on the image and not on the exam metadata, which is unreliable. Motivated by this problem, we successfully created an annotation platform and implemented a convolutional neural network (CNN) to automatically identify the CT scan phases in the HCFMUSP database in the city of São Paulo, Brazil. We improved this algorithm with hyperparameter tuning and evaluated it with cross validation methods. Comparing its predictions with the radiologists annotation, it achieved an accuracy of 94.6%, 98% and 100% in the testing dataset for the slice, volume and exam evaluation, respectively.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11042-022-14232-w,en,Cloud-based COVID-19 disease prediction system from X-Ray images using convolutional neural network on smartphone,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","COVID-19 has engulfed over 200 nations through human-to-human transmission, either directly or indirectly. Reverse Transcription-polymerase Chain Reaction (RT-PCR) has been endorsed as a standard COVID-19 diagnostic procedure but has caveats such as low sensitivity, the need for a skilled workforce, and is time-consuming. Coronaviruses show significant manifestation in Chest X-Ray (CX-Ray) images and, thus, can be a viable option for an alternate COVID-19 diagnostic strategy. An automatic COVID-19 detection system can be developed to detect the disease, thus reducing strain on the healthcare system. This paper discusses a real-time Convolutional Neural Network (CNN) based system for COVID-19 illness prediction from CX-Ray images on the cloud. The implemented CNN model displays exemplary results, with training accuracy being 99.94% and validation accuracy reaching 98.81%. The confusion matrix was utilized to assess the models’ outcome and achieved 99% precision, 98% recall, 99% F1 score, 100% training area under the curve (AUC) and 98.3% validation AUC. The same CX-Ray dataset was also employed to predict the COVID-19 disease with deep Convolution Neural Networks (DCNN), such as ResNet50, VGG19, InceptonV3, and Xception. The prediction outcome demonstrated that the present CNN was more capable than the DCNN models. The efficient CNN model was deployed to the Platform as a Service (PaaS) cloud.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40860-022-00196-z,en,Helix-HPSO approach for UAV path planning in a multi-building environment,"['OriginalPaper', 'Original Article']","Regular inspection of historic buildings is essential, while path planning of the building inspection is challenging because it requires comprehensive coverage at a low cost. Most of the previous research does not consider the multiple buildings’ environment. In this paper, a three-dimensional path planning approach is proposed to provide the inspection for multiple buildings. The proposed Helix-HPSO approach generates the helix-shaped path for each building and uses HPSO for path planning between buildings. The computational experiment validates the proposed approach. The helix-shaped path costs less than the traditional back-and-forth path for building inspection. HPSO is compared with other bio-inspired algorithms for optimization problems and PSO for path planning.","['Computer Science', 'Performance and Reliability', 'Software Engineering/Programming and Operating Systems', 'Artificial Intelligence', 'Simulation and Modeling', 'User Interfaces and Human Computer Interaction', 'Health Informatics']"
doi:10.1007/s11042-022-14187-y,en,Evolutionary neural architecture search based on efficient CNN models population for image classification,OriginalPaper,"The aim of this work is to search for a Convolutional Neural Network (CNN) architecture that performs optimally across all factors, including accuracy, memory footprint, and computing time, suitable for mobile devices. Although deep learning has evolved for use on devices with minimal resources, its implementation is hampered by that these devices are not designed to tackle complex tasks, such as CNN architectures. To address this limitation, a Network Architecture Search (NAS) strategy is considered, which employs a Multi-Objective Evolutionary Algorithm (MOEA) to create an efficient and robust CNN architecture by focusing on three objectives: fast processing times, reduced storage, and high accuracy. Furthermore, we proposed a new Efficient CNN Population Initialization (ECNN-PI) method that utilizes a combination of random and selected strong models to generate the first-generation population. To validate the proposed method, CNN models are trained using CIFAR-10, CIFAR-100, ImageNet, STL-10, FOOD-101, THFOOD-50, FGVC Aircraft, DTD, and Oxford-IIIT Pets benchmark datasets. The MOEA-Net algorithm outperformed other models on CIFAR-10, whereas MOEANet with the ECNN-PI method outperformed other models on CIFAR-10 and CIFAR-100. Furthermore, both the MOEA-Net algorithm and MOEA-Net with the ECNN-PI method outperformed DARTS, P-DARTS, and Relative-NAS for small-scale multi-class and fine-grained datasets.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s40747-022-00909-0,en,ST-MAE: robust lane detection in continuous multi-frame driving scenes based on a deep hybrid network,"['OriginalPaper', 'Original Article']","Lane detection is one of the key techniques to realize advanced driving assistance and automatic driving. However, lane detection networks based on deep learning have significant shortcomings. The detection results are often unsatisfactory when there are shadows, degraded lane markings, and vehicle occlusion lanes. Therefore, a continuous multi-frame image sequence lane detection network is proposed. Specifically, the continuous six-frame image sequence is input into the network, in which the scene information of each frame image is extracted by an encoder composed of Swin Transformer blocks and input into the PredRNN. Continuous multi-frame of the driving scene is modeled as time-series by ST-LSTM blocks, and then, the shape changes and motion trajectory in the spatiotemporal sequence are effectively modeled. Finally, through the decoder composed of Swin Transformer blocks, the features are obtained and reconstructed to complete the detection task. Extensive experiments on two large-scale datasets demonstrate that the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations. Experiments are carried out based on the TuSimple dataset. The results show: for easy scenes, the validation accuracy is 97.46%, the test accuracy is 97.37%, and the precision is 0.865. For complex scenes, the validation accuracy is 97.38%, the test accuracy is 97.29%, and the precision is 0.859. The running time is 4.4 ms. Experiments are carried out based on the CULane dataset. The results show that, for easy scenes, the validation accuracy is 97.03%, the test accuracy is 96.84%, and the precision is 0.837. For complex scenes, the validation accuracy is 96.18%, the test accuracy is 95.92%, and the precision is 0.829. The running time is 6.5 ms.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s40747-022-00893-5,en,GAN-based deep learning framework of network reconstruction,"['OriginalPaper', 'Original Article']","Inferring the topology of a network from network dynamics is a significant problem with both theoretical research significance and practical value. This paper considers how to reconstruct the network topology according to the continuous-time data on the network. Inspired by the generative adversarial network(GAN), we design a deep learning framework based on network continuous-time data. The framework predicts the edge connection probability between network nodes by learning the correlation between network node state vectors. To verify the accuracy and adaptability of our method, we conducted extensive experiments on scale-free networks and small-world networks at different network scales using three different dynamics: heat diffusion dynamics, mutualistic interaction dynamics, and gene regulation dynamics. Experimental results show that our method significantly outperforms the other five traditional correlation indices, which demonstrates that our method can reconstruct the topology of different scale networks well under different network dynamics.","['Engineering', 'Computational Intelligence', 'Complexity', 'Data Structures and Information Theory']"
doi:10.1007/s12145-022-00903-7,en,CleverRiver: an open source and free Google Colab toolkit for deep-learning river-flow models,"['OriginalPaper', 'Software']","In a period in which climate change is significantly varying rainfall regimes and their intensity all over the world, river-flow prediction is a major concern of geosciences. In recent years there has been an increase in the use of deep-learning models for river-flow prediction. However, in this field we can observe two main issues: i) many case studies use similar (or the same) strategies without sharing the codes, and ii) the application of these techniques requires good computer knowledge. This work proposes to employ a Google Colab notebook called CleverRiver, which allows the application of deep-learning for river-flow predictions. CleverRiver is a dynamic software that can be upgraded and modified not only by the authors but also by the users. The main advantages of CleverRiver are the following: the software is not limited by the client hardware, operating systems, etc.; the code is open-source; the toolkit is integrated with user-friendly interfaces; updated releases with new architectures, data management, and model parameters will be progressively uploaded. The software consists of three sections: the first one enables to train the models by means of some architectures, parameters, and data; the second section allows to create predictions by using the trained models; the third section allows to send feedback and to share experiences with the authors, providing a flux of precious information able to improve scientific research.","['Earth Sciences', 'Earth Sciences, general', 'Information Systems Applications (incl.Internet)', 'Simulation and Modeling', 'Ontology', 'Space Sciences (including Extraterrestrial Physics, Space Exploration and Astronautics)', 'Earth System Sciences']"
doi:10.1007/s13272-022-00622-1,en,An explanatory approach to modeling the fleet assignment in the global air transportation system,"['OriginalPaper', 'Original Paper']","Airlines’ fleet assignment heavily affects the economic and ecological performance of the global air transportation system (ATS). Consequently, it is inevitable to include potential changes of the fleet assignment when modeling and assessing future global ATS scenarios. Therefore, this article presents a novel explanatory approach to modeling the fleet assignment in the global ATS. The presented approach is based on formulating and solving an optimization problem, which describes the fleet assignment in the ATS through a suitable combination of objective function and constraints. While the objective function combines both the airline and the passenger perspective on the fleet assignment, the constraints include additional operational and technological aspects. In comparison to the available global fleet assignment models in the literature, which rely on statistical approaches, the advantages of the presented approach via an optimization problem lie in the overall scenario capability and the consideration of explicit aircraft types instead of simplifying seat categories. To calibrate and validate our model, we use 10 years of historic flight schedule data. The results underline the strengths and weaknesses of the presented approach and indicate potential for future improvement.","['Engineering', 'Aerospace Technology and Astronautics']"
doi:10.1007/s00158-022-03453-0,en,Wing jig shape optimisation with gradient-assisted metamodel building in a trust-region optimisation framework,"['OriginalPaper', 'Research Paper']","Significant computational resources are required to obtain an optimised wing jig shape by solving a high-fidelity large-scale aero-structural design optimisation problem. Gradient-based methods are efficient; however, some of the features of real-life engineering problems including numerical noise that pollutes the function values and occurrences of failed evaluations in the optimisation may limit their performance. To address these issues, this paper presents the latest developments in the multipoint approximation method (MAM) based on a gradient-assisted metamodel assembly technique within a trust-region optimisation framework. The proposed method is tested by a benchmark case first, and then, an aircraft wing jig shape optimisation problem is offered to demonstrate its performance. The gradient-based optimisation is used as a benchmark case, and the metamodel-based optimisation utilises the latest developments in MAM to solve the same problem. The results show that the proposed method can achieve the same design goal as the gradient-based method but with enhanced robustness and efficient performance. In the wing jig shape optimisation, the difference in the design objective, the global equivalent drag coefficient, between the two aforementioned optimisation approaches is 0.20 counts, whose relative difference is approximately 0.10%. Three approximate sub-optimisations have been conducted in every iteration of the metamodel-based optimisation to reduce the possibility of local optimality, while the overall elapsed time of the metamodel-based optimisation is approximately 1.98 times that of one gradient-based optimisation, which confirms the competitiveness of the proposed method bearing in mind the added safeguards for numerical noise, failed evaluations and possible local optimality.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1038/s41598-022-24836-9,en,Multi-scale fusion for RGB-D indoor semantic segmentation,"['OriginalPaper', 'Article']","In computer vision, convolution and pooling operations tend to lose high-frequency information, and the contour details will also disappear with the deepening of the network, especially in image semantic segmentation. For RGB-D image semantic segmentation, all the effective information of RGB and depth image can not be used effectively, while the form of wavelet transform can retain the low and high frequency information of the original image perfectly. In order to solve the information losing problems, we proposed an RGB-D indoor semantic segmentation network based on multi-scale fusion: designed a wavelet transform fusion module to retain contour details, a nonsubsampled contourlet transform to replace the pooling operation, and a multiple pyramid module to aggregate multi-scale information and context global information. The proposed method can retain the characteristics of multi-scale information with the help of wavelet transform, and make full use of the complementarity of high and low frequency information. As the depth of the convolutional neural network increases without losing the multi-frequency characteristics, the segmentation accuracy of image edge contour details is also improved. We evaluated our proposed efficient method on commonly used indoor datasets NYUv2 and SUNRGB-D, and the results showed that we achieved state-of-the-art performance and real-time inference.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07622-7,en,A novel methodology for the planning of charging infrastructure in the scenario of high EV penetration,"['OriginalPaper', 'Optimization']","This article presents a novel methodology for distribution network expansion planning (DNEP) considering the inclusion of electric vehicles (EVs), especially, electric bus (EB) charging loads. The proposed methodology addresses network congestion through an optimum time of charging, cost optimization, new charging infrastructure, and minimization of losses under a set of technical and physical constraints, which represents practical uncertainties. Along with load flow analysis, selection of the number of ports and technology at the host charging station is obtained through the application of response surface methodology. The proposed methodology provides coordinated planning for the development of EB charging station infrastructure that takes into account the effects of both the power dispersion framework and transportation framework. The effectiveness of the proposed methodology is investigated by applying it to the 69-node IEEE modified distribution test system considering three charging technologies, viz. fast charging, ultra-fast charging, and battery swapping. The results of the proposed model are compared with the direct statistical method, and it revealed that the right selection of technology for EB charging and the right planning of the charging infrastructure can effectively optimize the cost of EV charging infrastructure and thereby catalyze the decarbonization of the transportation sector.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11042-022-14124-z,en,Optimized face-emotion learning using convolutional neural network and binary whale optimization,OriginalPaper,"Human emotion detection using facial expressions might be easy for humans, but computing technology to accomplish the same task is more challenging. We can recognize emotions from images using the latest computer vision and machine learning (ML) advancements. This research proposes a novel optimized face emotion learning method with binary whale optimization (OFELBW). The OFELBW is implemented in three phases, the first phase with a convolutional neural network (CNN) in which from the image the background noise is removed in the initial phase, and the facial feature extraction is performed in the second phase. Finally, the binary whale optimization algorithm is used for the feature selection to obtain the most relevant feature subset. The proposed OFELBW method was examined with more than 750 K images using SFEW, CK+, JAFFE, and FERG datasets. We have compared our proposed OFELBW model with other existing techniques to examine the accuracy of our models with the above-mentioned datasets and received an accuracy of 98.35% with the CK+ dataset, 99.42% with the FERG dataset, 96.6% with the JAFFE dataset and 64.98% with the SFEW with 80% training, 10% testing, and 10% validation set. This technique will be useful in various applications such as human social/physiological interaction systems, mental disease diagnosis and military environment, etc.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s13369-022-07458-1,en,An Integrated Two-Stage Inventory and Workforce Planning Model with Variable Production Rates,"['OriginalPaper', 'Research Article-Systems Engineering ']","This paper addresses the economic lot-sizing and manpower planning in a two-stage manufacturing system. We refer to the first and second stages as the vendor and the manufacturer, respectively. Both the vendor and the manufacturer can operate at different production rates by varying the levels of their manpower. In Economics and Econometrics, the rate of production is usually modelled using the production function, a model which links company productivity to the level of existing resources, such as labour and capital. Namely, we utilize the Cobb–Douglas production function to represent the relationship between the production inputs (labour and capital) and the productivity. The impact of manpower planning decisions on the proposed supply chain system is studied. An efficient solution technique is proposed; then, sensitivity analysis and fractional factorial design were conducted to assess the performance of the proposed model. Our results suggest that the variable production rates may lead to substantial savings to the supply chain, especially in the case of low-profit margins. Furthermore, considering the vendor as a leader and the manufacturer as a follower in a Stackelberg sequential game, we present two contracts to share the resulting savings between the vendor and the manufacturer.","['Engineering', 'Engineering, general', 'Science, Humanities and Social Sciences, multidisciplinary']"
doi:10.1186/s12889-022-14642-3,en,"Comparison of ARIMA model, DNN model and LSTM model in predicting disease burden of occupational pneumoconiosis in Tianjin, China","['OriginalPaper', 'Research']","Background This study aims to explore appropriate model for predicting the disease burden of pneumoconiosis in Tianjin by comparing the prediction effects of Autoregressive Integrated Moving Average (ARIMA) model, Deep Neural Networks (DNN) model and multivariate Long Short-Term Memory Neural Network (LSTM) models. Methods Disability adjusted life year (DALY) was used to evaluate the disease burden of occupational pneumoconiosis. ARIMA model, DNN model and multivariate LSTM model were used to establish prediction model. Three performance evaluation metrics including Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) were used to compare the prediction effects of the three models. Results From 1990 to 2021, there were 10,694 cases of pneumoconiosis patients in Tianjin, resulting in a total of 112,725.52 person-years of DALY. During this period, the annual DALY showed a fluctuating trend, but it had a strong correlation with the number of pneumoconiosis patients, the average age of onset, the average age of receiving dust and the gross industrial product, and had a significant nonlinear relationship with them. The comparison of prediction results showed that the performance of multivariate LSTM model and DNN model is much better than that of traditional ARIMA model. Compared with the DNN model, the multivariate LSTM model performed better in the training set, showing lower RMES (42.30 vs. 380.96), MAE (29.53 vs. 231.20) and MAPE (1.63% vs. 2.93%), but performed less stable than the DNN on the test set, showing slightly higher RMSE (1309.14 vs. 656.44), MAE (886.98 vs. 594.47) and MAPE (36.86% vs. 22.43%). Conclusion The machine learning techniques of DNN and LSTM are an innovative method to accurately and efficiently predict the burden of pneumoconiosis with the simplest data. It has great application prospects in the monitoring and early warning system of occupational disease burden.","['Medicine & Public Health', 'Public Health', 'Medicine/Public Health, general', 'Epidemiology', 'Environmental Health', 'Biostatistics', 'Vaccine']"
doi:10.1007/s42107-022-00534-9,en,Salp swarm optimization of active tuned mass dampers in asymmetric plan buildings including SSI effects,"['OriginalPaper', 'Research']","Active tuned mass dampers (ATMDs) device is used for the structures where a mass is exited from the platform to create the structure to reduce vibrations. Active tuned mass dampers are generally used in many fields due to their high strength and vitality. Furthermore, the predicted effects of soil-structure interactions (SSI) on the machine structure are of paramount importance and should be considered sufficient for the best ATMD project not to be forgotten. This study aims to find the optimum parameters in ATMD design with the effects of SSI using mathematical and simulation approaches. To reduce more extreme transmission and speed up the story, we expand the plan factors, e.g. ATMD mass, damping coefficient and spring strength. Here, deep neural network (DNN) is used for find the optimum parameters; in this DNN the weights are optimized using opposition-based salp swarm optimization (OSSA). The optimized parameter for ATMD decreased vibrations of trembling high structures, including SSI effects. Improvement approaches are updated in the operational basis of MATLAB programming.","['Engineering', 'Civil Engineering', 'Building Materials', 'Sustainable Architecture/Green Buildings']"
doi:10.1186/s12951-022-01710-4,en,A modular and self-adjuvanted multivalent vaccine platform based on porcine circovirus virus-like nanoparticles,"['OriginalPaper', 'Research']","Background Virus-like particles (VLPs) are supramolecular structures composed of multiple protein subunits and resemble natural virus particles in structure and size, making them highly immunogenic materials for the development of next-generation subunit vaccines. The orderly and repetitive display of antigenic epitopes on particle surface allows efficient recognition and cross-link by B cell receptors (BCRs), thereby inducing higher levels of neutralizing antibodies and cellular immune responses than regular subunit vaccines. Here, we present a novel multiple antigen delivery system using SpyCatcher/Spytag strategy and self-assembled VLPs formed by porcine circovirus type 2 (PCV2) Cap, a widely used swine vaccine in solo. Results Cap-SC, recombinant Cap with a truncated SpyCatcher polypeptide at its C-terminal, self-assembled into 26-nm VLPs. Based on isopeptide bonds formed between SpyCatcher and SpyTag, classical swine fever virus (CSFV) E2, the antigen of interest, was linked to SpyTag and readily surface-displayed on SpyCatcher decorated Cap-SC via in vitro covalent conjugation. E2-conjugated Cap VLPs (Cap-E2 NPs) could be preferentially captured by antigen presenting cells (APCs) and effectively stimulate APC maturation and cytokine production. In vivo studies confirmed that Cap-E2 NPs elicited an enhanced E2 specific IgG response, which was significantly higher than soluble E2, or the admixture of Cap VLPs and E2. Moreover, E2 displayed on the surface did not mask the immunodominant epitopes of Cap-SC VLPs, and Cap-E2 NPs induced Cap-specific antibody levels and neutralizing antibody levels comparable to native Cap VLPs. Conclusion These results demonstrate that this modularly assembled Cap-E2 NPs retains the immune potential of Cap VLP backbone, while the surface-displayed antigen significantly elevated E2-induced immune potency. This immune strategy provides distinctly improved efficacy than conventional vaccine combination. It can be further applied to the development of dual or multiple nanoparticle vaccines to prevent co-infection of PCV2 and other swine pathogens.","['Chemistry', 'Biotechnology', 'Nanotechnology', 'Molecular Medicine']"
doi:10.1007/s11277-022-10093-6,en,Efficient Routing Protocol for Optimal Route Selection in Cognitive Radio Networks Over IoT Environment,OriginalPaper,"In recent times, Cognitive Radio Networks (CRNS) have been broadly investigated in light of the unceasingly growing demands of the Internet of Things (IoT) applications, paving the path to equipping IoT objects with Cognitive Radio (CR) technology. Thus, the implementation of these two technologies in unison has been attracting research interests. Today, CR technology is implemented in Ad-Hoc Networks (AHNs). This combination has several benefits, namely better coverage, lower costs, and simpler maintenance compared to infrastructure-based networks. Nonetheless, a limited number of researchers have lately paid attention to the area of Quality of Service (QoS) for being one of the key routing metrics adopted to define the optimal paths for the Cognitive Radio Ad Hoc Network (CRAHN). Accordingly, this work recommends the stability of a route in CRAHN and proposes Half -Duplex (HD) CRAHN routing protocol without Common Control Channel (CCC) using the control packets’ multicast transmission, referred to as Stability Based Multipath Downstream Quality Routing Protocol (SMDQRP). Interestingly, many CRN overheads have been reduced in the proposed routing protocol such as avoiding the overhead of transmitting the Route Request Packet (RRQP) over all channels and nodes. To be precise, we allow each node to transmit over all of its available channels instead of using all channels in the network. Further, every necessary calculation is made while the RRQP is being sent from the source to the destination not vice versa. Data transmission in HD mode with path recovery is used to test the proposed protocol, along with a compound accumulative routing metric named Stability Based Multipath Downstream Quality (SMDQ) to select the required QoS paths featured with the highest level of stability and maximal throughput. The path is recovered in data transmission by allowing the nodes in the path to use the already saved available channels between every two consecutive nodes in case of failure of any suggested channel. Moreover, a unique sensing technique based on energy level extracted from the received signals at a CR node along with waveform-based detection is factored in for their suitability to our proposed routing protocol as we aim to solve the problems in Multi-Cast-based Half Duplex Routing Protocol (MC-HDRP). The performance of the newly presented protocol is assessed and compared with the most relevant protocols called Probabilistic and Deterministic Path Selection (PDPS) and MC-HDRP by conducting several related simulation experiments and scenarios with the use of a special technologically advanced simulator based on Java language. Contrary to the MC-HDRP, the simulation’s results demonstrate an explicit significant improvement related to throughput, packet drop ratio, and the number of disconnected networks.","['Engineering', 'Communications Engineering, Networks', 'Signal,Image and Speech Processing', 'Computer Communication Networks']"
doi:10.1007/s00500-022-07645-0,en,"Optimal decision making, using interval uncertainty techniques, of a production-inventory model under warranty-linked demand and carbon tax regulations","['OriginalPaper', 'Soft computing in decision making and in modeling in economics']","The concepts of generalized Hukuhara difference and interval differential equation play important role in the theory of interval uncertainty. These concepts have many applications in different branches of research, viz. optimization, information theory, inventory control and others. The goal of this work is to study an application of Hukuhara difference and interval differential equation in inventory management. In this paper, an inventory model for imperfect production process under warranty-dependent demand and carbon tax regulatory mechanism, is presented with the help of Hukuhara difference and interval differential equation. Also, using the interval arithmetic, the generalized Hukuhara difference, and the existence and uniqueness theorem of interval differential equation, the corresponding average profit function of this model is obtained. In order to maximize the average profit, a center-radius optimization technique is proposed. Some numerical examples are considered and solved by using different variant of quantum-behaved particle swarm optimization algorithms.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11263-022-01712-7,en,Domain-Specific Bias Filtering for Single Labeled Domain Generalization,OriginalPaper,"Conventional Domain Generalization (CDG) utilizes multiple labeled source datasets to train a generalizable model for unseen target domains. However, due to expensive annotation costs, the requirements of labeling all the source data are hard to be met in real-world applications. In this paper, we investigate a Single Labeled Domain Generalization (SLDG) task with only one source domain being labeled, which is more practical and challenging than the CDG task. A major obstacle in the SLDG task is the discriminability-generalization bias: the discriminative information in the labeled source dataset may contain domain-specific bias, constraining the generalization of the trained model. To tackle this challenging task, we propose a novel framework called Domain-Specific Bias Filtering (DSBF), which initializes a discriminative model with the labeled source data and then filters out its domain-specific bias with the unlabeled source data for generalization improvement. We divide the filtering process into (1) feature extractor debiasing via k-means clustering-based semantic feature re-extraction and (2) classifier rectification through attention-guided semantic feature projection. DSBF unifies the exploration of the labeled and the unlabeled source data to enhance the discriminability and generalization of the trained model, resulting in a highly generalizable model. We further provide theoretical analysis to verify the proposed domain-specific bias filtering process. Extensive experiments on multiple datasets show the superior performance of DSBF in tackling both the challenging SLDG task and the CDG task.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s11682-022-00748-0,en,Automatic diagnosis of late-life depression by 3D convolutional neural networks and cross-sample Entropy analysis from resting-state fMRI,"['OriginalPaper', 'Original Research']","Resting-state fMRI has been widely used in investigating the pathophysiology of late-life depression (LLD). Unlike the conventional linear approach, cross-sample entropy (CSE) analysis shows the nonlinear property in fMRI signals between brain regions. Moreover, recent advances in deep learning, such as convolutional neural networks (CNNs), provide a timely application for understanding LLD. Accurate and prompt diagnosis is essential in LLD; hence, this study aimed to combine CNN and CSE analysis to discriminate LLD patients and non-depressed comparison older adults based on brain resting-state fMRI signals. Seventy-seven older adults, including 49 patients and 28 comparison older adults, were included for fMRI scans. Three-dimensional CSEs with volumes corresponding to 90 seed regions of interest of each participant were developed and fed into models for disease classification and depression severity prediction. We obtained a diagnostic accuracy > 85% in the superior frontal gyrus (left dorsolateral and right orbital parts), left insula, and right middle occipital gyrus. With a mean root-mean-square error (RMSE) of 2.41, three separate models were required to predict depressive symptoms in the severe, moderate, and mild depression groups. The CSE volumes in the left inferior parietal lobule, left parahippocampal gyrus, and left postcentral gyrus performed best in each respective model. Combined complexity analysis and deep learning algorithms can classify patients with LLD from comparison older adults and predict symptom severity based on fMRI data. Such application can be utilized in precision medicine for disease detection and symptom monitoring in LLD.","['Biomedicine', 'Neurosciences', 'Neuroradiology', 'Neuropsychology', 'Psychiatry']"
doi:10.1007/s40430-022-03916-x,en,Fault detection in rotating elements by using fuzzy integrated improved local binary pattern method,"['OriginalPaper', 'Technical Paper']","An infrared thermography method is a promising tool for defect detection in rotating machines, as this approach is a non-intrusive and no-contact kind of approach. Although the performance of infrared thermography is limited by strong noise signals and the irrelevant information found in infrared images. This issue can be efficiently addressed by using an image segmentation process that can enhance feature extraction in infrared thermography image analysis. In this paper, an image segmentation process named fuzzy integrated improved local binary pattern (ILBP-Fuzzy) for fault detection is proposed to enhance feature extraction in the thermography images. In the ILBP-Fuzzy method, the thermal image is first converted into a grey-scale image and thereafter, a median filter is applied to make the image noise-free. Later on, the region of interest is identified and the fault is detected by the application of the suggested ILBP-Fuzzy approach. In this work, two cases are performed. The first one is on synthetic images and the other one is on thermography images. In case 1, a synthetic image (star image) is used to evaluate the effectiveness of the edge detection method. The outcomes from the suggested approach are compared with other methods and various parameters such as accuracy, jacquard similarity index, sensitivity, dice similarity index, and specificity are calculated. In case 2, the proposed method is tested on three types of industrial thermo-graphic images of bearings named as healthy, fault-initialized, and unhealthy for estimating the capability of the ILBP-Fuzzy approach. From the results, it is evident that the ILBP-Fuzzy approach provides superior performance to identify the conditions of the rotating elements in a machine as compared to other segmentation methods such as IHLBP, Sobel, Canny, Laplace of Gaussian, Otsu and LTIHLBP method.","['Engineering', 'Mechanical Engineering']"
doi:10.1007/s11235-022-00973-4,en,Beamforming based algorithm for 5G applications,OriginalPaper,"5G applications such as the Internet of Things, high-resolution video streaming, robotic cars, smart cities, and telehealth care have a universal presence nowadays. These applications mandate higher data rates, large bandwidth, increased capacity, low latency and high throughput. The key element to meet this demand is to apply efficient frequency reuse and scheduling strategies based on adaptive beamforming. Employing efficient frequency reuse and scheduling techniques based on adaptive beamforming will significantly increase the cellular system capacity by eighteen times and substantially reduce the consumption power levels and interference. In cellular networks, the performance of the adaptive beamforming algorithms is severely degraded by the presence of interfering signals. In this paper, we introduce a beamforming-based algorithm for 5G applications named Direction Finding for Beamforming and Synthesizing. This algorithm combines the Direction of Arrival (DOA), adaptive beamforming, and radiation pattern synthesizing. The proposed algorithm uses the DOA technique to feed the adaptive beamforming algorithms with estimations of the desired user direction, desired user signal, and the interfering signals with their directions as initial values. In addition, we use the adaptive beamforming process to supply the radiation pattern synthesizing algorithms with an initial radiation pattern, and the required positions of nulls. At the beamformer output, we evaluate our proposed mechanism in terms of error convergence, tracking capabilities, and the obtained radiation pattern characteristics. At the synthesizer output, we carry out analysis in terms of the convergence speed and the resultant radiation pattern attributes to investigate the efficiency of the proposed algorithm. The simulation results show that our proposed algorithm has significantly fast convergence, reliable tracking capabilities, and radiation patterns with very low Side Lobe Levels.","['Business and Management', 'IT in Business', 'Computer Communication Networks', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes']"
doi:10.1038/s41587-022-01527-4,en,Drag-and-drop genome insertion of large sequences without double-strand DNA cleavage using CRISPR-directed integrases,"['OriginalPaper', 'Article']","Programmable genome integration of large, diverse DNA cargo without DNA repair of exposed DNA double-strand breaks remains an unsolved challenge in genome editing. We present programmable addition via site-specific targeting elements (PASTE), which uses a CRISPR–Cas9 nickase fused to both a reverse transcriptase and serine integrase for targeted genomic recruitment and integration of desired payloads. We demonstrate integration of sequences as large as ~36 kilobases at multiple genomic loci across three human cell lines, primary T cells and non-dividing primary human hepatocytes. To augment PASTE, we discovered 25,614 serine integrases and cognate attachment sites from metagenomes and engineered orthologs with higher activity and shorter recognition sequences for efficient programmable integration. PASTE has editing efficiencies similar to or exceeding those of homology-directed repair and non-homologous end joining-based methods, with activity in non-dividing cells and in vivo with fewer detectable off-target events. PASTE expands the capabilities of genome editing by allowing large, multiplexed gene insertion without reliance on DNA repair pathways. Large sequences are integrated site specifically into the human genome without double-strand DNA cleavage.","['Life Sciences', 'Life Sciences, general', 'Biotechnology', 'Biomedicine, general', 'Agriculture', 'Biomedical Engineering/Biotechnology', 'Bioinformatics']"
doi:10.1038/s41598-022-24541-7,en,Interpretable brain disease classification and relevance-guided deep learning,"['OriginalPaper', 'Article']","Deep neural networks are increasingly used for neurological disease classification by MRI, but the networks’ decisions are not easily interpretable by humans. Heat mapping by deep Taylor decomposition revealed that (potentially misleading) image features even outside of the brain tissue are crucial for the classifier’s decision. We propose a regularization technique to train convolutional neural network (CNN) classifiers utilizing relevance-guided heat maps calculated online during training. The method was applied using T1-weighted MR images from 128 subjects with Alzheimer’s disease (mean age = 71.9 ± 8.5 years) and 290 control subjects (mean age = 71.3 ± 6.4 years). The developed relevance-guided framework achieves higher classification accuracies than conventional CNNs but more importantly, it relies on less but more relevant and physiological plausible voxels within brain tissue. Additionally, preprocessing effects from skull stripping and registration are mitigated. With the interpretability of the decision mechanisms underlying CNNs, these results challenge the notion that unprocessed T1-weighted brain MR images in standard CNNs yield higher classification accuracy in Alzheimer’s disease than solely atrophy.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s12880-022-00933-z,en,Use of multimodal dataset in AI for detecting glaucoma based on fundus photographs assessed with OCT: focus group study on high prevalence of myopia,"['OriginalPaper', 'Research']","Background Glaucoma is one of the major causes of blindness; it is estimated that over 110 million people will be affected by glaucoma worldwide by 2040. Research on glaucoma detection using deep learning technology has been increasing, but the diagnosis of glaucoma in a large population with high incidence of myopia remains a challenge. This study aimed to provide a decision support system for the automatic detection of glaucoma using fundus images, which can be applied for general screening, especially in areas of high incidence of myopia. Methods A total of 1,155 fundus images were acquired from 667 individuals with a mean axial length of 25.60 ± 2.0 mm at the National Taiwan University Hospital, Hsinchu Br. These images were graded based on the findings of complete ophthalmology examinations, visual field test, and optical coherence tomography into three groups: normal (N, n = 596), pre-perimetric glaucoma (PPG, n = 66), and glaucoma (G, n = 493), and divided into a training-validation (N: 476, PPG: 55, G: 373) and test (N: 120, PPG: 11, G: 120) sets. A multimodal model with the Xception model as image feature extraction and machine learning algorithms [random forest (RF), support vector machine (SVM), dense neural network (DNN), and others] was applied. Results The Xception model classified the N, PPG, and G groups with 93.9% of the micro-average area under the receiver operating characteristic curve (AUROC) with tenfold cross-validation. Although normal and glaucoma sensitivity can reach 93.51% and 86.13% respectively, the PPG sensitivity was only 30.27%. The AUROC increased to 96.4% in the N + PPG and G groups. The multimodal model with the N + PPG and G groups showed that the AUROCs of RF, SVM, and DNN were 99.56%, 99.59%, and 99.10%, respectively; The N and PPG + G groups had less than 1% difference. The test set showed an overall 3%–5% less AUROC than the validation results. Conclusion The multimodal model had good AUROC while detecting glaucoma in a population with high incidence of myopia. The model shows the potential for general automatic screening and telemedicine, especially in Asia. Trial registration : The study was approved by the Institutional Review Board of the National Taiwan University Hospital, Hsinchu Branch (no. NTUHHCB 108-025-E).","['Medicine & Public Health', 'Imaging / Radiology']"
doi:10.1007/s12559-022-10069-5,en,Stein Variational Gradient Descent with Multiple Kernels,OriginalPaper,"Bayesian inference is an important research area in cognitive computation due to its ability to reason under uncertainty in machine learning. As a representative algorithm, Stein variational gradient descent (SVGD) and its variants have shown promising successes in approximate inference for complex distributions. In practice, we notice that the kernel used in SVGD-based methods has a decisive effect on the empirical performance. Radial basis function (RBF) kernel with median heuristics is a common choice in previous approaches, but unfortunately, this has proven to be sub-optimal. Inspired by the paradigm of Multiple Kernel Learning (MKL), our solution to this flaw is using a combination of multiple kernels to approximate the optimal kernel, rather than a single one which may limit the performance and flexibility. Specifically, we first extend Kernelized Stein Discrepancy (KSD) to its multiple kernels view called Multiple Kernelized Stein Discrepancy (MKSD) and then leverage MKSD to construct a general algorithm Multiple Kernel SVGD (MK-SVGD). Further, MK-SVGD can automatically assign a weight to each kernel without any other parameters, which means that our method not only gets rid of optimal kernel dependence but also maintains computational efficiency. Experiments on various tasks and models demonstrate that our proposed method consistently matches or outperforms the competing methods.","['Computer Science', 'Artificial Intelligence', 'Computation by Abstract Devices', 'Artificial Intelligence', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11263-022-01710-9,en,PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection,OriginalPaper,"3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for 3D object detection on point clouds. First, we propose a novel 3D detector, PV-RCNN, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e. , the voxel-to-keypoint scene encoding and the keypoint-to-grid RoI feature abstraction. Second, we propose an advanced framework, PV-RCNN++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and VectorPool aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our PV-RCNN++ is about $$3\times $$ 3 × faster than PV-RCNN, while also achieving better performance. The experiments demonstrate that our proposed PV-RCNN++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 FPS inference speed on the detection range of $$150m \times 150m$$ 150 m × 150 m .","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1007/s10489-022-04230-8,en,A new perspective for understanding generalization gap of deep neural networks trained with large batch sizes,OriginalPaper,"Deep neural networks (DNNs) are typically optimized using various forms of mini-batch gradient descent algorithm. A major motivation for mini-batch gradient descent is that with a suitably chosen batch size, available computing resources can be optimally utilized (including parallelization) for fast model training. However, many works report the progressive loss of model generalization when the training batch size is increased beyond some limits. This is a scenario commonly referred to as generalization gap. Although several works have proposed different methods for alleviating the generalization gap problem, a unanimous account for understanding generalization gap is still lacking in the literature. This is especially important given that recent works have observed that several proposed solutions for generalization gap problem such learning rate scaling and increased training budget do not indeed resolve it. As such, our main exposition in this paper is to investigate and provide new perspectives for the source of generalization loss for DNNs trained with a large batch size. Our analysis suggests that large training batch size results in increased near-rank loss of units’ activation (i.e. output) tensors, which consequently impacts model optimization and generalization. Extensive experiments are performed for validation on popular DNN models such as VGG-16, residual network (ResNet-56) and LeNet-5 using CIFAR-10, CIFAR-100, Fashion-MNIST and MNIST datasets.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s42979-022-01494-2,en,Introducing a New High-Resolution Handwritten Digits Data Set with Writer Characteristics,"['OriginalPaper', 'Original Research']","The contributions in this article are two-fold. First, we introduce a new handwritten digit data set that we collected. It contains high-resolution images of handwritten digits together with various writer characteristics which are not available in the well-known MNIST database. The multiple writer characteristics gathered are a novelty of our data set and create new research opportunities. The data set is publicly available online. Second, we analyse this new data set. We begin with simple supervised tasks. We assess the predictability of the writer characteristics gathered, the effect of using some of those characteristics as predictors in classification task and the effect of higher resolution images on classification accuracy. We also explore semi-supervised applications; we can leverage the high quantity of handwritten digits data sets already existing online to improve the accuracy of various classifications task with noticeable success. Finally, we also demonstrate the generative perspective offered by this new data set; we are able to generate images that mimics the writing style of specific writers. The data set has unique and distinct features and our analysis establishes benchmarks and showcases some of the new opportunities made possible with this new data set.","['Computer Science', 'Computer Science, general', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems', 'Data Structures and Information Theory', 'Information Systems and Communication Service', 'Computer Imaging, Vision, Pattern Recognition and Graphics']"
doi:10.1007/s10462-022-10309-y,en,A new hyper-heuristic based on ant lion optimizer and Tabu search algorithm for replica management in cloud environment,OriginalPaper,"Information can be shared across the Internet using cloud computing, a powerful paradigm for meeting the needs of individuals and organizations. To minimize access time and maximize load balancing for data nodes (DNs), a dynamic data replication algorithm is necessary. Even so, few of the existing algorithms consider each objective holistically during replication. An improved ant lion optimizer (ALO) algorithm and a fuzzy system are used in this paper to determine dynamically the number of replicas and the DNs for replication. Further, it balances the trade-offs among different objectives (e.g., service time, system availability, load, and monetary cost). The ALO algorithm has been widely applied to solve complex optimization problems due to its simplicity in implementation. However, ALO has premature convergence and can thus easily get trapped into the local optimum solution. In this paper, to overcome the shortcomings of ALO by balancing exploration and exploitation, a hybrid ant lion optimizer with Tabu search algorithm (ALO-Tabu) is proposed. There are several improvements of the ALO, in which the appropriate solutions are selected for the initial population based on chaotic maps (CMs) and opposition-based learning (OBL) strategies. On the other hand, there are many CMs, OBLs, and random walk strategies that make it difficult to select the best one for optimization. Generally, they are selected manually, which is time-consuming. As a result, this paper presents a hyper-heuristic ALO (HH-ALO-Tabu) that automatically chooses CMs, OBLs, and random walk strategies depending on the differential evolution (DE) algorithm. Based on 20 well-known test functions, the experiment results and statistical tests show that HH-ALO-Tabu can solve optimization problems effectively.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s10462-022-10324-z,en,Chaotic electromagnetic field optimization,OriginalPaper,"The search process in population-based metaheuristic algorithms (MAs) can be classified into two primary behaviours: diversification and intensification. In diversification behaviour, the search space will be explored considerably based on randomization. Whereas intensification alludes to the search for a promising region locally. The success of MAs relies on the balance between two search behaviours. Nonetheless, it is strenuous to get the right balance between these behaviours due to the scholastic nature of MAs. Chaotic maps are proven an excellent tool to enhance both behaviours. This work incorporates the Logistic chaotic map into the recently proposed population-based MA called Electromagnetic field optimization (EFO). This suggested algorithm is named chaotic EFO (CEFO). An improved diversification step with chaos in EFO is presented to efficiently control the global search and convergence to the global best solution. CEFO is tested on different case studies, 40 unconstrained CEC 2014 and CEC 2019 benchmark functions, seven real-world nonlinear systems and three mechanical engineering design frameworks. All experiments are compared with other recent and improved algorithms in the literature to show the performance and effectiveness of the proposed algorithm. Two nonparametric statistical tests, the Wilcoxon rank-sum and the Friedman test, are performed on CEFO and other compared algorithms to determine the significance of the results and show the efficiency of CEFO over other algorithms.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1186/s13014-022-02163-7,en,Introducing new plan evaluation indices for prostate dose painting IMRT plans based on apparent diffusion coefficient images,"['OriginalPaper', 'Research']","Background Dose painting planning would be more complicated due to different levels of prescribed doses and more complex evaluation with conventional plan quality indices considering uniform dose prescription. Therefore, we tried to introduce new indices for evaluating the dose distribution conformity and homogeneity of treatment volumes based on the tumoral cell density and relative volumes of each lesion in prostate IMRT. Methods CT and MRI scans of 20 male patients having local prostate cancer were used for IMRT DP planning. Apparent diffusion coefficient (ADC) images were imported to a MATLAB program to identify lesion regions based on ADC values automatically. Regions with ADC values lower than 750 mm 2 /s and regions with ADC values higher than 750 and less than 1500 mm 2 /s were considered CTV 70Gy (clinical tumor volume with 70 Gy prescribed dose), and CTV 60Gy , respectively. Other regions of the prostate were considered as CTV 53Gy . New plan evaluation indices based on evaluating the homogeneity (IOE(H)), and conformity (IOE(C)) were introduced, considering the relative volume of each lesion and cellular density obtained from ADC images. These indices were compared with conventional homogeneity and conformity indices and IOEs without considering cellular density. Furthermore, tumor control probability (TCP) was calculated for each patient, and the relationship of the assessed indices were evaluated with TCP values. Results IOE (H) and IOE (C) with considering cellular density had significantly lower values compared to conventional indices and IOEs without considering cellular density. (P < 0.05). TCP values had a stronger relationship with IOE(H) considering cell density (R 2  = -0.415), and IOE(C) without considering cell density (R 2  = 0.624). Conclusion IOE plan evaluation indices proposed in this study can be used for evaluating prostate IMRT dose painting plans. We suggested to consider cell densities in the IOE(H) calculation formula and it’s appropriate to calculate IOE(C) without considering cell density values.","['Biomedicine', 'Cancer Research', 'Oncology', 'Radiotherapy', 'Imaging / Radiology']"
doi:10.1007/s11042-022-14163-6,en,A deep neural network with hybrid spotted hyena optimizer and grasshopper optimization algorithm for copy move forgery detection,OriginalPaper,"Protecting data against tampering is a significant concern in modern times. Digital photographs are essential for displaying information. Digital picture forgeries include adding unusual patterns to real pictures, causing visual heterogeneity. CMF is a sort of digital image forgery in which a segment of an image is linked to a similar picture to cover or recreate forgeried components. The forgery seems authentic since the goal area has the same qualities as the original. Despite several ways to detect CMFD, there exist research gaps such as false detection, excessive execution time and low accuracy. Therefore, to address this issue, we present a hybrid optimization technique and classifier for CMFD. Proposed a novel deep learning technique stacked sparse denoising autoencoder (SSDAE) to classify the images as legitimate or fake. Additionally, the weight and bias parameters of the SSDAE model are optimized using the Grasshopper Optimization Algorithm (GOA) and the Spotted Hyena optimizer (SHO). The experiments are conducted on MICC-F220, MICC-F600, MICC-F2000 and CASIA2.0 datasets. Experimental results indicate that the proposed scheme find out image forgery region with Accuracy = 97.45%; Precision = 98.75%; Recall = 98.25% and F1 = 98.55% on MICC-F200 dataset, Accuracy = 98.92%; Precision = 88.45%; Recall = 85.21% and F1 = 91.41% on MICC-F600 dataset, Accuracy = 99.12%; Precision = 99.25%; Recall = 91.14% and F1 = 85.32% on MICC-F2000 dataset and Accuracy = 98.02%; Precision = 96.03%; Recall = 97.74% and F1 = 97.48% on CASIA 2.0 dataset.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1038/s41598-022-24369-1,en,DINI: data imputation using neural inversion for edge applications,"['OriginalPaper', 'Article']","The edge computing paradigm has recently drawn significant attention from industry and academia. Due to the advantages in quality-of-service metrics, namely, latency, bandwidth, energy efficiency, privacy, and security, deploying artificial intelligence (AI) models at the network edge has attracted widespread interest. Edge-AI has seen applications in diverse domains that involve large amounts of data. However, poor dataset quality plagues this compute regime owing to numerous data corruption sources, including missing data. As such systems are increasingly being deployed in mission-critical applications, mitigating the effects of corrupted data becomes important. In this work, we propose a strategy based on data imputation using neural inversion, DINI. It trains a surrogate model and runs data imputation in an interleaved fashion. Unlike previous works, DINI is a model-agnostic framework applicable to diverse deep learning architectures. DINI outperforms state-of-the-art methods by at least 10.7% in average imputation error. Applying DINI to mission-critical applications can increase prediction accuracy to up to 99% (F1 score of 0.99), resulting in significant gains compared to baseline methods.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11665-022-07570-1,en,Tribological Parameters Optimization of AZ31-SiC Composite Using Whale Optimization Algorithm,"['OriginalPaper', 'Technical Article']","In this paper, a Whale Optimization Algorithm (WOA) is proposed for the optimization of tribological parameters of AZ31-SiC metal matrix composite. Experiments were carried out by the surface of pin with the different loads of 10, 20, 30 N, velocities of 0.5, 1.0, 1.5 m s −1 and sliding distances of 500 m, 750 m, 1000 m respectively for optimization. The equations derived from the regression analysis are considered as objective functions to find the optimal parameters of wear and coefficients of friction (COF) using WOA. Considering these objective functions, the WOA is used to optimize the tribological parameters. The optimized parameters obtained from the WOA are compared with the parameters derived from PSO, DE, FFA and experimental results. The optimum wear values obtained from WOA, PSO, DE, FFA and experiment results are 3.55, 3.63, 3.60, 3.57 and 3.77 mg respectively. The optimum values of COF obtained from WOA, PSO, DE, FFA and experiment results are 0.311, 0.313, 0.314, 0.312, and 0.33 respectively. The optimum wear values obtained from WOA, PSO, DE and FFA are 6.01, 3.89, 4.69, 5.48% lower than the experimental value. The optimum COF values obtained from WOA, PSO, DE and FFA are 5.76, 5.15, 4.85, 5.45% lower than the experimental value. It is evidence from the results that WOA has provided the best wear and COF values for AZ31-SiC metal matrix composite when compared to other methods. This proposed method reduces the time and effort of the manufacturer to use the composite material in an application with the optimum operating condition for more life. The microstructural SEM micrographs reveal the distribution of reinforcement in the composite. The SEM micrographs of worn out surfaces present various wear mechanisms of the composites under variety of operating conditions.","['Materials Science', 'Characterization and Evaluation of Materials', 'Tribology, Corrosion and Coatings', 'Quality Control, Reliability, Safety and Risk', 'Engineering Design']"
doi:10.1038/s41598-022-24733-1,en,Nature inspired method for noninvasive fetal ECG extraction,"['OriginalPaper', 'Article']","This paper introduces a novel algorithm for effective and accurate extraction of non-invasive fetal electrocardiogram (NI-fECG). In NI-fECG based monitoring, the useful signal is measured along with other signals generated by the pregnant women’s body, especially maternal electrocardiogram (mECG). These signals are more distinct in magnitude and overlap in time and frequency domains, making the fECG extraction extremely challenging. The proposed extraction method combines the Grey wolf algorithm (GWO) with sequential analysis (SA). This innovative combination, forming the GWO-SA method, optimises the parameters required to create a template that matches the mECG, which leads to an accurate elimination of the said signal from the input composite signal. The extraction system was tested on two databases consisting of real signals, namely, Labour and Pregnancy. The databases used to test the algorithms are available on a server at the generalist repositories (figshare) integrated with Matonia et al. (Sci Data 7(1):1–14, 2020). The results show that the proposed method extracts the fetal ECG signal with an outstanding efficacy. The efficacy of the results was evaluated based on accurate detection of the fQRS complexes. The parameters used to evaluate are as follows: accuracy (ACC), sensitivity (SE), positive predictive value (PPV), and F1 score. Due to the stochastic nature of the GWO algorithm, ten individual runs were performed for each record in the two databases to assure stability as well as repeatability. Using these parameters, for the Labour dataset, we achieved an average ACC of 94.60%, F1 of 96.82%, SE of 97.49%, and PPV of 98.96%. For the Pregnancy database, we achieved an average ACC of 95.66%, F1 of 97.44%, SE of 98.07%, and PPV of 97.44%. The obtained results show that the fHR related parameters were determined accurately for most of the records, outperforming the other state-of-the-art approaches. The poorer quality of certain signals have caused deviation from the estimated fHR for certain records in the databases. The proposed algorithm is compared with certain well established algorithms, and has proven to be accurate in its fECG extractions.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11356-022-24051-z,en,"Environmental assessment of optimized renewable energy-based microgrids integrated desalination plant: considering human health, ecosystem quality, climate change, and resources","['OriginalPaper', 'Research Article']","Using hybrid renewable energy technology is an efficient method for greenhouse gas mitigation caused by fossil fuel combustion. However, these renewable microgrids are not free from environmental damages, especially during the lifetime of hybrid renewable energy systems (HRES). The main objective of this study is to assess the environmental impacts of three optimized HRES for the Sea Water Reverse Osmosis Desalination (SWROD) plant. An objective optimization was developed using the division algorithm, and the environmental impacts of the optimized HRES were investigated by the life cycle assessment approach. The results showed that producing 1 m 3 freshwater by an optimal size SWROD integrated with wind turbine/battery is responsible for 3.56E − 07 disability-adjusted life year (DALY). It is significantly less than 1 m 3 freshwater production by an optimal size SWROD integrated with solar PV/battery (5.88E − 07 DALY) and solar PV/wind turbine/battery (5.13E − 07 DALY) energy systems. Moreover, 1 m 3 freshwater by a SWROD integrated with proposed microgrids in this study led to a damage of 0.089 to 0.193 potentially disappeared fraction of species (PDF)*m 2 *yr to ecosystem quality. It also results in an emission of 0.143 to 0.339 kg CO 2 eq per 1 m 3 freshwater. Furthermore, resources for 1 m 3 freshwater production by a SWROD are calculated at 2.77 to 4.806 MJ primary. Freshwater production by an optimal size SWROD integrated with solar wind/battery compared with solar PV/battery and solar PV/wind turbine/battery had less damage to ecosystem quality, climate, and resources. The results showed reductions of 91.23% in human health, 73.51% in an ecosystem quality, 92.43% in climate change, and 90.08% in resources for producing 1 m 3 of freshwater using SWROD integrated with wind turbine/battery bank compared to fossil-based desalination. Finally, the result showed that solving the optimization problem using the division algorithm compared to other algorithms leads to less environmental damage in freshwater production.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s10994-022-06239-z,en,Local AdaGrad-type algorithm for stochastic convex-concave optimization,OriginalPaper,"Large scale convex-concave minimax problems arise in numerous applications, including game theory, robust training, and training of generative adversarial networks. Despite their wide applicability, solving such problems efficiently and effectively is challenging in the presence of large amounts of data using existing stochastic minimax methods. We study a class of stochastic minimax methods and develop a communication-efficient distributed stochastic extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable for solving convex-concave minimax problems in the Parameter-Server model. LocalAdaSEG has three main features: (1) a periodic communication strategy that reduces the communication cost between workers and the server; (2) an adaptive learning rate that is computed locally and allows for tuning-free implementation; and (3) theoretically, a nearly linear speed-up with respect to the dominant variance term, arising from the estimation of the stochastic gradient, is proven in both the smooth and nonsmooth convex-concave settings. LocalAdaSEG is used to solve a stochastic bilinear game, and train a generative adversarial network. We compare LocalAdaSEG against several existing optimizers for minimax problems and demonstrate its efficacy through several experiments in both homogeneous and heterogeneous settings.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s11071-022-08109-8,en,Early intelligent fault diagnosis of rotating machinery based on IWOA-VMD and DMKELM,"['OriginalPaper', 'Original Paper']","The effect of early fault vibration signals from rotating machinery is weak and easily drowned out by intense noise. Therefore, it is still a great challenge to make early fault diagnosis. An intelligent early fault diagnosis method for rotating machinery is proposed based on the parameter optimization of the variational mode decomposition (VMD) and deep multi-kernel extreme learning machine (DMKELM). Firstly, the improved whale optimization algorithm (IWOA) is designed by introducing the iterative chaotic mapping, nonlinear convergence factor and inertia weight to optimize the VMD parameters. Secondly, the optimized VMD (OVMD) with sample entropy is created to reduce noise and reconstruct the signals. Finally, the radial basis kernel function (RBF) and polynomial kernel (PK) are introduced to construct the mixed kernel function, which can enhance the classification performance and generalization ability of the model. Two experiments on bearings and gears show that the fault diagnosis accuracy by DMKELM is 99 and 98.5%, respectively, which is at least 1% higher than comparative methods and increases by 4% after noise reduction. The result shows that the proposed method has great superiority in the early fault diagnosis of rotating machinery.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s11042-022-14096-0,en,SSRFace: a face recognition framework against shallow data,OriginalPaper,"Most existing deep-learning-based approaches rely on high-resolution large-scale datasets to improve their performance. However, obtaining such datasets is challenging for tasks such as face recognition. The best way to address this is first to address the issue deep-learning-based approaches experience when trained on limited samples or shallow datasets (i.e., lack of diversity). We propose SSRFace, a framework for face recognition on shallow datasets. In detail, SSRFace leverages two novel components: Segregate-Representation (SR) and SimInstance. SR utilizes unlabeled data and angular-margin-based loss to increase inter-class distance, improving class discrimination. SimInstance, on the other hand, has a straightforward approach to improving intra-class diversity. Our proposed SimInstance starts by learning the unique class distribution from the few samples before randomly sampling a feature representation to serve as new intra-class samples. We train our model on TinyFace, a shallow dataset, to show its capabilities. We show SSRFace performed better than other existing approaches with Rank-1 accuracy and mean average precision (mAP) when trained on shallow datasets.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1186/s13037-022-00345-6,en,The value of artificial intelligence for detection and grading of prostate cancer in human prostatectomy specimens: a validation study,"['OriginalPaper', 'Research']","Background The Gleason grading system is an important clinical practice for diagnosing prostate cancer in pathology images. However, this analysis results in significant variability among pathologists, hence creating possible negative clinical impacts. Artificial intelligence methods can be an important support for the pathologist, improving Gleason grade classifications. Consequently, our purpose is to construct and evaluate the potential of a Convolutional Neural Network (CNN) to classify Gleason patterns. Methods The methodology included 6982 image patches with cancer, extracted from radical prostatectomy specimens previously analyzed by an expert uropathologist. A CNN was constructed to accurately classify the corresponding Gleason. The evaluation was carried out by computing the corresponding 3 classes confusion matrix; thus, calculating the percentage of precision, sensitivity, and specificity, as well as the overall accuracy. Additionally, k-fold three-way cross-validation was performed to enhance evaluation, allowing better interpretation and avoiding possible bias. Results The overall accuracy reached 98% for the training and validation stage, and 94% for the test phase. Considering the test samples, the true positive ratio between pathologist and computer method was 85%, 93%, and 96% for specific Gleason patterns. Finally, precision, sensitivity, and specificity reached values up to 97%. Conclusion The CNN model presented and evaluated has shown high accuracy for specifically pattern neighbors and critical Gleason patterns. The outcomes are in line and complement others in the literature. The promising results surpassed current inter-pathologist congruence in classical reports, evidencing the potential of this novel technology in daily clinical aspects.","['Medicine & Public Health', 'Surgery']"
doi:10.1007/s10489-022-04290-w,en,Differential evolution with variable leader-adjoint populations,OriginalPaper,"The performance of differential evolution (DE) is significantly affected by the selection of mutation strategies and control parameters. Inappropriate selection may lead to premature convergence and stagnation. Therefore, selecting appropriate mutation strategies and control parameters has always been a challenging task. In this paper, a differential evolution with variable leader-adjoint populations (LADE) is proposed. In LADE, a leader-adjoint model is used to divide the population in each generation into leader population and adjoint population. The leader population adopts a novel DE/current-best-rand/1 mutation strategy that can enhance the exploitation ability and avoid evolution stagnation. The adjoint population employs an improved DE/rand/1 mutation strategy that can not only strengthen the exploration ability, but also accelerate individual evolution by guiding the search process to the promising regions. Consequently, the leader-adjoint model can achieve a good balance between exploration and exploitation at different stages of evolution. Moreover, a parameter adaptation method is utilized to dynamically adjust the values of control parameters. To verify the performance of LADE, numerical experiments on the CEC2014 benchmark functions and Lennard-Jones potential real-world problem are executed. Experiment results show that the proposed LADE is significantly better than, or at least comparable to recent and advanced algorithms. In addition, experiments evaluate and analyze the effect of control parameters on the algorithm.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10586-022-03810-0,en,lIDS-SIoEL: intrusion detection framework for IoT-based smart environments security using ensemble learning,OriginalPaper,"Smart cities are being enabled all around the world by Internet of Things (IoT) applications. A smart city idea necessitates the integration of information and communication technologies and devices throughout a network in order to provide improved services to consumers. Because of their increasing amount and mobility, they are increasingly appealing to attackers. Therefore, several solutions, including as encryptions, authentication, availability, and data integrity, have been combined to protect IoT. Intrusion detection systems (IDSs) are a powerful security tool that may be improved by incorporating machine learning (ML) and deep learning (DP) techniques. This paper presents a novel intrusion detection framework for IoT-based smart environments with Ensemble Learning called IDS-SIoEL. Typically, the framework proposed an optimal anomaly detection model that uses AdaBoost, and combining different feature selection techniques Boruta, mutual information and correlation furthermore. The proposed model was evaluated on IoT-23, BoT-IoT, and Edge-IIoT datasets using the GPU. When compared to existing IDS, our approach provides good rating performance features of ACC, recall, and precision, with around 99.9% on record detection and calculation time of 33.68 s for learning and 0.02156 s for detection.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s40313-022-00972-5,en,An Intelligent Improvement Based on a Novel Configuration of Artificial Neural Network Model to Track the Maximum Power Point of a Photovoltaic Panel,OriginalPaper,"Maximum Power Point Tracking (MPPT) is one of the most challenging aspects of Photovoltaic (PV) system design. In fact, to improve the efficiency of solar panels, a viable MPPT approach is necessary. Many of these techniques are slow and imprecise in terms of functionality. The purpose of this paper is to give a performance study of a new configuration of Artificial Neural Network (ANN) models based on the Bayesian Regularization (BR) training algorithm, with the goal of outperforming the most widely used MPPT techniques. Consequently, the suggested approach based on the ANN-BR algorithm has been trained and analyzed for multiple model topologies, with the best generated configuration containing 19 neurons achieving 99.9997 % accuracy. In addition, it has shown an excellent power output convergence by reaching 99.9763 % of the PV’s Maximum Power Point (MPP), a better perturbation reduction, and a fast tracking speed of 37 ms compared to the most applicable MPPT algorithms, notably Perturb & Observe (P &O), Particle Swarm Optimization (PSO), Grey Wolf Optimization (GWO), Whale Optimization Algorithm (WOA). The obtained results have been evaluated using the Mean Square Error (MSE) and the Root Mean Square Error (RMSE) fitness functions, and the suggested algorithm’s potency and efficiency are examined using flow simulations in the MATLAB ®software.","['Engineering', 'Electrical Engineering', 'Control, Robotics, Mechatronics', 'Control and Systems Theory', 'Robotics and Automation']"
doi:10.1007/s00500-022-07511-z,en,Designing of neural network-based SoSMC for autonomous underwater vehicle: integrating hybrid optimization approach,"['OriginalPaper', 'Optimization']","The control of an autonomous underwater vehicle (AUV) is regarded as a difficult challenge, owing to the nonlinear and uncertain dynamics of the AUV. In this work, Optimized neural network (NN) is integrated with the “second-order sliding mode control (SoSMC) approach” for control of yaw angle in AUV. More particularly, the positive gain of SoSMC is predicted by an optimized NN model, where the training is performed by a novel Sea Lion Distance-based FireFly algorithm via tuning the optimal weights. At last, the supremacy of the adopted model is validated under various measures. Accordingly, the RMSE values accomplished by the proposed model is 40.94%, 1.39%, 0.69%, 0.69% and 0.41% better than existing models like “GW-SMC, FF-SoSMC, SLnO-SoSMC, POA-SoSMC and GW-SoSMC”, respectively, for set point 1.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s10489-022-04299-1,en,Hyper-parameter optimization of deep learning architectures using artificial bee colony (ABC) algorithm for high performance real-time automatic colorectal cancer (CRC) polyp detection,OriginalPaper,"Colorectal cancer (CRC) is one of the most common and malignant types of cancer worldwide. Colonoscopy, considered the gold standard for CRC screening, allows immediate removal of polyps, which are precursors to CRC. Many computer-aided diagnosis systems (CADs) have been proposed for automatic polyp detection. Most of these systems are based on traditional machine learning algorithms and their generalization ability, sensitivity and specificity are limited. On the other hand, with the widespread use of deep learning algorithms in medical image analysis and the successful results in the analysis of colonoscopy images, especially in the early and accurate detection of polyps, these problems are eliminated in recent years. In short, deep learning algorithms and applications have gained a critical role in CAD systems for real-time autonomous polyp detection. Here, we make significant improvements to object detection algorithms to improve the performance of CAD-based real-time polyp detection systems. We integrate the artificial bee colony algorithm (ABC) into the YOLO algorithm to optimize the hyper-parameters of YOLO-based algorithms. The proposed method can be easily integrated into all YOLO algorithms such as YOLOv3, YOLOv4, Scaled-YOLOv4, YOLOv5, YOLOR and YOLOv7. The proposed method improves the performance of the Scaled-YOLOv4 algorithm with an average of more than 3% increase in mAP and a more than 2% improvement in F1 value. In addition, the most comprehensive study is conducted by evaluating the performance of all existing models in the Scaled-YOLOv4 algorithm (YOLOv4s, YOLOv4m, YOLOV4-CSP, YOLOv4-P5, YOLOV4-P6 and YOLOv4-P7) on the novel SUN and PICCOLO polyp datasets. The proposed method is the first study for the optimization of YOLO-based algorithms in the literature and makes a significant contribution to the detection accuracy.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s11071-022-08095-x,en,Nonlinear hysteretic parameter identification using an attention-based long short-term memory network and principal component analysis,"['OriginalPaper', 'Original Paper']","Hysteretic models are used to describe the nonlinear memory-based relationship between the input and output of some physical systems. A long short-term memory neural network-based method is proposed to identify nonlinear hysteretic parameters. Either force or vibration response data are used as the input of the network and the nonlinear hysteresis parameters as the output. The principal component analysis technique is applied to eliminate the redundant dimensionality of the input data. The attention mechanism is utilized to enhance the generalization ability of the standard network. Three representative hysteretic models are employed to verify the effectiveness of the present method. Both numerical and experimental results demonstrate that the proposed method could yield accurate identification results in all cases, even when uncertain and limited input data are used. Compared with the sensitivity methods and heuristic algorithms, the proposed method is more computationally efficient and can obtain more accurate identification results.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Classical Mechanics', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s12083-022-01405-5,en,MPPT-EPO optimized solar energy harvesting for maximizing the WSN lifetime,OriginalPaper,"Recently, wireless sensor networks (WSNs) are the basic building blocks in smart cities, smart parking, and smart building applications. One of the major problems in the WSN node is the limited amount of energy that suffers the entire operation because it can only work for a few days based on the duty cycle. This research presents an efficient solar energy harvesting based WSN nodes using solar photovoltaic energy to overcome the problem mentioned above. The optimized solar energy harvesting based WSN will increase the network lifetime. In this paper, we have presented an MPPT-EPO optimized solar energy harvesting to maximize the WSN lifetime. The energy-efficient technique of the Emperor Penguin Optimization algorithm (EPO) is used to optimize the Maximum Power Point Tracking (MPPT) for tracking the maximum power from the solar panel. The SEPIC converter boosts the electrical energy generated through solar to get sufficient voltage to charge the battery. From this system, various sensor nodes are supplied with energy.","['Engineering', 'Communications Engineering, Networks', 'Information Systems and Communication Service', 'Computer Communication Networks', 'Signal,Image and Speech Processing']"
doi:10.1007/s00778-022-00769-7,en,Augmented lineage: traceability of data analysis including complex UDF processing,"['OriginalPaper', 'Regular Paper']","Data lineage allows information to be traced to its origin in data analysis by showing how the results were derived. Although many methods have been proposed to identify the source data from which the analysis results are derived, analysis is becoming increasingly complex both with regard to the target (e.g., images, videos, and texts) and technology (e.g., AI and machine learning (ML)). In such complex data analysis, simply showing the source data may not ensure traceability. For example, ML analysts building image classifier models often need to know which parts of images are relevant to the output and why the classifier made a decision. Recent studies have intensively investigated interpretability and explainability in the AI/ML domain. Integrating these techniques into the lineage framework will help analysts understand more precisely how the analysis results were derived and how the results are trustful. In this paper, we propose the concept of augmented lineage for this purpose, which is an extended lineage, and an efficient method to derive the augmented lineage for complex data analysis. We express complex data analysis flows using relational operators by combining user-defined functions (UDFs). UDFs can represent invocations of AI/ML models within the data analysis. Then, we present a method taking UDFs into consideration to derive the augmented lineage for arbitrarily chosen tuples among the analysis results. We also experimentally demonstrate the efficiency of the proposed method.","['Computer Science', 'Database Management']"
doi:10.1007/s11042-022-14212-0,en,VKCS: a pre-trained deep network with attention mechanism to diagnose acute lymphoblastic leukemia,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Leukemia is a prominent hematologic malignancy that causes mortality and complications at different ages. In this paper, to provide a more accurate diagnosis of Acute Lymphoblastic Leukemia (ALL), a three-stage model based on transfer learning called variable-kernel channel-spatial attention (VKCS) is proposed. First, a deep pre-trained network extracts high-level features from the blood smear images. In the second stage, two attention mechanisms of variable-kernel spatial attention and variable-kernel channel attention consider spatial and channel information in parallel to improve model performance. The last step is the classification module. The experiments are repeated for different pre-trained networks, as well as for images in RGB, HSV, L * a * b * , and YCbCr color spaces, and by applying morphological operators (erosion and dilation) to images in different color spaces. The best model efficiency accuracy was achieved for images in HSV color space and the use of EfficientNet-V2M for feature extraction. The VKCS model accuracy is 100% for the ALL-IDB1 dataset and 99.6% for the ALL-IDB2 dataset, which are promising results.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s42835-022-01314-w,en,Lightweight Deep Learning-Based Model for Traffic Prediction in Fog-Enabled Dense Deployed IoT Networks,"['OriginalPaper', 'Original Article']","Internet of Things (IoT) is one of the promising technologies, announced as one of the primary use cases of the fifth-generation cellular systems (5G). It has many applications that cover many fields, moving from indoor applications, e.g., smart homes, smart metering, and healthcare applications, to outdoor applications, including smart agriculture, smart city, and surveillance applications. This produces massive heterogeneous traffic that loads the IoT network and other integrated communication networks, e.g., 5G, which represents a significant challenge in designing IoT networks; especially, with dense deployment scenarios. To this end, this work considers developing a novel artificial intelligence (AI)-based framework for predicting traffic over IoT networks with dense deployment. This facilitates traffic management and avoids network congestion. The developed AI algorithm is a deep learning model based on the convolutional neural network, which is a lightweight algorithm to be implemented by a distributed edge computing node, e.g., a fog node, with limited computing capabilities. The considered IoT model deploys distributed edge computing to enable dense deployment, increase network availability, reliability, and energy efficiency, and reduce communication latency. The developed framework has been evaluated, and the results are introduced to validate the proposed prediction model.","['Engineering', 'Electrical Engineering', 'Electronics and Microelectronics, Instrumentation', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1007/s00502-022-01074-5,en,Spatio-temporal charging model for the identification of bottlenecks in planned highway charging infrastructure for passenger BEVs,"['OriginalPaper', 'Originalarbeit']","Fast-charging capacities must be sufficiently allocated to meet the charging demand of the growing battery electric vehicle (BEV) fleet. We present a methodology for testing the implementability of a planned charging infrastructure for highway networks in terms of underutilized charging capacities and bottlenecks. A linear optimization model for determining charging activities at a fast-charging infrastructure was developed to accomplish this. Using a bottom-up approach, we modeled the charging activities based on the traffic flow between starting and destination points in the network. The proposed model is applied to a planned fast-charging infrastructure along the highway network in the east of Austria. The obtained results reveal that the charging infrastructure is capable of meeting demand during all observed extreme traffic load and temperature conditions. Thus, no bottlenecks are detected, but locations of charging stations with overestimated capacities are discovered, implying that the local capacities would never be fully utilized. Our findings also highlight the importance of considering the spatio-temporal dynamics of charging activities and the traffic flow when expanding fast-charging infrastructure. Die Schnellladekapazitäten müssen regelmäßig erweitert werden, um den Ladebedarf der wachsenden Flotte von batterieelektrischen Fahrzeugen zu decken. In dieser Arbeit präsentieren wir eine Methodik, mit der die Umsetzbarkeit einer geplanten Autobahnladeinfrastruktur im Hinblick auf Ladekapazitäten, die kaum genutzt werden, und Engpässe in den Ladevorgängen getestet werden kann. Zu diesem Zweck wurde ein lineares Optimierungsmodell entwickelt, das die Ladeaktivitäten an einer vorgegebenen Schnellladeinfrastruktur modelliert. In dieser Modellformulierung wird ein Bottom-up-Ansatz verwendet, wobei die Ladeaktivitäten basierend auf dem Verkehrfluss modelliert werden. Das vorgeschlagene Modell wird auf eine geplante Schnellladeinfrastruktur entlang des Autobahnnetzes im Osten Österreichs angewendet. Die Ergebnisse zeigen, dass die betrachtete Ladeinfrastruktur ausreichend ist, um die Nachfrage unter verschiedenen extremen Bedingungen in Bezug auf Verkehrsbelastung und Temperatur zu decken. Daher werden hier keine Engpässe festgestellt, hingegen aber schon Standorte von Ladestationen mit überschätzten Ladekapazitäten entdeckt, d. h. die dort geplanten Kapazitäten würden nie vollständig genutzt werden. Darüber hinaus zeigen unsere Ergebnisse, dass die Beachtung von der räumlich-zeitlichen Dynamik von Ladeaktivitäten und dem Verkehrsfluss den Ausbau der Schnellladeinfrastruktur optimaler gestalten kann.","['Engineering', 'Electrical Engineering', 'Computer Hardware', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s11760-022-02396-9,en,Cancer prediction with gene expression profiling and differential evolution,"['OriginalPaper', 'Original Paper']","In the field of bioinformatics, the classification of tumors is a difficult and time-consuming task. When diagnosing cancer, gene expression levels are typically one of the most useful tools. However, the biological noise present in microarray data leads to unsatisfactory precision and accuracy. The utilization of thousands of genes in the process of diagnosing tumors is an important task. The two levels of feature selection have been proposed in order to determine the genes that are the most informative to diagnose cancer. Using three different statistical methods, the first level of selection reveals the prognostic genes. In the second level, the differential evolution algorithm considers the prognostic genes that were obtained from statistical measures as initial members to identify the most relevant features. The scaling factor in the modified differential evolution algorithm was made to vary in a dynamic manner in order to evolve the mutant member of the population. The proposed model is a hybrid of statistical approach and evolutionary computation with modified differential evolution algorithm that identifies the candidate genes from thousands of genes from gene expression data. The findings obtained through this hybrid approach upon testing five gene expression datasets provide evidence that it has outperformed when compared to the existing systems for DLBCL outcome, prostate outcome, prostate, and colon tumor datasets with improved classification accuracies of 14%, 4%, 0.62%, and 0.13%, respectively.","['Computer Science', 'Image Processing and Computer Vision', 'Signal,Image and Speech Processing', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Multimedia Information Systems']"
doi:10.1007/s11276-022-03182-8,en,Dynamic distributed generative adversarial network for intrusion detection system over internet of things,"['OriginalPaper', 'Original Paper']","The Internet of Things (IoT) intelligently facilitates individuals interacting with the real-world applications which forms smart environment through internet connectivity at anywhere anytime (dynamic in nature), the devices in an IoT environment encounters several security threats. To overcome these security challenges numerous state of art approaches have been implemented to ensure the security of IoT appliances, but still innovative methods are desirable. The traditional Machine learning (ML) integrates with deep learning algorithm exhibits a potential of detecting abnormal intrusion patterns by formulating a seamless option for anomaly-based detection. This work proposed a Dynamic Distributed—Generative Adversarial Network (DD-GAN) with Improved Firefly Optimization- Hybrid Deep Learning based Convolutional Neural Network -Adaptive Neuro-Fuzzy Inference System (IFFO-HDLCNN + ANFIS) that takes gain of IoT's power, offers enhanced behavior for efficiently examining the entire traffic which traverses in the IoT. Initially, Synthetic Minority Over-sampling Technique (SMOTE) is engaged for pre-processing of data and then Modified Principal Component Analysis (MPCA) is being applied for feature reduction. The optimal features are selected through the Improve Firefly Optimization (IFFO) for optimum fitness value to enhance the classification accuracy of HDLCNN. Finally the intrusion detection is carried out by HDLCNN + ANFIS model, which is competent in detecting threats. The experimental results have proven that model demonstrates ability to perceive any kind of probable intrusion and anomalous behavior. In comparison to existing methods, the suggested IFFO-HDLCNN + ANFIS algorithm delivers improved intrusion detection performance regarding higher accuracy, precision, recall, f-measure, reduced False Positive Rate (FPR).","['Engineering', 'Communications Engineering, Networks', 'Computer Communication Networks', 'Electrical Engineering', 'IT in Business']"
doi:10.1007/s00521-022-07961-4,en,Multistate time series imputation using generative adversarial network with applications to traffic data,"['OriginalPaper', 'Original Article']","Time series missing data is a pervasive problem in many fields, especially in intelligent transportation system, which hinders the application of timing analysis methods and the fine adjustment of control strategies. The prevalent imputation approaches reconstruct missing data with a high accuracy by exploiting a precise distribution model. But the multistate characteristic of time series data and the uncertainty of imputation process increase the difficulty of modeling temporal data distribution and reduce the imputation performance. In this paper, a novel time series generative adversarial imputation network (TGAIN) model is proposed to deal with time series data missing problem. The model combines the advantages of GAN's data distribution modeling and multiple imputation's uncertainty handling. Specifically, the TGAIN network is designed and adversarial trained to learn the multistate distribution of missing time series data. Through the conditional vector constraint and adversarial imputation process, the latent distribution for each missing position under different states can be effectively estimated based on implicit relationships with partial observation information. Then the corresponding multiple imputation strategy is proposed to deal with the uncertainty of imputation process and it can determine the best fill value from the learned distribution. Furthermore, sufficient experiments have been conducted in two real traffic flow datasets. The comparative results show the proposed TGAIN not only has better ability on time series data distribution modeling and imputation uncertainty handling, but also performs more robustly and stability even with the missing rate increases.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11263-022-01714-5,en,Animal Pose Tracking: 3D Multimodal Dataset and Token-based Pose Optimization,OriginalPaper,"Accurate tracking of the 3D pose of animals from video recordings is critical for many behavioral studies, yet there is a dearth of publicly available datasets that the computer vision community could use for model development. We here introduce the Rodent3D dataset that records animals exploring their environment and/or interacting with each other with multiple cameras and modalities (RGB, depth, thermal infrared). Rodent3D consists of 200 min of multimodal video recordings from up to three thermal and three RGB-D synchronized cameras (approximately 4 million frames). For the task of optimizing estimates of pose sequences provided by existing pose estimation methods, we provide a baseline model called OptiPose . While deep-learned attention mechanisms have been used for pose estimation in the past, with OptiPose , we propose a different way by representing 3D poses as tokens for which deep-learned context models pay attention to both spatial and temporal keypoint patterns. Our experiments show how OptiPose is highly robust to noise and occlusion and can be used to optimize pose sequences provided by state-of-the-art models for animal pose estimation.","['Computer Science', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Artificial Intelligence', 'Image Processing and Computer Vision', 'Pattern Recognition']"
doi:10.1038/s41586-022-05471-w,en,Structured cerebellar connectivity supports resilient pattern separation,"['OriginalPaper', 'Article']","The cerebellum is thought to help detect and correct errors between intended and executed commands 1 , 2 and is critical for social behaviours, cognition and emotion 3 – 6 . Computations for motor control must be performed quickly to correct errors in real time and should be sensitive to small differences between patterns for fine error correction while being resilient to noise 7 . Influential theories of cerebellar information processing have largely assumed random network connectivity, which increases the encoding capacity of the network’s first layer 8 – 13 . However, maximizing encoding capacity reduces the resilience to noise 7 . To understand how neuronal circuits address this fundamental trade-off, we mapped the feedforward connectivity in the mouse cerebellar cortex using automated large-scale transmission electron microscopy and convolutional neural network-based image segmentation. We found that both the input and output layers of the circuit exhibit redundant and selective connectivity motifs, which contrast with prevailing models. Numerical simulations suggest that these redundant, non-random connectivity motifs increase the resilience to noise at a negligible cost to the overall encoding capacity. This work reveals how neuronal network structure can support a trade-off between encoding capacity and redundancy, unveiling principles of biological network architecture with implications for the design of artificial neural networks. Mapping of the mouse cerebellar cortex using 3D reconstruction from electron microscopy, as well as numerical simulation of neuronal activity, shows non-random redundancy of connectivity that may favour resilient learning over encoding capacity.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s13059-022-02811-x,en,The genetic and biochemical determinants of mRNA degradation rates in mammals,"['OriginalPaper', 'Research']","Background Degradation rate is a fundamental aspect of mRNA metabolism, and the factors governing it remain poorly characterized. Understanding the genetic and biochemical determinants of mRNA half-life would enable more precise identification of variants that perturb gene expression through post-transcriptional gene regulatory mechanisms. Results We establish a compendium of 39 human and 27 mouse transcriptome-wide mRNA decay rate datasets. A meta-analysis of these data identified a prevalence of technical noise and measurement bias, induced partially by the underlying experimental strategy. Correcting for these biases allowed us to derive more precise, consensus measurements of half-life which exhibit enhanced consistency between species. We trained substantially improved statistical models based upon genetic and biochemical features to better predict half-life and characterize the factors molding it. Our state-of-the-art model, Saluki, is a hybrid convolutional and recurrent deep neural network which relies only upon an mRNA sequence annotated with coding frame and splice sites to predict half-life ( r =0.77). The key novel principle learned by Saluki is that the spatial positioning of splice sites, codons, and RNA-binding motifs within an mRNA is strongly associated with mRNA half-life. Saluki predicts the impact of RNA sequences and genetic mutations therein on mRNA stability, in agreement with functional measurements derived from massively parallel reporter assays. Conclusions Our work produces a more robust ground truth for transcriptome-wide mRNA half-lives in mammalian cells. Using these revised measurements, we trained Saluki, a model that is over 50% more accurate in predicting half-life from sequence than existing models. Saluki succinctly captures many of the known determinants of mRNA half-life and can be rapidly deployed to predict the functional consequences of arbitrary mutations in the transcriptome.","['Life Sciences', 'Animal Genetics and Genomics', 'Human Genetics', 'Plant Genetics and Genomics', 'Microbial Genetics and Genomics', 'Bioinformatics', 'Evolutionary Biology']"
doi:10.1007/s10915-022-02044-x,en,Solving Non-linear Kolmogorov Equations in Large Dimensions by Using Deep Learning: A Numerical Comparison of Discretization Schemes,OriginalPaper,"Non-linear partial differential Kolmogorov equations are successfully used to describe a wide range of time dependent phenomena, in natural sciences, engineering or even finance. For example, in physical systems, the Allen–Cahn equation describes pattern formation associated to phase transitions. In finance, instead, the Black–Scholes equation describes the evolution of the price of derivative investment instruments. Such modern applications often require to solve these equations in high-dimensional regimes in which classical approaches are ineffective. Recently, an interesting new approach based on deep learning has been introduced byby E, Han and Jentzen [ 1 , 2 ]. The main idea is to construct a deep network which is trained from the samples of discrete stochastic differential equations underlying Kolmogorov’s equation. The network is able to approximate, numerically at least, the solutions of the Kolmogorov equation with polynomial complexity in whole spatial domains. In this contribution we study variants of the deep networks by using different discretizations schemes of the stochastic differential equation. We compare the performance of the associated networks, on benchmarked examples, and show that, for some discretization schemes, improvements in the accuracy are possible without affecting the observed computational complexity.","['Mathematics', 'Algorithms', 'Computational Mathematics and Numerical Analysis', 'Mathematical and Computational Engineering', 'Theoretical, Mathematical and Computational Physics']"
doi:10.1007/s00158-022-03456-x,en,An adaptive and scalable artificial neural network-based model-order-reduction method for large-scale topology optimization designs,"['OriginalPaper', 'Research Paper']","Topology optimization (TO) provides a systematic approach for obtaining structure design with optimum performance of interest. However, the process requires the numerical evaluation of the objective function and constraints at each iteration, which is computationally expensive, especially for large-scale designs. Deep learning-based models have been developed to accelerate the process either by acting as surrogate models replacing the simulation process, or completely replacing the optimization process. However, most of them require a large set of labelled training data, which is generated mostly through simulations. The data generation time scales rapidly with the design size, decreasing the efficiency of the method itself. Another major issue is the weak generalizability of deep learning models. Most models are trained to work with the design problem similar to that used for data generation and require retraining if the design problem changes. In this work an adaptive, scalable deep learning-based model-order-reduction method is proposed to accelerate large-scale TO process, by utilizing MapNet, a neural network which maps the field of interest from coarse-scale to fine-scale. The proposed method allows for each simulation of the TO process to be performed at a coarser mesh, thereby greatly reducing the total computational time. More importantly, a crucial element, domain fragmentation, is introduced and integrated into the method, which greatly improves the transferability and scalability of the method. It has been demonstrated that the MapNet trained using data from one cantilever beam design with a specific loading condition can be directly applied to other structure design problems with different domain shapes, sizes, boundary and loading conditions.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1007/s11042-022-14208-w,en,A two-stage enhancement network with optimized effective receptive field for speckle image reconstruction,OriginalPaper,"Reconstructing target objects from strong speckle images is a key step for solving complex inverse scattering imaging problems. Deep learning (DL) methods are very effective for producing high quality object reconstruction, especially for speckle image reconstruction (SIR). Understanding the relationship between DL network structures and reconstruction results helps improve the reconstruction quality. Although previous studies have explored this issue, few of them considered dilated convolution adjustment and effective receptive field optimization of DL networks in image reconstruction for improving the reconstruction quality. In this paper, we propose a two stage enhancement network for speckle image reconstruction, in addition, we present an effective receptive field optimization method for maximizing the usage of the network capability. Specifically, in the first stage, we propose a growth model exploiting the dilation rates under the assumption that the central area pixels of images have a much bigger impact on the output field than the outer area pixels, and accordingly optimize the effective receptive field of the networks. Then, based on our growth model, in the second stage, the enhancement network jointly utilizes complementary information from the objective loss and perceptual loss when reconstructing objects. Extensive experiments show that our new network outperforms five state-of-the-art methods in the MAE, MSE, PSNR, and SSIM evaluating measures.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11042-022-14172-5,en,Residual spatial graph convolution and temporal sequence attention network for sign language translation,OriginalPaper,"Vision-based sign language translation technology (SLT) has brought the communication distance between deaf and ordinary people closer to a certain extent. The obstacle of SLT is mainly in two aspects: firstly, when capturing sign language action features, it is impossible to effectively overcome the shortcomings such as redundant information of sign language gesture features and motion ambiguity; secondly, it is difficult to define the alignment between action sequences and lexical sequences when processing sentence-level sign language videos. To overcome these problems, this paper proposes a sign language translation method based on residual spatial graph convolution network (Res-SGCN) and temporal attention model. Where, the Res-SGCN module is used to capture the spatial interaction feature information between the sign language skeleton nodes, and subsequently the temporal attention network is used to capture the temporal dimensional information fusion of the sign language spatial feature sequence and align it with the predicted vocabulary for translation. Experiments on public datasets show that the word error rate(WER) output by the proposed model reaches 4.17%, which is superior to other advanced sign language translation methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1038/s41598-022-24434-9,en,Winding-to-ground fault location in power transformer windings using combination of discrete wavelet transform and back-propagation neural network,"['OriginalPaper', 'Article']","Power transformers are important equipment in power systems and require a responsive and accurate protection system to ensure system reliability. In this paper, a fault location algorithm for power transformers based on the discrete wavelet transform and back-propagation neural network is presented. The system is modelled on part of Thailand’s transmission and distribution system. The ATP/EMTP software is used to simulate fault signals to validate the proposed algorithm, and the performance is evaluated under various conditions. In addition, various activation functions in the hidden and output layers are compared to select suitable functions for the algorithm. Test results show that the proposed algorithm can correctly locate faults on the transformer winding under different conditions with an average error of less than 0.1%. This result demonstrates the feasibility of implementing the proposed algorithm in actual protection systems for power transformers.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-08048-w,en,Semantic-aware multi-branch interaction network for deep multimodal learning,"['OriginalPaper', 'Original Article']","Deep multimodal learning has attracted increasing attention in artificial intelligence since it bridges vision and language. Most existing works only focus on specific multimodal tasks, which limits the ability to generalize to other tasks. Furthermore, these works only learn coarse-grained interactions at the object-level in images and the word-level in text, while ignoring to learn fine-grained interactions at relation-level and attribute-level. In this paper, to alleviate these issues, we propose a Semantic-aware Multi-Branch Interaction (SeMBI) network for various multimodal learning tasks. The SeMBI mainly consists of three modules, Multi-Branch Visual Semantics (MBVS) module, Multi-Branch Textual Semantics (MBTS) module and Multi-Branch Cross-modal Alignment (MBCA) module. The MBVS enhances the visual features and performs reasoning through three parallel branches, corresponding to the latent relationship branch, explicit relationship branch and attribute branch. The MBTS learns relation-level language context and attribute-level language context by textual relationship branch and textual attribute branch, respectively. The enhanced visual features then passed into MBCA to learn fine-grained cross-modal correspondence under the guidance of relation-level and attribute-level language context. We demonstrate the generalizability and effectiveness of the proposed SeMBI by applying it to three deep multimodal learning tasks, including Visual Question Answering (VQA), Referring Expression Comprehension (REC) and Cross-Modal Retrieval (CMR). Extensive experiments conducted on five common benchmark datasets indicate superior performance comparing with state-of-the-art works.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10462-022-10317-y,en,Automatic targetless LiDAR–camera calibration: a survey,OriginalPaper,"The recent trend of fusing complementary data from LiDARs and cameras for more accurate perception has made the extrinsic calibration between the two sensors critically important. Indeed, to align the sensors spatially for proper data fusion, the calibration process usually involves estimating the extrinsic parameters between them. Traditional LiDAR–camera calibration methods often depend on explicit targets or human intervention, which can be prohibitively expensive and cumbersome. Recognizing these weaknesses, recent methods usually adopt the autonomic targetless calibration approach, which can be conducted at a much lower cost. This paper presents a thorough review of these automatic targetless LiDAR–camera calibration methods. Specifically, based on how the potential cues in the environment are retrieved and utilized in the calibration process, we divide the methods into four categories: information theory based, feature based, ego-motion based, and learning based methods. For each category, we provide an in-depth overview with insights we have gathered, hoping to serve as a potential guidance for researchers in the related fields.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s12046-022-02027-6,en,Detection of heart arrhythmia based on UCMFB and deep learning technique,OriginalPaper,"Severe cardiovascular diseases (CVD) are the leading cause of death worldwide. In the emergency scenario, reliable electrocardiography (ECG) is critical for the rapid diagnosis and management of acute CVD. Deep learning (DL) is the most important leading technology for automatic computer-aided ECG detection of cardiovascular disorders. This work proposed a ResNet-50 model that classifies healthy people and patients with four types of CVD based on ECG abnormalities. The ECG signals were decomposed using a uniform cosine modulated filter bank (UCMFB) that helps in the easy identification of irregular and regular heartbeats. The study was performed on four different types of ECG databases specifically for short segmented (i.e., 2sec and 5sec) and long segmented (5min and 8min) time frames, and these sub-signals are converted into 2-D images using wavelet transform packet (WTP). The extensive tests result in the identification of AF, CHF, HT, and NSR classes with an accuracy, recall, precision, and F1-score of 99.93%, 99.96%, 99.89%, and 99.95%, respectively for multi-class classification. The proposed approach undergoes different fold cross-validation techniques and has achieved high classification accuracy when compared with different state-of-the-art models, demonstrating the superiority of our system over previous systems. It is discovered that the proposed technique achieves decayed computational complexity; thus, it is recommended for categorization challenges. The suggested approach has the potential to gain essential clinical acceptability and be used for ECG prioritisation of CVD detection in clinics and out-of-hospital situations.","['Engineering', 'Engineering, general']"
doi:10.1007/s11042-022-14234-8,en,Diabetic retinopathy detection and grading of retinal fundus images using coyote optimization algorithm with deep learning,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Diabetic retinopathy (DR) is a major reason of preventable blindness for diabetic patients. Regular retinal screening is recommended for diabetic persons to detect DR at the earlier stages. Manual retinal screening of DR is a difficult and laborious process, computer aided diagnosis models become essential. Recently deep learning (DL) methods enable effectual detection and classification of medical images, particularly retinal fundus images. With this motivation, this work presents an intelligent coyote optimization algorithm with DL based DR detection and grading (ICOA-DLDRD) model on retinal fundus images. The purpose of the ICOA-DLDRD approach is to identify the presence of DR on retinal fundus images. Primarily, the ICOA-DLDRD algorithm comprises Gabor filtering (GF) based noise removal and optimal region growing segmentation technique. Further, the primary seed points and thresholds of the region growing segmentation technique are optimally created utilizing the glowworm swarm optimization (GSO) algorithm. In addition, SqueezeNet with class attention learning (CAL) layer is derived for the extraction of feature vectors. Lastly, COA with a deep extreme learning machine (DELM) classifier is applied for the detection and grading of DR, in which the penalty parameter C and kernel parameter gamma γ of the DELM model are optimally adjusted by the use of COA. The performance validation of the ICOA-DLDRD method occurs utilizing the benchmark MESSIDOR dataset and the outcomes reported the betterment of the ICOA-DLDRD approach on the recent methods with maximum accuracy of 99.65%.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00466-022-02231-5,en,A data-driven multi-flaw detection strategy based on deep learning and boundary element method,"['OriginalPaper', 'Original Paper']","In this article, we propose a data-driven multi-flaw detection strategy based on deep learning and the boundary element method (BEM). In the training phase, BEM is implemented to generate the database, while the block LU decomposition technique is employed to reduce the computational cost. Then the Convolutional Neural Networks (CNNs) are adopted as a deep learning model to find the relationship between the input signals and the geometries of flaws through the training process. In the test phase, the performance of trained models will be evaluated with unseen data. As a typical inverse problem, the solution to a flaw detection problem is not always unique. In the present work, we demonstrate that such non-uniqueness is detrimental to the training process, and avoid them through some specific treatments. In order to enhance the robustness of the model, the idea of data augmentation is introduced to flaw detection tasks. The numerical results show that the presented model could produce accurate predictions in both single- and multi-flaw detection tasks with proper training. Additionally, data augmentation could significantly help against the noise.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Science and Engineering', 'Classical and Continuum Physics']"
doi:10.1038/s41467-022-34719-2,en,Differential compartmentalization of myeloid cell phenotypes and responses towards the CNS in Alzheimer’s disease,"['OriginalPaper', 'Article']","Myeloid cells are suggested as an important player in Alzheimer´s disease (AD). However, its continuum of phenotypic and functional changes across different body compartments and their use as a biomarker in AD remains elusive. Here, we perform multiple state-of-the-art analyses to phenotypically and metabolically characterize immune cells between peripheral blood ( n  = 117), cerebrospinal fluid (CSF, n  = 117), choroid plexus (CP, n  = 13) and brain parenchyma ( n  = 13). We find that CSF cells increase expression of markers involved in inflammation, phagocytosis, and metabolism. Changes in phenotype of myeloid cells from AD patients are more pronounced in CP and brain parenchyma and upon in vitro stimulation, suggesting that AD-myeloid cells are more vulnerable to environmental changes. Our findings underscore the importance of myeloid cells in AD and the detailed characterization across body compartments may serve as a resource for future studies focusing on the assessment of these cells as biomarkers in AD. Multiple state-of-the-art analyses of immune cells in 117 blood, 117 cerebrospinal fluid, 13 choroid plexus and 13 brain parenchyma samples reveal differential characteristics of immune cells in different body compartments and different diseases.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10055-022-00720-9,en,Emotion recognition using facial expressions in an immersive virtual reality application,"['OriginalPaper', 'Original Article']","Facial expression recognition (FER) is an important method to study and distinguish human emotions. In the virtual reality (VR) context, people’s emotions are instantly and naturally triggered and mobilized due to the high immersion and realism of VR. However, when people are wearing head mounted display (HMD) VR equipment, the eye regions will be covered. The FER accuracy will be reduced if the eye region information is discarded. Therefore, it is necessary to obtain the information of eye regions using other methods. The main difficulty in FER in an immersive VR context is that the conventional FER methods depend on public databases. The image facial information in the public databases is complete, so these methods are difficult to directly apply to the VR context. To solve this problem, this paper designs and implements a solution for FER in the VR context as follows. A real facial expression database collection scheme in the VR context is implemented by adding an infrared camera and infrared light source to the HMD. A virtual database construction method is presented for FER in the VR context, which can improve the generalization of models. A deep network named the multi-region facial expression recognition model is designed for FER in the VR context.","['Computer Science', 'Computer Graphics', 'Artificial Intelligence', 'Computer Science, general', 'Image Processing and Computer Vision', 'User Interfaces and Human Computer Interaction']"
doi:10.1007/s13042-022-01715-3,en,RADCU-Net: residual attention and dual-supervision cascaded U-Net for retinal blood vessel segmentation,"['OriginalPaper', 'Original Article']","The automated segmentation of retinal blood vessels plays an important role in the computer aided diagnosis of retinal diseases. In this study, we propose a novel retinal vessel segmentation method based on residual attention and dual-supervision cascaded U-Net (RADCU-Net). Specifically, a residual attention U-Net (RAU-Net), including a residual unit and an attention mechanism, is constructed to improve the feature representation ability by explicitly modelling the interdependency among the channels of the convolutional features. To boost the accuracy of retinal blood vessel segmentation, a cascaded RAU-Net framework is constructed by concatenating two RAU-Nets with the proposed residual attention modules. Moreover, a dual-supervision training strategy is designed to improve the supervision of the cascaded RAU-Net parameter learning by adding an additional balanced cross-entropy loss function in the middle of the cascaded RAU-Net. The results of extensive experiments on the DRIVE and STARE datasets demonstrate that the proposed method achieves better performance compared to state-of-the-art methods. Our method provides a meaningful attempt to improve blood vessel segmentation and can further facilitate the diagnosis of ophthalmological diseases.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Control, Robotics, Mechatronics', 'Complex Systems', 'Systems Biology', 'Pattern Recognition']"
doi:10.1007/s11235-022-00974-3,en,Resource allocation for UAV-assisted 5G mMTC slicing networks using deep reinforcement learning,OriginalPaper,"The Internet of Things (IoT) application scenarios is becoming extensive due to the quick evolution of smart devices with fifth-generation (5G) network slicing technologies. Hence, IoTs are becoming significantly important in 5G/6G networks. However, communication with IoT devices is more sensitive in disasters because the network depends on the main power supply and devices are fragile. In this paper, we consider Unmanned Aerial Vehicles (UAV) as a flying base station (BS) for the emergency communication system with 5G mMTC Network Slicing to improve the quality of user experience. The UAV-assisted mMTC creates a base station selection method to maximize the system energy efficiency. Then, the system model is reduced to the stochastic optimization-based problem using Markov Decision Process (MDP) theory. We propose a reinforcement learning-based dueling-deep-Q-networks (DDQN) technique to maximise energy efficiency and resource allocation. We compare the proposed model with DQN and Q-Learning models and found that the proposed DDQN-based model performs better for resource allocation in terms of low transmission power and maximum energy efficiency.","['Business and Management', 'IT in Business', 'Computer Communication Networks', 'Artificial Intelligence', 'Probability Theory and Stochastic Processes']"
doi:10.1007/s11042-022-14066-6,en,Facial expression recognition based on improved depthwise separable convolutional network,OriginalPaper,"A single network model can’t extract more complex and rich effective features. Meanwhile, the network structure is usually huge, and there are many parameters and consume more space resources, etc. Therefore, the combination of multiple network models to extract complementary features has attracted extensive attention. In order to solve the problems existing in the prior art that the network model can’t extract high spatial depth features, redundant network structure parameters, and weak generalization ability, this paper adopts two models of Xception module and inverted residual structure to build the neural network. Based on this, a face expression recognition method based on improved depthwise separable convolutional network is proposed in the paper. Firstly, Gaussian filtering is performed by Canny operator to remove noise, and combined with two original pixel feature maps to form a three-channel image. Secondly, the inverted residual structure of MobileNetV2 model is introduced into the network structure. Finally, the extracted features are classified by Softmax classifier, and the entire network model uses ReLU6 as the nonlinear activation function. The experimental results show that the recognition rate is 70.76% in Fer2013 dataset (facial expression recognition 2013) and 97.92% in CK+ dataset (extended Cohn Kanade). It can be seen that this method not only effectively mines the deeper and more abstract features of the image, but also prevents network over-fitting and improves the generalization ability.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s12369-022-00946-2,en,Utilizing an Emotional Robot Capable of Lip-Syncing in Robot-Assisted Speech Therapy Sessions for Children with Language Disorders,OriginalPaper,"This study scrutinizes the impacts of utilizing a socially assistive robot, the RASA robot, during speech therapy sessions for children with language disorders. Two capabilities were developed for the robotic platform to enhance children-robot interactions during speech therapy interventions: facial expression communication (containing recognition and expression) and lip-syncing. Facial expression recognition was conducted by training several well-known CNN architectures on one of the most extensive facial expressions databases, the AffectNet database, and then modifying them using the transfer learning strategy performed on the CK+ dataset. The robot’s lip-syncing capability was designed in two steps. The first step was concerned with designing precise schemes of the articulatory elements needed during the pronunciation of the Persian phonemes (i.e., consonants and vowels). The second step included developing an algorithm to pronounce words by disassembling them into their components (including consonants and vowels) and then morphing them into each other successively. To pursue the study’s primary goal, two comparable groups of children with language disorders were considered, the intervention and control groups. The intervention group attended therapy sessions in which the robot acted as the therapist’s assistant, while the control group only communicated with the human therapist. The study’s first purpose was to compare the children’s engagement while playing a mimic game with the affective robot and the therapist, conducted via video coding. The second objective was to assess the efficacy of the robot’s presence in the speech therapy sessions alongside the therapist, accomplished by administering the Persian Test of Language Development, Persian TOLD. According to the first scenario, playing with the affective robot is more engaging than playing with the therapist. Furthermore, the statistical analysis of the study’s results indicates that participating in robot-assisted speech therapy (RAST) sessions enhances children with language disorders’ achievements in comparison with taking part in conventional speech therapy interventions.","['Engineering', 'Control, Robotics, Mechatronics']"
doi:10.1038/s41598-022-24269-4,en,Deep neural network-based structural health monitoring technique for real-time crack detection and localization using strain gauge sensors,"['OriginalPaper', 'Article']","Structural health monitoring (SHM) techniques often require a large number of sensors to evaluate and monitor the structural health. In this paper, we propose a deep neural network (DNN)-based SHM method for accurate crack detection and localization in real time using a small number of strain gauge sensors and confirm its feasibility based on experimental data. The proposed method combines a DNN model with principal component analysis (PCA) to predict the strain field based on the local strains measured by strain gauge sensors located rather sparsely. We demonstrate the potential of the proposed technique via a cyclic 4-point bending test performed on a composite material specimen without cracks and seven specimens with different lengths of cracks. A dataset containing local strains measured with 12 strain gauge sensors and strain field measured with a digital image correlation (DIC) device was prepared. The strain field dataset from DIC is converted to a smaller dimension latent space with a few eigen basis via PCA, and a DNN model is trained to predict principal component values of each image with 12 strain gauge sensor measurements as input. The proposed method turns out to accurately predict the strain field for all specimens considered in the study.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-08036-0,en,"Dual-channel spatial–temporal difference graph neural network for PM
            
              
            
            $$_{2.5}$$
            
              
                
                
                  2.5
                
              
            
           forecasting","['OriginalPaper', 'Original Article']","Accurate PM $$_{2.5}$$ 2.5 forecasting is significant for improving quality of life and human health. However, it is very challenging to capture the high spatiotemporal correlations and the complex diffusion processes of PM $$_{2.5}$$ 2.5 . Most existing PM $$_{2.5}$$ 2.5 prediction methods only focus on spatiotemporal dependencies. In addition, the PM $$_{2.5}$$ 2.5 diffusion process with domain knowledge in deep learning is rarely considered. Therefore, how to simultaneously capture comprehensive spatiotemporal dependencies and model the complicated diffusion process of PM $$_{2.5}$$ 2.5 is still a challenge. To address this problem, we propose a dual-channel spatial–temporal difference graph neural network (DC-STDGN) to forecast future PM $$_{2.5}$$ 2.5 concentrations. DC-STDGN first constructs a dual-channel structure to obtain distance-based local neighboring information and the global hidden spatial correlation of the data. Then, a temporal convolution layer is designed to handle the long-term dependency. Finally, the spatial difference with domain knowledge is introduced to model the complex diffusion process and capture more comprehensive spatiotemporal correlations. The extensive experiments with three real-world datasets demonstrate the improved prediction performance of DC-STDGN over state-of-the-art baselines. DC-STDGN outperforms the second-best model by up to 16.9% improvement in mean absolute error, 8.9% improvement in root mean square error and 18.2% improvement in mean absolute scaled error.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11356-022-24240-w,en,"A comprehensive review of solar irradiation estimation and forecasting using artificial neural networks: data, models and trends","['ReviewPaper', 'Review Article']","Solar irradiation data are imperatively required for any solar energy-based project. The non-accessibility and uncertainty of these data can greatly affect the implementation, management, and performance of photovoltaic or thermal systems. Developing solar irradiation estimation and forecasting approaches is an effective way to overcome these issues. Practically, prediction approaches can help anticipate events by ensuring good operation of the power network and maintaining a precise balance between the demand and supply of the power at every moment. In the literature, various estimation and forecasting methods have been developed. Artificial Neural Network (ANN) models are the most commonly used methods in solar irradiation prediction. This paper aims to firstly review, analyze, and provide an overview of different aspects required to develop an ANN model for solar irradiation prediction, such as data types, data horizon, data preprocessing, forecasting horizon, feature selection, and model type. Secondly, a highly detailed state of the art of ANN-based approaches including deep learning and hybrid ANN models for solar irradiation estimation and forecasting is presented. Finally, the factors influencing prediction model performances are discussed in order to propose recommendations, trends, and outlooks for future research in this field.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s00371-022-02725-6,en,IV-Net: single-view 3D volume reconstruction by fusing features of image and recovered volume,"['OriginalPaper', 'Original article']","Single-view 3D reconstruction aims to recover the 3D shape from one image of an object and has attracted increasingly attention in recent years. Mostly, previous works are devoted to learning a mapping from 2 to 3D, and lack of spatial information of objects will cause inaccurate reconstruction on the details of objects. To address this issue, for single-view 3D reconstruction, we propose a novel voxel-based network by fusing features of image and recovered volume, named IV-Net. By a pre-trained baseline, it achieves image feature and a coarse volume from each image input, where the recovered volume contains spatial semantic information. Specially, the multi-scale convolutional block is designed to improve 2D encoder by extracting multi-scale image information. To recover more accurate shape and details of the object, an IV refiner is further used to reconstruct the final volume. We conduct experimental evaluations on both synthetic ShapeNet dataset and real-world Pix3D dataset, and results of comparative experiments indicate that our IV-Net outperforms state-of-the-art approaches about accuracy and parameters.","['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s11222-022-10172-5,en,Bayesian learning via neural Schrödinger–Föllmer flows,"['OriginalPaper', 'OriginalPaper']","In this work we explore a new framework for approximate Bayesian inference in large datasets based on stochastic control. We advocate stochastic control as a finite time and low variance alternative to popular steady-state methods such as stochastic gradient Langevin dynamics. Furthermore, we discuss and adapt the existing theoretical guarantees of this framework and establish connections to already existing VI routines in SDE-based models.","['Computer Science', 'Artificial Intelligence', 'Statistics and Computing/Statistics Programs', 'Statistical Theory and Methods', 'Probability and Statistics in Computer Science']"
doi:10.1007/s40808-022-01602-4,en,Managing groundwater demand through surface water and reuse strategies in an overexploited aquifer of Indian Punjab,"['OriginalPaper', 'Original Article']","Groundwater sustainability is one of the most critical issues to the State of Punjab, India. In this research, a numerical groundwater flow model (MODFLOW) was employed to simulate flow and groundwater levels in the Sirhind Canal Tract of Punjab between 1998 and 2030. Historical groundwater patterns were calibrated using reported groundwater data from 1998 to 2013 for aquifer parameters viz. hydraulic conductivity and specific yield. Thereafter, calibrated flow simulated model was validated for the years 2013–2018. Twelve possible strategies, including three irrigation conditions and four pumping scenarios, were postulated to evaluate the performance of groundwater resources through to 2030. During the study, it was found that if current groundwater abstraction continues there will be further steep decline of 21.49 m in groundwater level by 2030. Findings also suggest that canal water supplies will be beneficial to reverse groundwater level decline and help to increase the water level by 11% above that in year 2018. The projected increases in water level will reduce energy demand leading to reduced CO 2 emissions of approximately 966.6 thousand tonnes by 2030.","['Earth Sciences', 'Earth System Sciences', 'Math. Appl. in Environmental Science', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Applications in the Physical Sciences', 'Ecosystems', 'Environment, general']"
doi:10.1007/s11042-022-14138-7,en,Smartbanner: intelligent banner design framework that strikes a balance between creative freedom and design rules,OriginalPaper,"Companies use banners extensively to promote their products, and the intelligent automatic synthesis of banners is a challenging event. Under the premise of inputting only a small amount of information such as product, text and size, it can synthesize styles with high freedom and richness, but at the same time, it must satisfy the design specifications of advertisers for advertising and scenes. We propose an intelligent banner design framework that strikes a balance between creative freedom and design rules, called smartbanner. Smartbanner consists of planner, actuator, adjuster and generator. The banner is synthesized through the combined framework, which fully liberates the designer and reduces the threshold and cost of design. It increases the click-through rate by 30%, improves the human efficiency of designers by 500% under the condition of ensuring the quality of creation, and synthesizes hundreds of millions of pictures in batches throughout the year.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s10994-022-06261-1,en,SLISEMAP: supervised dimensionality reduction through local explanations,OriginalPaper,"Existing methods for explaining black box learning models often focus on building local explanations of the models’ behaviour for particular data items. It is possible to create global explanations for all data items, but these explanations generally have low fidelity for complex black box models. We propose a new supervised manifold visualisation method, slisemap , that simultaneously finds local explanations for all data items and builds a (typically) two-dimensional global visualisation of the black box model such that data items with similar local explanations are projected nearby. We provide a mathematical derivation of our problem and an open source implementation implemented using the GPU-optimised PyTorch library. We compare slisemap to multiple popular dimensionality reduction methods and find that slisemap is able to utilise labelled data to create embeddings with consistent local white box models. We also compare slisemap to other model-agnostic local explanation methods and show that slisemap provides comparable explanations and that the visualisations can give a broader understanding of black box regression and classification models.","['Computer Science', 'Machine Learning', 'Control, Robotics, Mechatronics', 'Artificial Intelligence', 'Simulation and Modeling', 'Natural Language Processing (NLP)']"
doi:10.1007/s10489-022-04310-9,en,KnAC: an approach for enhancing cluster analysis with background knowledge and explanations,OriginalPaper,"Pattern discovery in multidimensional data sets has been the subject of research for decades. There exists a wide spectrum of clustering algorithms that can be used for this purpose. However, their practical applications share a common post-clustering phase, which concerns expert-based interpretation and analysis of the obtained results. We argue that this can be the bottleneck in the process, especially in cases where domain knowledge exists prior to clustering. Such a situation requires not only a proper analysis of automatically discovered clusters but also conformance checking with existing knowledge. In this work, we present Knowledge Augmented Clustering ( KnAC ). Its main goal is to confront expert-based labelling with automated clustering for the sake of updating and refining the former. Our solution is not restricted to any existing clustering algorithm. Instead, KnAC can serve as an augmentation of an arbitrary clustering algorithm, making the approach robust and a model-agnostic improvement of any state-of-the-art clustering method. We demonstrate the feasibility of our method on artificially, reproducible examples and in a real life use case scenario. In both cases, we achieved better results than classic clustering algorithms without augmentation.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s42235-022-00288-9,en,Non-dominated Sorting Advanced Butterfly Optimization Algorithm for Multi-objective Problems,"['OriginalPaper', 'Research Article']","This paper uses the Butterfly Optimization Algorithm (BOA) with dominated sorting and crowding distance mechanisms to solve multi-objective optimization problems. There is also an improvement to the original version of BOA to alleviate its drawbacks before extending it into a multi-objective version. Due to better coverage and a well-distributed Pareto front, non-dominant rankings are applied to the modified BOA using the crowding distance strategy. Seven benchmark functions and eight real-world problems have been used to test the performance of multi-objective non-dominated advanced BOA (MONSBOA), including unconstrained, constrained, and real-world design multiple-objective, highly nonlinear constraint problems. Various performance metrics, such as Generational Distance (GD), Inverted Generational Distance (IGD), Maximum Spread (MS), and Spacing (S), have been used for performance comparison. It is demonstrated that the new MONSBOA algorithm is better than the compared algorithms in more than 80% occasions in solving problems with a variety of linear, nonlinear, continuous, and discrete characteristics based on the Pareto front when compared quantitatively. From all the analysis, it may be concluded that the suggested MONSBOA is capable of producing high-quality Pareto fronts with very competitive results with rapid convergence.","['Engineering', 'Biomedical Engineering and Bioengineering', 'Biomaterials', 'Artificial Intelligence', 'Biomedical Engineering/Biotechnology', 'Biochemical Engineering', 'Bioinformatics']"
doi:10.1007/s11227-022-04931-4,en,Independent vector analysis based on binary grey wolf feature selection and extreme learning machine for bearing fault diagnosis,OriginalPaper,"This paper develops a new architecture for bearing fault diagnosis based on independent vector analysis, feature selection, and extreme learning machines classifiers. The suggested method applied to vibration signals includes the following steps: First, the independent vector analysis is introduced to separate vibration signal components from each other. Second, statistical parameters are extracted from all the obtained sources. Then three binary optimisation algorithms such as binary bat algorithm, binary particle swarm optimisation and binary grey wolf optimisation are employed for feature selection one by one. Finally, three classifiers based on extreme learning, artificial neural networks and random forest are used to perform the classification step. The obtained results show that independent vector analysis followed by feature selection based on binary grey wolf optimisation and classification using an extreme learning machine provides an optimal input vector which contains only five features for each sample and a very small misdiagnosis rate equal to 0.76%. The obtained results also prove that the suggested methodology gives the best classification results and high visibility compared to the other studied approaches.","['Computer Science', 'Programming Languages, Compilers, Interpreters', 'Processor Architectures', 'Computer Science, general']"
doi:10.1007/s12046-022-02014-x,en,Machine learning solution of a coalitional game for optimal power allocation in multi-relay cooperative scenario,OriginalPaper,"This paper reports a novel Machine Learning (ML) solution to power allocation problem modelled as a Stackelberg game. We consider a multi-relay cooperative environment where the performance of the relays is dependent upon the resources allocated to them. As a first step, a game theoretic framework has been used for modelling cooperation and competition among the relays. This Stackelberg game-based framework considers benefits of source and relays jointly utilizing a strategy for optimal power allocation based on incentives provided to the cooperating relays. Subsequently, an optimal set of relays is identified through machine leaning based bilevel optimization of objective functions which define the aforementioned Stackelberg game. The proposed ML optimization helps the source increase its utility by allocating optimal power to the participating relays at an optimal price. Results from simulation experiments confirm that the ML based solution of Stackelberg game optimization problem provides consistently better performance in terms of system throughput as compared to the centralized scheme and an earlier reported heuristic scheme.","['Engineering', 'Engineering, general']"
doi:10.1038/s41598-022-24445-6,en,Damage assessment of suspension footbridge using vibration measurement data combined with a hybrid bee-genetic algorithm,"['OriginalPaper', 'Article']","Optimization algorithms (OAs) are a vital tool to deal with complex problems, and the improvement of OA is inseparable from practical strategies and mechanisms. Among the OAs, Bee Algorithm (BA) is an intelligent algorithm with a simple mechanism and easy implementation, in which effectiveness has been proven when handling optimization problems. Nevertheless, BA still has some fundamental drawbacks, which can hinder its effectiveness and accuracy. Therefore, this paper proposes a novel approach to tackle the shortcomings of BA by combining it with Genetic Algorithm (GA). The main intention is to combine the strengths of both optimization techniques, which are the exploitative search ability of BA and the robustness with the crossover and mutation capacity of GA. An investigation of a real-life suspension footbridge is considered to validate the effectiveness of the proposed method. A baseline Finite Element model of the bridge is constructed based on vibration measurement data and model updating, which is used to generate different hypothetical damage scenarios. The proposed HBGA is tested against BA, GA, and PSO to showcase its effectiveness in detecting damage for each scenario. The results show that the proposed algorithm is effective in dealing with the damage assessment problems of SHM.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-24668-7,en,Utilizing artificial intelligence to solving time – cost – quality trade-off problem,"['OriginalPaper', 'Article']","This study presents the Slime Mold Algorithm (SMA) to solve the time—cost—quality trade-off problem in a construction project. The proposed SMA is a flexible and efficient algorithm in exploration and exploitation to reach the best optimal solution to process the input model’s data. This paper aims to discuss and solve the optimization problem and compare the evaluation with other algorithms such as Opposition-based Multiple Objective Differential Evolution, Non-dominated sorting genetic algorithm, Multiple objective particle swarm optimization, Multiple objective differential evolution and Chaotic initialized multiple objective differential evolution (CAMODE) to verify the efficiency and potential of the proposed algorithm. According to the analysis results, the SMA model generated a diversification measure for case studies, producing superior outcomes to those of previous algorithms.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s40435-022-01076-3,en,Optimal fuzzy controller design for industrial gear system by gray wolf algorithm,OriginalPaper,"Chaotic behavior in gear system vibrations is deemed undesirable. Chaotic conduct must therefore be eliminated and controlled at all costs. This work proposes an optimal fuzzy controller to control the gear system's chaotic behavior. A gray wolf algorithm is used in this scheme to adjust the fuzzy membership functions and controller parameters. The simulation results in the presence and absence of noise were examined in order to evaluate this method. Finally, the proposed method is contrasted with other types of control methods. The results show that the proposed method outperformed similar methods in terms of mean square error and fluctuations in the goal of eliminating chaotic behavior.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Control and Systems Theory', 'Complexity']"
doi:10.1038/s42004-022-00770-9,en,A semi-automated material exploration scheme to predict the solubilities of tetraphenylporphyrin derivatives,"['OriginalPaper', 'Article']","Acceleration of material discovery has been tackled by informatics and laboratory automation. Here we show a semi-automated material exploration scheme to modelize the solubility of tetraphenylporphyrin derivatives. The scheme involved the following steps: definition of a practical chemical search space, prioritization of molecules in the space using an extended algorithm for submodular function maximization without requiring biased variable selection or pre-existing data, synthesis & automated measurement, and machine-learning model estimation. The optimal evaluation order selected using the algorithm covered several similar molecules (32% of all targeted molecules, whereas that obtained by random sampling and uncertainty sampling was ~7% and ~4%, respectively) with a small number of evaluations (10 molecules: 0.13% of all targeted molecules). The derived binary classification models predicted ‘good solvents’ with an accuracy >0.8. Overall, we confirmed the effectivity of the proposed semi-automated scheme in early-stage material search projects for accelerating a wider range of material research. Prediction of material properties is crucial for early stages of material research, but current experimental data-based strategies possess limited accuracy. Here, the authors develop a machine learning-based semi-automated material exploration scheme to predict the solubility of tetraphenylporphyrin derivatives with an accuracy above 0.8.","['Chemistry', 'Chemistry/Food Science, general']"
doi:10.1186/s44147-022-00159-4,en,MarianCG: a code generation transformer model inspired by machine translation,"['OriginalPaper', 'Research']","The idea that computers can build their own programs is extremely significant, and many researchers are working on this challenge. Code generation is described as the process of generating executable code that can be run directly on the computer and fulfills the natural language requirements. It is an intriguing topic that might assist developers to learn a new software technology or programming language, or it could be a simple technique to help in coding through the description of the natural language code developer. In this paper, we present MarianCG, a code generation Transformer model used to tackle the code generation challenge of generating python code from natural language descriptions. Marian neural machine translation (NMT), which is the core model of the Microsoft Translator, is the basis for our NL-to-Code translation engine and is the heart of the teaching model. MarianMT is the teacher language model in our study, and it is one of the most successful machine translation transformers. In our approach, we use a sinusoidal positional embedding technique to represent the position of each token in the text, as well as no layer normalization embedding. Our code generation approach, MarianCG, is based on fine-tuning a machine translation pre-trained language model. This allows us to demonstrate that the pre-trained translation model can also operate and work as a code generation model. The proposed model outperforms recent state-of-the-art models in the problem of code generation when trained on the CoNaLa and DJANGO datasets. MarianCG model scores a BLEU score of 34.43 and an exact match accuracy of 10.2% on the CoNaLa dataset. Also, this model records a BLEU score of 90.41 and an exact match accuracy of 81.83% on the DJANGO dataset. The implementation of MarianCG model and relevant resources are available at  https://www.github.com/AhmedSSoliman/MarianCG-NL-to-Code .","['Engineering', 'Engineering, general']"
doi:10.1007/s10845-022-02052-6,en,Dynamic spatial–temporal graph-driven machine remaining useful life prediction method using graph data augmentation,OriginalPaper,"It is beneficial to maintain the normal operation of machines by conducting remaining useful life (RUL) prediction. Recently, graph data-driven machine RUL prediction methods have made a great success, since graph can model spatial and temporal dependencies of signals. However, the constructed graphs still have some limitations: (1) In the practical industrial production, the installation of multi-sensor networks is expensive and hard to achieve, so the single sensor is commonly used for data monitoring. However, most of these methods constructed graphs by establishing relationships between the different sensors, which are completely unsuitable for prediction tasks in single-sensor scenarios. (2) The quality of constructed graph is low, where the graph structure is fixed, failing in representing the machine degradation process. To overcome these limitations, a dynamic spatial–temporal (ST) graph-driven machine RUL prediction method using graph data augmentation (GDA) is proposed. The ST graph is constructed using short-time Fourier transform, capturing the frequency-domain and time-domain information hidden in the signals. Then, a GDA framework is designed to generate dynamic ST graphs, enlarging the structural differences of subgraphs. Subsequently, a GDA-based graph deep learning prediction model is constructed for dynamic ST graph-based RUL prediction, where an autoencoder-based graph embedding module is designed to replace simple Readout. Verification experiments are conducted on two case studies, and the results show that the proposed prediction method achieves a competitive performance.","['Business and Management', 'Production', 'Manufacturing, Machines, Tools, Processes', 'Control, Robotics, Mechatronics']"
doi:10.1007/s00521-022-08005-7,en,A deep-learning-based facial expression recognition method using textural features,"['OriginalPaper', 'Original Article']","Human facial expression and emotion play pivotal roles in our day-to-day communication, and detecting them are one of the formidable tasks in the field of human–computer interfaces (HCI). This paper presents a new facial expressions detection method by exploiting textural image features such as local binary patterns (LBP), local ternary patterns (LTP) and completed local binary pattern (CLBP). This paper utilizes the advantages of textural features which are highly correlated with the facial expression changes and thereby trains a convolution neural network (CNN) model to detect facial expressions. The CNN model is trained on the images from the extended Cohn-Kanade (CK +), JAFEE and FER2013 datasets that are converted into LBP, LTP and CLBP image features. The performance of our facial expression recognition system is validated on modified CK+, JAFEE and FER2013 dataset. The results reported here illustrates that the CNN model yields better efficiency when we train the model with textural images. Moreover, we have shown that the CNN model trained with CLBP outperforms than that of with LBP and LTP images. In case of CLBP images, accuracies are 91.0%, 82.2% and 64.5% for CK+, JAFFE and FER2013 dataset, respectively. In case of LBP, accuracies are 79.5%, 75% and 58.45% and in case of LTP images accuracies are 89.2%, 77.3% and 62.79% for the datasets CK+, JAFFE and FER2013, respectively.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-07894-y,en,Improving automated latent fingerprint detection and segmentation using deep convolutional neural network,"['OriginalPaper', 'Original Article']","Latent fingerprint segmentation is a complex process of separating relevant areas called fingerprints from an irrelevant background in the latent fingerprint image which is of poor quality. A breakthrough in the field can be used to segment fingerprints accurately from the background by using optimal resources. Processing of unwanted background of the entire image can lead to false and missed detection of fingerprints. An early fingerprint distinction technique based on colour and saliency masks is proposed to detect potentially relevant areas out of the entire image area for further processing, using a non-learning approach. Later, the patches of early detected fingermarks are fed to a stacked convolutional autoencoder for separating imposters of fingerprint(s) region from relevant fingerprint(s) regions, using a deep learning approach. The inspiration to use the convolutional neural network in this hybrid approach is to effectively capture feature distinction from potential features similar to that of object detection and classification. The inspiration to use autoencoder in a stack is to provide better feature engineering for CNN. The use of the pre-trained convolutional neural network with a stack of autoencoders for image classification and segmentation produces better results than a naive convolutional neural network. The experiments are conducted on the IIIT-D database. The efficiency and effectiveness of the model over good quality images is evaluated by experimenting over different patch sizes, with and without the use of dropout in CNN, with and without use of Autoencoder with CNN. The early detection of contours along with patch-based classification-cum-segmentation using SCAE on good quality images produces 98.45% segmentation accuracy.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s10546-022-00761-2,en,Estimation of the Surface Fluxes for Heat and Momentum in Unstable Conditions with Machine Learning and Similarity Approaches for the LAFE Data Set,"['OriginalPaper', 'Research Article']","Measurements of three flux towers operated during the land atmosphere feedback experiment (LAFE) are used to investigate relationships between surface fluxes and variables of the land–atmosphere system. We study these relations by means of two machine learning (ML) techniques: multilayer perceptrons (MLP) and extreme gradient boosting (XGB). We compare their flux derivation performance with Monin–Obukhov similarity theory (MOST) and a similarity relationship using the bulk Richardson number (BRN). The ML approaches outperform MOST and BRN. Best agreement with the observations is achieved for the friction velocity. For the sensible heat flux and even more so for the latent heat flux, MOST and BRN deviate from the observations while MLP and XGB yield more accurate predictions. Using MOST and BRN for latent heat flux, the root mean square errors (RMSE) are 107 Wm $$^{-2}$$ - 2 and 121 Wm $$^{-2}$$ - 2 , respectively, as well as the intercepts of the regression lines are $$\approx 110$$ ≈ 110  Wm $$^{-2}$$ - 2 . For the ML methods, the RMSEs reduce to 31 Wm $$^{-2}$$ - 2 for MLP and 33 Wm $$^{-2}$$ - 2 for XGB as well as the intercepts to just 4 Wm $$^{-2}$$ - 2 for MLP and $$-1$$ - 1  Wm $$^{-2}$$ - 2 for XGB with slopes of the regression lines close to 1, respectively. These results indicate significant deficiencies of MOST and BRN, particularly for the derivation of the latent heat flux. In fact, in contrast to the established theories, feature importance weighting demonstrates that the ML methods base their improved derivations on net radiation, the incoming and outgoing shortwave radiations, the air temperature gradient, and the available water contents, but not on the water vapor gradient. The results imply that further studies of surface fluxes and other turbulent variables with ML techniques provide great promise for deriving advanced flux parameterizations and their implementation in land–atmosphere system models.","['Earth Sciences', 'Atmospheric Sciences', 'Meteorology', 'Atmospheric Protection/Air Quality Control/Air Pollution']"
doi:10.1007/s13349-022-00651-8,en,Artificial intelligence enhanced automatic identification for concrete cracks using acoustic impact hammer testing,"['OriginalPaper', 'Original Paper']","Impact hammer testing is a regular structure inspection method for detecting surface and internal damages. Inspectors use the sound from impact hammer testing to determine the damaged area. However, manual impact hammer testing cannot meet the reliable accuracy for small damages, such as concrete cracks, and due to the shortage of experienced workers, a reliable tool is needed to evaluate the hammering sound. Therefore, to improve the detection accuracy, this study proposes an automatic crack identification process of impact hammer testing. Three approaches are used to identify crack characteristics, such as width, depth, and location, based on fast Fourier transformation for the hammering sound. To determine the relationship between damaged and intact information values, the first and second approaches use dominant frequency ( $$D_{f}$$ D f ) and frequency feature value ( $$V_{f}$$ V f ), respectively, whereas the last one uses Mel-frequency cepstral coefficients (MFCCs). Six concrete specimens with different crack widths and depths were fabricated to validate the three approaches. The experimental results reveal that although $$D_{f}$$ D f can to detect the damage, it cannot classify its depth and width. Furthermore, $$V_{f}$$ V f indicates the cracks, which are 20-mm deep. Three different artificial-intelligence classification algorithms were used to validate the MFCC approach, fuzzy rule, gradient boosted trees, and support vector machine (SVM). The three algorithms are applied and evaluated to enhance the acoustic impact hammer testing. The results reveal that the SVM algorithm confirms the ability and effectiveness for accurately identifying the concrete fine cracks that are 0.2-mm wide and 40-mm deep.","['Engineering', 'Civil Engineering', 'Measurement Science and Instrumentation', 'Vibration, Dynamical Systems, Control']"
doi:10.1007/s12532-022-00231-3,en,A memetic procedure for global multi-objective optimization,"['OriginalPaper', 'Full Length Paper ']","In this paper we consider multi-objective optimization problems over a box. Several computational approaches to solve these problems have been proposed in the literature, that broadly fall into two main classes: evolutionary methods, which are usually very good at exploring the feasible region and retrieving good solutions even in the nonconvex case, and descent methods, which excel in efficiently approximating good quality solutions. In this paper, first we confirm, through numerical experiments, the advantages and disadvantages of these approaches. Then we propose a new method which combines the good features of both. The resulting algorithm, which we call Non-dominated Sorting Memetic Algorithm, besides enjoying interesting theoretical properties, excels in all of the numerical tests we performed on several, widely employed, test functions.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Theory of Computation', 'Mathematics of Computing']"
doi:10.1007/s10489-022-04261-1,en,Exploiting semantic-level affinities with a mask-guided network for temporal action proposal in videos,"['OriginalPaper', 'Original Submission']","Temporal action proposal (TAP) aims to detect the action instances’ starting and ending times in untrimmed videos, which is fundamental and critical for large-scale video analysis and human action understanding. The main challenge of the temporal action proposal lies in modeling representative temporal relations in long untrimmed videos. Existing state-of-the-art methods achieve temporal modeling by building local-level, proposal-level, or global-level temporal dependencies. Local methods lack a wider receptive field, while proposal and global methods lack the focalization of learning action frames and contain background distractions. In this paper, we propose that learning semantic-level affinities can capture more practical information. Specifically, by modeling semantic associations between frames and action units, action segments (foregrounds) can aggregate supportive cues from other co-occurring actions, and nonaction clips (backgrounds) can learn the discriminations between them and action frames. To this end, we propose a novel framework named the Mask-Guided Network (MGNet) to build semantic-level temporal associations for the TAP task. Specifically, we first propose a Foreground Mask Generation (FMG) module to adaptively generate the foreground mask, representing the locations of the action units throughout the video. Second, we design a Mask-Guided Transformer (MGT) by exploiting the foreground mask to guide the self-attention mechanism to focus on and calculate semantic affinities with the foreground frames. Finally, these two modules are jointly explored in a unified framework. MGNet models the intra-semantic similarities for foregrounds, extracting supportive action cues for boundary refinement; it also builds the inter-semantic distances for backgrounds, providing the semantic gaps to suppress false positives and distractions. Extensive experiments are conducted on two challenging datasets, ActivityNet-1.3 and THUMOS14, and the results demonstrate that our method achieves superior performance.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s10207-022-00640-4,en,User identification using deep learning and human activity mobile sensor data,"['OriginalPaper', 'Regular contribution']","The ownership of user actions in computer and mobile applications is an important concern, especially when using shared devices. User identification using physical biometric authentication methods permits the actual user to access the device. However, in cases where different users may access a shared device during the same active session, the person who owns the active session will be accountable for any actions performed by the other users. Thus, user identification using behavioral characteristics has come into the picture. Human activity recognition from mobile sensor data is gaining more interest with the advent of mobile devices and the emergence of the Internet of Things, where different applications such as elderly health monitoring, athletic evaluation, and context-aware behavior are being developed. In this paper, we show how human activity data can be utilized to identify the actual device user. We build deep learning models that are capable of identifying the users of mobile and wearable devices based on their body movements and daily activities. We use the Long Short-Term Memory classifier for building the user identification model based on time-series data from mobile motion sensors. The model targets the users that were involved in the training process. We tested our approach on two publicly available human activity datasets that contain daily activities and fall states data from accelerometer and gyroscope mobile sensors. The results show that the models are capable of identifying the actual mobile device users from their motion data with an accuracy of up to 90%. Further, the results show that the model from the accelerometer data outperforms the one from gyroscope data.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Coding and Information Theory', 'Management of Computing and Information Systems', 'Communications Engineering, Networks']"
doi:10.1038/s41598-022-24574-y,en,Quantifying deep neural network uncertainty for atrial fibrillation detection with limited labels,"['OriginalPaper', 'Article']","Atrial fibrillation (AF) is the most common arrhythmia found in the intensive care unit (ICU), and is associated with many adverse outcomes. Effective handling of AF and similar arrhythmias is a vital part of modern critical care, but obtaining knowledge about both disease burden and effective interventions often requires costly clinical trials. A wealth of continuous, high frequency physiological data such as the waveforms derived from electrocardiogram telemetry are promising sources for enriching clinical research. Automated detection using machine learning and in particular deep learning has been explored as a solution for processing these data. However, a lack of labels, increased presence of noise, and inability to assess the quality and trustworthiness of many machine learning model predictions pose challenges to interpretation. In this work, we propose an approach for training deep AF models on limited, noisy data and report uncertainty in their predictions. Using techniques from the fields of weakly supervised learning, we leverage a surrogate model trained on non-ICU data to create imperfect labels for a large ICU telemetry dataset. We combine these weak labels with techniques to estimate model uncertainty without the need for extensive human data annotation. AF detection models trained using this process demonstrated higher classification performance (0.64–0.67 F1 score) and improved calibration (0.05–0.07 expected calibration error).","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00521-022-07811-3,en,An accurate flexible process planning using an adaptive genetic algorithm,"['OriginalPaper', 'Original Article']","The increasing demand for products and services due to globalization has quickly increased through the years. Under such circumstances, the improvement in manufacturing processes has taken the attention of several areas of engineering. Different schemas have been introduced in the context of distributed manufacturing, such as the flexible use of tools, machines, and tool access directions which become a complex task considering the difficult combinatory process and rigorous restrictions. To overcome such complications, flexible process planning (FPP) has been treated as an optimization problem. Moreover, the problem difficulty compromises the proper balance between performance and computational cost which generates the proposal of different optimization techniques, statistical criteria, and hybridizations. Despite the good results of different methods, there are still several possibilities for improvement. In this work, a genetic algorithm (GA) is employed for an accurate FPP process where the GA operators are adapted in order to join up the combinatory optimization process of FPP with the main structure of GA (aGA). To carry out the experimentation, different scenarios of FPP problems using AND/OR networks, production time, and production cost are considered. The adapted genetic algorithm for flexible process planning (aGA-FPP) problems has shown competitive results regarding similar approaches and hybridizations reported in the literature.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00521-022-08028-0,en,Identifying human activities in megastores through postural data to monitor shoplifting events,"['OriginalPaper', 'Original Article']","In recent years, modeling activity patterns for understanding events and human behavior has drawn prominent attention in research. Multiple methods have been proposed for developing automated vision systems that are capable of inferring accurate semantics from the moving dynamics. The multi-disciplinary nature of Human Activity Recognition (HAR) methods and the expanding technologies in this field inspire continual updates in existing methods. However, a cost-effective solution is still needed to recognize human activities like shoplifting in an occluded environment. With this motivation, we present a novel approach to identify human stealing actions by analyzing the postural information of the human body. This approach involves extracting 2D postural body joints of a human being from the captured frame. Pose encoding and postural feature generation in parameter space are the foremost contributions of this work, which can handle the occluded actions too. The feature reduction is done to scale the features into a smaller dimension with an objective of the computationally efficient and real-time solution. Activity classification is done on the reduced feature sets to detect human shoplifting actions in real-time scenarios. Experiments are performed on the synthesized shoplifting dataset, where the results derived are found more promising compared to other state-of-the-art methods, with an accuracy of 96.87%. Additionally, this method exhibits commendable real-time performance in processing actual store camera footage.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s11669-022-01010-2,en,A Neural Network Approach to Predict Gibbs Free Energy of Ternary Solid Solutions,OriginalPaper,"We present a data-centric deep learning (DL) approach using neural networks (NNs) to predict the thermodynamics of ternary solid solutions. We explore how NNs can be trained with a dataset of Gibbs free energies computed from a CALPHAD database to predict ternary systems as a function of composition and temperature. We have chosen the energetics of the FCC solid solution phase in 226 binaries consisting of 23 elements at 11 different temperatures to demonstrate the feasibility. The number of binary data points included in the present study is 102,000. We select six ternaries to augment the binary dataset to investigate their influence on the NN prediction accuracy. We examine the sensitivity of data sampling on the prediction accuracy of NNs over selected ternary systems. It is anticipated that the current DL workflow can be further elevated by integrating advanced descriptors beyond the elemental composition and more curated training datasets to improve prediction accuracy and applicability.","['Physics', 'Crystallography and Scattering Methods', 'Thermodynamics', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Ceramics, Glass, Composites, Natural Materials', 'Metallic Materials']"
doi:10.1007/s00521-022-08040-4,en,MPCSAN: multi-head parallel channel-spatial attention network for facial expression recognition in the wild,"['OriginalPaper', 'Original Article']","Facial expression recognition (FER) in the wild is an exceedingly challenging task in computer vision due to subtle differences, poses, occlusions, label bias, and other uncontrollable factors. CNN-based deep learning networks are susceptible to the above factors, resulting in the inability to obtain highly discriminative features on the key regions of expressions, and most methods of learning in a single feature space may not fully capture the core regions of interest. These will directly affect the solution to the problem of intra-class variability and inter-class similarity of expressions, which ultimately affects the recognition performance. Therefore, we propose an effective multi-head parallel channel-spatial attention network (MPCSAN) for FER in the wild, which consists of a feature aggregation network (FAN), a multi-head parallel attention network (MPAN), and an expression forecasting network (EFN). First, the lightweight FAN network extracts basic expression features while optimizing intra-class and inter-class distribution. Then, MPAN forms a multi-attention subspace by a multi-head parallel channel-space attention fusion design and focuses on more accurate and comprehensive expression regions of interest by minimizing duplicate attention during subspace fusion. Finally, EFN performs the final expression classification under the optimization of label softening, which further improves the robustness problem caused by label bias. Our proposed method is evaluated on the three most widely used wild expression datasets (RAF-DB, FERPlus, and AffectNet). The extensive experimental results demonstrate that our method outperforms several current state-of-the-art methods, achieving accuracies of 90.16% on RAF-DB, 89.91% on FERPlus, and 61.58% on AffectNet, respectively. Occlusion and pose variation datasets evaluation and cross-dataset assessment further demonstrate the good comprehensive performance of our method.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s40808-022-01608-y,en,Crop type classification with hyperspectral images using deep learning : a transfer learning approach,"['OriginalPaper', 'Original Article']","Crop classification plays a vital role in felicitating agriculture statistics to the state and national government in decision-making. In recent years, due to advancements in remote sensing, high-resolution hyperspectral images (HSIs) are available for land cover classification. HSIs can classify the different crop categories precisely due to their narrow and continuous spectral band reflection. With improvements in computing power and evolution in deep learning technology, Deep learning is rapidly being used for HSIs classification. However, to train deep neural networks, many labeled samples are needed. The labeling of HSIs is time-consuming and costly. A transfer learning approach is used in many applications where a labeled dataset is challenging. This paper opts for the heterogeneous transfer learning models on benchmark HSIs datasets to discuss the performance accuracy of well-defined deep learning models—VGG16, VGG19, ResNet, and DenseNet for crop classification. Also, it discusses the performance accuracy of customized 2-dimensional Convolutional neural network (2DCNN) and 3-dimensional Convolutional neural network (3DCNN) deep learning models using homogeneous transfer learning models on benchmark HSIs datasets for crop classification. The results show that although HSIs datasets contain few samples, the transfer learning models perform better with limited labeled samples. The results achieved 99% of accuracy for the Indian Pines and Pavia University dataset with 15% of labeled training samples with heterogeneous transfer learning. As per the overall accuracy, homogeneous transfer learning with 2DCNN and 3DCNN models pre-trained on the Indian Pines dataset and adjusted on the Salinas scene dataset performs far better than heterogeneous transfer learning.","['Earth Sciences', 'Earth System Sciences', 'Math. Appl. in Environmental Science', 'Statistics for Engineering, Physics, Computer Science, Chemistry and Earth Sciences', 'Mathematical Applications in the Physical Sciences', 'Ecosystems', 'Environment, general']"
doi:10.1038/s41598-022-24059-y,en,Reconstruction of 3D topographic landscape in soft X-ray fluorescence microscopy through an inverse X-ray-tracing approach based on multiple detectors,"['OriginalPaper', 'Article']","The study of X-ray fluorescence (XRF) emission spectra is a powerful technique used in applications that range from biology to cultural heritage. Key objectives of this technique include identification and quantification of elemental traces composing the analyzed sample. However, precise derivation of elemental concentration is often hampered by self-absorption of the XRF signal emitted by light constituents. This attenuation depends on the amount of sample present between the radiation source and detection system and allows for the exploitation of self-absorption in order to recover a sample topography. In this work, an X-ray-tracing application based on the use of multiple silicon drift detectors, is introduced to inversely reconstruct a 3D sample with correct topographical landscape, from 2D XRF count rates maps obtained from spectroscopy. The reconstruction was tested on the XRF maps of a simulated sample, which is composed of three cells with different size but similar composition. We propose to use the recovered 3D sample topography in order to numerically compute the self-absorption effects on the X-ray fluorescence radiation, thereby showing that a quantitative correction is possible. Lastly, we present a web application which implements the suggested methodology, in order to demonstrate its feasibility and applicability, available at: https://github.com/ElettraSciComp/xrfstir .","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11042-022-14162-7,en,Cross-scale content-based full Transformer network with Bayesian inference for object tracking,OriginalPaper,"Visual tracking is fundamentally the problem of conditional probability regressing of the target location in each video frame. Convolutional neural network (CNN) have been dominant in visual tracking these years, but CNN-based trackers neglect long-range dependency in likelihood representation and prior information, these destroy the spatial consistency of target. Recently emerging Transformer-based trackers mitigate these, however, they do not possess the ability to build interactions among features of cross-scale. Moreover, the sine position encoding prior in Transformer-based tracker is content-unaware and fails to reflect the relative index of different positions. To address these issues and inspired by Bayesian probabilistic formulation, we propose a cross-scale full Transformer tracker with content-based prior bias (named BTT). There are four main contributions of the method, (i) we propose a hierarchical full Transformer tracking architecture to introduce long-range dependency, which enriches the likelihood representation of model, and alleviates the destruction of spatial consistency. (ii) An expanding layer without using convolution or interpolation operation is proposed to aggregate layer information of different scales to construct cross-scale likelihood estimation. (iii) We further demonstrate the defect of sine position encoding with mathematical derivation, and introduce a content-based positional encoding bias as prior in the Transformer architecture to reflect the relative index of inputs. (iv) And extensive experiments show that the proposed tracker achieves better performance compared with CNN-based trackers in cases of illumination, low resolution, deformation on various datasets, and achieves superior performance on others attributes. The proposed tracker obtains 70.3 % , 69.1 % , 63.4 % on OTB2015, UAV123, and LaSOT, respectively.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1038/s41598-022-24495-w,en,Fine-grained population mapping from coarse census counts and open geodata,"['OriginalPaper', 'Article']","Fine-grained population maps are needed in several domains, like urban planning, environmental monitoring, public health, and humanitarian operations. Unfortunately, in many countries only aggregate census counts over large spatial units are collected, moreover, these are not always up-to-date. We present P omelo , a deep learning model that employs coarse census counts and open geodata to estimate fine-grained population maps with $$100\,$$ 100 m ground sampling distance. Moreover, the model can also estimate population numbers when no census counts at all are available, by generalizing across countries. In a series of experiments for several countries in sub-Saharan Africa, the maps produced with P omelo are in good agreement with the most detailed available reference counts: disaggregation of coarse census counts reaches $$R^2$$ R 2 values of 85–89%; unconstrained prediction in the absence of any counts reaches 48–69%.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10973-022-11794-7,en,Theoretical and experimental study on image noise reduction for improving camera-based fire detection performance in thermal environments,OriginalPaper,"Fire is one of the most common hazards in the process industry. Timely and accurate fire detection is essential. The camera-based technics for fire detection are one of the promising technologies. However, its uncertainty of fire monitoring quality, such as noise artifacts within digital images caused by the inherent interference of hot environments, is always a key defect hindering the further application of this technology. Taking a simple fire scenario of the cable fire as an example, the noise reduction model (SA-DCGAN, Spatial Attention-Deep Convolution Generative Adversarial Network) is discussed for three kinds of typical fire image noise (white, black and mottled). Compared with traditional noise reduction algorithm, the model has greater advantages in restoring flame profile and texture. Through the verification process of applying this method in promoting fire detection based on image recognition, the effectiveness of the theoretical model is confirmed in improving the detection accuracy. It shows that the “True Detection” is increased by 375% and the “Missed Detection” and “False Detection” are decreased by 54% and 587%, respectively. These results show that the proposed theoretical model is of great significance for improving camera-based fire detection performance in thermal environments, which makes possible to further promote the intelligent fire protection in the process industry.","['Chemistry', 'Physical Chemistry', 'Analytical Chemistry', 'Polymer Sciences', 'Inorganic Chemistry', 'Measurement Science and Instrumentation']"
doi:10.1007/s42452-022-05227-1,en,A short utterance speaker recognition method with improved cepstrum–CNN,"['OriginalPaper', 'Research Article']","In this study, an improved cepstrum-convolutional neural network is proposed, which can solve the problem of low recognition accuracy of 1-s short utterance in speaker recognition technology. The audio feature Mel frequency cepstrum coefficient is extracted by using the improved cepstrum algorithm and the data of the two-dimensional acoustic feature vector matrix is preprocessed to convert the two-dimensional feature matrix into a three-dimensional tensor as the input data of the two-dimensional convolutional neural network model. Experiments are carried out on an Arabic digital English pronunciation dataset with an audio duration of less than one second in a specific experimental environment. Moreover, the performance of this model is evaluated by accuracy and F1-score. The simulation results show that the accuracy of our proposed model for speech recognition is as high as 100% and 99.60% on the training and test sets, respectively, as well as the F1- score, is 0.9985. It can be seen that the recognition method of this model solves the problem of accuracy degradation of short utterance speaker recognition due to the short duration of the corpus and improves the accuracy of short speech voice recognition. The model is simple but effective, generalization, superior, and has higher practical application value. Article Highlights. It is interesting to study how to improve the accuracy of 1-s short utterance speaker recognition. The improved cepstrum algorithm can solve the problem of not extracting enough discernible acoustic features. This paper proposed model obtained 100% accuracy on a spoken Arabic digit dataset with an audio duration about 0.3 s.","['Engineering', 'Engineering, general', 'Materials Science, general', 'Earth Sciences, general', 'Applied and Technical Physics', 'Chemistry/Food Science, general', 'Environment, general']"
doi:10.1007/s10462-022-10328-9,en,A survey on binary metaheuristic algorithms and their engineering applications,OriginalPaper,"This article presents a comprehensively state-of-the-art investigation of the engineering applications utilized by binary metaheuristic algorithms. Surveyed work is categorized based on application scenarios and solution encoding, and describes these algorithms in detail to help researchers choose appropriate methods to solve related applications. It is seen that transfer function is the main binary coding of metaheuristic algorithms, which usually adopts Sigmoid function. Among the contributions presented, there were different implementations and applications of metaheuristic algorithms, or the study of engineering applications by different objective functions such as the single- and multi-objective problems of feature selection, scheduling, layout and engineering structure optimization. The article identifies current troubles and challenges by the conducted review, and discusses that novel binary algorithm, transfer function, benchmark function, time-consuming problem and application integration are need to be resolved in future.","['Computer Science', 'Artificial Intelligence', 'Computer Science, general']"
doi:10.1007/s40815-022-01419-4,en,Design of Intelligent Controller Using Type-2 Fuzzy Cerebellar Model Articulation and 3D Membership Functions,OriginalPaper,"Intelligent control using interval type-2 fuzzy systems and cerebellar model articulation networks (CMAN) have been extensively studied. Although they perform well with nonlinear problems, they often have two significant issues: the choice of the learning rate and the design of adaptive laws to update the network parameters. To address these drawbacks, we proposed an improved gray wolf optimizer (IGWO) that optimizes the learning rate of type-2 fuzzy CMAN. We also designed an adaptation law to adjust the proposed network parameters online. Additionally, a three-dimensional Gaussian membership function (3DGMF) was developed to handle external disturbances and system uncertainties. Finally, numerical simulation results on micro-electro-mechanical system (MEMS) motion control were provided to validate the effectiveness of the proposed control method.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Operations Research, Management Science']"
doi:10.1007/s10489-022-04265-x,en,Firefighting multi strategy marine predators algorithm for the early-stage Forest fire rescue problem,OriginalPaper,"This paper proposes a firefighting multi strategy marine predators algorithm(FMMPA), which uses opposition-based learning (OBL) to make the initial population more uniform, introduces adaptive weight factors to balance exploration ability and exploitation ability, and combines DE/RAND/1 mutation mechanism to increase population diversity. Another purpose of our FMMPA is to add fire weighted selection, including flame edge suppression, combustion unit continuity and wind direction, as the rescue algorithm of forest fire rescue assemble to deal with the allocation of firefighting aircraft in forest fire rescue. We conduct this study based on real maps and the capabilities of the real firefighting aircraft. Compared with ten algorithms, the experimental results show that the proposed FMMPA has superior performance in reducing rescue time, controlling the spread speed of fire edge and minimizing loss cost.","['Computer Science', 'Artificial Intelligence', 'Mechanical Engineering', 'Manufacturing, Machines, Tools, Processes']"
doi:10.1007/s00366-022-01746-y,en,An enhanced hybrid seagull optimization algorithm with its application in engineering optimization,"['OriginalPaper', 'Original Article']","Aiming at the problems such as slow search speed, low optimization accuracy, and premature convergence of standard seagull optimization algorithm, an enhanced hybrid strategy seagull optimization algorithm was proposed. First, chaos mapping is used to generate the initial population to increase the diversity of the population, which lays the foundation for the global search. Then, a nonlinear convergence parameter and inertia weight are introduced to improve the convergence factor and to balance the global exploration and local development of the algorithm, so as to accelerate the convergence speed. Finally, an imitation crossover mutation strategy is introduced to avoid premature convergence of the algorithm. Comparison and verification between MSSOA and its incomplete algorithms are better than SOA, indicating that each improvement is effective and its incomplete algorithms all improve SOA to different degrees in both exploration and exploitation. 25 classic functions and the CEC2014 benchmark functions were tested, and compared with seven well-known meta-heuristic algorithms and its improved algorithm to evaluate the validity of the algorithm. The algorithm can explore different regions of the search space, avoid local optimum and converge to global optimum. Compared with other algorithms, the results of non-parametric statistical analysis and performance index show that the enhanced algorithm in this paper has better comprehensive optimization performance, significantly improves the search speed and convergence precision, and has strong ability to get rid of the local optimal solution. At the same time, in order to prove its applicability and feasibility, it is used to solve two constrained mechanical engineering design problems contain the interpolation curve engineering design and the aircraft wing design. The engineering curve shape with minimum energy, minimum curvature, and the smoother shape of airfoil with low drag are obtained. It is proved that enhanced algorithm in this paper can solve practical problems with constrained and unknown search space highly effectively.","['Computer Science', 'Computer-Aided Engineering (CAD, CAE) and Design', 'Math. Applications in Chemistry', 'Systems Theory, Control', 'Calculus of Variations and Optimal Control; Optimization', 'Classical Mechanics', 'Mathematical and Computational Engineering']"
doi:10.1007/s00466-022-02251-1,en,Physics-informed machine learning for surrogate modeling of wind pressure and optimization of pressure sensor placement,"['OriginalPaper', 'Original Paper']","This paper presents a predictive computational framework for surrogate modeling of pressure field and optimization of pressure sensor placement for wind engineering applications. Firstly, a machine learning-derived surrogate model, trained by high-fidelity simulation data using finite element-based CFD and informed by a turbulence model, is developed to construct the full-field pressure from scattered sensor measurements in near real-time. Then, the surrogate pressure model is embedded in another neural network (NN) for optimizing pressure sensor placement. The goal of the NN-based optimizer is to learn the best layout of a fixed number of pressure sensors over the structural surface to deliver the most accurate full-field pressure prediction for various inflow wind conditions. We deploy the model to a representative low-rise building subjected to different wind conditions. The performance of the proposed framework is assessed by comparing the predicted results with finite element-based CFD simulation results. The framework shows excellent accuracy and efficiency, which could be potentially integrated with structural health monitoring to enable digital twins of civil structures.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Science and Engineering', 'Classical and Continuum Physics']"
doi:10.1007/s12065-022-00794-z,en,I/F-Race tuned firefly algorithm and particle swarm optimization for K-medoids-based clustering,"['OriginalPaper', 'Research Paper']","Clustering is still one of the most common unsupervised learning techniques in data mining since it allows the discovery of meaningful and interesting patterns, knowledge, rules and associations from large-scale datasets. K-medoids, a variant of K-means, is a popular clustering method that attempts to find the optimal combination of K medoids from among a set of potential combinations. It has been successfully applied to solve various real-life problems owing to its simplicity and effectiveness. Nevertheless, due to the exponential number of possible combinations of K medoids, it is extremely challenging to produce the optimal one within a reasonable amount of time. Therefore, in this work, we propose to formulate the problem of K-medoids clustering as an optimization problem and then combine two effective and powerful Swarm Intelligence (SI) algorithms, namely Firefly Algorithm (FA) and Particle Swarm Optimization (PSO), to select the appropriate combination of K medoids. We extensively evaluate the proposed FA-PSO for K-medoids-based clustering, abbreviated as FA-PSO-KMED, using 10 UCI datasets. We first use the Iterated F-Race (I/F-Race) algorithm to determine the optimal parameter settings for FA and PSO. Then, we compare the results of the proposed FA-PSO-KMED with those obtained using the well-known state-of-the-art K-medoids-based clustering algorithms: PAM, CLARA and CLARANS. We also compare the results with 11 popular swarm intelligence algorithms: PSO, ABC, CS, FA, BA, APSO, EHO, HHO, SMA, AO and RSA. Experimental results and statistical analysis show that the proposed FA-PSO-KMED is very promising and demonstrates a significant improvement over the other clustering algorithms.","['Engineering', 'Mathematical and Computational Engineering', 'Artificial Intelligence', 'Statistical Physics and Dynamical Systems', 'Control, Robotics, Mechatronics', 'Bioinformatics', 'Applications of Mathematics']"
doi:10.1038/s41598-022-24317-z,en,Bias reduction in representation of histopathology images using deep feature selection,"['OriginalPaper', 'Article']","Appearing traces of bias in deep networks is a serious reliability issue which can play a significant role in ethics and generalization related concerns. Recent studies report that the deep features extracted from the histopathology images of The Cancer Genome Atlas (TCGA), the largest publicly available archive, are surprisingly able to accurately classify the whole slide images (WSIs) based on their acquisition site while these features are extracted to primarily discriminate cancer types. This is clear evidence that the utilized Deep Neural Networks (DNNs) unexpectedly detect the specific patterns of the source site, i.e, the hospital of origin, rather than histomorphologic patterns, a biased behavior resulting in degraded trust and generalization. This observation motivated us to propose a method to alleviate the destructive impact of hospital bias through a novel feature selection process. To this effect, we have proposed an evolutionary strategy to select a small set of optimal features to not only accurately represent the histological patterns of tissue samples but also to eliminate the features contributing to internal bias toward the institution. The defined objective function for an optimal subset selection of features is to minimize the accuracy of the model to classify the source institutions which is basically defined as a bias indicator. By the conducted experiments, the selected features extracted by the state-of-the-art network trained on TCGA images (i.e., the KimiaNet), considerably decreased the institutional bias, while improving the quality of features to discriminate the cancer types. In addition, the selected features could significantly improve the results of external validation compared to the entire set of features which has been negatively affected by bias. The proposed scheme is a model-independent approach which can be employed when it is possible to define a bias indicator as a participating objective in a feature selection process; even with unknown bias sources.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s12530-022-09475-9,en,A finger vein feature extraction network fusing global/local features and its lightweight network,"['OriginalPaper', 'Original Paper']","Finger vein recognition technology has been widely used in various identity authentication scenarios due to its convenience, fast recognition speed, and high security. However, there is an inevitable problem that seriously affects its recognition, which is the change of finger poses, such as shift and rotation during the image acquisition process. Therefore, how to extract finger vein features that are more robust to finger pose changes is a more concerned issue. We analyze the multi-pose finger vein images in practical application scenarios, and find the fact that the global vein features of the same finger are quite different while the local features are highly similar. In most existing finger vein recognition algorithms based on convolutional neural network (CNN), only global finger vein features are extracted by global average pooling (GAP), which fails to take full account of the above fact. In this paper, we proposed a finger vein feature extraction network fusing global and local features (FGL-Net) and its lightweight network KD-FGL-MobileNet, which effectively improves finger vein recognition performance in different finger poses. Firstly, FGL-Net consists of two parts, the backbone network based on the designed ResBlock with Mish is used to extract the high-level semantic features of finger veins, and then the global and local feature extraction module with three independent branches are designed to fully learn the global and local finger vein features at different granularities, and finally all the features of the three branches are fused into a fusion feature with greater robustness to pose change for recognition. To improve the generalization ability of the network, the CurricularFace loss is added to train FGL-Net with the cross-entropy loss. Such design not only aggregates homologous features and separates heterologous features, but also mines finger vein image samples under special poses online for intensive training. Secondly, according to the characteristics of the finger veins, we design a lightweight residual block based on fast receptive field (SE-FrfResBlock) to build a more lightweight FGL-MobileNet. A knowledge distillation loss and a feature map loss are added to FGL-MobileNet to address the generalization performance degradation of FGL-MobileNet, and we named it KD-FGL-MobileNet. On FV-USM, FV-Normal and FV-Specical datasets, compared with VGG-Net and InceptionResnet, the Top1 ranking of FGL-Net are improved by 2.38%, 8.26%, 13.42% and 0.00%, 1.17%, 9.95%, and the recognition rate are improved by 9.42%, 18.59%, 28.33% and 0.72%, 7.12%, 20.88%. On FV-Specical datasets, compared with MobileNetv3, FGL-MobileNet, the recognition rate and Top1 ranking of KD-FGL-MobileNet are improved by 24.12% ,6.19% and 12.46%, 3.78%. The above results show that the proposed FGL-Net effectively improves the recognition performance of finger vein images in different poses, and KD-FGL-MobileNe requires less storage space while remaining basically consistent with the performance of FGL-Net.","['Engineering', 'Complexity', 'Artificial Intelligence', 'Complex Systems']"
doi:10.1038/s41598-022-21250-z,en,"White blood cell detection, classification and analysis using phase imaging with computational specificity (PICS)","['OriginalPaper', 'Article']","Treatment of blood smears with Wright’s stain is one of the most helpful tools in detecting white blood cell abnormalities. However, to diagnose leukocyte disorders, a clinical pathologist must perform a tedious, manual process of locating and identifying individual cells. Furthermore, the staining procedure requires considerable preparation time and clinical infrastructure, which is incompatible with point-of-care diagnosis. Thus, rapid and automated evaluations of unlabeled blood smears are highly desirable. In this study, we used color spatial light interference microcopy (cSLIM), a highly sensitive quantitative phase imaging (QPI) technique, coupled with deep learning tools, to localize, classify and segment white blood cells (WBCs) in blood smears. The concept of combining QPI label-free data with AI for the purpose of extracting cellular specificity has recently been introduced in the context of fluorescence imaging as phase imaging with computational specificity (PICS). We employed AI models to first translate SLIM images into brightfield micrographs, then ran parallel tasks of locating and labelling cells using EfficientNet, which is an object detection model. Next, WBC binary masks were created using U-net, a convolutional neural network that performs precise segmentation. After training on digitally stained brightfield images of blood smears with WBCs, we achieved a mean average precision of 75% for localizing and classifying neutrophils, eosinophils, lymphocytes, and monocytes, and an average pixel-wise majority-voting F1 score of 80% for determining the cell class from semantic segmentation maps. Therefore, PICS renders and analyzes synthetically stained blood smears rapidly, at a reduced cost of sample preparation, providing quantitative clinical information.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s12517-022-11005-5,en,Performance efficiency of data-based hybrid intelligent approaches to predict crest settlement in rockfill dams,"['OriginalPaper', 'Original Paper']","In the present study, intelligent methods including artificial neural network (ANN) support vector machine (SVM) optimization and their combinations artificial neural network-particle swarm optimization (ANN-PSO), wavelet-artificial neural network (W-ANN), and W-ANN-PSO were investigated to predict the performance of rockfill dam crest settlements. Input parameters were based on the crest settlement data from a rockfill dam with a central core and the dam height and compressibility index. The results showed that the artificial neural network with 66% accuracy is the basis of the effectiveness of the optimization process and data preprocessing. The minimum error values by the neural network method are 1.88%, and the maximum value is 37.44%. Also, the average error was 14.23%. SVM optimization method and radial basis function (RBF) performance are often superior to other functions due to their radial nature. The reason for the greater compatibility of RBF performance and better fit to data is the lower absolute mean error value compared to other methods. With the ANN-PSO method, the maximum error is 11.2%, the minimum error value is 1.17%, and the average is 4.66%. By examining the validation data, it can be concluded that the errors are consistently in the range of 5–11%. The preprocessed neural network method and the performance of the bior 6.8 wavelet function has superior performance compared to other W-ANN models, so its average absolute error is about 29%. The db4 wavelet function performs better than other functions in the W-ANN-PSO model. The W-ANN-PSO model performed better than the model without PSO optimizer because the particle aggregation method dealt with complexity by increasing the number of inputs to the neural network and reducing their effects.","['Earth Sciences', 'Earth Sciences, general']"
doi:10.1007/s12273-022-0953-5,en,DeepRadiation: An intelligent augmented reality platform for predicting urban energy performance just through 360 panoramic streetscape images utilizing various deep learning models,"['OriginalPaper', 'Research Article']","Urban energy simulation is critical for understanding and managing energy performance in cities. In this research, we design a novel framework called DeepRadiation, to enable automatic urban environmental performance prediction. By incorporating deep learning strategies, DeepRadiation predicts solar radiation on an urban scale using just panoramic streetscape images without any 3D modeling and simulation. New York City was chosen as the case study for this research. DeepRadiation is comprised of three different deep learning models organized into two stages. The first stage, named DeepRadiation modeling, serves as the framework’s brain. At this stage, solar radiation analysis was performed using a Pix2Pix model, a type of conditional generative adversarial networks (GANs). After extracting GIS data and performing energy simulation analysis to prepare the dataset, the Pix2Pix model was trained on 10000 paired panoramic depth images of streetscapes with only building blocks and related panoramic images of streetscapes with only solar radiation analysis. Two GAN generator evaluation measures named qualitative evaluation and quantitative evaluation were used to validate the trained Pix2Pix model. Both demonstrated high levels of accuracy (qualitative evaluation: 93%, quantitative evaluation: 89%). DeepRadiation application as the DeepRadiation’s sescond stage is the framework’s eyes. At this stage, two convolutional neural network (CNN) models (DeepLabv3 and MiDaS) were used to perform computer vision tasks on panoramic streetscape images, such as semantic segmentation and depth estimation. The DeepRadiation application stage allows urban designers, architects, and urban policymakers to use the DeepRadiation framework and experience the final output via augmented reality.","['Engineering', 'Building Construction and Design', 'Engineering Thermodynamics, Heat and Mass Transfer', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Monitoring/Environmental Analysis']"
doi:10.1186/s13014-022-02156-6,en,Treatment plan comparison for irradiation of multiple brain metastases with hippocampal avoidance whole brain radiotherapy and simultaneous integrated boost using the Varian Halcyon and the Elekta Synergy platforms,"['OriginalPaper', 'Research']",,"['Biomedicine', 'Cancer Research', 'Oncology', 'Radiotherapy', 'Imaging / Radiology']"
doi:10.1007/s00530-022-01025-2,en,Multimodal metadata assignment for cultural heritage artifacts,"['OriginalPaper', 'Regular Paper']","We develop a multimodal classifier for the cultural heritage domain using a late fusion approach and introduce a novel dataset. The three modalities are Image, Text, and Tabular data. We based the image classifier on a ResNet convolutional neural network architecture and the text classifier on a multilingual transformer architecture (XML-Roberta). Both are trained as multitask classifiers. Tabular data and late fusion are handled by Gradient Tree Boosting. We also show how we leveraged a specific data model and taxonomy in a Knowledge Graph to create the dataset and to store classification results.","['Computer Science', 'Cryptology', 'Computer Communication Networks', 'Operating Systems', 'Data Storage Representation', 'Multimedia Information Systems', 'Computer Graphics']"
doi:10.1038/s41467-022-34807-3,en,DeepPROTACs is a deep learning-based targeted degradation predictor for PROTACs,"['OriginalPaper', 'Article']","The rational design of PROTACs is difficult due to their obscure structure-activity relationship. This study introduces a deep neural network model - DeepPROTACs to help design potent PROTACs molecules. It can predict the degradation capacity of a proposed PROTAC molecule based on structures of given target protein and E3 ligase. The experimental dataset is mainly collected from PROTAC-DB and appropriately labeled according to the DC 50 and Dmax values. In the model of DeepPROTACs, the ligands as well as the ligand binding pockets are generated and represented with graphs and fed into Graph Convolutional Networks for feature extraction. While SMILES representations of linkers are fed into a Bidirectional Long Short-Term Memory layer to generate the features. Experiments show that DeepPROTACs model achieves 77.95% average prediction accuracy and 0.8470 area under receiver operating characteristic curve on the test set. DeepPROTACs is available online at a web server ( https://bailab.siais.shanghaitech.edu.cn/services/deepprotacs/ ) and at github ( https://github.com/fenglei104/DeepPROTACs ). The rational design of PROTACs is difficult due to their obscure structure-activity relationship. Here the authors present a deep neural network model - DeepPROTACs - for predicting the degradation capacity of a proposed PROTAC molecule.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s41601-022-00266-7,en,Cascade controller based modeling of a four area thermal: gas AGC system with dependency of wind turbine generator and PEVs under restructured environment,"['OriginalPaper', 'Original research']","This paper investigates automatic generation control (AGC) of a realistic hybrid four-control area system with a distinct arrangement of thermal units, gas units and additional power generation. A proportional-integral-double derivative cascaded with proportional-integral (PIDD-PI) controller is employed as secondary controller in each control area for robust restructured AGC considering bilateral transactions and contract violations. The Harris Hawks algorithm is used to determine the optimal controller gains and system parameters under several scenarios. Electric vehicle (EV) aggregators are employed in each area to participate fully along with thermal and gas units to compensate for the unscheduled system demand in the local area. A comparison of non-cascaded controllers such as PI-PD, PD-PID and the proposed PIDD-PI proves the superiority of the last. The effect of the decline in inertia is closely examined because of the sudden outage of a generating unit while at the same time considering the change in area frequency response characteristics and area control error. EV fleets make significant contributions to improving the system dynamics during system inertia loss. The use of EVs in the presence of a wind energy-supported grid can provide a stable efficacy to the power grid. Numerous simulations with higher load demands, stochastic communication delays in presence of the WTG plant, and violations in system loadings and changes in gas turbine time constants in the absence of WTG demonstrate the robustness of the proposed control approach.","['Energy', 'Energy Systems', 'Renewable and Green Energy', 'Power Electronics, Electrical Machines and Networks']"
doi:10.1186/s40537-022-00662-8,en,Spoofing keystroke dynamics authentication through synthetic typing pattern extracted from screen-recorded video,"['OriginalPaper', 'Research']","As the inter-connectivity in cyberspace continues to increase exponentially, privacy and security have become two of the most concerning issues that need to be tackled in today’s state of technology. Therefore, password-based authentication should no longer be used without an additional layer of authentication, namely two-factor authentication (2FA). One of the most promising 2FA approaches is Keystroke Dynamics, which relies on the unique typing behaviour of the users. Since its discovery, Keystroke Dynamics adoption has been continuously growing to many use cases: generally to obtain access to a platform through typing behaviour similarity, into a continuous keystroke monitoring on e-learning or e-exams platforms to detect illegitimate participants or cheaters. As the adoption of Keystroke Dynamics continues to grow, so does the threats that are lurking in. This paper proposes a novel exploitation method that utilizes computer vision to extract and learn a user’s typing pattern just from a screen-recorded video that captures their typing process. By using a screen-recorded video, an attacker could eliminate the needs to inject a keylogger into the victim’s computer, thus rendering the attack easier to perform and more difficult to detect. Furthermore, the extracted typing pattern can be used to spoof a Keystroke Dynamics authentication mechanism with an evasion rate as high as 64%, a considerably alarming rate given the impact it yields if the attacks are successful, which allows an attacker to pretend, mimic, and falsely authenticate as the victim (i.e., total account takeover). This paper also shows that from a screen-recorded video, one can produce a staggering statistical similarity in keystroke timing patterns as if they used an actual keylogger, and the extracted patterns can be potentially used to spoof the Keystroke Dynamics authentication service. To the author’s best knowledge, there is no precedence of previous research that suggests this kind of attack (i.e. using video to spoof keystroke dynamics). This research can be used as the baseline for future research in this area.","['Computer Science', 'Database Management', 'Information Storage and Retrieval', 'Data Mining and Knowledge Discovery', 'Computational Science and Engineering', 'Mathematical Applications in Computer Science', 'Communications Engineering, Networks']"
doi:10.1007/s40998-022-00569-3,en,Efficient MPPT Controller for Solar PV System Using GWO-CS Optimized Fuzzy Logic Control and Conventional Incremental Conductance Technique,"['OriginalPaper', 'Research Paper']","In this work, an efficient MPPT controller is designed using expert fuzzy system and conventional incremental conductance (INC) algorithm for solar photovoltaic system. The proposed controller possesses adaptive capability due to the inclusion of optimized fuzzy logic expert knowledge and simultaneously preserves the simplicity of conventional MPPT. The membership functions in proposed INC-Fuzzy controller are optimized using hybrid grey wolf cuckoo search optimization so as to increase the tracking accuracy of controller. The basic INC MPPT and INC-PID MPPT techniques are also implemented for comparative analysis. The performance of proposed hybrid INC-Fuzzy MPPT controller is validated on a string of multi-crystalline solar cell KC130GT by M/s Kyocera in 4S1P configuration. The effectiveness of proposed technique is justified in terms of power tracking accuracy and average efficiency.","['Engineering', 'Electrical Engineering']"
doi:10.1007/s40891-022-00418-7,en,Probabilistic Assessment on the Performance of Tiered Geosynthetic Reinforced Soil Walls Using RLEM and RFEM,"['OriginalPaper', 'Original Paper']","Geosynthetic reinforced soil (GRS) structures are constructed in the tiered configurations for various reasons, particularly reducing displacement, economic aspects, flexibility, and construction constraints. This paper aims to investigate the probabilistic stability analysis of tiered GRS walls under seismic loading conditions, considering soil spatial variability, and cross-correlation among input variables. Several sensitive and parametric analyses were used to examine the effect of uncertainties on stability indicators. In this regard, calibrated random limit equilibrium methods (RLEM) were employed to determine critical slip surface. Supplementary optimization search techniques, in conjunction with non-circular methods, are among the advantages of this investigation to improve the accuracy of determining stability indicators over the previous research. It was concluded that the GRS walls with modular concrete block facing tend to perform well under seismic loads due to their ability to reduce failure probabilities. Statistical analyses also showed that cross-correlation between cohesion and friction angle, as well as the coefficient of the variation of random variables, significantly affect the probability of failure ( P f ). The results indicated that failure probability increased from zero to about 9% because of increasing the coefficient of variation from 5 to 15%, which presents a severe risk to the GRS wall's stability. Furthermore, it was determined that as the negative value of cross-correlation between soil cohesion (c) and soil friction angle ( φ ) increased, the probability of failure decreased. This study compared the seismic performance of tiered GRS walls using RLEM methods and the random finite-element method (RFEM). In addition, it was shown that RFEM approach results in a higher factor of safety values than circular and non-circular RLEMs, with a difference of about 6% compared to Sarma’s method. Graphical abstract ","['Engineering', 'Geoengineering, Foundations, Hydraulics', 'Environmental Science and Engineering', 'Building Materials']"
doi:10.1007/s11044-022-09852-x,en,Predictive multibody dynamic simulation of human neuromusculoskeletal systems: a review,ReviewPaper,"Over the past decade, there has been a rapid increase in applications of multibody system dynamics to the predictive simulation of human movement. Using predictive “what-if” human dynamic simulations that do not rely on experimental testing or prototypes, new medical interventions and devices can be developed more quickly, cheaply, and safely. In this paper, we provide a comprehensive review of research into the predictive multibody dynamic simulation of human movements, with applications in clinical practice, medical and assistive device design, sports, and industrial ergonomics. Multibody models of human neuromusculoskeletal systems are reviewed, including models of joints, contacts, and muscle forces or torques, followed by a review of simulation approaches that use optimal control methods and a cost function to predict human movements. Modelling and optimal control software are also reviewed, and directions for future research are suggested.","['Engineering', 'Vibration, Dynamical Systems, Control', 'Optimization', 'Electrical Engineering', 'Mechanical Engineering', 'Automotive Engineering']"
doi:10.1007/s11947-022-02945-7,en,A Novel Machine Learning–Based Approach for Characterising the Micromechanical Properties of Food Material During Drying,"['OriginalPaper', 'Research']","Plant-based food materials (PBFMs) such as fruits and vegetables contain various irregular cellular compartments. Like other engineering materials, the characterisation of micromechanical properties (MMPs) of PBFMs is intensely important for accurately estimating the functionality of dried food products. The application of a machine learning (ML)–based approach to characterise the MMPs is a promising idea. However, no intensive research in this regard has been attempted yet. Therefore, we proposed an ML-based modelling framework to characterise the MMPs of PBFMs during drying. A feed-forward artificial neural network (ANN) model with a backpropagation algorithm was developed and optimised with a genetic algorithm (GA)–based optimisation tool for characterising PBFMs, specifically carrots. Moreover, the accuracy of the ANN model was compared with a multiple nonlinear regression (MNLR) model. It was found that the developed network model agreed very well with the experimental data when predicting the elastic modulus, stiffness and hardness, with an accuracy of the goodness of fit ( R 2 ) values of 0.992, 0.993 and 0.802, respectively. It is expected that the developed model has incredible potential to characterise the MMPs of similar food products.","['Chemistry', 'Food Science', 'Chemistry/Food Science, general', 'Agriculture', 'Biotechnology']"
doi:10.1038/s41598-022-24674-9,en,SNAL: sensitive non-associative learning network configuration for the automatic driving strategy,"['OriginalPaper', 'Article']","Nowadays, there is a huge gap between autonomous vehicles and mankind in terms of the decision response against some dangerous scenarios, which would has stressed the potential users out and even made them nervous. To efficiently identify the possible sensitivity scenarios, a new neural network configuration, named sensitive non-associative learning network (SNAL), is proposed. In such structure, the modulated interneurons, excited by abnormal scene stimulation for scene processing, are well processed and utilized to improve the training structure which refers to the sensitization mechanism in non-associative learning in neurobiology and the neural structure of Aplysia. When encountering the sensitivity scenes that the automatic driving agent is not good at or has not seen, the modulated interneuron facilitates the full connection layer neurons for the decision-making process, so as to change the final automatic driving strategy. In the process of constructing the model, a method to measure the similarity of the convolution feature map is proposed, which provides a new investigation tool for the properties of convolution networks after the feature extraction. Based on the Morris–Lecar equation in neurobiology, the dynamic model of modulating interneurons in the network is constructed. The automatic control optimization of the model is carried out by imitating the biological properties. The optimization method provides a reference for introducing neurobiological mechanism into deep learning and automatic control. To validate the effectiveness of the proposed method, the simulation test are executed and the existing methods are compared accordingly. The results show that the proposed SNAL algorithm can effectively recognize the sensitivity mechanism. Furthermore, compared with the existing algorithms, such as CNN, LSTM, ViT, the proposed algorithm can make better defensive strategies for potentially dangerous scenes rarely seen or not seen in the training stage. This sensitivity mechanism is more in line with the human driving intuition when dealing with abnormal driving scenes, and makes the decision more interpretable, significantly improving the traffic ability of autonomous vehicles under the sensitive scenes. In addition, this configuration can be easily combined with the existing mainstream neural network models and has good expansibility.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11042-022-14177-0,en,An ensemble framework of deep neural networks for colorectal polyp classification,"['OriginalPaper', 'Track 2: Medical Applications of Multimedia']","Colorectal cancer (CRC) is caused by malignant polyps which must be resected and examined for accurate classification. Biopsy, the manual workflow of polyp classification is time-intensive task and requires an automated solution. The objective of this study is to develop an accurate virtual biopsy tool for polyp classification. Moreover, automated assessment of polyps is a challenging task due to the similarities in their patterns, and in contrast to existing studies on binary classification, the outcome of multi-class classification requires evaluation through advanced evaluation measures. The proposed method combined the strength of individual weak learner for an accurate weighted-average ensemble deep learning classification. At first, base-classifiers were pretrained on the ImageNet database. Second, an average ensemble was built and evaluated for enhancing the performance, an appropriate combination of weights was chosen through grid search and assigned to the models. The performance evaluation of the proposed method in terms of F1-micro (0.80), F1-macro (0.81), F1-weighted (0.84) metrics, model reliability using Cohen’s Kappa Coefficient (0.60) and Mathew Correlation Co-efficient value (0.49) for binary dataset shows the superiority over existing models. The higher rates of precision and recall show potential usage of the proposed system in the development of a virtual biopsy tool.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s13222-022-00425-y,en,Benchmarking JSON Document Stores in Practice,"['OriginalPaper', 'Schwerpunktbeitrag']","The increasing dissemination of JSON as exchange and storage format through its popularity in business and analytical applications requires efficient storage and processing of JSON documents. Consequently, this led to the development of specialized JSON document stores and the extension of existing relational stores, while no JSON-specific benchmarks were available to assess these systems. In this work, we assess currently available JSON document store benchmarks and select the recently developed DeepBench benchmark to experimentally study important dimensions like analytical querying capabilities, object nesting and array unnesting. To make the computational complexity of array unnesting more tractable, we introduce an improvement that we evaluate within a commercial system as part of the common, performance-oriented development process in practice. We conclude our evaluation of well-known document stores with DeepBench and give new insights into strengths and potential weaknesses of those systems that were not found by existing, non-JSON benchmarking practices. In particular the algebraic optimization of JSON query processing is still limited despite prior work on hierarchical data models in the XML context.","['Computer Science', 'Information Storage and Retrieval', 'Data Mining and Knowledge Discovery', 'Database Management', 'Data Structures and Information Theory', 'IT in Business', 'Computer Systems Organization and Communication Networks']"
doi:10.1007/s13201-022-01815-z,en,Prediction of lake water-level fluctuations using adaptive neuro-fuzzy inference system hybridized with metaheuristic optimization algorithms,"['OriginalPaper', 'Original Article']","Lakes help increase the sustainability of the natural environment and decrease food chain risk, agriculture, ecosystem services, and leisure recreational activities locally and globally. Reliable simulation of monthly lake water levels is still an ongoing demand for multiple environmental and hydro-informatics engineering applications. The current research aims to utilize newly developed hybrid data-intelligence models based on the ensemble adaptive neuro-fuzzy inference system (ANFIS) coupled with metaheuristics algorithms for lake water-level simulation by considering the effect of seasonality on Titicaca Lake water-level fluctuations. The classical ANFIS model was trained using three metaheuristics nature-inspired optimization algorithms, including the genetic algorithm (ANFIS-GA), particle swarm optimizer (ANFIS-PSO), and whale optimization algorithm (ANFIS-WOA). For determining the best set of the input variables, an evolutionary approach based on several lag months has been utilized prior to the lake water-level simulation process using the hybrid models. The proposed hybrid models were investigated for accurately simulating the monthly water levels at Titicaca Lake. The ANFIS-WOA model exhibited the best prediction performance for lake water-level pattern measurement in this study. For the best scenario (the inputs were $${X}_{t-1},\; {X}_{t-2}, \;{X}_{t-3}, \;{X}_{t-4}, \; {X}_{t-12}$$ X t - 1 , X t - 2 , X t - 3 , X t - 4 , X t - 12 ) the ANFIS-WOA model attained root mean square error (RMSE $$\approx$$ ≈ 0.08 m), mean absolute error (MAE $$\approx$$ ≈ 0.06 m), and coefficient of determination ( R 2 $$\approx$$ ≈ 0.96). Also, the results showed that long-term seasonal memory for this lake is suitable input for lake water-level models so that the long-term dynamic memory of 1-year time series for lake water-level data is the best input for estimating the water level of Titicaca Lake.","['Earth Sciences', 'Hydrogeology', 'Water Industry/Water Technologies', 'Industrial and Production Engineering', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution', 'Nanotechnology', 'Private International Law, International & Foreign Law, Comparative Law']"
doi:10.1038/s41598-022-21813-0,en,Multi-modal wound classification using wound image and location by deep neural network,"['OriginalPaper', 'Article']","Wound classification is an essential step of wound diagnosis. An efficient classifier can assist wound specialists in classifying wound types with less financial and time costs and help them decide on an optimal treatment procedure. This study developed a deep neural network-based multi-modal classifier using wound images and their corresponding locations to categorize them into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map was also developed to prepare the location data, which can help wound specialists tag wound locations more efficiently. Three datasets containing images and their corresponding location information were designed with the help of wound specialists. The multi-modal network was developed by concatenating the image-based and location-based classifier outputs with other modifications. The maximum accuracy on mixed-class classifications (containing background and normal skin) varies from 82.48 to 100% in different experiments. The maximum accuracy on wound-class classifications (containing only diabetic, pressure, surgical, and venous) varies from 72.95 to 97.12% in various experiments. The proposed multi-modal network also showed a significant improvement in results from the previous works of literature.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41467-022-34595-w,en,Leveraging data-driven self-consistency for high-fidelity gene expression recovery,"['OriginalPaper', 'Article']","Single cell RNA sequencing is a promising technique to determine the states of individual cells and classify novel cell subtypes. In current sequence data analysis, however, genes with low expressions are omitted, which leads to inaccurate gene counts and hinders downstream analysis. Recovering these omitted expression values presents a challenge because of the large size of the data. Here, we introduce a data-driven gene expression recovery framework, referred to as self-consistent expression recovery machine (SERM), to impute the missing expressions. Using a neural network, the technique first learns the underlying data distribution from a subset of the noisy data. It then recovers the overall expression data by imposing a self-consistency on the expression matrix, thus ensuring that the expression levels are similarly distributed in different parts of the matrix. We show that SERM improves the accuracy of gene imputation with orders of magnitude enhancement in computational efficiency in comparison to the state-of-the-art imputation techniques. Recovering dropout-affected gene expression values is a challenging problem in bioinformatics. Here, the authors propose a data-driven framework, that first learns the underlying data distribution and then recovers the expression values by imposing a self-consistency on the expression matrix.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1186/s12911-022-02049-4,en,An imConvNet-based deep learning model for Chinese medical named entity recognition,"['OriginalPaper', 'Research']","Background With the development of current medical technology, information management becomes perfect in the medical field. Medical big data analysis is based on a large amount of medical and health data stored in the electronic medical system, such as electronic medical records and medical reports. How to fully exploit the resources of information included in these medical data has always been the subject of research by many scholars. The basis for text mining is named entity recognition (NER), which has its particularities in the medical field, where issues such as inadequate text resources and a large number of professional domain terms continue to face significant challenges in medical NER. Methods We improved the convolutional neural network model (imConvNet) to obtain additional text features. Concurrently, we continue to use the classical Bert pre-training model and BiLSTM model for named entity recognition. We use imConvNet model to extract additional word vector features and improve named entity recognition accuracy. The proposed model, named BERT-imConvNet-BiLSTM-CRF, is composed of four layers: BERT embedding layer—getting word embedding vector; imConvNet layer—capturing the context feature of each character; BiLSTM (Bidirectional Long Short-Term Memory) layer—capturing the long-distance dependencies; CRF (Conditional Random Field) layer—labeling characters based on their features and transfer rules. Results The average F1 score on the public medical data set yidu-s4k reached 91.38% when combined with the classical model; when real electronic medical record text in impacted wisdom teeth is used as the experimental object, the model's F1 score is 93.89%. They all show better results than classical models. Conclusions The suggested novel model (imConvNet) significantly improves the recognition accuracy of Chinese medical named entities and applies to various medical corpora.","['Medicine & Public Health', 'Health Informatics', 'Information Systems and Communication Service', 'Management of Computing and Information Systems']"
doi:10.1007/s00170-022-10459-x,en,Application of cutting power consumption in tool condition monitoring and wear prediction based on Gaussian process regression under variable cutting parameters,"['OriginalPaper', 'ORIGINAL ARTICLE']","Tool wear is inevitable in actual manufacturing, especially in extreme processing conditions for machining difficult-to-cut materials. The monitoring of the tool state has an important influence on the surface quality and dimensional accuracy of the precision parts. In the previous studies, the original total power consumption is usually used to predict tool wear while ignoring the cutting power consumption accounts for a small proportion of the total power consumption of machine tools. Therefore, the accuracy is difficult to achieve the expected target. For better prediction results, a novel prediction method based on net cutting power consumption by Gaussian process regression (GPR) with ARD Matern 5/2 kernel is proposed in this study. Firstly, the physical model of net cutting power consumption is established. Then, tool wear under fixed working conditions is predicted by using the net cutting power consumption and GPR, and the advantage of the proposed method in this study is verified by comparing it with the existing methods. Finally, the proposed method is verified to obtain better prediction performance with variable cutting parameters than using total power consumption with the neural network. This study reveals that low-cost sensors like power meter can be used as an important supplement to monitoring tool conditions in the industry and also provides a research basis for predicting tool wear under different cutting conditions.","['Engineering', 'Industrial and Production Engineering', 'Media Management', 'Mechanical Engineering', 'Computer-Aided Engineering (CAD, CAE) and Design']"
doi:10.1007/s10586-022-03796-9,en,Task processing optimization using cuckoo particle swarm (CPS) algorithm in cloud computing infrastructure,OriginalPaper,"Recently, cloud computing infrastructure (CCI) models have received much attention for their exceptional scalability, dependability, Data Information Sharing (DIS), and low cost rate. There are many hardware and software elements that are accessed over the internet by cloud data centers. Modern data centers utilize Virtualization Techniques (VT) to offer a dispersed CI that employs Virtual Machines (VM) based on Physical Hosts (PH). With the increasing number of centers, optimizing energy consumption has become vital to saving costs due to DCC's high energy consumption. In our CPS algorithm, we combine the Cuckoo algorithm and the particle swarm optimization (PSO). It is determined which virtual machine can be assigned to each host, thus choosing the best virtual machine. As a result, if the selected host is overloaded, it is determined which virtual machines are generating high loads and migrated to another host, which is determined based on the cuckoo algorithm and PSO. In testing each algorithm separately, the combination method proved to consume less energy and execute faster than the other methods in the CloudSim simulation environment. Fault tolerance for our network and evaluation of VMs have also been emphasized in vSphereTM.","['Computer Science', 'Processor Architectures', 'Operating Systems', 'Computer Communication Networks']"
doi:10.1007/s10579-022-09621-4,en,CORAA ASR: a large corpus of spontaneous and prepared speech manually validated for speech recognition in Brazilian Portuguese,"['OriginalPaper', 'Original Paper']","Automatic Speech recognition (ASR) is a complex and challenging task. In recent years, there have been significant advances in the area. In particular, for the Brazilian Portuguese (BP) language, there were around 376 h publicly available for the ASR task until the second half of 2020. With the release of new datasets in early 2021, this number increased to 574 h. The existing resources, however, are composed of audios containing only read and prepared speech. There is a lack of datasets including spontaneous speech, which are essential in several ASR applications. This paper presents CORAA (Corpus of Annotated Audios) ASR with 290 h, a publicly available dataset for ASR in BP containing validated pairs of audio-transcription. CORAA ASR also contains European Portuguese audios (4.6 h). We also present a public ASR model based on Wav2Vec 2.0 XLSR-53, fine-tuned over CORAA ASR. Our model achieved a Word Error Rate (WER) of 24.18% on CORAA ASR test set and 20.08% on Common Voice test set. When measuring the Character Error Rate (CER), we obtained 11.02% and 6.34% for CORAA ASR and Common Voice, respectively. CORAA ASR corpora were assembled to both improve ASR models in BP with phenomena from spontaneous speech and motivate young researchers to start their studies on ASR for Portuguese. All the corpora are publicly available at https://github.com/nilc-nlp/CORAA under the CC BY-NC-ND 4.0 license.","['Linguistics', 'Computational Linguistics', 'Computer Science, general', 'Linguistics, general', 'Language and Literature']"
doi:10.1007/s00500-022-07631-6,en,Hybrid deeper neural network model for detection of the Domain Name System over Hypertext markup language protocol traffic flooding attacks,"['OriginalPaper', 'Application of soft computing']","Domain Name System flood attacks are generally carried out in the application layer with the user datagram protocol. This type of attack is a vulnerability that concerns almost all web assets of web services. Various studies provide different security solutions to handle these attacks, but cyber-attackers find some unique approaches to exploit Domain Name Systems. In this study, we focused on Domain Name System flood attacks, which involve delaying or blocking services by increasing the memory and processor usage of Domain Name System servers. In order to separate the measurements of Domain Name System server traffic from legitimate network traffic, flooding attacks were detected with an innovative hybrid deep learning model consisting of a convolutional neural network with a long short-term memory model. This proposed model has been validated with the CIRA-CIC-DoHBrw-2020 dataset obtained from traffic data that causes flooding of the Domain Name System over HyperText Markup Language requests. Validation metrics were compared with widely used support vector machines, shallow neural networks and deep learning classifiers with long short-time model. As a result, very low false alarms and significantly high detection accuracy (99.54%) were achieved using the proposed hybrid deep learning classification. The proposed method for direct detection of domain system attacks without feature optimization and statistical methods such as coding of tags, normalization and standardization of data offers a comprehensive solution based on these metrics.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1038/s41598-022-23306-6,en,Prediction of severe thunderstorm events with ensemble deep learning and radar data,"['OriginalPaper', 'Article']","The problem of nowcasting extreme weather events can be addressed by applying either numerical methods for the solution of dynamic model equations or data-driven artificial intelligence algorithms. Within this latter framework, the most used techniques rely on video prediction deep learning methods which take in input time series of radar reflectivity images to predict the next future sequence of reflectivity images, from which the predicted rainfall quantities are extrapolated. Differently from the previous works, the present paper proposes a deep learning method, exploiting videos of radar reflectivity frames as input and lightning data to realize a warning machine able to sound timely alarms of possible severe thunderstorm events. The problem is recast in a classification one in which the extreme events to be predicted are characterized by a an high level of precipitation and lightning density. From a technical viewpoint, the computational core of this approach is an ensemble learning method based on the recently introduced value-weighted skill scores for both transforming the probabilistic outcomes of the neural network into binary predictions and assessing the forecasting performance. Such value-weighted skill scores are particularly suitable for binary predictions performed over time since they take into account the time evolution of events and predictions paying attention to the value of the prediction for the forecaster. The result of this study is a warning machine validated against weather radar data recorded in the Liguria region, in Italy.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s43674-022-00047-7,en,A novel hybrid dimension reduction and deep learning-based classification for neuromuscular disorder,"['OriginalPaper', 'Original Article']","Correct classification of neuromuscular disorders is essential to provide accurate diagnosis. Presently, gene microarray technology is a widely accepted technology to monitor the expression level of a large number of genes simultaneously. The gene microarray data are a high dimensional data, which usually contains small samples having a large number of genes. Therefore, dimension reduction is a crucial task for correct classification of diseases. Dimension reduction eliminates those genes which are less expressive and enhances the efficiency of the classification model. In the present paper, we developed a novel hybrid dimension reduction method and a deep learning-based classification model for neuromuscular disorders. The hybrid dimension reduction method is deployed in three phase: in the first phase, the expressive genes are selected using F test method, and the mutual information method and the best one among them are selected for further processing. In second phase, the gene selected by the best model is further transformed to low dimension by PCA. In third phase, the deep learning-based classification model is deployed. For experimentation, two diseased and multi-diseased micro array data sets, which is publicly available, is used. The best accuracy by 50-100-50-25-13 deep learning architecture with hybrid dimension reduction, where 100 genes select by F test and PCA with 50 principal components is 89% for NMD data set. The best accuracy by 50-100-2 deep learning architecture with hybrid dimension reduction, where 100 genes select by F test and PCA with 50 principal components is 97% for FSHD data set. The proposed hybrid method gives better classification accuracy result and reduces the search space and time complexity as well for both two diseased and multi-diseased micro array data sets.","['Engineering', 'Computational Intelligence', 'Machine Learning', 'Artificial Intelligence']"
doi:10.1007/s13349-022-00640-x,en,Deformation prediction model based on an improved CNN + LSTM model for the first impoundment of super-high arch dams,"['OriginalPaper', 'Original Paper']","Herein, we propose a one-dimensional convolutional neural network (CNN) + long short-term memory (LSTM) model optimised by L1 regularisation and the dropout method to solve the problem of acquiring both computational speed and accuracy in a deformation prediction analysis model of a super-high arch dam’s first impoundment. The calculation results of one class (OC) + LSTM, traditional LSTM, optimised LSTM, CNN + LSTM and multilayer perceptron are compared with the actual measurement results using deformation monitoring data from the first impoundment of a super-high arch dam in southwest China. The results show that the proposed OC-LSTM model can reduce the computational time without sacrificing computational accuracy, providing a new computational model for super-high arch dam deformation prediction during the first impoundment.","['Engineering', 'Civil Engineering', 'Measurement Science and Instrumentation', 'Vibration, Dynamical Systems, Control']"
doi:10.1057/s41272-022-00407-5,en,Market segmentation and dynamic price discrimination in the U.S. airline industry,"['OriginalPaper', 'Research Article']","Airfares are affected by a variety of factors, but it is less clear which factors are the key determinants and how they interact. Based on a unique transaction level data set, this paper introduces a widely used, machine learning based pricing tool to investigate the airline market segmentation and dynamic price discrimination problems. The empirical results suggest that purchasing time, city distance, market structure, market size, and seat availability are the five most important pricing factors in order. Airlines first partition their markets into an early market and a late market, and split the market further by city distance and other factors. While intertemporal price discrimination explains the majority of fare variations, there are strong indications that airlines use their market power in the late market and charge higher fares on late-arriving consumers (but not on early consumers), in response to extra seats sold.","['Business and Management', 'Business and Management, general']"
doi:10.1007/s10696-022-09477-4,en,Balancing and sequencing of mixed-model assembly line considering preventive maintenance scenarios: mathematical model and a migrating birds optimization algorithm,OriginalPaper,"In the mixed-model assembly line balancing and sequencing problem (MALBSP), workstations are assumed to be constantly available. The failure of any workstation will make the entire assembly line stop working. Preventive maintenance (PM) is a way to maintain the workstation before its failure, reduce unexpected downtime, and prolong its useful life. Previous studies have considered PM scenarios (PMS) in the simple and U-shaped assembly line to improve production efficiency and smoothness effectively, but not in the mixed-model assembly line. This paper fills this research gap, and the MALBSP considering PMS (MALBSP_PMS) is studied in this paper. A mixed-integer linear programming model is proposed to minimize makespan and task alteration. A migrating birds optimization algorithm is improved (IMBO) to obtain well-distributed Pareto frontier solutions. This algorithm designs a restart mechanism and an intra-population crossover operator to avoid falling into the local optimal and enhance its searchability. Experimental results demonstrate the effectiveness of two improvements and the IMBO algorithm. In addition, a real-world case study is introduced to illustrate the importance of considering PM scenarios in MALBSP.","['Engineering', 'Manufacturing, Machines, Tools, Processes', 'Operations Management', 'Operations Research/Decision Theory']"
doi:10.1140/epjqt/s40507-022-00149-8,en,On the learnability of quantum state fidelity,"['OriginalPaper', 'Research']","Current quantum processing technology is generally noisy with a limited number of qubits, stressing the importance of quantum state fidelity estimation. The complexity of this problem is mainly due to not only accounting for single gates and readout errors but also for interactions among which. Existing methods generally rely on either reconstructing the given circuit state, ideal state, and computing the distance of which; or forcing the system to be on a specific state. Both rely on conducting circuit measurements, in which computational efficiency is traded off with obtained fidelity details, requiring an exponential number of experiments for full information. This paper poses the question: Is the mapping between a given quantum circuit and its state fidelity learnable? If learnable, this would be a step towards an alternative approach that relies on machine learning, providing much more efficient computation. To answer this question, we propose three deep learning models for 1-, 3-, and 5-qubit circuits and experiment on the following real-quantum processors: ibmq_armonk (1-qubit), ibmq_lima (5-qubit) and ibmq_quito (5-qubit) backends, respectively. Our models achieved a mean correlation factor of 0.74, 0.67 and 0.66 for 1-, 3-, and 5-qubit random circuits, respectively, with the exponential state tomography method. Additionally, our 5-qubit model outperforms simple baseline state fidelity estimation method on three quantum benchmarks. Our method, trained on random circuits only, achieved a mean correlation factor of 0.968 while the baseline method achieved 0.738. Furthermore, we investigate the effect of dynamic noise on state fidelity estimation. The correlation factor substantially improved to 0.82 and 0.74 for the 3- and 5-qubit models, respectively. The results show that machine learning is promising for predicting state fidelity from circuit representation and this work may be considered a step towards efficient end-to-end learning.","['Physics', 'Quantum Physics', 'Quantum Information Technology, Spintronics', 'Nanotechnology and Microengineering']"
doi:10.1007/s00521-022-08018-2,en,A shapelet-based framework for large-scale word-level sign language database auto-construction,"['ReviewPaper', 'Review']","Sign language recognition is a challenging and often underestimated problem that includes the asynchronous integration of multimodal articulators. Learning powerful applied statistical models requires much training data. However, well-labelled sign language databases are a scarce resource due to the high cost of manual labelling and performing. On the other hand, there exist a lot of sign language-interpreted videos on the Internet. This work aims to propose a framework to automatically learn a large-scale sign language database from sign language-interpreted videos. We achieved this by exploring the correspondence between subtitles and motions by discovering shapelets which are the most discriminative subsequences within the data sequences. In this paper, two modified shapelet methods were used to identify the target signs for 1000 words from 89 (96 h, 8 naive signers) sign language-interpreted videos in terms of brute force search and parameter learning. Then, an augmented (3–5 times larger) large-scale word-level sign database was finally constructed using an adaptive sample augmentation strategy that collected all similar video clips of the target sign as valid samples. Experiments on a subset of 100 words revealed a considerable speedup and 14% improvement in recall rate. The evaluation of three state-of-the-art sign language classifiers demonstrates the good discrimination of the database, and the sample augmentation strategy can significantly increase the recognition accuracy of all classifiers by 10–33% by increasing the number, variety, and balance of the data.","['Computer Science', 'Artificial Intelligence', 'Data Mining and Knowledge Discovery', 'Probability and Statistics in Computer Science', 'Computational Science and Engineering', 'Image Processing and Computer Vision', 'Computational Biology/Bioinformatics']"
doi:10.1007/s00354-022-00195-x,en,Combined Cloud-Based Inference System for the Classification of COVID-19 in CT-Scan and X-Ray Images,OriginalPaper,"In the past few years, most of the work has been done around the classification of covid-19 using different images like CT-scan, X-ray, and ultrasound. But none of that is capable enough to deal with each of these image types on a single common platform and can identify the possibility that a person is suffering from COVID or not. Thus, we realized there should be a platform to identify COVID-19 in CT-scan and X-ray images on the fly. So, to fulfill this need, we proposed an AI model to identify CT-scan and X-ray images from each other and then use this inference to classify them of COVID positive or negative. The proposed model uses the inception architecture under the hood and trains on the open-source extended covid-19 dataset. The dataset consists of plenty of images for both image types and is of size 4 GB. We achieved an accuracy of 100%, average macro-Precision of 100%, average macro-Recall of 100%, average macro f1-score of 100%, and AUC score of 99.6%. Furthermore, in this work, cloud-based architecture is proposed to massively scale and load balance as the Number of user requests rises. As a result, it will deliver a service with minimal latency to all users.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s12524-022-01624-6,en,Oil Spill Identification based on Dual Attention UNet Model Using Synthetic Aperture Radar Images,"['OriginalPaper', 'Research Article']","Oil spills cause tremendous damage to marine, coastal environments, and ecosystems. Previous deep learning-based studies have addressed the task of detecting oil spills as a semantic segmentation problem. However, further improvement is still required to address the noisy nature of the Synthetic Aperture Radar (SAR) imagery problem, which limits segmentation performance. In this study, a new deep learning model based on the Dual Attention Model (DAM) is developed to automatically detect oil spills in a water body. We enhanced a conventional UNet segmentation network by integrating a dual attention model DAM to selectively highlight the relevant and discriminative global and local characteristics of oil spills in SAR imagery. DAM is composed of a Channel Attention Map and a Position Attention Map which are stacked in the decoder network of UNet. The proposed DAM-UNet is compared with four baselines, namely fully convolutional network, PSPNet, LinkNet, and traditional UNet. The proposed DAM-UNet outperforms the four baselines, as demonstrated empirically. Moreover, the EG-Oil Spill dataset includes a large set of SAR images with 3000 image pairs. The obtained overall accuracy of the proposed method increased by 3.2% and reaches 94.2% compared with that of the traditional UNet. The study opens new development ideas for integrating attention modules into other deep learning tasks, including machine translation, image-based analysis, action recognition, and speech recognition.","['Earth Sciences', 'Earth Sciences, general', 'Remote Sensing/Photogrammetry']"
doi:10.1007/s11998-022-00687-x,en,Deep learning study of induced stochastic pattern formation in the gravure printing fluid splitting process,OriginalPaper,"We use deep learning (DL) algorithms for the phenomenological classification of Saffman-Taylor-instability-driven spontaneous pattern formation at the liquid meniscus in the fluid splitting in a gravure printing press. The DL algorithms are applied to high-speed video recordings of the fluid splitting process between the rotating gravure cylinder and the co-moving planar target substrate. Depending on rotation velocity or printing velocity and gravure raster of the engraved printing cylinder, a variety of transient liquid wetting patterns, e.g., a raster of separate drops, viscous fingers, or more complex, branched liquid bridges appear in the printing nip. We discuss how these patterns are classified with DL methods, and how this could serve the identification of different hydrodynamic flow regimes in the nip, e.g., point or lamella splitting.","['Materials Science', 'Tribology, Corrosion and Coatings', 'Surfaces and Interfaces, Thin Films', 'Polymer Sciences', 'Industrial Chemistry/Chemical Engineering', 'Materials Science, general']"
doi:10.1038/s41534-022-00625-0,en,Long-time simulations for fixed input states on quantum hardware,"['OriginalPaper', 'Article']","Publicly accessible quantum computers open the exciting possibility of experimental dynamical quantum simulations. While rapidly improving, current devices have short coherence times, restricting the viable circuit depth. Despite these limitations, we demonstrate long-time, high fidelity simulations on current hardware. Specifically, we simulate an XY-model spin chain on Rigetti and IBM quantum computers, maintaining a fidelity over 0.9 for 150 times longer than is possible using the iterated Trotter method. Our simulations use an algorithm we call fixed state Variational Fast Forwarding (fsVFF). Recent work has shown an approximate diagonalization of a short time evolution unitary allows a fixed-depth simulation. fsVFF substantially reduces the required resources by only diagonalizing the energy subspace spanned by the initial state, rather than over the total Hilbert space. We further demonstrate the viability of fsVFF through large numerical simulations, and provide an analysis of the noise resilience and scaling of simulation errors.","['Physics', 'Physics, general', 'Quantum Physics', 'Quantum Information Technology, Spintronics', 'Quantum Computing', 'Quantum Field Theories, String Theory', 'Classical and Quantum Gravitation, Relativity Theory']"
doi:10.1007/s11042-022-14203-1,en,Alzheimer disease diagnosis for magnetic resonance brain images using deep learning neural networks,OriginalPaper,"In this work, a Deep Convolutional Neural Network (DCNN) framework for Alzheimer’s Disease (AD) diagnosis based on brain Magnetic Resonance Imaging (MRI) scans is presented. A multiclass DCNN classifier is used to discriminate between Normal Controls (NC), Mild Cognitive Impairment (MCI), and AD. The Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset was used to train and test the proposed DCNN. Different train-test ratios have been examined. Average accuracies of 100% for AD/NC, 92.93% for NC/MCI, and 99.21% for AD/MCI were obtained. The proposed system achieved an average accuracy of 93.86% for a three-way AD/MCI/NC classification. To further examine the proposed system performance, Receiver Operation Characteristics (ROC) analysis and Confusion Matrix (CM) were also used. For certain scenarios, the Area Under ROC Curve (AUC) values of 1, 1, and 0.989 were obtained for AD, NC, and MCI, respectively. The results show higher metrics compared to previously published studies concerning AD diagnosis.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s11042-022-14083-5,en,Deep auto encoder based on a transient search capsule network for student performance prediction,OriginalPaper,"Prediction of Student performance through a machine predicts a student’s future success. It can be considered an essential procedure to determine the students’ academic excellence and identify them at high risk for academic performance. Prediction of student performance also provides universities with a high reputation and ranking. The evaluation of ‘What students can do with their learning’ is still a tedious task. There are many challenging factors to solve this problem, mainly owing to the enormous amount of data collected from students. Most of the research works have focused on developing new methodologies for student performance prediction. But all the existing work has some performance limitations. Here, a new model called transient search capsule network based on the deep Autoencoder (TSCNDE ) is introduced to detect student performance. The TSCNDE method is implemented with the help of the PYTHON tool. The performance prediction process has been completed with the help of the OULA dataset. The obtained results are assessed on accuracy (99.2%), precision, (99.8%), specificity (98.7%), and sensitivity (98.9%) parameters. The results obtained showed that the TSCNDE method is about 99.2% more accurate than the other related method. Also, the obtained results are compared with some existing deep learning and machine learning methods.","['Computer Science', 'Multimedia Information Systems', 'Computer Communication Networks', 'Data Structures and Information Theory', 'Special Purpose and Application-Based Systems']"
doi:10.1007/s00354-022-00194-y,en,Variational Autoencoder Based Imbalanced COVID-19 Detection Using Chest X-Ray Images,OriginalPaper,"Early and fast detection of disease is essential for the fight against COVID-19 pandemic. Researchers have focused on developing robust and cost-effective detection methods using Deep learning based chest X-Ray image processing. However, such prediction models are often not well suited to address the challenge of highly imabalanced datasets. The current work is an attempt to address the issue by utilizing unsupervised Variational Auto Encoders (VAEs). Firstly, chest X-Ray images are converted to a latent space by learning the most important features using VAEs. Secondly, a wide range of well established data resampling techniques are used to balance the preexisting imbalanced classes in the latent vector form of the dataset. Finally, the modified dataset in the new feature space is used to train well known classification models to classify chest X-Ray images into three different classes viz., ”COVID-19”, ”Pneumonia”, and ”Normal”. In order to capture the quality of resampling methods, 10-folds cross validation technique is applied on the dataset. Extensive experimental analysis have been carried out and results so obtained indicate significant improvement in COVID-19 detection using the proposed VAE based method. Furthermore, the ingenuity of the results have been established by performing Wilcoxon rank test with 95% level of significance.","['Computer Science', 'Artificial Intelligence', 'Computer Hardware', 'Computer Systems Organization and Communication Networks', 'Software Engineering/Programming and Operating Systems']"
doi:10.1007/s00158-022-03451-2,en,3D shape optimization of loudspeaker cabinets for uniform directivity,"['OriginalPaper', 'Research Paper']","This paper presents a method to perform gradient-based shape optimization to minimize the root mean square deviation of the exterior acoustic sound pressure level distribution in front of an initially spherically shaped loudspeaker. The work includes several examples of how different multi-frequency optimization strategies can affect the final optimized design performance. This includes testing, averaging, and weighting of multi-frequency cost functions or using a minimax formulation. The shape optimization technique is based on an acoustic Boundary Element Method coupled to a Lumped Parameter loudspeaker model. To control and alter the deformation of the loudspeaker cabinet the optimization method adapts a spherical free-form deformation approach based on Bernstein polynomials. For the particular optimization problems presented, it is shown that improvements in the root mean square deviation of the sound pressure level in front of the loudspeaker can be achieved between 1 and 5 kHz. In the best-case scenario, less than a 1 dB sound pressure level (SPL) variation is observed between on-axis and a 70° off-axis response in the range 2 to 5 kHz. The widest frequency bandwidth and smoothest response of the root mean square deviation is found by utilizing the minimax formulation.","['Engineering', 'Theoretical and Applied Mechanics', 'Computational Mathematics and Numerical Analysis', 'Engineering Design']"
doi:10.1007/s11590-022-01935-0,en,On ambiguity-averse market equilibrium,"['OriginalPaper', 'Original Paper']","We develop a Nash equilibrium problem representing a perfectly competitive market wherein all players are subject to the same source of uncertainty with an unknown probability distribution. Each player—depending on her individual access to and confidence over empirical data—builds an ambiguity set containing a family of potential probability distributions describing the uncertain event. The ambiguity set of different players is not necessarily identical, yielding a market with potentially heterogeneous ambiguity aversion. Built upon recent developments in the field of Wasserstein distributionally robust chance-constrained optimization, each ambiguity-averse player maximizes her own expected payoff under the worst-case probability distribution within her ambiguity set. Using an affine policy and a conditional value-at-risk approximation of chance constraints, we define a tractable Nash game. We prove that under certain conditions a unique Nash equilibrium point exists, which coincides with the solution of a single optimization problem. Numerical results indicate that players with comparatively lower consumption utility are highly exposed to rival ambiguity aversion.","['Mathematics', 'Optimization', 'Operations Research/Decision Theory', 'Computational Intelligence', 'Numerical and Computational Physics, Simulation']"
doi:10.1007/s12046-022-01964-6,en,Explainable sentiment analysis for product reviews using causal graph embeddings,OriginalPaper,"Sentiment analysis is used to extract opinions expressed in product reviews. Aspect level sentiment analysis extracts opinions about features of a product. However, such analysis cannot infer the underlying reason for the opinions expressed. The following useful applications can be realized, if reasons for opinions are inferred: (i) To perform root cause analysis of negative opinions. (ii) Sentiment inference of causes helps in building a phrase-level sentiment lexicon, and (iii) Creation of causality-aware word embeddings to enhance the accuracy of sentiment analyzers. To realize above-cited use-cases, we have proposed to use a deep neural sequence model to extract cause phrases and designed a novel hybrid model based on graph neural networks and Bayesian reasoning for inferring the sentiments implied by cause phrases. We tested our models on three annotated datasets and observed mean accuracies of 96.34%, 96.12%, 97.14% and 82.14%, 85.23%, 87.21% for the cause phrase extraction and sentiment inference tasks respectively. We have also investigated the impact of over smoothing in graph neural network through an ablation study and reported the results.","['Engineering', 'Engineering, general']"
doi:10.1007/s00371-022-02721-w,en,STAN: spatiotemporal attention network for video-based facial expression recognition,"['OriginalPaper', 'Original article']","Video-based facial expression recognition is a very challenging task. The expression features portrayed by traditional ResNet18 are not rich enough, while the classical method LSTM to process expression videos may not extract effective temporal features for cases with weaker emotion intensity. This paper proposes a spatiotemporal attention network to extract more diverse spatial features and more effective temporal relationships. Firstly, a spatial attention module is used to enhance the expression features extracted by the ResNet18 and remove redundancy. Then, multiple levels of information are combined to extract richer expression features. Meanwhile, the video stream is divided into a series of video clips using a sliding window, and the temporal features are extracted from each small clip by the LSTM, which is a simple but effective way to divide the video. Third, considering the different importance of expression features extracted from each window for the results of expression recognition of the whole video, an attention-based score fusion module is proposed to fuse expression information from multiple windows. We perform comprehensive experiments on in-the-wild FER benchmarks (AFEW8.0 and HUST-MM). Quantitative and qualitative analyses demonstrate the effectiveness of our proposed method.","['Computer Science', 'Computer Graphics', 'Computer Science, general', 'Artificial Intelligence', 'Image Processing and Computer Vision']"
doi:10.1007/s12205-022-1241-8,en,Prediction and Feature Importance of Earth Pressure in Shields Using Machine Learning Algorithms,"['OriginalPaper', 'Tunnel Engineering']","To reduce subjectivity and uncertainty when maintaining suitable earth pressure in earth pressure shields that can prevent heave or collapse, many prediction models using machine learning algorithms were proposed, but little research into the effects of other parameters on earth pressure has been undertaken, and soil conditioning parameters are always ignored. To establish a model with thorough parameters and probe into influences of other parameters, multiple machine learning algorithms were attempted. Given the accuracy, diversity and functions, random forest (RF), LightGBM and Attention-back-propagation neural network (Attention-BPNN) were further analyzed. Then, two RF models were compared in this research, one with soil conditioning parameters and the other without. Meanwhile, a case study was utilized to verify the reliability of the model. Finally, the feature importance of three models was compared and the variation rules of the most four important features were discussed by controlling variates. The results showed that soil conditioning parameters delivered a significant reduction in the prediction error. The case study demonstrated that the proposed model can satisfy engineering requirements. More earth pressure should give priority to increasing propulsion pressure, advance rate, and reducing foam air flow, rotational speed of screw conveyor, and vice versa.","['Engineering', 'Civil Engineering', 'Industrial Pollution Prevention', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s11600-022-00948-8,en,"Comparison of machine learning models for predicting groundwater level, case study: Najafabad region","['OriginalPaper', 'Research Article - Hydrology']","Water resources, consisting of surface water and groundwater, are considered to be among the crucial natural resources in most arid and semiarid regions. Groundwater resources as the sustainable yields can be predicted, whereas this is one of the important stages in water resource management. To this end, several models such as mathematical, statistical, empirical, and conceptual can be employed. In this paper, machine learning and deep learning methods as conceptual ones are applied for the simulations. The selected models are support vector regression (SVR), adaptive neuro-fuzzy inference system (ANFIS), and multilayer perceptron (MLP). Next, these models are optimized with the adaptive moment estimation (ADAM) optimization algorithm which results in hybrid models. The hyper-parameters of the stated models are optimized with the ADAM method. The root mean squared error (RMSE), mean absolute error (MAE), mean squared error (MSE), and coefficient of determination (R 2 ) are used to evaluate the accuracy of the simulated groundwater level. To this end, the aquifer hydrograph is used to compare the results with observations data. So, the RMSE and R 2 show that the accuracy of the machine learning and deep learning models is better than the numerical model for the given data. Moreover, the MSE is approximately the same in all three cases (ranging from 0.7113 to 0.6504). Also, the total value of R 2 and RMSE for the best hybrid model is 0.9617 and 0.7313, respectively, which are obtained from the model output. The results show that all three techniques are useful tools for modeling hydrological processes in agriculture and their computational capabilities and memory are similar.","['Earth Sciences', 'Geophysics/Geodesy', 'Structural Geology', 'Geotechnical Engineering & Applied Earth Sciences']"
doi:10.1007/s10514-022-10072-7,en,Provident vehicle detection at night for advanced driver assistance systems,OriginalPaper,"In recent years, computer vision algorithms have become more powerful, which enabled technologies such as autonomous driving to evolve rapidly. However, current algorithms mainly share one limitation: They rely on directly visible objects. This is a significant drawback compared to human behavior, where visual cues caused by objects (e. g., shadows) are already used intuitively to retrieve information or anticipate occurring objects. While driving at night, this performance deficit becomes even more obvious: Humans already process the light artifacts caused by the headlamps of oncoming vehicles to estimate where they appear, whereas current object detection systems require that the oncoming vehicle is directly visible before it can be detected. Based on previous work on this subject, in this paper, we present a complete system that can detect light artifacts caused by the headlights of oncoming vehicles so that it detects that a vehicle is approaching providently (denoted as provident vehicle detection). For that, an entire algorithm architecture is investigated, including the detection in the image space, the three-dimensional localization, and the tracking of light artifacts. To demonstrate the usefulness of such an algorithm, the proposed algorithm is deployed in a test vehicle to use the detected light artifacts to control the glare-free high beam system proactively (react before the oncoming vehicle is directly visible). Using this experimental setting, the provident vehicle detection system’s time benefit compared to an in-production computer vision system is quantified. Additionally, the glare-free high beam use case provides a real-time and real-world visualization interface of the detection results by considering the adaptive headlamps as projectors. With this investigation of provident vehicle detection, we want to put awareness on the unconventional sensing task of detecting objects providently (detection based on observable visual cues the objects cause before they are visible) and further close the performance gap between human behavior and computer vision algorithms to bring autonomous and automated driving a step forward.","['Engineering', 'Robotics and Automation', 'Artificial Intelligence', 'Computer Imaging, Vision, Pattern Recognition and Graphics', 'Control, Robotics, Mechatronics']"
doi:10.1007/s12046-022-02026-7,en,DeepHands: Dynamic hand gesture detection with depth estimation and 3D reconstruction from monocular RGB data,OriginalPaper,"Human hand gestures are the most important tools for interacting with the real environment. Capturing hand motion is critical for a wide range of applications in Augmented Reality (AR)/Virtual Reality (VR), Human-computer Interface (HCI), and many other disciplines. This paper presents a 3 module pipeline for effective hand gesture detection in real-time at the speed of 100 frames per second (fps).Various hand gestures can be captured by simple RGB camera and then processed to first detect the palm and then find essential 3D landmarks, which helps in creating skeletal representation of hand. In order to form a 3D mesh around the skeletal hand 2D and 3D annotations of Hand gestures are merged and in the final module 3D animated hand gestures are presented using advanced neural network. 3D representation of hand gestures ensures greater understanding of depth ambiguity problem in monocular pose estimations and can be effectively used in computer vision and graphics applications. The proposed design is compared with several benchmarks to highlight improvements in the results achieved over conventional methods.","['Engineering', 'Engineering, general']"
doi:10.1038/s41467-022-34603-z,en,Deep learning to decompose macromolecules into independent Markovian domains,"['OriginalPaper', 'Article']","The increasing interest in modeling the dynamics of ever larger proteins has revealed a fundamental problem with models that describe the molecular system as being in a global configuration state. This notion limits our ability to gather sufficient statistics of state probabilities or state-to-state transitions because for large molecular systems the number of metastable states grows exponentially with size. In this manuscript, we approach this challenge by introducing a method that combines our recent progress on independent Markov decomposition (IMD) with VAMPnets, a deep learning approach to Markov modeling. We establish a training objective that quantifies how well a given decomposition of the molecular system into independent subdomains with Markovian dynamics approximates the overall dynamics. By constructing an end-to-end learning framework, the decomposition into such subdomains and their individual Markov state models are simultaneously learned, providing a data-efficient and easily interpretable summary of the complex system dynamics. While learning the dynamical coupling between Markovian subdomains is still an open issue, the present results are a significant step towards learning Ising models of large molecular complexes from simulation data. Modeling the dynamics of large proteins reveals a fundamental scaling problem. Here, the authors tackle this challenge by decomposing a large system into smaller independent subsystems, simultaneously modeling each subsystem’s kinetics and ensuring their mutual independence.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1038/s41598-022-22057-8,en,Accurate discharge and water level forecasting using ensemble learning with genetic algorithm and singular spectrum analysis-based denoising,"['OriginalPaper', 'Article']","Forecasting discharge (Q) and water level (H) are essential factors in hydrological research and flood prediction. In recent years, deep learning has emerged as a viable technique for capturing the non-linear relationship of historical data to generate highly accurate prediction results. Despite the success in various domains, applying deep learning in Q and H prediction is hampered by three critical issues: a shortage of training data, the occurrence of noise in the collected data, and the difficulty in adjusting the model’s hyper-parameters. This work proposes a novel deep learning-based Q–H prediction model that overcomes all the shortcomings encountered by existing approaches. Specifically, to address data scarcity and increase prediction accuracy, we design an ensemble learning architecture that takes advantage of multiple deep learning techniques. Furthermore, we leverage the Singular-Spectrum Analysis (SSA) to remove noise and outliers from the original data. Besides, we exploit the Genetic Algorithm (GA) to propose a novel mechanism that can automatically determine the prediction model’s optimal hyper-parameters. We conducted extensive experiments on two datasets collected from Vietnam’s Red and Dakbla rivers. The results show that our proposed solution outperforms current techniques across a wide range of metrics, including NSE, MSE, MAE, and MAPE. Specifically, by exploiting the ensemble learning technique, we can improve the NSE by at least $$2\%$$ 2 % . Moreover, with the aid of the SSA-based data preprocessing technique, the NSE is further enhanced by more than $$5\%$$ 5 % . Finally, thanks to GA-based optimization, our proposed model increases the NSE by at least $$6\%$$ 6 % and up to $$40\%$$ 40 % in the best case.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s00500-022-07604-9,en,A hybrid learning-based genetic and grey-wolf optimizer for global optimization,"['OriginalPaper', 'Optimization']","The grey-wolf optimizer (GWO) is a comparatively recent and competent algorithm in Swarm Intelligence (SI) to solve numerical and real-world optimization problems. However, the biggest challenge is the quick stabilization of its search agents to the local optima. Therefore, to bring effectiveness in the global search, it is imperative to relocate the leading agents through the procreation of their positions in the search space. This paper proposes GL-GWO, a genetic learning (GL)-based GWO, which imitates the genetic offspring generation scheme to improve the intelligence of GWO’s leading agents. The GL scheme expedites the global effectiveness of leading agents by constructing the exemplars for them through genetic operators using their historical information. The obtained exemplars are well diversified and highly intelligent; therefore, the rest of the population’s global searchability and search efficiency are enhanced under their guidance. The GL-GWO is tested on widely adopted 20 benchmark functions from the IEEE-CEC-2005 dataset and 38 functions from the IEEE-CEC-2014 dataset. The efficacy of GL-GWO is tested on four real-world engineering problems, namely recommendation systems, face image super-resolution, tension/compression spring, and welded beam. The obtained results on benchmark functions and considered engineering problems conclude that the GL-GWO is an efficient, effective, and reliable algorithm for solving real-world optimization problems.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1007/s11356-022-24044-y,en,A review of recent developments in the application of machine learning in solar thermal collector modelling,"['ReviewPaper', 'Review Article']","Over the past few decades, the popularity of solar thermal collectors has increased dramatically because of many significant advantages like being a free, natural, environmentally friendly and permanent energy source. Today, developing and optimising different solar thermal energy systems are more important than before. Thus, there are various methods for investigating the performance of these systems, such as experimental, numerical and mathematical methods. One of the cutting-edge methods is artificial intelligence, which can predict key and effective parameters in solar collector efficiency. This review identified recent machine learning modelling, including multilayer perceptron artificial neural network (MLP-ANN), group method of data handling (GMDH), radial basis function (RBF), artificial neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and studies regarding different types of solar thermal collectors, namely non-concentration and concentration. Furthermore, it investigated the effect of various essential factors on the accuracy, potential issues and challenges facing the application of artificial intelligence in these systems. Finally, it will also be recommended opportunities for future research.","['Environment', 'Environment, general', 'Environmental Chemistry', 'Ecotoxicology', 'Environmental Health', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Waste Water Technology / Water Pollution Control / Water Management / Aquatic Pollution']"
doi:10.1007/s44196-022-00160-y,en,Enhanced Salp Search Algorithm for Optimization Extreme Learning Machine and Application to Dew Point Temperature Prediction,"['OriginalPaper', 'Research Article']","Extreme learning machine (ELM) is popular as a method of training single hidden layer feedforward neural networks. However, the ELMs optimized by the traditional gradient descent algorithms cannot fundamentally solve the influence of the random selection of the input weights and biases. Therefore, this paper proposes a method of extreme learning machine optimized by an enhanced salp search algorithm (NSSA-ELM). Salp search algorithm (SSA) is a metaheuristic algorithm, to improve the performance of SSA exploration and avoid getting stuck in local optima, the neighborhood centroid opposite‑based learning is used to optimize SSA. This method maintains the diversity of the population, which is conducive to avoid local optimization and accelerate convergence. This paper performs classification tests on NSSA and other metaheuristic-optimized ELMs on ten datasets, and regression tests on 5 datasets. Finally, the prediction ability of dew point temperature is evaluated. The meteorological data of five climatically representative cities in China from 2016 to 2022 were collected to predict the dew point temperature. The experimental results show that the NSSA-ELM is the best model, and its generalization performance and accuracy are better than other models.","['Engineering', 'Computational Intelligence', 'Artificial Intelligence', 'Mathematical Logic and Foundations', 'Control, Robotics, Mechatronics']"
doi:10.1038/s41598-022-24250-1,en,Low-cost quasi-global optimization of expensive electromagnetic simulation models by inverse surrogates and response features,"['OriginalPaper', 'Article']","Conceptual design of contemporary high-frequency structures is typically followed by a careful tuning of their parameters, predominantly the geometry ones. The process aims at improving the relevant performance figures, and may be quite expensive. The reason is that conventional design methods, e.g., based on analytical or equivalent network models, often only yield rough initial designs. This is especially the case for miniaturized components featuring considerable electromagnetic (EM) cross couplings, or antenna systems with non-negligible radiator coupling (e.g., MIMO, closely-spaced arrays). For reliability reasons, parametric optimization is carried out using EM simulation tools, which is a time-consuming task. In many cases, designer needs to resort to a global search, especially when handling several objectives and constraints is necessary, or the high-frequency structure under design is overly complex. Combination of both aforementioned factors makes it no longer possible to rely on engineering insight, even to detect a promising region of the design space. Unfortunately, nature-inspired algorithms, commonly employed for solving these tasks typically exhibit significant computational expenditures. This paper proposes a simple yet efficient method for globalized search using a response feature approach and inverse regression surrogates. Owing to less nonlinear dependence of the feature point coordinates on the system variables (as compared to the original responses, e.g., S -parameter frequency characteristics), our methodology permits a rapid identification of the most appropriate regions of the parametric space, and further design tuning by means of local routines. At the same time, the overall optimization cost is comparable to the cost of local procedures. The proposed approach is validated using several high-frequency structures (a dual-band antenna, a microstrip coupler, an impedance matching transformer) optimized under different design scenarios. Global search capability and computational efficiency are demonstrated through comprehensive comparisons with multiple-start local search, as well as particle swarm optimizer, a representative nature-inspired algorithm.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s11063-022-11072-5,en,Scene Level Image Classification: A Literature Review,ReviewPaper,"Convolutional neural networks (CNNs) have made significant contributions to natural and remote sensing imaging since the development of deep learning. Scene-level image classification is a challenge that affects both the natural and remote sensing domains and has numerous applications. The number of possible scene entities in the image content that could match the dataset images is the main focus. Scene-level classification is significant and fascinating because of open problems like intraclass heterogeneity, interclass homogeneity, background cluttering, high spatial resolution, and different imaging conditions. Additionally, the multi-label scene dataset’s imbalance, lack of preservation of complex semantic relations, and higher label-to-label correlation are all apparent. The article discusses a meta-analysis of the state-of-the-art scene classification literature practices. We discuss CNNs, attention mechanisms, capsule networks, and generative adversarial networks. The article also delivers an overview of the various activations, losses, optimization techniques, and regularization schemes pertinent to the scene domain. The standard benchmark datasets based on single- and multi-label themes are collated. The performance metrics for scene classification are explained as well. The implementation of the multi-label scene classification utilizing several CNN models on the UC Merced multi-label dataset is also covered in the paper. The proposed MobileNet-based model performs better than the recognized cutting-edge methodologies.","['Computer Science', 'Artificial Intelligence', 'Complex Systems', 'Computational Intelligence']"
doi:10.1038/s41598-022-24405-0,en,Stacked kinship CNN vs. GBLUP for genomic predictions of additive and complex continuous phenotypes,"['OriginalPaper', 'Article']","Deep learning is impacting many fields of data science with often spectacular results. However, its application to whole-genome predictions in plant and animal science or in human biology has been rather limited, with mostly underwhelming results. While most works focus on exploring alternative network architectures, in this study we propose an innovative representation of marker genotype data and tested it against the GBLUP (Genomic BLUP) benchmark with linear and nonlinear phenotypes. From publicly available cattle SNP genotype data, different types of genomic kinship matrices are stacked together in a 3D pile from where 2D grayscale slices are extracted and fed to a deep convolutional neural network (DNN). We simulated nine phenotype scenarios with combinations of additivity, dominance and epistasis, and compared the DNN to GBLUP-A (computed using only the additive kinship matrix) and GBLUP-optim (additive, dominance, and epistasis kinship matrices, as needed). Results varied depending on the accuracy metric employed, with DNN performing better in terms of root mean squared error (1–12% lower than GBLUP-A; 1–9% lower than GBLUP-optim) but worse in terms of Pearson’s correlation (0.505 for DNN compared to 0.672 and 0.669 of GBLUP-A and GBLUP-optim for fully additive case; 0.274 for DNN, 0.279 for GBLUP-A, and 0.477 for GBLUP-optim for fully dominant case). The proposed approach offers a basis to explore further the application of DNN to tabular data in whole-genome predictions.","['Science, Humanities and Social Sciences, multidisciplinary', 'Science, Humanities and Social Sciences, multidisciplinary', 'Science, multidisciplinary']"
doi:10.1007/s10661-022-10656-x,en,Deep learning system for paddy plant disease detection and classification,OriginalPaper,"Automatic detection and analysis of rice crop diseases is widely required in the farming industry, which can be utilized to avoid squandering financial and other resources, reduce yield losses, and improve treatment efficiency, resulting in healthier crop output. An automated approach was proposed for accurately detecting and classifying diseases from a supplied photograph. The proposed system for the recognition of rice plant diseases adopts a computer vision–based approach that employs the techniques of image processing, machine learning, and deep learning, reducing the reliance on conventional methods to protect paddy crops from diseases like bacterial leaf blight, false smut, brown leaf spot, rice blast, and sheath rot, the five primary diseases that frequently plague the Indian rice fields. Following image pre-processing, image segmentation is employed to determine the diseased section of the paddy plant, with the diseases listed above being identified purely on the basis of their visual contents. An integration of a support vector machine classifier and convolutional neural networks are used to recognize and classify specific varieties of paddy plant diseases. With ReLU and softmax functions, the suggested deep learning–based strategy attained the highest validation accuracy of 0.9145. Following recognition, a predictive remedy is recommended, which can assist agriculture-related individuals and organizations in taking suitable measures to combat these diseases.","['Environment', 'Monitoring/Environmental Analysis', 'Environmental Management', 'Ecotoxicology', 'Atmospheric Protection/Air Quality Control/Air Pollution', 'Ecology']"
doi:10.1038/s42003-022-04076-3,en,UnMICST: Deep learning with real augmentation for robust segmentation of highly multiplexed images of human tissues,"['OriginalPaper', 'Article']","Upcoming technologies enable routine collection of highly multiplexed (20–60 channel), subcellular resolution images of mammalian tissues for research and diagnosis. Extracting single cell data from such images requires accurate image segmentation, a challenging problem commonly tackled with deep learning. In this paper, we report two findings that substantially improve image segmentation of tissues using a range of machine learning architectures. First, we unexpectedly find that the inclusion of intentionally defocused and saturated images in training data substantially improves subsequent image segmentation. Such real augmentation outperforms computational augmentation (Gaussian blurring). In addition, we find that it is practical to image the nuclear envelope in multiple tissues using an antibody cocktail thereby better identifying nuclear outlines and improving segmentation. The two approaches cumulatively and substantially improve segmentation on a wide range of tissue types. We speculate that the use of real augmentations will have applications in image processing outside of microscopy. Presenting UnMICST, strategies for robust single-cell segmentation in challenging human tissues.","['Life Sciences', 'Life Sciences, general']"
