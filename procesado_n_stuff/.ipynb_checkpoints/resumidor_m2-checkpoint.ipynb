{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c696c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml6team/keyphrase-extraction-kbir-inspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da4e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed8ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.19k/1.19k [00:00<00:00, 480kB/s]\n",
      "c:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\G\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 1.92M/1.92M [00:02<00:00, 738kB/s] \n",
      "Downloading: 100%|██████████| 3.51M/3.51M [00:08<00:00, 430kB/s] \n",
      "Downloading: 100%|██████████| 775/775 [00:00<00:00, 258kB/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\G/.cache\\huggingface\\hub\\models--google--bigbird-pegasus-large-arxiv\\snapshots\\cc842da8398b85a3ffc9b30521216d4c043a6a10\\pytorch_model.bin' at 'C:\\Users\\G/.cache\\huggingface\\hub\\models--google--bigbird-pegasus-large-arxiv\\snapshots\\cc842da8398b85a3ffc9b30521216d4c043a6a10\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:399\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    400\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:777\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    776\u001b[0m orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[1;32m--> 777\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    778\u001b[0m     \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:282\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name_or_buffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_reader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileReader(name_or_buffer))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:403\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    404\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    405\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    406\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2006: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/bigbird-pegasus-large-arxiv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m BigBirdPegasusForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgoogle/bigbird-pegasus-large-arxiv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# decoder attention type can't be changed & will be \"original_full\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# you can change `attention_type` (encoder only) to full attention like this:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m BigBirdPegasusForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/bigbird-pegasus-large-arxiv\u001b[39m\u001b[39m\"\u001b[39m, attention_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moriginal_full\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2230\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2228\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2229\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2230\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2232\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2237\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\G\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:415\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    411\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\G/.cache\\huggingface\\hub\\models--google--bigbird-pegasus-large-arxiv\\snapshots\\cc842da8398b85a3ffc9b30521216d4c043a6a10\\pytorch_model.bin' at 'C:\\Users\\G/.cache\\huggingface\\hub\\models--google--bigbird-pegasus-large-arxiv\\snapshots\\cc842da8398b85a3ffc9b30521216d4c043a6a10\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "# decoder attention type can't be changed & will be \"original_full\"\n",
    "# you can change `attention_type` (encoder only) to full attention like this:\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
    "\n",
    "# you can change `block_size` & `num_random_blocks` like this:\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58221ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"spring/\")\n",
    "dir_list = [x for x in dir_list if   \".csv\" in x]\n",
    "data = pd.DataFrame()\n",
    "for i in dir_list:\n",
    "    if i == \"total.csv\":\n",
    "        continue\n",
    "    print(i)\n",
    "    data_temp = pd.read_csv(\"spring/\"+i)\n",
    "    data = data.append(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8086c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = data[\"abstract\"].tolist()[962]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "prediction = model.generate(**inputs)\n",
    "prediction = tokenizer.batch_decode(prediction)\n",
    "print(\"time =\",time.time() - start_time)\n",
    "print(\"---------------------\")\n",
    "print(text)\n",
    "print(\"---------------------\")\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2451bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe448d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = data[\"abstract\"].tolist()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcd387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe583a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748513b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784257d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41befb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e02a394f46a34fb7f702684e07532c65eb0e3cf841f78604b97865c14e9683e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
