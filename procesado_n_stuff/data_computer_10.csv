titulo,abstract,clase_pri,clase_otr
Large image datasets: A pyrrhic win for computer vision?,"  In this paper we investigate problematic practices and consequences of large
scale vision datasets. We examine broad issues such as the question of consent
and justice as well as specific concerns such as the inclusion of verifiably
pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an
example, we perform a cross-sectional model-based quantitative census covering
factors such as age, gender, NSFW content scoring, class-wise accuracy,
human-cardinality-analysis, and the semanticity of the image class information
in order to statistically investigate the extent and subtleties of ethical
transgressions. We then use the census to help hand-curate a look-up-table of
images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of
verifiably pornographic: shot in a non-consensual setting (up-skirt), beach
voyeuristic, and exposed private parts. We survey the landscape of harm and
threats both society broadly and individuals face due to uncritical and
ill-considered dataset curation practices. We then propose possible courses of
correction and critique the pros and cons of these. We have duly open-sourced
all of the code and the census meta-datasets generated in this endeavor for the
computer vision community to build on. By unveiling the severity of the
threats, our hope is to motivate the constitution of mandatory Institutional
Review Boards (IRB) for large scale dataset curation processes.

    ",Computers and Society (cs.CY),; Applications (stat.AP); Machine Learning (stat.ML)
The Heisenberg Representation of Quantum Computers,"  Since Shor's discovery of an algorithm to factor numbers on a quantum
computer in polynomial time, quantum computation has become a subject of
immense interest. Unfortunately, one of the key features of quantum computers -
the difficulty of describing them on classical computers - also makes it
difficult to describe and understand precisely what can be done with them. A
formalism describing the evolution of operators rather than states has proven
extremely fruitful in understanding an important class of quantum operations.
States used in error correction and certain communication protocols can be
described by their stabilizer, a group of tensor products of Pauli matrices.
Even this simple group structure is sufficient to allow a rich range of quantum
effects, although it falls short of the full power of quantum computation.

    ",Quantum Physics (quant-ph),
MNIST-C: A Robustness Benchmark for Computer Vision,"  We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions
applied to the MNIST test set, for benchmarking out-of-distribution robustness
in computer vision. Through several experiments and visualizations we
demonstrate that our corruptions significantly degrade performance of
state-of-the-art computer vision models while preserving the semantic content
of the test images. In contrast to the popular notion of adversarial
robustness, our model-agnostic corruptions do not seek worst-case performance
but are instead designed to be broad and diverse, capturing multiple failure
modes of modern models. In fact, we find that several previously published
adversarial defenses significantly degrade robustness as measured by MNIST-C.
We hope that our benchmark serves as a useful tool for future work in designing
systems that are able to learn robust feature representations that capture the
underlying semantics of the input.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG)
How will quantum computers provide an industrially relevant computational advantage in quantum chemistry?,"  Numerous reports claim that quantum advantage, which should emerge as a
direct consequence of the advent of quantum computers, will herald a new era of
chemical research because it will enable scientists to perform the kinds of
quantum chemical simulations that have not been possible before. Such
simulations on quantum computers, promising a significantly greater accuracy
and speed, are projected to exert a great impact on the way we can probe
reality, predict the outcomes of chemical experiments, and even drive design of
drugs, catalysts, and materials. In this work we review the current status of
quantum hardware and algorithm theory and examine whether such popular claims
about quantum advantage are really going to be transformative. We go over
subtle complications of quantum chemical research that tend to be overlooked in
discussions involving quantum computers. We estimate quantum computer resources
that will be required for performing calculations on quantum computers with
chemical accuracy for several types of molecules. In particular, we directly
compare the resources and timings associated with classical and quantum
computers for the molecules H$_2$ for increasing basis set sizes, and Cr$_2$
for a variety of complete active spaces (CAS) within the scope of the CASCI and
CASSCF methods. The results obtained for the chromium dimer enable us to
estimate the size of the active space at which computations of non-dynamic
correlation on a quantum computer should take less time than analogous
computations on a classical computer. Using this result, we speculate on the
types of chemical applications for which the use of quantum computers would be
both beneficial and relevant to industrial applications in the short term.

    ",Quantum Physics (quant-ph),; Chemical Physics (physics.chem-ph)
DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems,"  Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique
used to increase the interpretability and explainability of black box Machine
Learning (ML) algorithms. LIME typically generates an explanation for a single
prediction by any ML model by learning a simpler interpretable model (e.g.
linear classifier) around the prediction through generating simulated data
around the instance by random perturbation, and obtaining feature importance
through applying some form of feature selection. While LIME and similar local
algorithms have gained popularity due to their simplicity, the random
perturbation and feature selection methods result in ""instability"" in the
generated explanations, where for the same prediction, different explanations
can be generated. This is a critical issue that can prevent deployment of LIME
in a Computer-Aided Diagnosis (CAD) system, where stability is of utmost
importance to earn the trust of medical professionals. In this paper, we
propose a deterministic version of LIME. Instead of random perturbation, we
utilize agglomerative Hierarchical Clustering (HC) to group the training data
together and K-Nearest Neighbour (KNN) to select the relevant cluster of the
new instance that is being explained. After finding the relevant cluster, a
linear model is trained over the selected cluster to generate the explanations.
Experimental results on three different medical datasets show the superiority
for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME),
where we quantitatively determine the stability of DLIME compared to LIME
utilizing the Jaccard similarity among multiple generated explanations.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Using Computer Vision to enhance Safety of Workforce in Manufacturing in a Post COVID World,"  The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Florence: A New Foundation Model for Computer Vision,"  Automated visual understanding of our diverse and open world demands computer
vision models to generalize well with minimal customization for specific tasks,
similar to human vision. Computer vision foundation models, which are trained
on diverse, large-scale dataset and can be adapted to a wide range of
downstream tasks, are critical for this mission to solve real-world computer
vision applications. While existing vision foundation models such as CLIP,
ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual
representations to a cross-modal shared representation, we introduce a new
computer vision foundation model, Florence, to expand the representations from
coarse (scene) to fine (object), from static (images) to dynamic (videos), and
from RGB to multiple modalities (caption, depth). By incorporating universal
visual-language representations from Web-scale image-text data, our Florence
model can be easily adapted for various computer vision tasks, such as
classification, retrieval, object detection, VQA, image caption, video
retrieval and action recognition. Moreover, Florence demonstrates outstanding
performance in many types of transfer learning: fully sampled fine-tuning,
linear probing, few-shot transfer and zero-shot transfer for novel images and
objects. All of these properties are critical for our vision foundation model
to serve general purpose vision tasks. Florence achieves new state-of-the-art
results in majority of 44 representative benchmarks, e.g., ImageNet-1K
zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of
97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.

    ",Computer Vision and Pattern Recognition (cs.CV),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,"  Humans read and write hundreds of billions of messages every day. Further,
due to the availability of large datasets, large computing systems, and better
neural network models, natural language processing (NLP) technology has made
significant strides in understanding, proofreading, and organizing these
messages. Thus, there is a significant opportunity to deploy NLP in myriad
applications to help web users, social networks, and businesses. In particular,
we consider smartphones and other mobile devices as crucial platforms for
deploying NLP models at scale. However, today's highly-accurate NLP neural
network models such as BERT and RoBERTa are extremely computationally
expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a
Pixel 3 smartphone. In this work, we observe that methods such as grouped
convolutions have yielded significant speedups for computer vision networks,
but many of these techniques have not been adopted by NLP neural network
designers. We demonstrate how to replace several operations in self-attention
layers with grouped convolutions, and we use this technique in a novel network
architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the
Pixel 3 while achieving competitive accuracy on the GLUE test set. The
SqueezeBERT code will be released.

    ",Computation and Language (cs.CL),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
Better Computer Go Player with Neural Network and Long-term Prediction,"  Competing with top human players in the ancient game of Go has been a
long-term goal of artificial intelligence. Go's high branching factor makes
traditional search techniques ineffective, even on leading-edge hardware, and
Go's evaluation function could change drastically with one stone change. Recent
works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not
strictly necessary for machine Go players. A pure pattern-matching approach,
based on a Deep Convolutional Neural Network (DCNN) that predicts the next
move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source
Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is
limited. We extend this idea in our bot named darkforest, which relies on a
DCNN designed for long-term predictions. Darkforest substantially improves the
win rate for pattern-matching approaches against MCTS-based approaches, even
with looser search budgets. Against human players, the newest versions,
darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a
substantial improvement upon the estimated 4k-5k ranks for DCNN reported in
Clark & Storkey (2015) based on games against other machine players. Adding
MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000
rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts
it achieves a stable 5d level in KGS server, on par with state-of-the-art Go
AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al.
(2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI)
Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot,"  The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal ""Bulletin of Mathematical Biophysics"", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.

    ",Computers and Society (cs.CY),
Simulating quantum field theory with a quantum computer,"  Forthcoming exascale digital computers will further advance our knowledge of
quantum chromodynamics, but formidable challenges will remain. In particular,
Euclidean Monte Carlo methods are not well suited for studying real-time
evolution in hadronic collisions, or the properties of hadronic matter at
nonzero temperature and chemical potential. Digital computers may never be able
to achieve accurate simulations of such phenomena in QCD and other
strongly-coupled field theories; quantum computers will do so eventually,
though I'm not sure when. Progress toward quantum simulation of quantum field
theory will require the collaborative efforts of quantumists and field
theorists, and though the physics payoff may still be far away, it's worthwhile
to get started now. Today's research can hasten the arrival of a new era in
which quantum simulation fuels rapid progress in fundamental physics.

    ",High Energy Physics - Lattice (hep-lat),; High Energy Physics - Theory (hep-th); Quantum Physics (quant-ph)
"The Computer Science and Physics of Community Detection: Landscapes, Phase Transitions, and Hardness","  Community detection in graphs is the problem of finding groups of vertices
which are more densely connected than they are to the rest of the graph. This
problem has a long history, but it is undergoing a resurgence of interest due
to the need to analyze social and biological networks. While there are many
ways to formalize it, one of the most popular is as an inference problem, where
there is a ""ground truth"" community structure built into the graph somehow. The
task is then to recover the ground truth knowing only the graph.
",Computational Complexity (cs.CC),; Statistical Mechanics (cond-mat.stat-mech); Social and Information Networks (cs.SI); Probability (math.PR); Physics and Society (physics.soc-ph)
Computer Algebra Algorithms for Special Functions in Particle Physics,"  This work deals with special nested objects arising in massive higher order
perturbative calculations in renormalizable quantum field theories. On the one
hand we work with nested sums such as harmonic sums and their generalizations
(S-sums, cyclotomic harmonic sums, cyclotomic S-sums) and on the other hand we
treat iterated integrals of the PoincarÃ© and Chen-type, such as harmonic
polylogarithms and their generalizations (multiple polylogarithms, cyclotomic
harmonic polylogarithms). The iterated integrals are connected to the nested
sums via (generalizations of) the Mellin-transformation and we show how this
transformation can be computed. We derive algebraic and structural relations
between the nested sums as well as relations between the values of the sums at
infinity and connected to it the values of the iterated integrals evaluated at
special constants. In addition we state algorithms to compute asymptotic
expansions of these nested objects and we state an algorithm which rewrites
certain types of nested sums into expressions in terms of cyclotomic S-sums.
Moreover we summarize the main functionality of the computer algebra package
HarmonicSums in which all these algorithms and transformations are implemented.
Furthermore, we present application of and enhancements of the multivariate
Almkvist-Zeilberger algorithm to certain types of Feynman integrals and the
corresponding computer algebra package MultiIntegrate.

    ",Mathematical Physics (math-ph),; High Energy Physics - Phenomenology (hep-ph); High Energy Physics - Theory (hep-th)
Play and Learn: Using Video Games to Train Computer Vision Models,"  Video games are a compelling source of annotated data as they can readily
provide fine-grained groundtruth for diverse tasks. However, it is not clear
whether the synthetically generated data has enough resemblance to the
real-world images to improve the performance of computer vision models in
practice. We present experiments assessing the effectiveness on real-world data
of systems trained on synthetic RGB images that are extracted from a video
game. We collected over 60000 synthetic samples from a modern video game with
similar conditions to the real-world CamVid and Cityscapes datasets. We provide
several experiments to demonstrate that the synthetically generated RGB images
can be used to improve the performance of deep neural networks on both image
segmentation and depth estimation. These results show that a convolutional
network trained on synthetic data achieves a similar test error to a network
that is trained on real-world data for dense image classification. Furthermore,
the synthetically generated RGB images can provide similar or better results
compared to the real-world datasets if a simple domain adaptation technique is
applied. Our results suggest that collaboration with game developers for an
accessible interface to gather data is potentially a fruitful direction for
future work in computer vision.

    ",Computer Vision and Pattern Recognition (cs.CV),
A Computer Algebra Toolbox for Harmonic Sums Related to Particle Physics,"  In this work we present the computer algebra package HarmonicSums and its
theoretical background for the manipulation of harmonic sums and some related
quantities as for example Euler-Zagier sums and harmonic polylogarithms.
Harmonic sums and generalized harmonic sums emerge as special cases of
so-called d'Alembertian solutions of recurrence relations. We show that
harmonic sums form a quasi-shuffle algebra and describe a method how we can
find algebraically independent harmonic sums. In addition, we define a
differentiation on harmonic sums via an extended version of the Mellin
transform. Along with that, new relations between harmonic sums will arise.
Furthermore, we present an algorithm which rewrites certain types of nested
sums into expressions in terms of harmonic sums. We illustrate by nontrivial
examples how these algorithms in cooperation with the summation package Sigma
support the evaluation of Feynman integrals.

    ",Mathematical Physics (math-ph),
Quantum Neuron: an elementary building block for machine learning on quantum computers,"  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the ""quantum neuron"", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.

    ",Quantum Physics (quant-ph),; Neural and Evolutionary Computing (cs.NE)
Correspondence principle for idempotent calculus and some computer applications,"  This paper is devoted to heuristic aspects of the so-called idempotent
calculus. There is a correspondence between important, useful and interesting
constructions and results over the field of real (or complex) numbers and
similar constructions and results over idempotent semirings in the spirit of N.
Bohr's correspondence principle in Quantum Mechanics.
",General Mathematics (math.GM),
An Applied Study on Educational Use of Facebook as a Web 2.0 Tool: The Sample Lesson of Computer Networks and Communication,"  The main aim of the research was to examine educational use of Facebook. The
Computer Networks and Communication lesson was taken as the sample and the
attitudes of the students included in the study group towards Facebook were
measured in a semi-experimental setup. The students on Facebook platform were
examined for about three months and they continued their education
interactively in that virtual environment. After the-three-month-education
period, observations for the students were reported and the attitudes of the
students towards Facebook were measured by three different measurement tools.
As a result, the attitudes of the students towards educational use of Facebook
and their views were heterogeneous. When the average values of the group were
examined, it was reported that the attitudes towards educational use of
Facebook was above a moderate level. Therefore, it might be suggested that
social networks in virtual environments provide continuity in life long
learning.

    ",Social and Information Networks (cs.SI),
Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems,"  Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV)
Fansmitter: Acoustic Data Exfiltration from (Speakerless) Air-Gapped Computers,"  Because computers may contain or interact with sensitive information, they
are often air-gapped and in this way kept isolated and disconnected from the
Internet. In recent years the ability of malware to communicate over an air-gap
by transmitting sonic and ultrasonic signals from a computer speaker to a
nearby receiver has been shown. In order to eliminate such acoustic channels,
current best practice recommends the elimination of speakers (internal or
external) in secure computers, thereby creating a so-called 'audio-gap'. In
this paper, we present Fansmitter, a malware that can acoustically exfiltrate
data from air-gapped computers, even when audio hardware and speakers are not
present. Our method utilizes the noise emitted from the CPU and chassis fans
which are present in virtually every computer today. We show that a software
can regulate the internal fans' speed in order to control the acoustic waveform
emitted from a computer. Binary data can be modulated and transmitted over
these audio signals to a remote microphone (e.g., on a nearby mobile phone). We
present Fansmitter's design considerations, including acoustic signature
analysis, data modulation, and data transmission. We also evaluate the acoustic
channel, present our results, and discuss countermeasures. Using our method we
successfully transmitted data from air-gapped computer without audio hardware,
to a smartphone receiver in the same room. We demonstrated the effective
transmission of encryption keys and passwords from a distance of zero to eight
meters, with bit rate of up to 900 bits/hour. We show that our method can also
be used to leak data from different types of IT equipment, embedded systems,
and IoT devices that have no audio hardware, but contain fans of various types
and sizes.

    ",Cryptography and Security (cs.CR),
Survey on the attention based RNN model and its applications in computer vision,"  The recurrent neural networks (RNN) can be used to solve the sequence to
sequence problem, where both the input and the output have sequential
structures. Usually there are some implicit relations between the structures.
However, it is hard for the common RNN model to fully explore the relations
between the sequences. In this survey, we introduce some attention based RNN
models which can focus on different parts of the input for each output item, in
order to explore and take advantage of the implicit relations between the input
and the output items. The different attention mechanisms are described in
detail. We then introduce some applications in computer vision which apply the
attention based RNN models. The superiority of the attention based RNN model is
shown by the experimental results. At last some future research directions are
given.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG)
Five Experimental Tests on the 5-Qubit IBM Quantum Computer,"  The 5-qubit quantum computer prototypes that IBM has given open access to on
the cloud allow the implementation of real experiments on a quantum processor.
We present the results obtained in five experimental tests performed on these
computers: dense coding, quantum Fourier transforms, Bell's inequality,
Mermin's inequalities (up to $n=5$) and the construction of the prime state
$|p_3\rangle$. These results serve to assess the functioning of the IBM 5Q
chips.

    ",Quantum Physics (quant-ph),
QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer,"  Quantum Natural Language Processing (QNLP) deals with the design and
implementation of NLP models intended to be run on quantum hardware. In this
paper, we present results on the first NLP experiments conducted on Noisy
Intermediate-Scale Quantum (NISQ) computers for datasets of size >= 100
sentences. Exploiting the formal similarity of the compositional model of
meaning by Coecke et al. (2010) with quantum theory, we create representations
for sentences that have a natural mapping to quantum circuits. We use these
representations to implement and successfully train two NLP models that solve
simple sentence classification tasks on quantum hardware. We describe in detail
the main principles, the process and challenges of these experiments, in a way
accessible to NLP researchers, thus paving the way for practical Quantum
Natural Language Processing.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantum Physics (quant-ph)
A New Generation of Brain-Computer Interface Based on Riemannian Geometry,"  Based on the cumulated experience over the past 25 years in the field of
Brain-Computer Interface (BCI) we can now envision a new generation of BCI.
Such BCIs will not require training; instead they will be smartly initialized
using remote massive databases and will adapt to the user fast and effectively
in the first minute of use. They will be reliable, robust and will maintain
good performances within and across sessions. A general classification
framework based on recent advances in Riemannian geometry and possessing these
characteristics is presented. It applies equally well to BCI based on
event-related potentials (ERP), sensorimotor (mu) rhythms and steady-state
evoked potential (SSEP). The framework is very simple, both algorithmically and
computationally. Due to its simplicity, its ability to learn rapidly (with
little training data) and its good across-subject and across-session
generalization, this strategy a very good candidate for building a new
generation of BCIs, thus we hereby propose it as a benchmark method for the
field.

    ",Human-Computer Interaction (cs.HC),; Differential Geometry (math.DG)
HoloLens 2 Research Mode as a Tool for Computer Vision Research,"  Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful
sensing devices with integrated compute capabilities, which makes it an ideal
platform for computer vision research. In this technical report, we present
HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw
sensor streams. We provide an overview of the API and explain how it can be
used to build mixed reality applications based on processing sensor data. We
also show how to combine the Research Mode sensor data with the built-in eye
and hand tracking capabilities provided by HoloLens 2. By releasing the
Research Mode API and a set of open-source tools, we aim to foster further
research in the fields of computer vision as well as robotics and encourage
contributions from the research community.

    ",Computer Vision and Pattern Recognition (cs.CV),
ZX-calculus for the working quantum computer scientist,"  The ZX-calculus is a graphical language for reasoning about quantum
computation that has recently seen an increased usage in a variety of areas
such as quantum circuit optimisation, surface codes and lattice surgery,
measurement-based quantum computation, and quantum foundations. The first half
of this review gives a gentle introduction to the ZX-calculus suitable for
those familiar with the basics of quantum computing. The aim here is to make
the reader comfortable enough with the ZX-calculus that they could use it in
their daily work for small computations on quantum circuits and states. The
latter sections give a condensed overview of the literature on the ZX-calculus.
We discuss Clifford computation and graphically prove the Gottesman-Knill
theorem, we discuss a recently introduced extension of the ZX-calculus that
allows for convenient reasoning about Toffoli gates, and we discuss the recent
completeness theorems for the ZX-calculus that show that, in principle, all
reasoning about quantum computation can be done using ZX-diagrams.
Additionally, we discuss the categorical and algebraic origins of the
ZX-calculus and we discuss several extensions of the language which can
represent mixed states, measurement, classical control and higher-dimensional
qudits.

    ",Quantum Physics (quant-ph),
A Case for Variability-Aware Policies for NISQ-Era Quantum Computers,"  Recently, IBM, Google, and Intel showcased quantum computers ranging from 49
to 72 qubits. While these systems represent a significant milestone in the
advancement of quantum computing, existing and near-term quantum computers are
not yet large enough to fully support quantum error-correction. Such systems
with few tens to few hundreds of qubits are termed as Noisy Intermediate Scale
Quantum computers (NISQ), and these systems can provide benefits for a class of
quantum algorithms. In this paper, we study the problems of Qubit-Allocation
(mapping of program qubits to machine qubits) and Qubit-Movement(routing qubits
from one location to another to perform entanglement).
",Quantum Physics (quant-ph),; Emerging Technologies (cs.ET)
Addition on a Quantum Computer,"  A new method for computing sums on a quantum computer is introduced. This
technique uses the quantum Fourier transform and reduces the number of qubits
necessary for addition by removing the need for temporary carry bits. This
approach also allows the addition of a classical number to a quantum
superposition without encoding the classical number in the quantum register.
This method also allows for massive parallelization in its execution.

    ",Quantum Physics (quant-ph),
Review of the Use of Electroencephalography as an Evaluation Method for Human-Computer Interaction,"  Evaluating human-computer interaction is essential as a broadening population
uses machines, sometimes in sensitive contexts. However, traditional evaluation
methods may fail to combine real-time measures, an ""objective"" approach and
data contextualization. In this review we look at how adding neuroimaging
techniques can respond to such needs. We focus on electroencephalography (EEG),
as it could be handled effectively during a dedicated evaluation phase. We
identify workload, attention, vigilance, fatigue, error recognition, emotions,
engagement, flow and immersion as being recognizable by EEG. We find that
workload, attention and emotions assessments would benefit the most from EEG.
Moreover, we advocate to study further error recognition through neuroimaging
to enhance usability and increase user experience.

    ",Human-Computer Interaction (cs.HC),
Next Steps in Quantum Computing: Computer Science's Role,"  The computing ecosystem has always had deep impacts on society and technology
and profoundly changed our lives in myriads of ways. Despite decades of
impressive Moore's Law performance scaling and other growth in the computing
ecosystem there are nonetheless still important potential applications of
computing that remain out of reach of current or foreseeable conventional
computer systems. Specifically, there are computational applications whose
complexity scales super-linearly, even exponentially, with the size of their
input data such that the computation time or memory requirements for these
problems become intractably large to solve for useful data input sizes. Such
problems can have memory requirements that exceed what can be built on the most
powerful supercomputers, and/or runtimes on the order of tens of years or more.
Quantum computing (QC) is viewed by many as a possible future option for
tackling these high-complexity or seemingly-intractable problems by
complementing classical computing with a fundamentally different compute
paradigm.
",Emerging Technologies (cs.ET),; Quantum Physics (quant-ph)
Simulating quantum computers with probabilistic methods,"  We investigate the boundary between classical and quantum computational
power. This work consists of two parts. First we develop new classical
simulation algorithms that are centered on sampling methods. Using these
techniques we generate new classes of classically simulatable quantum circuits
where standard techniques relying on the exact computation of measurement
probabilities fail to provide efficient simulations. For example, we show how
various concatenations of matchgate, Toffoli, Clifford, bounded-depth, Fourier
transform and other circuits are classically simulatable. We also prove that
sparse quantum circuits as well as circuits composed of CNOT and exp[iaX] gates
can be simulated classically. In a second part, we apply our results to the
simulation of quantum algorithms. It is shown that a recent quantum algorithm,
concerned with the estimation of Potts model partition functions, can be
simulated efficiently classically. Finally, we show that the exponential
speed-ups of Simon's and Shor's algorithms crucially depend on the very last
stage in these algorithms, dealing with the classical postprocessing of the
measurement outcomes. Specifically, we prove that both algorithms would be
classically simulatable if the function classically computed in this step had a
sufficiently peaked Fourier spectrum.

    ",Quantum Physics (quant-ph),
Image recognition with an adiabatic quantum computer I. Mapping to quadratic unconstrained binary optimization,"  Many artificial intelligence (AI) problems naturally map to NP-hard
optimization problems. This has the interesting consequence that enabling
human-level capability in machines often requires systems that can handle
formally intractable problems. This issue can sometimes (but possibly not
always) be resolved by building special-purpose heuristic algorithms, tailored
to the problem in question. Because of the continued difficulties in automating
certain tasks that are natural for humans, there remains a strong motivation
for AI researchers to investigate and apply new algorithms and techniques to
hard AI problems. Recently a novel class of relevant algorithms that require
quantum mechanical hardware have been proposed. These algorithms, referred to
as quantum adiabatic algorithms, represent a new approach to designing both
complete and heuristic solvers for NP-hard optimization problems. In this work
we describe how to formulate image recognition, which is a canonical NP-hard AI
problem, as a Quadratic Unconstrained Binary Optimization (QUBO) problem. The
QUBO format corresponds to the input format required for D-Wave superconducting
adiabatic quantum computing (AQC) processors.

    ",Quantum Physics (quant-ph),
Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks,"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.

    ",Human-Computer Interaction (cs.HC),; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)
FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning,"  The computational demands of computer vision tasks based on state-of-the-art
Convolutional Neural Network (CNN) image classification far exceed the energy
budgets of mobile devices. This paper proposes FixyNN, which consists of a
fixed-weight feature extractor that generates ubiquitous CNN features, and a
conventional programmable CNN accelerator which processes a dataset-specific
CNN. Image classification models for FixyNN are trained end-to-end via transfer
learning, with the common feature extractor representing the transfered part,
and the programmable part being learnt on the target dataset. Experimental
results demonstrate FixyNN hardware can achieve very high energy efficiencies
up to 26.6 TOPS/W ($4.81 \times$ better than iso-area programmable
accelerator). Over a suite of six datasets we trained models via transfer
learning with an accuracy loss of $<1\%$ resulting in up to 11.2 TOPS/W -
nearly $2 \times$ more efficient than a conventional programmable CNN
accelerator of the same area.

    ",Computer Vision and Pattern Recognition (cs.CV),; Hardware Architecture (cs.AR); Machine Learning (cs.LG); Machine Learning (stat.ML)
Unity Perception: Generate Synthetic Data for Computer Vision,"  We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.

    ",Computer Vision and Pattern Recognition (cs.CV),
Dynamical mean field theory algorithm and experiment on quantum computers,"  The developments of quantum computing algorithms and experiments for atomic
scale simulations have largely focused on quantum chemistry for molecules,
while their application in condensed matter systems is scarcely explored. Here
we present a quantum algorithm to perform dynamical mean field theory (DMFT)
calculations for condensed matter systems on currently available quantum
computers, and demonstrate it on two quantum hardware platforms. DMFT is
required to properly describe the large class of materials with strongly
correlated electrons. The computationally challenging part arises from solving
the effective problem of an interacting impurity coupled to a bath, which
scales exponentially with system size on conventional computers. An exponential
speedup is expected on quantum computers, but the algorithms proposed so far
are based on real time evolution of the wavefunction, which requires high-depth
circuits and hence very low noise levels in the quantum hardware. Here we
propose an alternative approach, which uses the variational quantum eigensolver
(VQE) method for ground and excited states to obtain the needed quantities as
part of an exact diagonalization impurity solver. We present the algorithm for
a two site DMFT system, which we benchmark using simulations on conventional
computers as well as experiments on superconducting and trapped ion qubits,
demonstrating that this method is suitable for running DMFT calculations on
currently available quantum hardware.

    ",Quantum Physics (quant-ph),; Strongly Correlated Electrons (cond-mat.str-el)
Computer Stereo Vision for Autonomous Driving,"  As an important component of autonomous systems, autonomous car perception
has had a big leap with recent advances in parallel computing architectures.
With the use of tiny but full-feature embedded supercomputers, computer stereo
vision has been prevalently applied in autonomous cars for depth perception.
The two key aspects of computer stereo vision are speed and accuracy. They are
both desirable but conflicting properties, as the algorithms with better
disparity accuracy usually have higher computational complexity. Therefore, the
main aim of developing a computer stereo vision algorithm for resource-limited
hardware is to improve the trade-off between speed and accuracy. In this
chapter, we introduce both the hardware and software aspects of computer stereo
vision for autonomous car systems. Then, we discuss four autonomous car
perception tasks, including 1) visual feature detection, description and
matching, 2) 3D information acquisition, 3) object detection/recognition and 4)
semantic image segmentation. The principles of computer stereo vision and
parallel computing on multi-threading CPU and GPU architectures are then
detailed.

    ",Computer Vision and Pattern Recognition (cs.CV),
Introducing Cadabra: a symbolic computer algebra system for field theory problems,"  Cadabra is a new computer algebra system designed specifically for the
solution of problems encountered in field theory. It has extensive
functionality for tensor polynomial simplification taking care of Bianchi and
Schouten identities, for fermions and anti-commuting variables, Clifford
algebras and Fierz transformations, implicit coordinate dependence, multiple
index types and many other field theory related concepts. The input format is a
subset of TeX and thus easy to learn. Both a command-line and a graphical
interface are available. The present paper is an introduction to the program
using several concrete problems from gravity, supergravity and quantum field
theory.

    ",High Energy Physics - Theory (hep-th),; General Relativity and Quantum Cosmology (gr-qc); High Energy Physics - Phenomenology (hep-ph)
A Real-time Hand Gesture Recognition and Human-Computer Interaction System,"  In this project, we design a real-time human-computer interaction system
based on hand gesture. The whole system consists of three components: hand
detection, gesture recognition and human-computer interaction (HCI) based on
recognition; and realizes the robust control of mouse and keyboard events with
a higher accuracy of gesture recognition. Specifically, we use the
convolutional neural network (CNN) to recognize gestures and makes it
attainable to identify relatively complex gestures using only one cheap
monocular camera. We introduce the Kalman filter to estimate the hand position
based on which the mouse cursor control is realized in a stable and smooth way.
During the HCI stage, we develop a simple strategy to avoid the false
recognition caused by noises - mostly transient, false gestures, and thus to
improve the reliability of interaction. The developed system is highly
extendable and can be used in human-robotic or other human-machine interaction
scenarios with more complex command formats rather than just mouse and keyboard
events.

    ",Computer Vision and Pattern Recognition (cs.CV),
