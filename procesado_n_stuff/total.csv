titulo,abstract,clase_pri,clase_otr,clases,categorias
An Actor-Critic Algorithm for Sequence Prediction,"  We present an approach to training neural networks to generate sequences
using actor-critic methods from reinforcement learning (RL). Current
log-likelihood training methods are limited by the discrepancy between their
training and testing modes, as models must generate tokens conditioned on their
previous guesses rather than the ground-truth tokens. We address this problem
by introducing a \textit{critic} network that is trained to predict the value
of an output token, given the policy of an \textit{actor} network. This results
in a training procedure that is much closer to the test phase, and allows us to
directly optimize for a task-specific score such as BLEU. Crucially, since we
leverage these techniques in the supervised learning setting rather than the
traditional RL setting, we condition the critic network on the ground-truth
output. We show that our method leads to improved performance on both a
synthetic task, and for German-English machine translation. Our analysis paves
the way for such methods to be applied in natural language generation tasks,
such as machine translation, caption generation, and dialogue modelling.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Playing Flappy Bird via Asynchronous Advantage Actor Critic Algorithm,"  Flappy Bird, which has a very high popularity, has been trained in many
algorithms. Some of these studies were trained from raw pixel values of game
and some from specific attributes. In this study, the model was trained with
raw game images, which had not been seen before. The trained model has learned
as reinforcement when to make which decision. As an input to the model, the
reward or penalty at the end of each step was returned and the training was
completed. Flappy Bird game was trained with the Reinforcement Learning
algorithm Deep Q-Network and Asynchronous Advantage Actor Critic (A3C)
algorithms.

    ",Machine Learning (cs.LG),; Neural and Evolutionary Computing (cs.NE),"['MachineLearning(cs.LG)', 'NeuralandEvolutionaryComputing(cs.NE)']","['cs.LG', 'cs.NE']"
Distributional Advantage Actor-Critic,"  In traditional reinforcement learning, an agent maximizes the reward
collected during its interaction with the environment by approximating the
optimal policy through the estimation of value functions. Typically, given a
state s and action a, the corresponding value is the expected discounted sum of
rewards. The optimal action is then chosen to be the action a with the largest
value estimated by value function. However, recent developments have shown both
theoretical and experimental evidence of superior performance when value
function is replaced with value distribution in context of deep Q learning [1].
In this paper, we develop a new algorithm that combines advantage actor-critic
with value distribution estimated by quantile regression. We evaluated this new
algorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a
variety of tasks, and observed it to achieve at least as good as baseline
algorithms, and outperforming baseline in some tasks with smaller variance and
increased stability.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management,"  Deep reinforcement learning (RL) methods have significant potential for
dialogue policy optimisation. However, they suffer from a poor performance in
the early stages of learning. This is especially problematic for on-line
learning with real users. Two approaches are introduced to tackle this problem.
Firstly, to speed up the learning process, two sample-efficient neural networks
algorithms: trust region actor-critic with experience replay (TRACER) and
episodic natural actor-critic with experience replay (eNACER) are presented.
For TRACER, the trust region helps to control the learning step size and avoid
catastrophic model changes. For eNACER, the natural gradient identifies the
steepest ascent direction in policy space to speed up the convergence. Both
models employ off-policy learning with experience replay to improve
sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of
demonstration data is utilised to pre-train the models prior to on-line
reinforcement learning. Combining these two approaches, we demonstrate a
practical approach to learn deep RL-based dialogue policies and demonstrate
their effectiveness in a task-oriented information seeking domain.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.AI', 'cs.LG']"
Towards Automatic Actor-Critic Solutions to Continuous Control,"  Model-free off-policy actor-critic methods are an efficient solution to
complex continuous control tasks. However, these algorithms rely on a number of
design tricks and hyperparameters, making their application to new domains
difficult and computationally expensive. This paper creates an evolutionary
approach that automatically tunes these design decisions and eliminates the
RL-specific hyperparameters from the Soft Actor-Critic algorithm. Our design is
sample efficient and provides practical advantages over baseline approaches,
including improved exploration, generalization over multiple control
frequencies, and a robust ensemble of high-performance policies. Empirically,
we show that our agent outperforms well-tuned hyperparameter settings in
popular benchmarks from the DeepMind Control Suite. We then apply it to less
common control tasks outside of simulated robotics to find high-performance
solutions with minimal compute and research effort.

    ",Machine Learning (cs.LG),; Neural and Evolutionary Computing (cs.NE); Systems and Control (eess.SY),"['MachineLearning(cs.LG)', 'NeuralandEvolutionaryComputing(cs.NE)', 'SystemsandControl(eess.SY)']","['cs.LG', 'cs.NE', 'eess.SY']"
Off-Policy Actor-Critic,"  This paper presents the first actor-critic algorithm for off-policy
reinforcement learning. Our algorithm is online and incremental, and its
per-time-step complexity scales linearly with the number of learned weights.
Previous work on actor-critic algorithms is limited to the on-policy setting
and does not take advantage of the recent advances in off-policy gradient
temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable
a target policy to be learned while following and obtaining data from another
(behavior) policy. For many problems, however, actor-critic methods are more
practical than action value methods (like Greedy-GQ) because they explicitly
represent the policy; consequently, the policy can be stochastic and utilize a
large action space. In this paper, we illustrate how to practically combine the
generality and learning potential of off-policy learning with the flexibility
in action selection given by actor-critic methods. We derive an incremental,
linear time and space complexity algorithm that includes eligibility traces,
prove convergence under assumptions similar to previous off-policy algorithms,
and empirically show better or comparable performance to existing algorithms on
standard reinforcement-learning benchmark problems.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Policy Networks with Two-Stage Training for Dialogue Systems,"  In this paper, we propose to use deep policy networks which are trained with
an advantage actor-critic method for statistically optimised dialogue systems.
First, we show that, on summary state and action spaces, deep Reinforcement
Learning (RL) outperforms Gaussian Processes methods. Summary state and action
spaces lead to good performance but require pre-engineering effort, RL
knowledge, and domain expertise. In order to remove the need to define such
summary spaces, we show that deep RL can also be trained efficiently on the
original state and action spaces. Dialogue systems based on partially
observable Markov decision processes are known to require many dialogues to
train, which makes them unappealing for practical deployment. We show that a
deep RL method based on an actor-critic architecture can exploit a small amount
of data very efficiently. Indeed, with only a few hundred dialogues collected
with a handcrafted policy, the actor-critic deep learner is considerably
bootstrapped from a combination of supervised and batch RL. In addition,
convergence to an optimal policy is significantly sped up compared to other
deep RL methods initialized on the data with batch RL. All experiments are
performed on a restaurant domain derived from the Dialogue State Tracking
Challenge 2 (DSTC2) dataset.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI),"['ComputationandLanguage(cs.CL)', 'ArtificialIntelligence(cs.AI)']","['cs.CL', 'cs.AI']"
Asymmetric Actor Critic for Image-Based Robot Learning,"  Deep reinforcement learning (RL) has proven a powerful technique in many
sequential decision making domains. However, Robotics poses many challenges for
RL, most notably training on a physical system can be expensive and dangerous,
which has sparked significant interest in learning control policies using a
physics simulator. While several recent works have shown promising results in
transferring policies trained in simulation to the real world, they often do
not fully utilize the advantage of working with a simulator. In this work, we
exploit the full state observability in the simulator to train better policies
which take as input only partial observations (RGBD images). We do this by
employing an actor-critic training algorithm in which the critic is trained on
full states while the actor (or policy) gets rendered images as input. We show
experimentally on a range of simulated tasks that using these asymmetric inputs
significantly improves performance. Finally, we combine this method with domain
randomization and show real robot experiments for several tasks like picking,
pushing, and moving a block. We achieve this simulation to real world transfer
without training on any real world data.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.AI', 'cs.LG']"
Recursive Least Squares Advantage Actor-Critic Algorithms,"  As an important algorithm in deep reinforcement learning, advantage actor
critic (A2C) has been widely succeeded in both discrete and continuous control
tasks with raw pixel inputs, but its sample efficiency still needs to improve
more. In traditional reinforcement learning, actor-critic algorithms generally
use the recursive least squares (RLS) technology to update the parameter of
linear function approximators for accelerating their convergence speed.
However, A2C algorithms seldom use this technology to train deep neural
networks (DNNs) for improving their sample efficiency. In this paper, we
propose two novel RLS-based A2C algorithms and investigate their performance.
Both proposed algorithms, called RLSSA2C and RLSNA2C, use the RLS method to
train the critic network and the hidden layers of the actor network. The main
difference between them is at the policy learning step. RLSSA2C uses an
ordinary first-order gradient descent algorithm and the standard policy
gradient to learn the policy parameter. RLSNA2C uses the Kronecker-factored
approximation, the RLS method and the natural policy gradient to learn the
compatible parameter and the policy parameter. In addition, we analyze the
complexity and convergence of both algorithms, and present three tricks for
further improving their convergence speed. Finally, we demonstrate the
effectiveness of both algorithms on 40 games in the Atari 2600 environment and
11 tasks in the MuJoCo environment. From the experimental results, it is shown
that our both algorithms have better sample efficiency than the vanilla A2C on
most games or tasks, and have higher computational efficiency than other two
state-of-the-art algorithms.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Combining policy gradient and Q-learning,"  Policy gradient is an efficient technique for improving a policy in a
reinforcement learning setting. However, vanilla online variants are on-policy
only and not able to take advantage of off-policy data. In this paper we
describe a new technique that combines policy gradient with off-policy
Q-learning, drawing experience from a replay buffer. This is motivated by
making a connection between the fixed points of the regularized policy gradient
algorithm and the Q-values. This connection allows us to estimate the Q-values
from the action preferences of the policy, to which we apply Q-learning
updates. We refer to the new technique as 'PGQL', for policy gradient and
Q-learning. We also establish an equivalency between action-value fitting
techniques and actor-critic algorithms, showing that regularized policy
gradient techniques can be interpreted as advantage function learning
algorithms. We conclude with some numerical examples that demonstrate improved
data efficiency and stability of PGQL. In particular, we tested PGQL on the
full suite of Atari games and achieved performance exceeding that of both
asynchronous advantage actor-critic (A3C) and Q-learning.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'OptimizationandControl(math.OC)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']"
A Deeper Look at Discounting Mismatch in Actor-Critic Algorithms,"  We investigate the discounting mismatch in actor-critic algorithm
implementations from a representation learning perspective. Theoretically,
actor-critic algorithms usually have discounting for both actor and critic,
i.e., there is a $\gamma^t$ term in the actor update for the transition
observed at time $t$ in a trajectory and the critic is a discounted value
function. Practitioners, however, usually ignore the discounting ($\gamma^t$)
for the actor while using a discounted critic. We investigate this mismatch in
two scenarios. In the first scenario, we consider optimizing an undiscounted
objective $(\gamma = 1)$ where $\gamma^t$ disappears naturally $(1^t = 1)$. We
then propose to interpret the discounting in critic in terms of a
bias-variance-representation trade-off and provide supporting empirical
results. In the second scenario, we consider optimizing a discounted objective
($\gamma < 1$) and propose to interpret the omission of the discounting in the
actor update from an auxiliary task perspective and provide supporting
empirical results.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Guide Actor-Critic for Continuous Control,"  Actor-critic methods solve reinforcement learning problems by updating a
parameterized policy known as an actor in a direction that increases an
estimate of the expected return known as a critic. However, existing
actor-critic methods only use values or gradients of the critic to update the
policy parameter. In this paper, we propose a novel actor-critic method called
the guide actor-critic (GAC). GAC firstly learns a guide actor that locally
maximizes the critic and then it updates the policy parameter based on the
guide actor by supervised learning. Our main theoretical contributions are two
folds. First, we show that GAC updates the guide actor by performing
second-order optimization in the action space where the curvature matrix is
based on the Hessians of the critic. Second, we show that the deterministic
policy gradient method is a special case of GAC when the Hessians are ignored.
Through experiments, we show that our method is a promising reinforcement
learning method for continuous controls.

    ",Machine Learning (stat.ML),,['MachineLearning(stat.ML)'],['stat.ML']
Efficient Parallel Methods for Deep Reinforcement Learning,"  We propose a novel framework for efficient parallelization of deep
reinforcement learning algorithms, enabling these algorithms to learn from
multiple actors on a single machine. The framework is algorithm agnostic and
can be applied to on-policy, off-policy, value based and policy gradient based
algorithms. Given its inherent parallelism, the framework can be efficiently
implemented on a GPU, allowing the usage of powerful models while significantly
reducing training time. We demonstrate the effectiveness of our framework by
implementing an advantage actor-critic algorithm on a GPU, using on-policy
experiences and employing synchronous updates. Our algorithm achieves
state-of-the-art performance on the Atari domain after only a few hours of
training. Our framework thus opens the door for much faster experimentation on
demanding problem domains. Our implementation is open-source and is made public
at ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations,"  Pretraining with expert demonstrations have been found useful in speeding up
the training process of deep reinforcement learning algorithms since less
online simulation data is required. Some people use supervised learning to
speed up the process of feature learning, others pretrain the policies by
imitating expert demonstrations. However, these methods are unstable and not
suitable for actor-critic reinforcement learning algorithms. Also, some
existing methods rely on the global optimum assumption, which is not true in
most scenarios. In this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and meanwhile ensure that the
performance is not affected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method for computing policy gradients
and value estimators with only expert demonstrations. Our method is
theoretically plausible for actor-critic reinforcement learning algorithms that
pretrains both policy and value functions. We apply our method to two of the
typical actor-critic reinforcement learning algorithms, DDPG and ACER, and
demonstrate with experiments that our method not only outperforms the RL
algorithms without pretraining process, but also is more simulation efficient.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.AI', 'cs.LG', 'stat.ML']"
The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,"  In this work we present a new agent architecture, called Reactor, which
combines multiple algorithmic and architectural contributions to produce an
agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al.,
2016) and Categorical DQN (Bellemare et al., 2017), while giving better
run-time performance than A3C (Mnih et al., 2016). Our first contribution is a
new policy evaluation algorithm called Distributional Retrace, which brings
multi-step off-policy updates to the distributional reinforcement learning
setting. The same approach can be used to convert several classes of multi-step
policy evaluation algorithms designed for expected value evaluation into
distributional ones. Next, we introduce the \b{eta}-leave-one-out policy
gradient algorithm which improves the trade-off between variance and bias by
using action values as a baseline. Our final algorithmic contribution is a new
prioritized replay algorithm for sequences, which exploits the temporal
locality of neighboring observations for more efficient replay prioritization.
Using the Atari 2600 benchmarks, we show that each of these innovations
contribute to both the sample efficiency and final agent performance. Finally,
we demonstrate that Reactor reaches state-of-the-art performance after 200
million frames and less than a day of training.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Hybrid actor-critic algorithm for quantum reinforcement learning at CERN beam lines,"  Free energy-based reinforcement learning (FERL) with clamped quantum
Boltzmann machines (QBM) was shown to significantly improve the learning
efficiency compared to classical Q-learning with the restriction, however, to
discrete state-action space environments. In this paper, the FERL approach is
extended to multi-dimensional continuous state-action space environments to
open the doors for a broader range of real-world applications. First, free
energy-based Q-learning is studied for discrete action spaces, but continuous
state spaces and the impact of experience replay on sample efficiency is
assessed. In a second step, a hybrid actor-critic scheme for continuous
state-action spaces is developed based on the Deep Deterministic Policy
Gradient algorithm combining a classical actor network with a QBM-based critic.
The results obtained with quantum annealing, both simulated and with D-Wave
quantum annealing hardware, are discussed, and the performance is compared to
classical reinforcement learning methods. The environments used throughout
represent existing particle accelerator beam lines at the European Organisation
for Nuclear Research (CERN). Among others, the hybrid actor-critic agent is
evaluated on the actual electron beam line of the Advanced Plasma Wakefield
Experiment (AWAKE).

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
MoTiAC: Multi-Objective Actor-Critics for Real-Time Bidding,"  Online Real-Time Bidding (RTB) is a complex auction game among which
advertisers struggle to bid for ad impressions when a user request occurs.
Considering display cost, Return on Investment (ROI), and other influential Key
Performance Indicators (KPIs), large ad platforms try to balance the trade-off
among various goals in dynamics. To address the challenge, we propose a
Multi-ObjecTive Actor-Critics algorithm based on reinforcement learning (RL),
named MoTiAC, for the problem of bidding optimization with various goals. In
MoTiAC, objective-specific agents update the global network asynchronously with
different goals and perspectives, leading to a robust bidding policy. Unlike
previous RL models, the proposed MoTiAC can simultaneously fulfill
multi-objective tasks in complicated bidding environments. In addition, we
mathematically prove that our model will converge to Pareto optimality.
Finally, experiments on a large-scale real-world commercial dataset from
Tencent verify the effectiveness of MoTiAC versus a set of recent approaches

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Demand-Side Scheduling Based on Multi-Agent Deep Actor-Critic Learning for Smart Grids,"  We consider the problem of demand-side energy management, where each
household is equipped with a smart meter that is able to schedule home
appliances online. The goal is to minimize the overall cost under a real-time
pricing scheme. While previous works have introduced centralized approaches in
which the scheduling algorithm has full observability, we propose the
formulation of a smart grid environment as a Markov game. Each household is a
decentralized agent with partial observability, which allows scalability and
privacy-preservation in a realistic setting. The grid operator produces a price
signal that varies with the energy demand. We propose an extension to a
multi-agent, deep actor-critic algorithm to address partial observability and
the perceived non-stationarity of the environment from the agent's viewpoint.
This algorithm learns a centralized critic that coordinates training of
decentralized agents. Our approach thus uses centralized learning but
decentralized execution. Simulation results show that our online deep
reinforcement learning method can reduce both the peak-to-average ratio of
total energy consumed and the cost of electricity for all households based
purely on instantaneous observations and a price signal.

    ",Machine Learning (cs.LG),; Systems and Control (eess.SY); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'SystemsandControl(eess.SY)', 'MachineLearning(stat.ML)']","['cs.LG', 'eess.SY', 'stat.ML']"
Stein Variational Policy Gradient,"  Policy gradient methods have been successfully applied to many complex
reinforcement learning problems. However, policy gradient methods suffer from
high variance, slow convergence, and inefficient exploration. In this work, we
introduce a maximum entropy policy optimization framework which explicitly
encourages parameter exploration, and show that this framework can be reduced
to a Bayesian inference problem. We then propose a novel Stein variational
policy gradient method (SVPG) which combines existing policy gradient methods
and a repulsive functional to generate a set of diverse but well-behaved
policies. SVPG is robust to initialization and can easily be implemented in a
parallel manner. On continuous control problems, we find that implementing SVPG
on top of REINFORCE and advantage actor-critic algorithms improves both average
return and data efficiency.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent Reinforcement Learning,"  Deep reinforcement learning methods have shown great performance on many
challenging cooperative multi-agent tasks. Two main promising research
directions are multi-agent value function decomposition and multi-agent policy
gradients. In this paper, we propose a new decomposed multi-agent soft
actor-critic (mSAC) method, which effectively combines the advantages of the
aforementioned two methods. The main modules include decomposed Q network
architecture, discrete probabilistic policy and counterfactual advantage
function (optinal). Theoretically, mSAC supports efficient off-policy learning
and addresses credit assignment problem partially in both discrete and
continuous action spaces. Tested on StarCraft II micromanagement cooperative
multiagent benchmark, we empirically investigate the performance of mSAC
against its variants and analyze the effects of the different components.
Experimental results demonstrate that mSAC significantly outperforms
policy-based approach COMA, and achieves competitive results with SOTA
value-based approach Qmix on most tasks in terms of asymptotic perfomance
metric. In addition, mSAC achieves pretty good results on large action space
tasks, such as 2c_vs_64zg and MMM2.

    ",Artificial Intelligence (cs.AI),; Multiagent Systems (cs.MA); Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MultiagentSystems(cs.MA)', 'MachineLearning(stat.ML)']","['cs.AI', 'cs.MA', 'stat.ML']"
Simultaneous Double Q-learning with Conservative Advantage Learning for Actor-Critic Methods,"  Actor-critic Reinforcement Learning (RL) algorithms have achieved impressive
performance in continuous control tasks. However, they still suffer two
nontrivial obstacles, i.e., low sample efficiency and overestimation bias. To
this end, we propose Simultaneous Double Q-learning with Conservative Advantage
Learning (SDQ-CAL). Our SDQ-CAL boosts the Double Q-learning for off-policy
actor-critic RL based on a modification of the Bellman optimality operator with
Advantage Learning. Specifically, SDQ-CAL improves sample efficiency by
modifying the reward to facilitate the distinction from experience between the
optimal actions and the others. Besides, it mitigates the overestimation issue
by updating a pair of critics simultaneously upon double estimators. Extensive
experiments reveal that our algorithm realizes less biased value estimation and
achieves state-of-the-art performance in a range of continuous control
benchmark tasks. We release the source code of our method at:
\url{",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
A Brief Survey of Deep Reinforcement Learning,"  Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']"
Variance Reduction in Actor Critic Methods (ACM),"  After presenting Actor Critic Methods (ACM), we show ACM are control variate
estimators. Using the projection theorem, we prove that the Q and Advantage
Actor Critic (A2C) methods are optimal in the sense of the $L^2$ norm for the
control variate estimators spanned by functions conditioned by the current
state and action. This straightforward application of Pythagoras theorem
provides a theoretical justification of the strong performance of QAC and AAC
most often referred to as A2C methods in deep policy gradient methods. This
enables us to derive a new formulation for Advantage Actor Critic methods that
has lower variance and improves the traditional A2C method.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Policy Regularization via Noisy Advantage Values for Cooperative Multi-agent Actor-Critic methods,"  Recent works have applied the Proximal Policy Optimization (PPO) to the
multi-agent cooperative tasks, such as Independent PPO (IPPO); and vanilla
Multi-agent PPO (MAPPO) which has a centralized value function. However,
previous literature shows that MAPPO may not perform as well as Independent PPO
(IPPO) and the Fine-tuned QMIX on Starcraft Multi-Agent Challenge (SMAC).
MAPPO-Feature-Pruned (MAPPO-FP) improves the performance of MAPPO by the
carefully designed agent-specific features, which may be not friendly to
algorithmic utility. By contrast, we find that MAPPO may face the problem of
\textit{The Policies Overfitting in Multi-agent Cooperation(POMAC)}, as they
learn policies by the sampled advantage values. Then POMAC may lead to updating
the multi-agent policies in a suboptimal direction and prevent the agents from
exploring better trajectories. In this paper, to mitigate the multi-agent
policies overfitting, we propose a novel policy regularization method, which
disturbs the advantage values via random Gaussian noise. The experimental
results show that our method outperforms the Fine-tuned QMIX, MAPPO-FP, and
achieves SOTA on SMAC without agent-specific features. We open-source the code
at \url{",Multiagent Systems (cs.MA),,['MultiagentSystems(cs.MA)'],['cs.MA']
Soft Actor-Critic Algorithms and Applications,"  Model-free deep reinforcement learning (RL) algorithms have been successfully
applied to a range of challenging sequential decision making and control tasks.
However, these methods typically suffer from two major challenges: high sample
complexity and brittleness to hyperparameters. Both of these challenges limit
the applicability of such methods to real-world domains. In this paper, we
describe Soft Actor-Critic (SAC), our recently introduced off-policy
actor-critic algorithm based on the maximum entropy RL framework. In this
framework, the actor aims to simultaneously maximize expected return and
entropy. That is, to succeed at the task while acting as randomly as possible.
We extend SAC to incorporate a number of modifications that accelerate training
and improve stability with respect to the hyperparameters, including a
constrained formulation that automatically tunes the temperature
hyperparameter. We systematically evaluate SAC on a range of benchmark tasks,
as well as real-world challenging tasks such as locomotion for a quadrupedal
robot and robotic manipulation with a dexterous hand. With these improvements,
SAC achieves state-of-the-art performance, outperforming prior on-policy and
off-policy methods in sample-efficiency and asymptotic performance.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving similar performance across different
random seeds. These results suggest that SAC is a promising candidate for
learning in real-world robotics tasks.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'Robotics(cs.RO)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']"
Deep Reinforcement learning for real autonomous mobile robot navigation in indoor environments,"  Deep Reinforcement Learning has been successfully applied in various computer
games [8]. However, it is still rarely used in real-world applications,
especially for the navigation and continuous control of real mobile robots
[13]. Previous approaches lack safety and robustness and/or need a structured
environment. In this paper we present our proof of concept for autonomous
self-learning robot navigation in an unknown environment for a real robot
without a map or planner. The input for the robot is only the fused data from a
2D laser scanner and a RGB-D camera as well as the orientation to the goal. The
map of the environment is unknown. The output actions of an Asynchronous
Advantage Actor-Critic network (GA3C) are the linear and angular velocities for
the robot. The navigator/controller network is pretrained in a high-speed,
parallel, and self-implemented simulation environment to speed up the learning
process and then deployed to the real robot. To avoid overfitting, we train
relatively small networks, and we add random Gaussian noise to the input laser
data. The sensor data fusion with the RGB-D camera allows the robot to navigate
in real environments with real 3D obstacle avoidance and without the need to
fit the environment to the sensory capabilities of the robot. To further
increase the robustness, we train on environments of varying difficulties and
run 32 training instances simultaneously. Video: supplementary File / YouTube,
Code: GitHub

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.AI', 'cs.LG']"
Unbiased Asymmetric Reinforcement Learning under Partial Observability,"  In partially observable reinforcement learning, offline training gives access
to latent information which is not available during online training and/or
execution, such as the system state. Asymmetric actor-critic methods exploit
such information by training a history-based policy via a state-based critic.
However, many asymmetric methods lack theoretical foundation, and are only
evaluated on limited domains. We examine the theory of asymmetric actor-critic
methods which use state-based critics, and expose fundamental issues which
undermine the validity of a common variant, and limit its ability to address
partial observability. We propose an unbiased asymmetric actor-critic variant
which is able to exploit state information while remaining theoretically sound,
maintaining the validity of the policy gradient theorem, and introducing no
bias and relatively low variance into the training process. An empirical
evaluation performed on domains which exhibit significant partial observability
confirms our analysis, demonstrating that unbiased asymmetric actor-critic
converges to better policies and/or faster than symmetric and biased asymmetric
baselines.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Deep Primal-Dual Reinforcement Learning: Accelerating Actor-Critic using Bellman Duality,"  We develop a parameterized Primal-Dual $\pi$ Learning method based on deep
neural networks for Markov decision process with large state space and
off-policy reinforcement learning. In contrast to the popular Q-learning and
actor-critic methods that are based on successive approximations to the
nonlinear Bellman equation, our method makes primal-dual updates to the policy
and value functions utilizing the fundamental linear Bellman duality. Naive
parametrization of the primal-dual $\pi$ learning method using deep neural
networks would encounter two major challenges: (1) each update requires
computing a probability distribution over the state space and is intractable;
(2) the iterates are unstable since the parameterized Lagrangian function is no
longer linear. We address these challenges by proposing a relaxed Lagrangian
formulation with a regularization penalty using the advantage function. We show
that the dual policy update step in our method is equivalent to the policy
gradient update in the actor-critic method in some special case, while the
value updates differ substantially. The main advantage of the primal-dual $\pi$
learning method lies in that the value and policy updates are closely coupled
together using the Bellman duality and therefore more informative. Experiments
on a simple cart-pole problem show that the algorithm significantly outperforms
the one-step temporal-difference actor-critic method, which is the most
relevant benchmark method to compare with. We believe that the primal-dual
updates to the value and policy functions would expedite the learning process.
The proposed methods might open a door to more efficient algorithms and sharper
theoretical analysis.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
An Adaptive Threshold for the Canny Edge Detection with Actor-Critic Algorithm,"  Visual surveillance aims to perform robust foreground object detection
regardless of the time and place. Object detection shows good results using
only spatial information, but foreground object detection in visual
surveillance requires proper temporal and spatial information processing. In
deep learning-based foreground object detection algorithms, the detection
ability is superior to classical background subtraction (BGS) algorithms in an
environment similar to training. However, the performance is lower than that of
the classical BGS algorithm in the environment different from training. This
paper proposes a spatio-temporal fusion network (STFN) that could extract
temporal and spatial information using a temporal network and a spatial
network. We suggest a method using a semi-foreground map for stable training of
the proposed STFN. The proposed algorithm shows excellent performance in an
environment different from training, and we show it through experiments with
various public datasets. Also, STFN can generate a compliant background image
in a semi-supervised method, and it can operate in real-time on a desktop with
GPU. The proposed method shows 11.28% and 18.33% higher FM than the latest deep
learning method in the LASIESTA and SBI dataset, respectively.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Adversarially Guided Actor-Critic,"  Despite definite success in deep reinforcement learning problems,
actor-critic algorithms are still confronted with sample inefficiency in
complex environments, particularly in tasks where efficient exploration is a
bottleneck. These methods consider a policy (the actor) and a value function
(the critic) whose respective losses are built using different motivations and
approaches. This paper introduces a third protagonist: the adversary. While the
adversary mimics the actor by minimizing the KL-divergence between their
respective action distributions, the actor, in addition to learning to solve
the task, tries to differentiate itself from the adversary predictions. This
novel objective stimulates the actor to follow strategies that could not have
been correctly predicted from previous trajectories, making its behavior
innovative in tasks where the reward is extremely rare. Our experimental
analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC)
algorithm leads to more exhaustive exploration. Notably, AGAC outperforms
current state-of-the-art methods on a set of various hard-exploration and
procedurally-generated tasks.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Reinforcement learning for automatic quadrilateral mesh generation: a soft actor-critic approach,"  This paper proposes, implements, and evaluates a reinforcement learning
(RL)-based computational framework for automatic mesh generation. Mesh
generation plays a fundamental role in numerical simulations in the area of
computer aided design and engineering (CAD/E). It is identified as one of the
critical issues in the NASA CFD Vision 2030 Study. Existing mesh generation
methods suffer from high computational complexity, low mesh quality in complex
geometries, and speed limitations. These methods and tools, including
commercial software packages, are typically semiautomatic and they need inputs
or help from human experts. By formulating the mesh generation as a Markov
decision process (MDP) problem, we are able to use a state-of-the-art
reinforcement learning (RL) algorithm called ""soft actor-critic"" to
automatically learn from trials the policy of actions for mesh generation. The
implementation of this RL algorithm for mesh generation allows us to build a
fully automatic mesh generation system without human intervention and any extra
clean-up operations, which fills the gap in the existing mesh generation tools.
In the experiments to compare with two representative commercial software
packages, our system demonstrates promising performance with respect to
scalability, generalizability, and effectiveness.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computational Geometry (cs.CG),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'ComputationalGeometry(cs.CG)']","['cs.LG', 'cs.AI', 'cs.CG']"
Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning,"  Deep reinforcement learning for multi-agent cooperation and competition has
been a hot topic recently. This paper focuses on cooperative multi-agent
problem based on actor-critic methods under local observations settings. Multi
agent deep deterministic policy gradient obtained state of art results for some
multi-agent games, whereas, it cannot scale well with growing amount of agents.
In order to boost scalability, we propose a parameter sharing deterministic
policy gradient method with three variants based on neural networks, including
actor-critic sharing, actor sharing and actor sharing with partially shared
critic. Benchmarks from rllab show that the proposed method has advantages in
learning speed and memory efficiency, well scales with growing amount of
agents, and moreover, it can make full use of reward sharing and
exchangeability if possible.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Federated Reinforcement Distillation with Proxy Experience Memory,"  In distributed reinforcement learning, it is common to exchange the
experience memory of each agent and thereby collectively train their local
models. The experience memory, however, contains all the preceding state
observations and their corresponding policies of the host agent, which may
violate the privacy of the agent. To avoid this problem, in this work, we
propose a privacy-preserving distributed reinforcement learning (RL) framework,
termed federated reinforcement distillation (FRD). The key idea is to exchange
a proxy experience memory comprising a pre-arranged set of states and
time-averaged policies, thereby preserving the privacy of actual experiences.
Based on an advantage actor-critic RL architecture, we numerically evaluate the
effectiveness of FRD and investigate how the performance of FRD is affected by
the proxy memory structure and different memory exchanging rules.

    ",Machine Learning (cs.LG),; Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MultiagentSystems(cs.MA)', 'NetworkingandInternetArchitecture(cs.NI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.MA', 'cs.NI', 'stat.ML']"
Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback,"  Machine translation is a natural candidate problem for reinforcement learning
from human feedback: users provide quick, dirty ratings on candidate
translations to guide a system to improve. Yet, current neural machine
translation training focuses on expensive human-generated reference
translations. We describe a reinforcement learning algorithm that improves
neural machine translation systems from simulated human feedback. Our algorithm
combines the advantage actor-critic algorithm (Mnih et al., 2016) with the
attention-based neural encoder-decoder architecture (Luong et al., 2015). This
algorithm (a) is well-designed for problems with a large action space and
delayed rewards, (b) effectively optimizes traditional corpus-level machine
translation metrics, and (c) is robust to skewed, high-variance, granular
feedback modeled after actual human behaviors.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'ArtificialIntelligence(cs.AI)', 'Human-ComputerInteraction(cs.HC)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.AI', 'cs.HC', 'cs.LG']"
"Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game","  Score following is the process of tracking a musical performance (audio) with
respect to a known symbolic representation (a score). We start this paper by
formulating score following as a multimodal Markov Decision Process, the
mathematical foundation for sequential decision making. Given this formal
definition, we address the score following task with state-of-the-art deep
reinforcement learning (RL) algorithms such as synchronous advantage actor
critic (A2C). In particular, we design multimodal RL agents that simultaneously
learn to listen to music, read the scores from images of sheet music, and
follow the audio along in the sheet, in an end-to-end fashion. All this
behavior is learned entirely from scratch, based on a weak and potentially
delayed reward signal that indicates to the agent how close it is to the
correct position in the score. Besides discussing the theoretical advantages of
this learning paradigm, we show in experiments that it is in fact superior
compared to previously proposed methods for score following in raw sheet music
images.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'Sound(cs.SD)', 'AudioandSpeechProcessing(eess.AS)']","['cs.AI', 'cs.LG', 'cs.SD', 'eess.AS']"
Backpropagation through the Void: Optimizing control variates for black-box gradient estimation,"  Gradient-based optimization is the foundation of deep learning and
reinforcement learning. Even when the mechanism being optimized is unknown or
not differentiable, optimization using high-variance or biased gradient
estimates is still often the best strategy. We introduce a general framework
for learning low-variance, unbiased gradient estimators for black-box functions
of random variables. Our method uses gradients of a neural network trained
jointly with model parameters or policies, and is applicable in both discrete
and continuous settings. We demonstrate this framework for training discrete
latent-variable models. We also give an unbiased, action-conditional extension
of the advantage actor-critic reinforcement learning algorithm.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,"  Legal Artificial Intelligence (LegalAI) focuses on applying the technology of
artificial intelligence, especially natural language processing, to benefit
tasks in the legal domain. In recent years, LegalAI has drawn increasing
attention rapidly from both AI researchers and legal professionals, as LegalAI
is beneficial to the legal system for liberating legal professionals from a
maze of paperwork. Legal professionals often think about how to solve tasks
from rule-based and symbol-based methods, while NLP researchers concentrate
more on data-driven and embedding methods. In this paper, we introduce the
history, the current state, and the future directions of research in LegalAI.
We illustrate the tasks from the perspectives of legal professionals and NLP
researchers and show several representative applications in LegalAI. We conduct
experiments and provide an in-depth analysis of the advantages and
disadvantages of existing works to explore possible future directions. You can
find the implementation of our work from ",Computation and Language (cs.CL),,['ComputationandLanguage(cs.CL)'],['cs.CL']
Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review,"  With the advances in information technology (IT) criminals are using
cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly
vulnerable to intrusions and other threats. Physical devices and human
intervention are not sufficient for monitoring and protection of these
infrastructures; hence, there is a need for more sophisticated cyber defense
systems that need to be flexible, adaptable and robust, and able to detect a
wide variety of threats and make intelligent real-time decisions. Numerous
bio-inspired computing methods of Artificial Intelligence have been
increasingly playing an important role in cyber crime detection and prevention.
The purpose of this study is to present advances made so far in the field of
applying AI techniques for combating cyber crimes, to demonstrate how these
techniques can be an effective tool for detection and prevention of cyber
attacks, as well as to give the scope for future work.

    ",Artificial Intelligence (cs.AI),; Cryptography and Security (cs.CR); Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CR', 'cs.CY']"
Artificial Intelligence and its Role in Near Future,"  AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades.

    ",Artificial Intelligence (cs.AI),; Computer Vision and Pattern Recognition (cs.CV),"['ArtificialIntelligence(cs.AI)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.AI', 'cs.CV']"
The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey,"  Detection of non-technical losses (NTL) which include electricity theft,
faulty meters or billing errors has attracted increasing attention from
researchers in electrical engineering and computer science. NTLs cause
significant harm to the economy, as in some countries they may range up to 40%
of the total electricity distributed. The predominant research direction is
employing artificial intelligence to predict whether a customer causes NTL.
This paper first provides an overview of how NTLs are defined and their impact
on economies, which include loss of revenue and profit of electricity providers
and decrease of the stability and reliability of electrical power grids. It
then surveys the state-of-the-art research efforts in a up-to-date and
comprehensive review of algorithms, features and data sets used. It finally
identifies the key scientific and engineering challenges in NTL detection and
suggests how they could be addressed in the future.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Building Ethics into Artificial Intelligence,"  As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
From Statistical Relational to Neuro-Symbolic Artificial Intelligence,"  Neuro-symbolic and statistical relational artificial intelligence both
integrate frameworks for learning with logical reasoning. This survey
identifies several parallels across seven different dimensions between these
two fields. These cannot only be used to characterize and position
neuro-symbolic artificial intelligence approaches but also to identify a number
of directions for further research.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Detection of Covid-19 From Chest X-ray Images Using Artificial Intelligence: An Early Review,"  In 2019, the entire world is facing a situation of health emergency due to a
newly emerged coronavirus (COVID-19). Almost 196 countries are affected by
covid-19, while USA, Italy, China, Spain, Iran, and France have the maximum
active cases of COVID-19. The issues, medical and healthcare departments are
facing in delay of detecting the COVID-19. Several artificial intelligence
based system are designed for the automatic detection of COVID-19 using chest
x-rays. In this article we will discuss the different approaches used for the
detection of COVID-19 and the challenges we are facing. It is mandatory to
develop an automatic detection system to prevent the transfer of the virus
through contact. Several deep learning architecture are deployed for the
detection of COVID-19 such as ResNet, Inception, Googlenet etc. All these
approaches are detecting the subjects suffering with pneumonia while its hard
to decide whether the pneumonia is caused by COVID-19 or due to any other
bacterial or fungal attack.

    ",Image and Video Processing (eess.IV),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG),"['ImageandVideoProcessing(eess.IV)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['eess.IV', 'cs.CV', 'cs.LG']"
"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report","  In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Linking Artificial Intelligence Principles,"  Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.

    ",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CY']"
"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence","  Perhaps the most ambitious scientific quest in human history is the creation
of general artificial intelligence, which roughly means AI that is as smart or
smarter than humans. The dominant approach in the machine learning community is
to attempt to discover each of the pieces required for intelligence, with the
implicit assumption that some future group will complete the Herculean task of
figuring out how to combine all of those pieces into a complex thinking
machine. I call this the ""manual AI approach"". This paper describes another
exciting path that ultimately may be more successful at producing general AI.
It is based on the clear trend in machine learning that hand-designed solutions
eventually are replaced by more effective, learned solutions. The idea is to
create an AI-generating algorithm (AI-GA), which automatically learns how to
produce general AI. Three Pillars are essential for the approach: (1)
meta-learning architectures, (2) meta-learning the learning algorithms
themselves, and (3) generating effective learning environments. I argue that
either approach could produce general AI first, and both are scientifically
worthwhile irrespective of which is the fastest path. Because both are
promising, yet the ML community is currently committed to the manual approach,
I argue that our community should increase its research investment in the AI-GA
approach. To encourage such research, I describe promising work in each of the
Three Pillars. I also discuss AI-GA-specific safety and ethical considerations.
Because it it may be the fastest path to general AI and because it is
inherently scientifically interesting to understand the conditions in which a
simple algorithm can produce general AI (as happened on Earth where Darwinian
evolution produced human intelligence), I argue that the pursuit of AI-GAs
should be considered a new grand challenge of computer science research.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures,"  In this work, we present and analyze reported failures of artificially
intelligent systems and extrapolate our analysis to future AIs. We suggest that
both the frequency and the seriousness of future AI failures will steadily
increase. AI Safety can be improved based on ideas developed by cybersecurity
experts. For narrow AIs safety failures are at the same, moderate, level of
criticality as in cybersecurity, however for general AI, failures have a
fundamentally different impact. A single failure of a superintelligent system
may cause a catastrophic event without a chance for recovery. The goal of
cybersecurity is to reduce the number of successful attacks on the system; the
goal of AI Safety is to make sure zero attacks succeed in bypassing the safety
mechanisms. Unfortunately, such a level of performance is unachievable. Every
security system will eventually fail; there is no such thing as a 100% secure
system.

    ",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CY']"
A comparative study of artificial intelligence and human doctors for the purpose of triage and diagnosis,"  Online symptom checkers have significant potential to improve patient care,
however their reliability and accuracy remain variable. We hypothesised that an
artificial intelligence (AI) powered triage and diagnostic system would compare
favourably with human doctors with respect to triage and diagnostic accuracy.
We performed a prospective validation study of the accuracy and safety of an AI
powered triage and diagnostic system. Identical cases were evaluated by both an
AI system and human doctors. Differential diagnoses and triage outcomes were
evaluated by an independent judge, who was blinded from knowing the source (AI
system or human doctor) of the outcomes. Independently of these cases,
vignettes from publicly available resources were also assessed to provide a
benchmark to previous studies and the diagnostic component of the MRCGP exam.
Overall we found that the Babylon AI powered Triage and Diagnostic System was
able to identify the condition modelled by a clinical vignette with accuracy
comparable to human doctors (in terms of precision and recall). In addition, we
found that the triage advice recommended by the AI System was, on average,
safer than that of human doctors, when compared to the ranges of acceptable
triage provided by independent expert judges, with only a minimal reduction in
appropriateness.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'Applications(stat.AP)', 'MachineLearning(stat.ML)']","['cs.AI', 'cs.LG', 'stat.AP', 'stat.ML']"
"CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models","  As artificial intelligence plays an increasingly important role in our
society, there are ethical and moral obligations for both businesses and
researchers to ensure that their machine learning models are designed,
deployed, and maintained responsibly. These models need to be rigorously
audited for fairness, robustness, transparency, and interpretability. A variety
of methods have been developed that focus on these issues in isolation,
however, managing these methods in conjunction with model development can be
cumbersome and timeconsuming. In this paper, we introduce a unified and
model-agnostic approach to address these issues: Counterfactual Explanations
for Robustness, Transparency, Interpretability, and Fairness of Artificial
Intelligence models (CERTIFAI). Unlike previous methods in this domain,
CERTIFAI is a general tool that can be applied to any black-box model and any
type of input data. Given a model and an input instance, CERTIFAI uses a custom
genetic algorithm to generate counterfactuals: instances close to the input
that change the prediction of the model. We demonstrate how these
counterfactuals can be used to examine issues of robustness, interpretability,
transparency, and fairness. Additionally, we introduce CERScore, the first
black-box model robustness score that performs comparably to methods that have
access to model internals.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Smart Sustainable Agriculture (SSA) Solution Underpinned by Internet of Things (IoT) and Artificial Intelligence (AI),"  The Internet of Things (IoT) and Artificial Intelligence (AI) have been
employed in agriculture over a long period of time, alongside other advanced
computing technologies. However, increased attention is currently being paid to
the use of such smart technologies. Agriculture has provided an important
source of food for human beings over many thousands of years, including the
development of appropriate farming methods for different types of crops. The
emergence of new advanced IoT technologies has the potential to monitor the
agricultural environment to ensure high-quality products. However, there
remains a lack of research and development in relation to Smart Sustainable
Agriculture (SSA), accompanied by complex obstacles arising from the
fragmentation of agricultural processes, i.e. the control and operation of
IoT/AI machines; data sharing and management; interoperability; and large
amounts of data analysis and storage. This study firstly, explores existing
IoT/AI technologies adopted for SSA and secondly, identifies IoT/AI technical
architecture capable of underpinning the development of SSA platforms. As well
as contributing to the current body of knowledge, this research reviews
research and development within SSA and provides an IoT/AI architecture to
establish a Smart, Sustainable Agriculture platform as a solution.

    ",Signal Processing (eess.SP),,['SignalProcessing(eess.SP)'],['eess.SP']
A Grounded Interaction Protocol for Explainable Artificial Intelligence,"  Explainable Artificial Intelligence (XAI) systems need to include an
explanation model to communicate the internal decisions, behaviours and actions
to the interacting humans. Successful explanation involves both cognitive and
social processes. In this paper we focus on the challenge of meaningful
interaction between an explainer and an explainee and investigate the
structural aspects of an interactive explanation to propose an interaction
protocol. We follow a bottom-up approach to derive the model by analysing
transcripts of different explanation dialogue types with 398 explanation
dialogues. We use grounded theory to code and identify key components of an
explanation dialogue. We formalize the model using the agent dialogue framework
(ADF) as a new dialogue type and then evaluate it in a human-agent interaction
study with 101 dialogues from 14 participants. Our results show that the
proposed model can closely follow the explanation dialogues of human-agent
conversations.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Blockchain and Artificial Intelligence,"  It is undeniable that artificial intelligence (AI) and blockchain concepts
are spreading at a phenomenal rate. Both technologies have distinct degree of
technological complexity and multi-dimensional business implications. However,
a common misunderstanding about blockchain concept, in particular, is that
blockchain is decentralized and is not controlled by anyone. But the underlying
development of a blockchain system is still attributed to a cluster of core
developers. Take smart contract as an example, it is essentially a collection
of codes (or functions) and data (or states) that are programmed and deployed
on a blockchain (say, Ethereum) by different human programmers. It is thus,
unfortunately, less likely to be free of loopholes and flaws. In this article,
through a brief overview about how artificial intelligence could be used to
deliver bug-free smart contract so as to achieve the goal of blockchain 2.0, we
to emphasize that the blockchain implementation can be assisted or enhanced via
various AI techniques. The alliance of AI and blockchain is expected to create
numerous possibilities.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
"Innateness, AlphaZero, and Artificial Intelligence","  The concept of innateness is rarely discussed in the context of artificial
intelligence. When it is discussed, or hinted at, it is often the context of
trying to reduce the amount of innate machinery in a given system. In this
paper, I consider as a test case a recent series of papers by Silver et al
(Silver et al., 2017a) on AlphaGo and its successors that have been presented
as an argument that a ""even in the most challenging of domains: it is possible
to train to superhuman level, without human examples or guidance"", ""starting
tabula rasa.""
",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Artificial Intelligence for Social Good: A Survey,"  Artificial intelligence for social good (AI4SG) is a research theme that aims
to use and advance artificial intelligence to address societal issues and
improve the well-being of the world. AI4SG has received lots of attention from
the research community in the past decade with several successful applications.
Building on the most comprehensive collection of the AI4SG literature to date
with over 1000 contributed papers, we provide a detailed account and analysis
of the work under the theme in the following ways. (1) We quantitatively
analyze the distribution and trend of the AI4SG literature in terms of
application domains and AI techniques used. (2) We propose three conceptual
methods to systematically group the existing literature and analyze the eight
AI4SG application domains in a unified framework. (3) We distill five research
topics that represent the common challenges in AI4SG across various application
domains. (4) We discuss five issues that, we hope, can shed light on the future
development of the AI4SG research.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)']","['cs.CY', 'cs.AI']"
"Impact of Artificial Intelligence on Businesses: from Research, Innovation, Market Deployment to Future Shifts in Business Models","  The fast pace of artificial intelligence (AI) and automation is propelling
strategists to reshape their business models. This is fostering the integration
of AI in the business processes but the consequences of this adoption are
underexplored and need attention. This paper focuses on the overall impact of
AI on businesses - from research, innovation, market deployment to future
shifts in business models. To access this overall impact, we design a
three-dimensional research model, based upon the Neo-Schumpeterian economics
and its three forces viz. innovation, knowledge, and entrepreneurship. The
first dimension deals with research and innovation in AI. In the second
dimension, we explore the influence of AI on the global market and the
strategic objectives of the businesses and finally, the third dimension
examines how AI is shaping business contexts. Additionally, the paper explores
AI implications on actors and its dark sides.

    ",General Economics (econ.GN),; Artificial Intelligence (cs.AI); Computers and Society (cs.CY),"['GeneralEconomics(econ.GN)', 'ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['econ.GN', 'cs.AI', 'cs.CY']"
Machine Learning in Artificial Intelligence: Towards a Common Understanding,"  The application of ""machine learning"" and ""artificial intelligence"" has
become popular within the last decade. Both terms are frequently used in
science and media, sometimes interchangeably, sometimes with different
meanings. In this work, we aim to clarify the relationship between these terms
and, in particular, to specify the contribution of machine learning to
artificial intelligence. We review relevant literature and present a conceptual
framework which clarifies the role of machine learning to build (artificial)
intelligent agents. Hence, we seek to provide more terminological clarity and a
starting point for (interdisciplinary) discussions and future research.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research,"  Increasingly, modern Artificial Intelligence (AI) research has become more
computationally intensive. However, a growing concern is that due to unequal
access to computing power, only certain firms and elite universities have
advantages in modern AI research. Using a novel dataset of 171394 papers from
57 prestigious computer science conferences, we document that firms, in
particular, large technology firms and elite universities have increased
participation in major AI conferences since deep learning's unanticipated rise
in 2012. The effect is concentrated among elite universities, which are ranked
1-50 in the QS World University Rankings. Further, we find two strategies
through which firms increased their presence in AI research: first, they have
increased firm-only publications; and second, firms are collaborating primarily
with elite universities. Consequently, this increased presence of firms and
elite universities in AI research has crowded out mid-tier (QS ranked 201-300)
and lower-tier (QS ranked 301-500) universities. To provide causal evidence
that deep learning's unanticipated rise resulted in this divergence, we
leverage the generalized synthetic control method, a data-driven counterfactual
estimator. Using machine learning based text analysis methods, we provide
additional evidence that the divergence between these two groups - large firms
and non-elite universities - is driven by access to computing power or compute,
which we term as the ""compute divide"". This compute divide between large firms
and non-elite universities increases concerns around bias and fairness within
AI technology, and presents an obstacle towards ""democratizing"" AI. These
results suggest that a lack of access to specialized equipment such as compute
can de-democratize knowledge production.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
Explainable Artificial Intelligence (XAI) on TimeSeries Data: A Survey,"  Most of state of the art methods applied on time series consist of deep
learning methods that are too complex to be interpreted. This lack of
interpretability is a major drawback, as several applications in the real world
are critical tasks, such as the medical field or the autonomous driving field.
The explainability of models applied on time series has not gather much
attention compared to the computer vision or the natural language processing
fields. In this paper, we present an overview of existing explainable AI (XAI)
methods applied on time series and illustrate the type of explanations they
produce. We also provide a reflection on the impact of these explanation
methods to provide confidence and trust in the AI systems.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
"Hi Sigma, do I have the Coronavirus?: Call for a New Artificial Intelligence Approach to Support Health Care Professionals Dealing With The COVID-19 Pandemic","  Just like your phone can detect what song is playing in crowded spaces, we
show that Artificial Intelligence transfer learning algorithms trained on cough
phone recordings results in diagnostic tests for COVID-19. To gain adoption by
the health care community, we plan to validate our results in a clinical trial
and three other venues in Mexico, Spain and the USA . However, if we had data
from other on-going clinical trials and volunteers, we may do much more. For
example, for confirmed stay-at-home COVID-19 patients, a longitudinal audio
test could be developed to determine contact-with-hospital recommendations, and
for the most critical COVID-19 patients a success ratio forecast test,
including patient clinical data, to prioritize ICU allocation. As a challenge
to the engineering community and in the context of our clinical trial, the
authors suggest distributing cough recordings daily, hoping other trials and
crowdsourcing users will contribute more data. Previous approaches to complex
AI tasks have either used a static dataset or were private efforts led by large
corporations. All existing COVID-19 trials published also follow this paradigm.
Instead, we suggest a novel open collective approach to large-scale real-time
health care AI. We will be posting updates at ",Computers and Society (cs.CY),; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'Human-ComputerInteraction(cs.HC)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.HC', 'cs.LG']"
Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence,"  The ethical implications and social impacts of artificial intelligence have
become topics of compelling interest to industry, researchers in academia, and
the public. However, current analyses of AI in a global context are biased
toward perspectives held in the U.S., and limited by a lack of research,
especially outside the U.S. and Western Europe.
",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
A Definition of Artificial Intelligence,"  In this paper we offer a formal definition of Artificial Intelligence and
this directly gives us an algorithm for construction of this object. Really,
this algorithm is useless due to the combinatory explosion.
",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Diagnosis of Coronary Artery Disease Using Artificial Intelligence Based Decision Support System,"  This research is about the development a fuzzy decision support system for
the diagnosis of coronary artery disease based on evidence. The coronary artery
disease data sets taken from University California Irvine (UCI) are used. The
knowledge base of fuzzy decision support system is taken by using rules
extraction method based on Rough Set Theory. The rules then are selected and
fuzzified based on information from discretization of numerical attributes.
Fuzzy rules weight is proposed using the information from support of extracted
rules. UCI heart disease data sets collected from U.S., Switzerland and
Hungary, data from Ipoh Specialist Hospital Malaysia are used to verify the
proposed system. The results show that the system is able to give the
percentage of coronary artery blocking better than cardiologists and
angiography. The results of the proposed system were verified and validated by
three expert cardiologists and are considered to be more efficient and useful.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
A Theory of Universal Artificial Intelligence based on Algorithmic Complexity,"  Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.

    ",Artificial Intelligence (cs.AI),; Information Theory (cs.IT); Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'InformationTheory(cs.IT)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.IT', 'cs.LG']"
Explainable Artificial Intelligence Approaches: A Survey,"  The lack of explainability of a decision from an Artificial Intelligence (AI)
based ""black box"" system/model, despite its superiority in many real-world
applications, is a key stumbling block for adopting AI in many high stakes
applications of different domain or industry. While many popular Explainable
Artificial Intelligence (XAI) methods or approaches are available to facilitate
a human-friendly explanation of the decision, each has its own merits and
demerits, with a plethora of open challenges. We demonstrate popular XAI
methods with a mutual case study/task (i.e., credit default prediction),
analyze for competitive advantages from multiple perspectives (e.g., local,
global), provide meaningful insight on quantifying explainability, and
recommend paths towards responsible or human-centered AI using XAI as a medium.
Practitioners can use this work as a catalog to understand, compare, and
correlate competitive advantages of popular XAI methods. In addition, this
survey elicits future research directions towards responsible or human-centric
AI systems, which is crucial to adopt AI in high stakes applications.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Computational Power and the Social Impact of Artificial Intelligence,"  Machine learning is a computational process. To that end, it is inextricably
tied to computational power - the tangible material of chips and semiconductors
that the algorithms of machine intelligence operate on. Most obviously,
computational power and computing architectures shape the speed of training and
inference in machine learning, and therefore influence the rate of progress in
the technology. But, these relationships are more nuanced than that: hardware
shapes the methods used by researchers and engineers in the design and
development of machine learning models. Characteristics such as the power
consumption of chips also define where and how machine learning can be used in
the real world.
",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CY']"
A 20-Year Community Roadmap for Artificial Intelligence Research in the US,"  Decades of research in artificial intelligence (AI) have produced formidable
technologies that are providing immense benefit to industry, government, and
society. AI systems can now translate across multiple languages, identify
objects in images and video, streamline manufacturing processes, and control
cars. The deployment of AI systems has not only created a trillion-dollar
industry that is projected to quadruple in three years, but has also exposed
the need to make AI systems fair, explainable, trustworthy, and secure. Future
AI systems will rightfully be expected to reason effectively about the world in
which they (and people) operate, handling complex tasks and responsibilities
effectively and ethically, engaging in meaningful communication, and improving
their awareness through experience.
",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)']","['cs.CY', 'cs.AI']"
"Deep learning, deep change? Mapping the development of the Artificial Intelligence General Purpose Technology","  General Purpose Technologies (GPTs) that can be applied in many industries
are an important driver of economic growth and national and regional
competitiveness. In spite of this, the geography of their development and
diffusion has not received significant attention in the literature. We address
this with an analysis of Deep Learning (DL), a core technique in Artificial
Intelligence (AI) increasingly being recognized as the latest GPT. We identify
DL papers in a novel dataset from ArXiv, a popular preprints website, and use
CrunchBase, a technology business directory to measure industrial capabilities
related to it. After showing that DL conforms with the definition of a GPT,
having experienced rapid growth and diffusion into new fields where it has
generated an impact, we describe changes in its geography. Our analysis shows
China's rise in AI rankings and relative decline in several European countries.
We also find that initial volatility in the geography of DL has been followed
by consolidation, suggesting that the window of opportunity for new entrants
might be closing down as new DL research hubs become dominant. Finally, we
study the regional drivers of DL clustering. We find that competitive DL
clusters tend to be based in regions combining research and industrial
activities related to it. This could be because GPT developers and adopters
located close to each other can collaborate and share knowledge more easily,
thus overcoming coordination failures in GPT deployment. Our analysis also
reveals a Chinese comparative advantage in DL after we control for other
explanatory factors, perhaps underscoring the importance of access to data and
supportive policies for the successful development of this complex, `omni-use'
technology.

    ",Computers and Society (cs.CY),; Econometrics (econ.EM),"['ComputersandSociety(cs.CY)', 'Econometrics(econ.EM)']","['cs.CY', 'econ.EM']"
Artificial Intelligence Governance and Ethics: Global Perspectives,"  Artificial intelligence (AI) is a technology which is increasingly being
utilised in society and the economy worldwide, and its implementation is
planned to become more prevalent in coming years. AI is increasingly being
embedded in our lives, supplementing our pervasive use of digital technologies.
But this is being accompanied by disquiet over problematic and dangerous
implementations of AI, or indeed, even AI itself deciding to do dangerous and
problematic actions, especially in fields such as the military, medicine and
criminal justice. These developments have led to concerns about whether and how
AI systems adhere, and will adhere to ethical standards. These concerns have
stimulated a global conversation on AI ethics, and have resulted in various
actors from different countries and sectors issuing ethics and governance
initiatives and guidelines for AI. Such developments form the basis for our
research in this report, combining our international and interdisciplinary
expertise to give an insight into what is happening in Australia, China,
Europe, India and the US.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.AI', 'cs.LG']"
Neuro-Symbolic Artificial Intelligence: Current Trends,"  Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods
with methods that are based on artificial neural networks -- has a
long-standing history. In this article, we provide a structured overview of
current trends, by means of categorizing recent publications from key
conferences. The article is meant to serve as a convenient starting point for
research on the general topic.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Improving content marketing processes with the approaches by artificial intelligence,"  Content marketing is todays one of the most remarkable approaches in the
context of marketing processes of companies. Value of this kind of marketing
has improved in time, thanks to the latest developments regarding to computer
and communication technologies. Nowadays, especially social media based
platforms have a great importance on enabling companies to design multimedia
oriented, interactive content. But on the other hand, there is still something
more to do for improved content marketing approaches. In this context,
objective of this study is to focus on intelligent content marketing, which can
be done by using artificial intelligence. Artificial Intelligence is todays one
of the most remarkable research fields and it can be used easily as
multidisciplinary. So, this study has aimed to discuss about its potential on
improving content marketing. In detail, the study has enabled readers to
improve their awareness about the intersection point of content marketing and
artificial intelligence. Furthermore, the authors have introduced some example
models of intelligent content marketing, which can be achieved by using current
Web technologies and artificial intelligence techniques.

    ",Artificial Intelligence (cs.AI),; Social and Information Networks (cs.SI),"['ArtificialIntelligence(cs.AI)', 'SocialandInformationNetworks(cs.SI)']","['cs.AI', 'cs.SI']"
Security and Privacy for Artificial Intelligence: Opportunities and Challenges,"  The increased adoption of Artificial Intelligence (AI) presents an
opportunity to solve many socio-economic and environmental challenges; however,
this cannot happen without securing AI-enabled technologies. In recent years,
most AI models are vulnerable to advanced and sophisticated hacking techniques.
This challenge has motivated concerted research efforts into adversarial AI,
with the aim of developing robust machine and deep learning models that are
resilient to different types of adversarial scenarios. In this paper, we
present a holistic cyber security review that demonstrates adversarial attacks
against AI applications, including aspects such as adversarial knowledge and
capabilities, as well as existing methods for generating adversarial examples
and existing cyber defence models. We explain mathematical AI models,
especially new variants of reinforcement and federated learning, to demonstrate
how attack vectors would exploit vulnerabilities of AI models. We also propose
a systematic framework for demonstrating attack techniques against AI
applications and reviewed several cyber defences that would protect AI
applications against those attacks. We also highlight the importance of
understanding the adversarial goals and their capabilities, especially the
recent attacks against industry applications, to develop adaptive defences that
assess to secure AI applications. Finally, we describe the main challenges and
future research directions in the domain of security and privacy of AI
technologies.

    ",Cryptography and Security (cs.CR),; Artificial Intelligence (cs.AI),"['CryptographyandSecurity(cs.CR)', 'ArtificialIntelligence(cs.AI)']","['cs.CR', 'cs.AI']"
Artificial Intelligence for Social Good,"  The Computing Community Consortium (CCC), along with the White House Office
of Science and Technology Policy (OSTP), and the Association for the
Advancement of Artificial Intelligence (AAAI), co-sponsored a public workshop
on Artificial Intelligence for Social Good on June 7th, 2016 in Washington, DC.
This was one of five workshops that OSTP co-sponsored and held around the
country to spur public dialogue on artificial intelligence, machine learning,
and to identify challenges and opportunities related to AI. In the AI for
Social Good workshop, the successful deployments and the potential use of AI in
various topics that are essential for social good were discussed, including but
not limited to urban computing, health, environmental sustainability, and
public welfare. This report highlights each of these as well as a number of
crosscutting issues.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
The Dataset Nutrition Label (2nd Gen): Leveraging Context to Mitigate Harms in Artificial Intelligence,"  As the production of and reliance on datasets to produce automated
decision-making systems (ADS) increases, so does the need for processes for
evaluating and interrogating the underlying data. After launching the Dataset
Nutrition Label in 2018, the Data Nutrition Project has made significant
updates to the design and purpose of the Label, and is launching an updated
Label in late 2020, which is previewed in this paper. The new Label includes
context-specific Use Cases &Alerts presented through an updated design and user
interface targeted towards the data scientist profile. This paper discusses the
harm and bias from underlying training data that the Label is intended to
mitigate, the current state of the work including new datasets being labeled,
new and existing challenges, and further directions of the work, as well as
Figures previewing the new label.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
"Beyond STEM, How Can Women Engage Big Data, Analytics, Robotics and Artificial Intelligence? An Exploratory Analysis of Confidence and Educational Factors in the Emerging Technology Waves Influencing the Role of, and Impact Upon, Women","  In spite of the rapidly advancing global technological environment, the
professional participation of women in technology, big data, analytics,
artificial intelligence and information systems related domains remains
proportionately low. Furthermore, it is of no less concern that the number of
women in leadership in these domains are in even lower proportions. In spite of
numerous initiatives to improve the participation of women in technological
domains, there is an increasing need to gain additional insights into this
phenomenon especially since it occurs in nations and geographies which have
seen a sharp rise in overall female education, without such increase
translating into a corresponding spurt in information systems and technological
roles for women. The present paper presents findings from an exploratory
analysis and outlines a framework to gain insights into educational factors in
the emerging technology waves influencing the role of, and impact upon, women.
We specifically identify ways for learning and self-efficacy as key factors,
which together lead us to the Advancement of Women in Technology (AWT) insights
framework. Based on the AWT framework, we also proposition principles that can
be used to encourage higher professional engagement of women in emerging and
advanced technologies. Key Words- Women's Education, Technology, Artificial
Intelligence, Knowing, Confidence, Self-Efficacy, Learning.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Artificial Intelligence MArkup Language: A Brief Tutorial,"  The purpose of this paper is to serve as a reference guide for the
development of chatterbots implemented with the AIML language. In order to
achieve this, the main concepts in Pattern Recognition area are described
because the AIML uses such theoretical framework in their syntactic and
semantic structures. After that, AIML language is described and each AIML
command/tag is followed by an application example. Also, the usage of AIML
embedded tags for the handling of sequence dialogue limitations between humans
and machines is shown. Finally, computer systems that assist in the design of
chatterbots with the AIML language are classified and described.

    ",Artificial Intelligence (cs.AI),; Software Engineering (cs.SE),"['ArtificialIntelligence(cs.AI)', 'SoftwareEngineering(cs.SE)']","['cs.AI', 'cs.SE']"
Large image datasets: A pyrrhic win for computer vision?,"  In this paper we investigate problematic practices and consequences of large
scale vision datasets. We examine broad issues such as the question of consent
and justice as well as specific concerns such as the inclusion of verifiably
pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an
example, we perform a cross-sectional model-based quantitative census covering
factors such as age, gender, NSFW content scoring, class-wise accuracy,
human-cardinality-analysis, and the semanticity of the image class information
in order to statistically investigate the extent and subtleties of ethical
transgressions. We then use the census to help hand-curate a look-up-table of
images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of
verifiably pornographic: shot in a non-consensual setting (up-skirt), beach
voyeuristic, and exposed private parts. We survey the landscape of harm and
threats both society broadly and individuals face due to uncritical and
ill-considered dataset curation practices. We then propose possible courses of
correction and critique the pros and cons of these. We have duly open-sourced
all of the code and the census meta-datasets generated in this endeavor for the
computer vision community to build on. By unveiling the severity of the
threats, our hope is to motivate the constitution of mandatory Institutional
Review Boards (IRB) for large scale dataset curation processes.

    ",Computers and Society (cs.CY),; Applications (stat.AP); Machine Learning (stat.ML),"['ComputersandSociety(cs.CY)', 'Applications(stat.AP)', 'MachineLearning(stat.ML)']","['cs.CY', 'stat.AP', 'stat.ML']"
The Heisenberg Representation of Quantum Computers,"  Since Shor's discovery of an algorithm to factor numbers on a quantum
computer in polynomial time, quantum computation has become a subject of
immense interest. Unfortunately, one of the key features of quantum computers -
the difficulty of describing them on classical computers - also makes it
difficult to describe and understand precisely what can be done with them. A
formalism describing the evolution of operators rather than states has proven
extremely fruitful in understanding an important class of quantum operations.
States used in error correction and certain communication protocols can be
described by their stabilizer, a group of tensor products of Pauli matrices.
Even this simple group structure is sufficient to allow a rich range of quantum
effects, although it falls short of the full power of quantum computation.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
MNIST-C: A Robustness Benchmark for Computer Vision,"  We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions
applied to the MNIST test set, for benchmarking out-of-distribution robustness
in computer vision. Through several experiments and visualizations we
demonstrate that our corruptions significantly degrade performance of
state-of-the-art computer vision models while preserving the semantic content
of the test images. In contrast to the popular notion of adversarial
robustness, our model-agnostic corruptions do not seek worst-case performance
but are instead designed to be broad and diverse, capturing multiple failure
modes of modern models. In fact, we find that several previously published
adversarial defenses significantly degrade robustness as measured by MNIST-C.
We hope that our benchmark serves as a useful tool for future work in designing
systems that are able to learn robust feature representations that capture the
underlying semantics of the input.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['cs.CV', 'cs.LG']"
How will quantum computers provide an industrially relevant computational advantage in quantum chemistry?,"  Numerous reports claim that quantum advantage, which should emerge as a
direct consequence of the advent of quantum computers, will herald a new era of
chemical research because it will enable scientists to perform the kinds of
quantum chemical simulations that have not been possible before. Such
simulations on quantum computers, promising a significantly greater accuracy
and speed, are projected to exert a great impact on the way we can probe
reality, predict the outcomes of chemical experiments, and even drive design of
drugs, catalysts, and materials. In this work we review the current status of
quantum hardware and algorithm theory and examine whether such popular claims
about quantum advantage are really going to be transformative. We go over
subtle complications of quantum chemical research that tend to be overlooked in
discussions involving quantum computers. We estimate quantum computer resources
that will be required for performing calculations on quantum computers with
chemical accuracy for several types of molecules. In particular, we directly
compare the resources and timings associated with classical and quantum
computers for the molecules H$_2$ for increasing basis set sizes, and Cr$_2$
for a variety of complete active spaces (CAS) within the scope of the CASCI and
CASSCF methods. The results obtained for the chromium dimer enable us to
estimate the size of the active space at which computations of non-dynamic
correlation on a quantum computer should take less time than analogous
computations on a classical computer. Using this result, we speculate on the
types of chemical applications for which the use of quantum computers would be
both beneficial and relevant to industrial applications in the short term.

    ",Quantum Physics (quant-ph),; Chemical Physics (physics.chem-ph),"['QuantumPhysics(quant-ph)', 'ChemicalPhysics(physics.chem-ph)']","['quant-ph', 'physics.chem-ph']"
DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems,"  Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique
used to increase the interpretability and explainability of black box Machine
Learning (ML) algorithms. LIME typically generates an explanation for a single
prediction by any ML model by learning a simpler interpretable model (e.g.
linear classifier) around the prediction through generating simulated data
around the instance by random perturbation, and obtaining feature importance
through applying some form of feature selection. While LIME and similar local
algorithms have gained popularity due to their simplicity, the random
perturbation and feature selection methods result in ""instability"" in the
generated explanations, where for the same prediction, different explanations
can be generated. This is a critical issue that can prevent deployment of LIME
in a Computer-Aided Diagnosis (CAD) system, where stability is of utmost
importance to earn the trust of medical professionals. In this paper, we
propose a deterministic version of LIME. Instead of random perturbation, we
utilize agglomerative Hierarchical Clustering (HC) to group the training data
together and K-Nearest Neighbour (KNN) to select the relevant cluster of the
new instance that is being explained. After finding the relevant cluster, a
linear model is trained over the selected cluster to generate the explanations.
Experimental results on three different medical datasets show the superiority
for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME),
where we quantitatively determine the stability of DLIME compared to LIME
utilizing the Jaccard similarity among multiple generated explanations.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Using Computer Vision to enhance Safety of Workforce in Manufacturing in a Post COVID World,"  The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'stat.ML']"
Florence: A New Foundation Model for Computer Vision,"  Automated visual understanding of our diverse and open world demands computer
vision models to generalize well with minimal customization for specific tasks,
similar to human vision. Computer vision foundation models, which are trained
on diverse, large-scale dataset and can be adapted to a wide range of
downstream tasks, are critical for this mission to solve real-world computer
vision applications. While existing vision foundation models such as CLIP,
ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual
representations to a cross-modal shared representation, we introduce a new
computer vision foundation model, Florence, to expand the representations from
coarse (scene) to fine (object), from static (images) to dynamic (videos), and
from RGB to multiple modalities (caption, depth). By incorporating universal
visual-language representations from Web-scale image-text data, our Florence
model can be easily adapted for various computer vision tasks, such as
classification, retrieval, object detection, VQA, image caption, video
retrieval and action recognition. Moreover, Florence demonstrates outstanding
performance in many types of transfer learning: fully sampled fine-tuning,
linear probing, few-shot transfer and zero-shot transfer for novel images and
objects. All of these properties are critical for our vision foundation model
to serve general purpose vision tasks. Florence achieves new state-of-the-art
results in majority of 44 representative benchmarks, e.g., ImageNet-1K
zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of
97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.

    ",Computer Vision and Pattern Recognition (cs.CV),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['ComputerVisionandPatternRecognition(cs.CV)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CV', 'cs.AI', 'cs.LG']"
SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,"  Humans read and write hundreds of billions of messages every day. Further,
due to the availability of large datasets, large computing systems, and better
neural network models, natural language processing (NLP) technology has made
significant strides in understanding, proofreading, and organizing these
messages. Thus, there is a significant opportunity to deploy NLP in myriad
applications to help web users, social networks, and businesses. In particular,
we consider smartphones and other mobile devices as crucial platforms for
deploying NLP models at scale. However, today's highly-accurate NLP neural
network models such as BERT and RoBERTa are extremely computationally
expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a
Pixel 3 smartphone. In this work, we observe that methods such as grouped
convolutions have yielded significant speedups for computer vision networks,
but many of these techniques have not been adopted by NLP neural network
designers. We demonstrate how to replace several operations in self-attention
layers with grouped convolutions, and we use this technique in a novel network
architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the
Pixel 3 while achieving competitive accuracy on the GLUE test set. The
SqueezeBERT code will be released.

    ",Computation and Language (cs.CL),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.CV', 'cs.LG']"
Better Computer Go Player with Neural Network and Long-term Prediction,"  Competing with top human players in the ancient game of Go has been a
long-term goal of artificial intelligence. Go's high branching factor makes
traditional search techniques ineffective, even on leading-edge hardware, and
Go's evaluation function could change drastically with one stone change. Recent
works [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not
strictly necessary for machine Go players. A pure pattern-matching approach,
based on a Deep Convolutional Neural Network (DCNN) that predicts the next
move, can perform as well as Monte Carlo Tree Search (MCTS)-based open source
Go engines such as Pachi [Baudis & Gailly (2012)] if its search budget is
limited. We extend this idea in our bot named darkforest, which relies on a
DCNN designed for long-term predictions. Darkforest substantially improves the
win rate for pattern-matching approaches against MCTS-based approaches, even
with looser search budgets. Against human players, the newest versions,
darkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a
substantial improvement upon the estimated 4k-5k ranks for DCNN reported in
Clark & Storkey (2015) based on games against other machine players. Adding
MCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000
rollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts
it achieves a stable 5d level in KGS server, on par with state-of-the-art Go
AIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al.
(2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of the Age of Camelot,"  The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal ""Bulletin of Mathematical Biophysics"", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Simulating quantum field theory with a quantum computer,"  Forthcoming exascale digital computers will further advance our knowledge of
quantum chromodynamics, but formidable challenges will remain. In particular,
Euclidean Monte Carlo methods are not well suited for studying real-time
evolution in hadronic collisions, or the properties of hadronic matter at
nonzero temperature and chemical potential. Digital computers may never be able
to achieve accurate simulations of such phenomena in QCD and other
strongly-coupled field theories; quantum computers will do so eventually,
though I'm not sure when. Progress toward quantum simulation of quantum field
theory will require the collaborative efforts of quantumists and field
theorists, and though the physics payoff may still be far away, it's worthwhile
to get started now. Today's research can hasten the arrival of a new era in
which quantum simulation fuels rapid progress in fundamental physics.

    ",High Energy Physics - Lattice (hep-lat),; High Energy Physics - Theory (hep-th); Quantum Physics (quant-ph),"['HighEnergyPhysics-Lattice(hep-lat)', 'HighEnergyPhysics-Theory(hep-th)', 'QuantumPhysics(quant-ph)']","['hep-lat', 'hep-th', 'quant-ph']"
"The Computer Science and Physics of Community Detection: Landscapes, Phase Transitions, and Hardness","  Community detection in graphs is the problem of finding groups of vertices
which are more densely connected than they are to the rest of the graph. This
problem has a long history, but it is undergoing a resurgence of interest due
to the need to analyze social and biological networks. While there are many
ways to formalize it, one of the most popular is as an inference problem, where
there is a ""ground truth"" community structure built into the graph somehow. The
task is then to recover the ground truth knowing only the graph.
",Computational Complexity (cs.CC),; Statistical Mechanics (cond-mat.stat-mech); Social and Information Networks (cs.SI); Probability (math.PR); Physics and Society (physics.soc-ph),"['ComputationalComplexity(cs.CC)', 'StatisticalMechanics(cond-mat.stat-mech)', 'SocialandInformationNetworks(cs.SI)', 'Probability(math.PR)', 'PhysicsandSociety(physics.soc-ph)']","['cs.CC', 'cond-mat.stat-mech', 'cs.SI', 'math.PR', 'physics.soc-ph']"
Computer Algebra Algorithms for Special Functions in Particle Physics,"  This work deals with special nested objects arising in massive higher order
perturbative calculations in renormalizable quantum field theories. On the one
hand we work with nested sums such as harmonic sums and their generalizations
(S-sums, cyclotomic harmonic sums, cyclotomic S-sums) and on the other hand we
treat iterated integrals of the Poincar and Chen-type, such as harmonic
polylogarithms and their generalizations (multiple polylogarithms, cyclotomic
harmonic polylogarithms). The iterated integrals are connected to the nested
sums via (generalizations of) the Mellin-transformation and we show how this
transformation can be computed. We derive algebraic and structural relations
between the nested sums as well as relations between the values of the sums at
infinity and connected to it the values of the iterated integrals evaluated at
special constants. In addition we state algorithms to compute asymptotic
expansions of these nested objects and we state an algorithm which rewrites
certain types of nested sums into expressions in terms of cyclotomic S-sums.
Moreover we summarize the main functionality of the computer algebra package
HarmonicSums in which all these algorithms and transformations are implemented.
Furthermore, we present application of and enhancements of the multivariate
Almkvist-Zeilberger algorithm to certain types of Feynman integrals and the
corresponding computer algebra package MultiIntegrate.

    ",Mathematical Physics (math-ph),; High Energy Physics - Phenomenology (hep-ph); High Energy Physics - Theory (hep-th),"['MathematicalPhysics(math-ph)', 'HighEnergyPhysics-Phenomenology(hep-ph)', 'HighEnergyPhysics-Theory(hep-th)']","['math-ph', 'hep-ph', 'hep-th']"
Play and Learn: Using Video Games to Train Computer Vision Models,"  Video games are a compelling source of annotated data as they can readily
provide fine-grained groundtruth for diverse tasks. However, it is not clear
whether the synthetically generated data has enough resemblance to the
real-world images to improve the performance of computer vision models in
practice. We present experiments assessing the effectiveness on real-world data
of systems trained on synthetic RGB images that are extracted from a video
game. We collected over 60000 synthetic samples from a modern video game with
similar conditions to the real-world CamVid and Cityscapes datasets. We provide
several experiments to demonstrate that the synthetically generated RGB images
can be used to improve the performance of deep neural networks on both image
segmentation and depth estimation. These results show that a convolutional
network trained on synthetic data achieves a similar test error to a network
that is trained on real-world data for dense image classification. Furthermore,
the synthetically generated RGB images can provide similar or better results
compared to the real-world datasets if a simple domain adaptation technique is
applied. Our results suggest that collaboration with game developers for an
accessible interface to gather data is potentially a fruitful direction for
future work in computer vision.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
A Computer Algebra Toolbox for Harmonic Sums Related to Particle Physics,"  In this work we present the computer algebra package HarmonicSums and its
theoretical background for the manipulation of harmonic sums and some related
quantities as for example Euler-Zagier sums and harmonic polylogarithms.
Harmonic sums and generalized harmonic sums emerge as special cases of
so-called d'Alembertian solutions of recurrence relations. We show that
harmonic sums form a quasi-shuffle algebra and describe a method how we can
find algebraically independent harmonic sums. In addition, we define a
differentiation on harmonic sums via an extended version of the Mellin
transform. Along with that, new relations between harmonic sums will arise.
Furthermore, we present an algorithm which rewrites certain types of nested
sums into expressions in terms of harmonic sums. We illustrate by nontrivial
examples how these algorithms in cooperation with the summation package Sigma
support the evaluation of Feynman integrals.

    ",Mathematical Physics (math-ph),,['MathematicalPhysics(math-ph)'],['math-ph']
Quantum Neuron: an elementary building block for machine learning on quantum computers,"  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the ""quantum neuron"", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.

    ",Quantum Physics (quant-ph),; Neural and Evolutionary Computing (cs.NE),"['QuantumPhysics(quant-ph)', 'NeuralandEvolutionaryComputing(cs.NE)']","['quant-ph', 'cs.NE']"
Correspondence principle for idempotent calculus and some computer applications,"  This paper is devoted to heuristic aspects of the so-called idempotent
calculus. There is a correspondence between important, useful and interesting
constructions and results over the field of real (or complex) numbers and
similar constructions and results over idempotent semirings in the spirit of N.
Bohr's correspondence principle in Quantum Mechanics.
",General Mathematics (math.GM),,['GeneralMathematics(math.GM)'],['math.GM']
An Applied Study on Educational Use of Facebook as a Web 2.0 Tool: The Sample Lesson of Computer Networks and Communication,"  The main aim of the research was to examine educational use of Facebook. The
Computer Networks and Communication lesson was taken as the sample and the
attitudes of the students included in the study group towards Facebook were
measured in a semi-experimental setup. The students on Facebook platform were
examined for about three months and they continued their education
interactively in that virtual environment. After the-three-month-education
period, observations for the students were reported and the attitudes of the
students towards Facebook were measured by three different measurement tools.
As a result, the attitudes of the students towards educational use of Facebook
and their views were heterogeneous. When the average values of the group were
examined, it was reported that the attitudes towards educational use of
Facebook was above a moderate level. Therefore, it might be suggested that
social networks in virtual environments provide continuity in life long
learning.

    ",Social and Information Networks (cs.SI),,['SocialandInformationNetworks(cs.SI)'],['cs.SI']
Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems,"  Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV),"['CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.CR', 'cs.CV']"
Fansmitter: Acoustic Data Exfiltration from (Speakerless) Air-Gapped Computers,"  Because computers may contain or interact with sensitive information, they
are often air-gapped and in this way kept isolated and disconnected from the
Internet. In recent years the ability of malware to communicate over an air-gap
by transmitting sonic and ultrasonic signals from a computer speaker to a
nearby receiver has been shown. In order to eliminate such acoustic channels,
current best practice recommends the elimination of speakers (internal or
external) in secure computers, thereby creating a so-called 'audio-gap'. In
this paper, we present Fansmitter, a malware that can acoustically exfiltrate
data from air-gapped computers, even when audio hardware and speakers are not
present. Our method utilizes the noise emitted from the CPU and chassis fans
which are present in virtually every computer today. We show that a software
can regulate the internal fans' speed in order to control the acoustic waveform
emitted from a computer. Binary data can be modulated and transmitted over
these audio signals to a remote microphone (e.g., on a nearby mobile phone). We
present Fansmitter's design considerations, including acoustic signature
analysis, data modulation, and data transmission. We also evaluate the acoustic
channel, present our results, and discuss countermeasures. Using our method we
successfully transmitted data from air-gapped computer without audio hardware,
to a smartphone receiver in the same room. We demonstrated the effective
transmission of encryption keys and passwords from a distance of zero to eight
meters, with bit rate of up to 900 bits/hour. We show that our method can also
be used to leak data from different types of IT equipment, embedded systems,
and IoT devices that have no audio hardware, but contain fans of various types
and sizes.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
Survey on the attention based RNN model and its applications in computer vision,"  The recurrent neural networks (RNN) can be used to solve the sequence to
sequence problem, where both the input and the output have sequential
structures. Usually there are some implicit relations between the structures.
However, it is hard for the common RNN model to fully explore the relations
between the sequences. In this survey, we introduce some attention based RNN
models which can focus on different parts of the input for each output item, in
order to explore and take advantage of the implicit relations between the input
and the output items. The different attention mechanisms are described in
detail. We then introduce some applications in computer vision which apply the
attention based RNN models. The superiority of the attention based RNN model is
shown by the experimental results. At last some future research directions are
given.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['cs.CV', 'cs.LG']"
Five Experimental Tests on the 5-Qubit IBM Quantum Computer,"  The 5-qubit quantum computer prototypes that IBM has given open access to on
the cloud allow the implementation of real experiments on a quantum processor.
We present the results obtained in five experimental tests performed on these
computers: dense coding, quantum Fourier transforms, Bell's inequality,
Mermin's inequalities (up to $n=5$) and the construction of the prime state
$|p_3\rangle$. These results serve to assess the functioning of the IBM 5Q
chips.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer,"  Quantum Natural Language Processing (QNLP) deals with the design and
implementation of NLP models intended to be run on quantum hardware. In this
paper, we present results on the first NLP experiments conducted on Noisy
Intermediate-Scale Quantum (NISQ) computers for datasets of size >= 100
sentences. Exploiting the formal similarity of the compositional model of
meaning by Coecke et al. (2010) with quantum theory, we create representations
for sentences that have a natural mapping to quantum circuits. We use these
representations to implement and successfully train two NLP models that solve
simple sentence classification tasks on quantum hardware. We describe in detail
the main principles, the process and challenges of these experiments, in a way
accessible to NLP researchers, thus paving the way for practical Quantum
Natural Language Processing.

    ",Computation and Language (cs.CL),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantum Physics (quant-ph),"['ComputationandLanguage(cs.CL)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'QuantumPhysics(quant-ph)']","['cs.CL', 'cs.AI', 'cs.LG', 'quant-ph']"
A New Generation of Brain-Computer Interface Based on Riemannian Geometry,"  Based on the cumulated experience over the past 25 years in the field of
Brain-Computer Interface (BCI) we can now envision a new generation of BCI.
Such BCIs will not require training; instead they will be smartly initialized
using remote massive databases and will adapt to the user fast and effectively
in the first minute of use. They will be reliable, robust and will maintain
good performances within and across sessions. A general classification
framework based on recent advances in Riemannian geometry and possessing these
characteristics is presented. It applies equally well to BCI based on
event-related potentials (ERP), sensorimotor (mu) rhythms and steady-state
evoked potential (SSEP). The framework is very simple, both algorithmically and
computationally. Due to its simplicity, its ability to learn rapidly (with
little training data) and its good across-subject and across-session
generalization, this strategy a very good candidate for building a new
generation of BCIs, thus we hereby propose it as a benchmark method for the
field.

    ",Human-Computer Interaction (cs.HC),; Differential Geometry (math.DG),"['Human-ComputerInteraction(cs.HC)', 'DifferentialGeometry(math.DG)']","['cs.HC', 'math.DG']"
HoloLens 2 Research Mode as a Tool for Computer Vision Research,"  Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful
sensing devices with integrated compute capabilities, which makes it an ideal
platform for computer vision research. In this technical report, we present
HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw
sensor streams. We provide an overview of the API and explain how it can be
used to build mixed reality applications based on processing sensor data. We
also show how to combine the Research Mode sensor data with the built-in eye
and hand tracking capabilities provided by HoloLens 2. By releasing the
Research Mode API and a set of open-source tools, we aim to foster further
research in the fields of computer vision as well as robotics and encourage
contributions from the research community.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
ZX-calculus for the working quantum computer scientist,"  The ZX-calculus is a graphical language for reasoning about quantum
computation that has recently seen an increased usage in a variety of areas
such as quantum circuit optimisation, surface codes and lattice surgery,
measurement-based quantum computation, and quantum foundations. The first half
of this review gives a gentle introduction to the ZX-calculus suitable for
those familiar with the basics of quantum computing. The aim here is to make
the reader comfortable enough with the ZX-calculus that they could use it in
their daily work for small computations on quantum circuits and states. The
latter sections give a condensed overview of the literature on the ZX-calculus.
We discuss Clifford computation and graphically prove the Gottesman-Knill
theorem, we discuss a recently introduced extension of the ZX-calculus that
allows for convenient reasoning about Toffoli gates, and we discuss the recent
completeness theorems for the ZX-calculus that show that, in principle, all
reasoning about quantum computation can be done using ZX-diagrams.
Additionally, we discuss the categorical and algebraic origins of the
ZX-calculus and we discuss several extensions of the language which can
represent mixed states, measurement, classical control and higher-dimensional
qudits.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
A Case for Variability-Aware Policies for NISQ-Era Quantum Computers,"  Recently, IBM, Google, and Intel showcased quantum computers ranging from 49
to 72 qubits. While these systems represent a significant milestone in the
advancement of quantum computing, existing and near-term quantum computers are
not yet large enough to fully support quantum error-correction. Such systems
with few tens to few hundreds of qubits are termed as Noisy Intermediate Scale
Quantum computers (NISQ), and these systems can provide benefits for a class of
quantum algorithms. In this paper, we study the problems of Qubit-Allocation
(mapping of program qubits to machine qubits) and Qubit-Movement(routing qubits
from one location to another to perform entanglement).
",Quantum Physics (quant-ph),; Emerging Technologies (cs.ET),"['QuantumPhysics(quant-ph)', 'EmergingTechnologies(cs.ET)']","['quant-ph', 'cs.ET']"
Addition on a Quantum Computer,"  A new method for computing sums on a quantum computer is introduced. This
technique uses the quantum Fourier transform and reduces the number of qubits
necessary for addition by removing the need for temporary carry bits. This
approach also allows the addition of a classical number to a quantum
superposition without encoding the classical number in the quantum register.
This method also allows for massive parallelization in its execution.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Review of the Use of Electroencephalography as an Evaluation Method for Human-Computer Interaction,"  Evaluating human-computer interaction is essential as a broadening population
uses machines, sometimes in sensitive contexts. However, traditional evaluation
methods may fail to combine real-time measures, an ""objective"" approach and
data contextualization. In this review we look at how adding neuroimaging
techniques can respond to such needs. We focus on electroencephalography (EEG),
as it could be handled effectively during a dedicated evaluation phase. We
identify workload, attention, vigilance, fatigue, error recognition, emotions,
engagement, flow and immersion as being recognizable by EEG. We find that
workload, attention and emotions assessments would benefit the most from EEG.
Moreover, we advocate to study further error recognition through neuroimaging
to enhance usability and increase user experience.

    ",Human-Computer Interaction (cs.HC),,['Human-ComputerInteraction(cs.HC)'],['cs.HC']
Next Steps in Quantum Computing: Computer Science's Role,"  The computing ecosystem has always had deep impacts on society and technology
and profoundly changed our lives in myriads of ways. Despite decades of
impressive Moore's Law performance scaling and other growth in the computing
ecosystem there are nonetheless still important potential applications of
computing that remain out of reach of current or foreseeable conventional
computer systems. Specifically, there are computational applications whose
complexity scales super-linearly, even exponentially, with the size of their
input data such that the computation time or memory requirements for these
problems become intractably large to solve for useful data input sizes. Such
problems can have memory requirements that exceed what can be built on the most
powerful supercomputers, and/or runtimes on the order of tens of years or more.
Quantum computing (QC) is viewed by many as a possible future option for
tackling these high-complexity or seemingly-intractable problems by
complementing classical computing with a fundamentally different compute
paradigm.
",Emerging Technologies (cs.ET),; Quantum Physics (quant-ph),"['EmergingTechnologies(cs.ET)', 'QuantumPhysics(quant-ph)']","['cs.ET', 'quant-ph']"
Simulating quantum computers with probabilistic methods,"  We investigate the boundary between classical and quantum computational
power. This work consists of two parts. First we develop new classical
simulation algorithms that are centered on sampling methods. Using these
techniques we generate new classes of classically simulatable quantum circuits
where standard techniques relying on the exact computation of measurement
probabilities fail to provide efficient simulations. For example, we show how
various concatenations of matchgate, Toffoli, Clifford, bounded-depth, Fourier
transform and other circuits are classically simulatable. We also prove that
sparse quantum circuits as well as circuits composed of CNOT and exp[iaX] gates
can be simulated classically. In a second part, we apply our results to the
simulation of quantum algorithms. It is shown that a recent quantum algorithm,
concerned with the estimation of Potts model partition functions, can be
simulated efficiently classically. Finally, we show that the exponential
speed-ups of Simon's and Shor's algorithms crucially depend on the very last
stage in these algorithms, dealing with the classical postprocessing of the
measurement outcomes. Specifically, we prove that both algorithms would be
classically simulatable if the function classically computed in this step had a
sufficiently peaked Fourier spectrum.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Image recognition with an adiabatic quantum computer I. Mapping to quadratic unconstrained binary optimization,"  Many artificial intelligence (AI) problems naturally map to NP-hard
optimization problems. This has the interesting consequence that enabling
human-level capability in machines often requires systems that can handle
formally intractable problems. This issue can sometimes (but possibly not
always) be resolved by building special-purpose heuristic algorithms, tailored
to the problem in question. Because of the continued difficulties in automating
certain tasks that are natural for humans, there remains a strong motivation
for AI researchers to investigate and apply new algorithms and techniques to
hard AI problems. Recently a novel class of relevant algorithms that require
quantum mechanical hardware have been proposed. These algorithms, referred to
as quantum adiabatic algorithms, represent a new approach to designing both
complete and heuristic solvers for NP-hard optimization problems. In this work
we describe how to formulate image recognition, which is a canonical NP-hard AI
problem, as a Quadratic Unconstrained Binary Optimization (QUBO) problem. The
QUBO format corresponds to the input format required for D-Wave superconducting
adiabatic quantum computing (AQC) processors.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks,"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.

    ",Human-Computer Interaction (cs.HC),; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML),"['Human-ComputerInteraction(cs.HC)', 'MachineLearning(cs.LG)', 'NeuronsandCognition(q-bio.NC)', 'MachineLearning(stat.ML)']","['cs.HC', 'cs.LG', 'q-bio.NC', 'stat.ML']"
FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning,"  The computational demands of computer vision tasks based on state-of-the-art
Convolutional Neural Network (CNN) image classification far exceed the energy
budgets of mobile devices. This paper proposes FixyNN, which consists of a
fixed-weight feature extractor that generates ubiquitous CNN features, and a
conventional programmable CNN accelerator which processes a dataset-specific
CNN. Image classification models for FixyNN are trained end-to-end via transfer
learning, with the common feature extractor representing the transfered part,
and the programmable part being learnt on the target dataset. Experimental
results demonstrate FixyNN hardware can achieve very high energy efficiencies
up to 26.6 TOPS/W ($4.81 \times$ better than iso-area programmable
accelerator). Over a suite of six datasets we trained models via transfer
learning with an accuracy loss of $<1\%$ resulting in up to 11.2 TOPS/W -
nearly $2 \times$ more efficient than a conventional programmable CNN
accelerator of the same area.

    ",Computer Vision and Pattern Recognition (cs.CV),; Hardware Architecture (cs.AR); Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'HardwareArchitecture(cs.AR)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.AR', 'cs.LG', 'stat.ML']"
Unity Perception: Generate Synthetic Data for Computer Vision,"  We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Dynamical mean field theory algorithm and experiment on quantum computers,"  The developments of quantum computing algorithms and experiments for atomic
scale simulations have largely focused on quantum chemistry for molecules,
while their application in condensed matter systems is scarcely explored. Here
we present a quantum algorithm to perform dynamical mean field theory (DMFT)
calculations for condensed matter systems on currently available quantum
computers, and demonstrate it on two quantum hardware platforms. DMFT is
required to properly describe the large class of materials with strongly
correlated electrons. The computationally challenging part arises from solving
the effective problem of an interacting impurity coupled to a bath, which
scales exponentially with system size on conventional computers. An exponential
speedup is expected on quantum computers, but the algorithms proposed so far
are based on real time evolution of the wavefunction, which requires high-depth
circuits and hence very low noise levels in the quantum hardware. Here we
propose an alternative approach, which uses the variational quantum eigensolver
(VQE) method for ground and excited states to obtain the needed quantities as
part of an exact diagonalization impurity solver. We present the algorithm for
a two site DMFT system, which we benchmark using simulations on conventional
computers as well as experiments on superconducting and trapped ion qubits,
demonstrating that this method is suitable for running DMFT calculations on
currently available quantum hardware.

    ",Quantum Physics (quant-ph),; Strongly Correlated Electrons (cond-mat.str-el),"['QuantumPhysics(quant-ph)', 'StronglyCorrelatedElectrons(cond-mat.str-el)']","['quant-ph', 'cond-mat.str-el']"
Computer Stereo Vision for Autonomous Driving,"  As an important component of autonomous systems, autonomous car perception
has had a big leap with recent advances in parallel computing architectures.
With the use of tiny but full-feature embedded supercomputers, computer stereo
vision has been prevalently applied in autonomous cars for depth perception.
The two key aspects of computer stereo vision are speed and accuracy. They are
both desirable but conflicting properties, as the algorithms with better
disparity accuracy usually have higher computational complexity. Therefore, the
main aim of developing a computer stereo vision algorithm for resource-limited
hardware is to improve the trade-off between speed and accuracy. In this
chapter, we introduce both the hardware and software aspects of computer stereo
vision for autonomous car systems. Then, we discuss four autonomous car
perception tasks, including 1) visual feature detection, description and
matching, 2) 3D information acquisition, 3) object detection/recognition and 4)
semantic image segmentation. The principles of computer stereo vision and
parallel computing on multi-threading CPU and GPU architectures are then
detailed.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Introducing Cadabra: a symbolic computer algebra system for field theory problems,"  Cadabra is a new computer algebra system designed specifically for the
solution of problems encountered in field theory. It has extensive
functionality for tensor polynomial simplification taking care of Bianchi and
Schouten identities, for fermions and anti-commuting variables, Clifford
algebras and Fierz transformations, implicit coordinate dependence, multiple
index types and many other field theory related concepts. The input format is a
subset of TeX and thus easy to learn. Both a command-line and a graphical
interface are available. The present paper is an introduction to the program
using several concrete problems from gravity, supergravity and quantum field
theory.

    ",High Energy Physics - Theory (hep-th),; General Relativity and Quantum Cosmology (gr-qc); High Energy Physics - Phenomenology (hep-ph),"['HighEnergyPhysics-Theory(hep-th)', 'GeneralRelativityandQuantumCosmology(gr-qc)', 'HighEnergyPhysics-Phenomenology(hep-ph)']","['hep-th', 'gr-qc', 'hep-ph']"
A Real-time Hand Gesture Recognition and Human-Computer Interaction System,"  In this project, we design a real-time human-computer interaction system
based on hand gesture. The whole system consists of three components: hand
detection, gesture recognition and human-computer interaction (HCI) based on
recognition; and realizes the robust control of mouse and keyboard events with
a higher accuracy of gesture recognition. Specifically, we use the
convolutional neural network (CNN) to recognize gestures and makes it
attainable to identify relatively complex gestures using only one cheap
monocular camera. We introduce the Kalman filter to estimate the hand position
based on which the mouse cursor control is realized in a stable and smooth way.
During the HCI stage, we develop a simple strategy to avoid the false
recognition caused by noises - mostly transient, false gestures, and thus to
improve the reliability of interaction. The developed system is highly
extendable and can be used in human-robotic or other human-machine interaction
scenarios with more complex command formats rather than just mouse and keyboard
events.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Thumbs up? Sentiment Classification using Machine Learning Techniques,"  We consider the problem of classifying documents not by topic, but by overall
sentiment, e.g., determining whether a review is positive or negative. Using
movie reviews as data, we find that standard machine learning techniques
definitively outperform human-produced baselines. However, the three machine
learning methods we employed (Naive Bayes, maximum entropy classification, and
support vector machines) do not perform as well on sentiment classification as
on traditional topic-based categorization. We conclude by examining factors
that make the sentiment classification problem more challenging.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.LG']"
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"  MXNet is a multi-language machine learning (ML) library to ease the
development of ML algorithms, especially for deep neural networks. Embedded in
the host language, it blends declarative symbolic expression with imperative
tensor computation. It offers auto differentiation to derive gradients. MXNet
is computation and memory efficient and runs on various heterogeneous systems,
ranging from mobile devices to distributed GPU clusters.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG); Mathematical Software (cs.MS); Neural and Evolutionary Computing (cs.NE),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)', 'NeuralandEvolutionaryComputing(cs.NE)']","['cs.DC', 'cs.LG', 'cs.MS', 'cs.NE']"
API design for machine learning software: experiences from the scikit-learn project,"  Scikit-learn is an increasingly popular machine learning li- brary. Written
in Python, it is designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this paper, we present and
discuss our design choices for the application programming interface (API) of
the project. In particular, we describe the simple and elegant interface shared
by all learning and processing units in the library and then discuss its
advantages in terms of composition and reusability. The paper also comments on
implementation details specific to the Python ecosystem and analyzes obstacles
faced by users and developers of the library.

    ",Machine Learning (cs.LG),; Mathematical Software (cs.MS),"['MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['cs.LG', 'cs.MS']"
Foolbox: A Python toolbox to benchmark the robustness of machine learning models,"  Even todays most advanced machine learning models are easily fooled by almost
imperceptible perturbations of their inputs. Foolbox is a new Python package to
generate such adversarial perturbations and to quantify and compare the
robustness of machine learning models. It is build around the idea that the
most comparable robustness measure is the minimum perturbation needed to craft
an adversarial example. To this end, Foolbox provides reference implementations
of most published adversarial attack methods alongside some new ones, all of
which perform internal hyperparameter tuning to find the minimum adversarial
perturbation. Additionally, Foolbox interfaces with most popular deep learning
frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows
different adversarial criteria such as targeted misclassification and top-k
misclassification as well as different distance measures. The code is licensed
under the MIT license and is openly available at
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"  TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands of
computational devices such as GPU cards. The system is flexible and can be used
to express a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been used for conducting
research and for deploying machine learning systems into production across more
than a dozen areas of computer science and other fields, including speech
recognition, computer vision, robotics, information retrieval, natural language
processing, geographic information extraction, and computational drug
discovery. This paper describes the TensorFlow interface and an implementation
of that interface that we have built at Google. The TensorFlow API and a
reference implementation were released as an open-source package under the
Apache 2.0 license in November, 2015 and are available at ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Taking Human out of Learning Applications: A Survey on Automated Machine Learning,"  Machine learning techniques have deeply rooted in our everyday life. However,
since it is knowledge- and labor-intensive to pursue good learning performance,
human experts are heavily involved in every aspect of machine learning. In
order to make machine learning techniques easier to apply and reduce the demand
for experienced human experts, automated machine learning (AutoML) has emerged
as a hot topic with both industrial and academic interest. In this paper, we
provide an up to date survey on AutoML. First, we introduce and define the
AutoML problem, with inspiration from both realms of automation and machine
learning. Then, we propose a general AutoML framework that not only covers most
existing approaches to date but also can guide the design for new methods.
Subsequently, we categorize and review the existing works from two aspects,
i.e., the problem setup and the employed techniques. Finally, we provide a
detailed analysis of AutoML approaches and explain the reasons underneath their
successful applications. We hope this survey can serve as not only an
insightful guideline for AutoML beginners but also an inspiration for future
research.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.AI', 'cs.LG', 'stat.ML']"
Universal Differential Equations for Scientific Machine Learning,"  In the context of science, the well-known adage ""a picture is worth a
thousand words"" might well be ""a model is worth a thousand datasets."" In this
manuscript we introduce the SciML software ecosystem as a tool for mixing the
information of physical laws and scientific models with data-driven machine
learning approaches. We describe a mathematical object, which we denote
universal differential equations (UDEs), as the unifying framework connecting
the ecosystem. We show how a wide variety of applications, from automatically
discovering biological mechanisms to solving high-dimensional
Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled
through the UDE formalism and its tooling. We demonstrate the generality of the
software tooling to handle stochasticity, delays, and implicit constraints.
This funnels the wide variety of SciML applications into a core set of training
mechanisms which are highly optimized, stabilized for stiff equations, and
compatible with distributed parallelism and GPU accelerators.

    ",Machine Learning (cs.LG),; Dynamical Systems (math.DS); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'DynamicalSystems(math.DS)', 'QuantitativeMethods(q-bio.QM)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.DS', 'q-bio.QM', 'stat.ML']"
Distributed GraphLab: A Framework for Machine Learning in the Cloud,"  While high-level data parallel frameworks, like MapReduce, simplify the
design and implementation of large-scale data processing systems, they do not
naturally or efficiently support many important data mining and machine
learning algorithms and can lead to inefficient learning systems. To help fill
this critical void, we introduced the GraphLab abstraction which naturally
expresses asynchronous, dynamic, graph-parallel computation while ensuring data
consistency and achieving a high degree of parallel performance in the
shared-memory setting. In this paper, we extend the GraphLab framework to the
substantially more challenging distributed setting while preserving strong data
consistency guarantees. We develop graph based extensions to pipelined locking
and data versioning to reduce network congestion and mitigate the effect of
network latency. We also introduce fault tolerance to the GraphLab abstraction
using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can
be easily implemented by exploiting the GraphLab abstraction itself. Finally,
we evaluate our distributed implementation of the GraphLab abstraction on a
large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains
over Hadoop-based implementations.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning,"  The nascent field of fair machine learning aims to ensure that decisions
guided by algorithms are equitable. Over the last several years, three formal
definitions of fairness have gained prominence: (1) anti-classification,
meaning that protected attributes---like race, gender, and their proxies---are
not explicitly used to make decisions; (2) classification parity, meaning that
common measures of predictive performance (e.g., false positive and false
negative rates) are equal across groups defined by the protected attributes;
and (3) calibration, meaning that conditional on risk estimates, outcomes are
independent of protected attributes. Here we show that all three of these
fairness definitions suffer from significant statistical limitations. Requiring
anti-classification or classification parity can, perversely, harm the very
groups they were designed to protect; and calibration, though generally
desirable, provides little guarantee that decisions are equitable. In contrast
to these formal fairness criteria, we argue that it is often preferable to
treat similarly risky people similarly, based on the most statistically
accurate estimates of risk that one can produce. Such a strategy, while not
universally applicable, often aligns well with policy objectives; notably, this
strategy will typically violate both anti-classification and classification
parity. In practice, it requires significant effort to construct suitable risk
estimates. One must carefully define and measure the targets of prediction to
avoid retrenching biases in the data. But, importantly, one cannot generally
address these difficulties by requiring that algorithms satisfy popular
mathematical formalizations of fairness. By highlighting these challenges in
the foundation of fair machine learning, we hope to help researchers and
practitioners productively advance the area.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning","  The correct use of model evaluation, model selection, and algorithm selection
techniques is vital in academic machine learning research as well as in many
industrial settings. This article reviews different techniques that can be used
for each of these three subtasks and discusses the main advantages and
disadvantages of each technique with references to theoretical and empirical
studies. Further, recommendations are given to encourage best yet feasible
practices in research and applications of machine learning. Common methods such
as the holdout method for model evaluation and selection are covered, which are
not recommended when working with small datasets. Different flavors of the
bootstrap technique are introduced for estimating the uncertainty of
performance estimates, as an alternative to confidence intervals via normal
approximation if bootstrapping is computationally feasible. Common
cross-validation techniques such as leave-one-out cross-validation and k-fold
cross-validation are reviewed, the bias-variance trade-off for choosing k is
discussed, and practical tips for the optimal choice of k are given based on
empirical evidence. Different statistical tests for algorithm comparisons are
presented, and strategies for dealing with multiple comparisons such as omnibus
tests and multiple-comparison corrections are discussed. Finally, alternative
methods for algorithm selection, such as the combined F-test 5x2
cross-validation and nested cross-validation, are recommended for comparing
machine learning algorithms when datasets are small.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Coronavirus (COVID-19) Classification using CT Images by Machine Learning Methods,"  This study presents early phase detection of Coronavirus (COVID-19), which is
named by World Health Organization (WHO), by machine learning methods. The
detection process was implemented on abdominal Computed Tomography (CT) images.
The expert radiologists detected from CT images that COVID-19 shows different
behaviours from other viral pneumonia. Therefore, the clinical experts specify
that COVD-19 virus needs to be diagnosed in early phase. For detection of
the COVID-19, four different datasets were formed by taking patches sized as
16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process
was applied to patches to increase the classification performance. Grey Level
Co-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run
Length Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete
Wavelet Transform (DWT) algorithms were used as feature extraction methods.
Support Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold
and 10-fold cross-validations were implemented during the classification
process. Sensitivity, specificity, accuracy, precision, and F-score metrics
were used to evaluate the classification performance. The best classification
accuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature
extraction method.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'ImageandVideoProcessing(eess.IV)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
InterpretML: A Unified Framework for Machine Learning Interpretability,"  InterpretML is an open-source Python package which exposes machine learning
interpretability algorithms to practitioners and researchers. InterpretML
exposes two types of interpretability - glassbox models, which are machine
learning models designed for interpretability (ex: linear models, rule lists,
generalized additive models), and blackbox explainability techniques for
explaining existing systems (ex: Partial Dependence, LIME). The package enables
practitioners to easily compare interpretability algorithms by exposing
multiple methods under a unified API, and by having a built-in, extensible
visualization platform. InterpretML also includes the first implementation of
the Explainable Boosting Machine, a powerful, interpretable, glassbox model
that can be as accurate as many blackbox models. The MIT licensed source code
can be downloaded from ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
GraphLab: A New Framework For Parallel Machine Learning,"  Designing and implementing efficient, provably correct parallel machine
learning (ML) algorithms is challenging. Existing high-level parallel
abstractions like MapReduce are insufficiently expressive while low-level tools
like MPI and Pthreads leave ML experts repeatedly solving the same design
challenges. By targeting common patterns in ML, we developed GraphLab, which
improves upon abstractions like MapReduce by compactly expressing asynchronous
iterative algorithms with sparse computational dependencies while ensuring data
consistency and achieving a high degree of parallel performance. We demonstrate
the expressiveness of the GraphLab framework by designing and implementing
parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and
Compressed Sensing. We show that using GraphLab we can achieve excellent
parallel performance on large scale real-world problems.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.LG', 'cs.DC']"
Adversarial Machine Learning at Scale,"  Adversarial examples are malicious inputs designed to fool machine learning
models. They often transfer from one model to another, allowing attackers to
mount black box attacks without knowledge of the target model's parameters.
Adversarial training is the process of explicitly training a model on
adversarial examples, in order to make it more robust to attack or to reduce
its test error on clean inputs. So far, adversarial training has primarily been
applied to small problems. In this research, we apply adversarial training to
ImageNet. Our contributions include: (1) recommendations for how to succesfully
scale adversarial training to large models and datasets, (2) the observation
that adversarial training confers robustness to single-step attack methods, (3)
the finding that multi-step attack methods are somewhat less transferable than
single-step attack methods, so single-step attacks are the best for mounting
black-box attacks, and (4) resolution of a ""label leaking"" effect that causes
adversarially trained models to perform better on adversarial examples than on
clean examples, because the adversarial example construction process uses the
true label and the model can learn to exploit regularities in the construction
process.

    ",Computer Vision and Pattern Recognition (cs.CV),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.CR', 'cs.LG', 'stat.ML']"
"Interpretable machine learning: definitions, methods, and applications","  Machine-learning models have demonstrated great success in learning complex
patterns that enable them to make predictions about unobserved data. In
addition to using models for prediction, the ability to interpret what a model
has learned is receiving an increasing amount of attention. However, this
increased focus has led to considerable confusion about the notion of
interpretability. In particular, it is unclear how the wide array of proposed
interpretation methods are related, and what common concepts can be used to
evaluate them.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'Applications(stat.AP)']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP']"
Pylearn2: a machine learning research library,"  Pylearn2 is a machine learning research library. This does not just mean that
it is a collection of machine learning algorithms that share a common API; it
means that it has been designed for flexibility and extensibility in order to
facilitate research projects that involve new or unusual use cases. In this
paper we give a brief history of the library, an overview of its basic
philosophy, a summary of the library's architecture, and a description of how
the Pylearn2 community functions socially.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Mathematical Software (cs.MS),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['stat.ML', 'cs.LG', 'cs.MS']"
The Frontiers of Fairness in Machine Learning,"  The last few years have seen an explosion of academic and popular interest in
algorithmic fairness. Despite this interest and the volume and velocity of work
that has been produced recently, the fundamental science of fairness in machine
learning is still in a nascent state. In March 2018, we convened a group of
experts as part of a CCC visioning workshop to assess the state of the field,
and distill the most promising research directions going forward. This report
summarizes the findings of that workshop. Along the way, it surveys recent
theoretical work in the field and points towards promising directions for
research.

    ",Machine Learning (cs.LG),; Data Structures and Algorithms (cs.DS); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'DataStructuresandAlgorithms(cs.DS)', 'ComputerScienceandGameTheory(cs.GT)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DS', 'cs.GT', 'stat.ML']"
Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data,"  On-device machine learning (ML) enables the training process to exploit a
massive amount of user-generated private data samples. To enjoy this benefit,
inter-device communication overhead should be minimized. With this end, we
propose federated distillation (FD), a distributed model training algorithm
whose communication payload size is much smaller than a benchmark scheme,
federated learning (FL), particularly when the model size is large. Moreover,
user-generated data samples are likely to become non-IID across devices, which
commonly degrades the performance compared to the case with an IID dataset. To
cope with this, we propose federated augmentation (FAug), where each device
collectively trains a generative model, and thereby augments its local data
towards yielding an IID dataset. Empirical studies demonstrate that FD with
FAug yields around 26x less communication overhead while achieving 95-98% test
accuracy compared to FL.

    ",Machine Learning (cs.LG),; Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.NI', 'stat.ML']"
ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models,"  Machine learning (ML) has become a core component of many real-world
applications and training data is a key factor that drives current progress.
This huge success has led Internet companies to deploy machine learning as a
service (MLaaS). Recently, the first membership inference attack has shown that
extraction of information on the training set is possible in such MLaaS
settings, which has severe security and privacy implications.
",Cryptography and Security (cs.CR),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.AI', 'cs.LG']"
BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,"  Deep learning-based techniques have achieved state-of-the-art performance on
a wide variety of recognition and classification tasks. However, these networks
are typically computationally expensive to train, requiring weeks of
computation on many GPUs; as a result, many users outsource the training
procedure to the cloud or rely on pre-trained models that are then fine-tuned
for a specific task. In this paper we show that outsourced training introduces
new security risks: an adversary can create a maliciously trained network (a
backdoored neural network, or a \emph{BadNet}) that has state-of-the-art
performance on the user's training and validation samples, but behaves badly on
specific attacker-chosen inputs. We first explore the properties of BadNets in
a toy example, by creating a backdoored handwritten digit classifier. Next, we
demonstrate backdoors in a more realistic scenario by creating a U.S. street
sign classifier that identifies stop signs as speed limits when a special
sticker is added to the stop sign; we then show in addition that the backdoor
in our US street sign detector can persist even if the network is later
retrained for another task and cause a drop in accuracy of {25}\% on average
when the backdoor trigger is present. These results demonstrate that backdoors
in neural networks are both powerful and---because the behavior of neural
networks is difficult to explicate---stealthy. This work provides motivation
for further research into techniques for verifying and inspecting neural
networks, just as we have developed tools for verifying and debugging software.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"  We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.AI', 'cs.LG']"
Federated Optimization: Distributed Machine Learning for On-Device Intelligence,"  We introduce a new and increasingly relevant setting for distributed
optimization in machine learning, where the data defining the optimization are
unevenly distributed over an extremely large number of nodes. The goal is to
train a high-quality centralized model. We refer to this setting as Federated
Optimization. In this setting, communication efficiency is of the utmost
importance and minimizing the number of rounds of communication is the
principal goal.
",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Machine Learning that Matters,"  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution,"  Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
"Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge","  Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic
core, active and non-enhancing core. This intrinsic heterogeneity is also
portrayed in their radio-phenotype, as their sub-regions are depicted by
varying intensity profiles disseminated across multi-parametric magnetic
resonance imaging (mpMRI) scans, reflecting varying biological properties.
Their heterogeneous shape, extent, and location are some of the factors that
make these tumors difficult to resect, and in some cases inoperable. The amount
of resected tumor is a factor also considered in longitudinal scans, when
evaluating the apparent tumor for potential diagnosis of progression.
Furthermore, there is mounting evidence that accurate segmentation of the
various tumor sub-regions can offer the basis for quantitative image analysis
towards prediction of patient overall survival. This study assesses the
state-of-the-art machine learning (ML) methods used for brain tumor image
analysis in mpMRI scans, during the last seven instances of the International
Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we
focus on i) evaluating segmentations of the various glioma sub-regions in
pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue
of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO
criteria, and iii) predicting the overall survival from pre-operative mpMRI
scans of patients that underwent gross total resection. Finally, we investigate
the challenge of identifying the best ML algorithms for each of these tasks,
considering that apart from being diverse on each instance of the challenge,
the multi-institutional mpMRI BraTS dataset has also been a continuously
evolving/growing dataset.

    ",Computer Vision and Pattern Recognition (cs.CV),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']"
Quantifying the Carbon Emissions of Machine Learning,"  From an environmental standpoint, there are a few crucial aspects of training
a neural network that have a major impact on the quantity of carbon that it
emits. These factors include: the location of the server used for training and
the energy grid that it uses, the length of the training procedure, and even
the make and model of hardware on which the training takes place. In order to
approximate these emissions, we present our Machine Learning Emissions
Calculator, a tool for our community to better understand the environmental
impact of training ML models. We accompany this tool with an explanation of the
factors cited above, as well as concrete actions that individual practitioners
and organizations can take to mitigate their carbon emissions.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
Fairness in Machine Learning: A Survey,"  As Machine Learning technologies become increasingly used in contexts that
affect citizens, companies as well as researchers need to be confident that
their application of these methods will not have unexpected social
implications, such as bias towards gender, ethnicity, and/or people with
disabilities. There is significant literature on approaches to mitigate bias
and promote fairness, yet the area is complex and hard to penetrate for
newcomers to the domain. This article seeks to provide an overview of the
different schools of thought and approaches to mitigating (social) biases and
increase fairness in the Machine Learning literature. It organises approaches
into the widely accepted framework of pre-processing, in-processing, and
post-processing methods, subcategorizing into a further 11 method areas.
Although much of the literature emphasizes binary classification, a discussion
of fairness in regression, recommender systems, unsupervised learning, and
natural language processing is also provided along with a selection of
currently available open source libraries. The article concludes by summarising
open challenges articulated as four dilemmas for fairness research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning,"  Current advances in Artificial Intelligence and machine learning in general,
and deep learning in particular have reached unprecedented impact not only
across research communities, but also over popular media channels. However,
concerns about interpretability and accountability of AI have been raised by
influential thinkers. In spite of the recent impact of AI, several works have
identified the need for principled knowledge representation and reasoning
mechanisms integrated with deep learning-based systems to provide sound and
explainable models for such systems. Neural-symbolic computing aims at
integrating, as foreseen by Valiant, two most fundamental cognitive abilities:
the ability to learn from the environment, and the ability to reason from what
has been learned. Neural-symbolic computing has been an active topic of
research for many years, reconciling the advantages of robust learning in
neural networks and reasoning and interpretability of symbolic representation.
In this paper, we survey recent accomplishments of neural-symbolic computing as
a principled methodology for integrated machine learning and reasoning. We
illustrate the effectiveness of the approach by outlining the main
characteristics of the methodology: principled integration of neural learning
with symbolic knowledge representation and reasoning allowing for the
construction of explainable AI systems. The insights provided by
neural-symbolic computing shed new light on the increasingly prominent need for
interpretable and accountable AI systems.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Towards the Science of Security and Privacy in Machine Learning,"  Advances in machine learning (ML) in recent years have enabled a dizzying
array of applications such as data analytics, autonomous systems, and security
diagnostics. ML is now pervasive---new systems and models are being deployed in
every domain imaginable, leading to rapid and widespread deployment of software
based inference and decision making. There is growing recognition that ML
exposes new vulnerabilities in software systems, yet the technical community's
understanding of the nature and extent of these vulnerabilities remains
limited. We systematize recent findings on ML security and privacy, focusing on
attacks identified on these systems and defenses crafted to date. We articulate
a comprehensive threat model for ML, and categorize attacks and defenses within
an adversarial framework. Key insights resulting from works both in the ML and
security communities are identified and the effectiveness of approaches are
related to structural elements of ML algorithms and the data used to train
them. We conclude by formally exploring the opposing relationship between model
accuracy and resilience to adversarial manipulation. Through these
explorations, we show that there are (possibly unavoidable) tensions between
model complexity, accuracy, and resilience that must be calibrated for the
environments in which they will be used.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
"Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology","  Performance metrics (error measures) are vital components of the evaluation
frameworks in various fields. The intention of this study was to overview of a
variety of performance metrics and approaches to their classification. The main
goal of the study was to develop a typology that will help to improve our
knowledge and understanding of metrics and facilitate their selection in
machine learning regression, forecasting and prognostics. Based on the analysis
of the structure of numerous performance metrics, we propose a framework of
metrics which includes four (4) categories: primary metrics, extended metrics,
composite metrics, and hybrid sets of metrics. The paper identified three (3)
key components (dimensions) that determine the structure and properties of
primary metrics: method of determining point distance, method of normalization,
method of aggregation of point distances over a data set.

    ",Methodology (stat.ME),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['Methodology(stat.ME)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['stat.ME', 'cs.LG', 'stat.ML']"
Opportunities and Challenges for Machine Learning in Materials Science,"  Advances in machine learning have impacted myriad areas of materials science,
ranging from the discovery of novel materials to the improvement of molecular
simulations, with likely many more important developments to come. Given the
rapid changes in this field, it is challenging to understand both the breadth
of opportunities as well as best practices for their use. In this review, we
address aspects of both problems by providing an overview of the areas where
machine learning has recently had significant impact in materials science, and
then provide a more detailed discussion on determining the accuracy and domain
of applicability of some common types of machine learning models. Finally, we
discuss some opportunities and challenges for the materials community to fully
utilize the capabilities of machine learning.

    ",Materials Science (cond-mat.mtrl-sci),; Computational Physics (physics.comp-ph),"['MaterialsScience(cond-mat.mtrl-sci)', 'ComputationalPhysics(physics.comp-ph)']","['cond-mat.mtrl-sci', 'physics.comp-ph']"
TensorFlow Quantum: A Software Framework for Quantum Machine Learning,"  We introduce TensorFlow Quantum (TFQ), an open source library for the rapid
prototyping of hybrid quantum-classical models for classical or quantum data.
This framework offers high-level abstractions for the design and training of
both discriminative and generative quantum models under TensorFlow and supports
high-performance quantum circuit simulators. We provide an overview of the
software architecture and building blocks through several examples and review
the theory of hybrid quantum-classical neural networks. We illustrate TFQ
functionalities via several basic applications including supervised learning
for quantum classification, quantum control, simulating noisy quantum circuits,
and quantum approximate optimization. Moreover, we demonstrate how one can
apply TFQ to tackle advanced quantum learning tasks including meta-learning,
layerwise learning, Hamiltonian learning, sampling thermal states, variational
quantum eigensolvers, classification of quantum phase transitions, generative
adversarial networks, and reinforcement learning. We hope this framework
provides the necessary tools for the quantum computing and machine learning
research communities to explore models of both natural and artificial quantum
systems, and ultimately discover new quantum algorithms which could potentially
yield a quantum advantage.

    ",Quantum Physics (quant-ph),; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Programming Languages (cs.PL),"['QuantumPhysics(quant-ph)', 'DisorderedSystemsandNeuralNetworks(cond-mat.dis-nn)', 'MachineLearning(cs.LG)', 'ProgrammingLanguages(cs.PL)']","['quant-ph', 'cond-mat.dis-nn', 'cs.LG', 'cs.PL']"
Augmentor: An Image Augmentation Library for Machine Learning,"  The generation of artificial data based on existing observations, known as
data augmentation, is a technique used in machine learning to improve model
accuracy, generalisation, and to control overfitting. Augmentor is a software
package, available in both Python and Julia versions, that provides a high
level API for the expansion of image data using a stochastic, pipeline-based
approach which effectively allows for images to be sampled from a distribution
of augmented images at runtime. Augmentor provides methods for most standard
augmentation practices as well as several advanced features such as
label-preserving, randomised elastic distortions, and provides many helper
functions for typical augmentation tasks used in machine learning.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'stat.ML']"
FedML: A Research Library and Benchmark for Federated Machine Learning,"  Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models,"  This paper describes EMBER: a labeled benchmark dataset for training machine
learning models to statically detect malicious Windows portable executable
files. The dataset includes features extracted from 1.1M binary files: 900K
training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test
samples (100K malicious, 100K benign). To accompany the dataset, we also
release open source code for extracting features from additional binaries so
that additional sample features can be appended to the dataset. This dataset
fills a void in the information security machine learning community: a
benign/malicious dataset that is large, open and general enough to cover
several interesting use cases. We enumerate several use cases that we
considered when structuring the dataset. Additionally, we demonstrate one use
case wherein we compare a baseline gradient boosted decision tree model trained
using LightGBM with default settings to MalConv, a recently published
end-to-end (featureless) deep learning model for malware detection. Results
show that even without hyper-parameter optimization, the baseline EMBER model
outperforms MalConv. The authors hope that the dataset, code and baseline model
provided by EMBER will help invigorate machine learning research for malware
detection, in much the same way that benchmark datasets have advanced computer
vision research.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
Differential Privacy and Machine Learning: a Survey and Review,"  The objective of machine learning is to extract useful information from data,
while privacy is preserved by concealing information. Thus it seems hard to
reconcile these competing interests. However, they frequently must be balanced
when mining sensitive data. For example, medical research represents an
important application where it is necessary both to extract useful information
and protect patient privacy. One way to resolve the conflict is to extract
general characteristics of whole populations without disclosing the private
information of individuals.
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Databases (cs.DB),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'Databases(cs.DB)']","['cs.LG', 'cs.CR', 'cs.DB']"
Unsupervised Machine Learning on a Hybrid Quantum Computer,"  Machine learning techniques have led to broad adoption of a statistical model
of computing. The statistical distributions natively available on quantum
processors are a superset of those available classically. Harnessing this
attribute has the potential to accelerate or otherwise improve machine learning
relative to purely classical performance. A key challenge toward that goal is
learning to hybridize classical computing resources and traditional learning
techniques with the emerging capabilities of general purpose quantum
processors. Here, we demonstrate such hybridization by training a 19-qubit gate
model processor to solve a clustering problem, a foundational challenge in
unsupervised learning. We use the quantum approximate optimization algorithm in
conjunction with a gradient-free Bayesian optimization to train the quantum
machine. This quantum/classical hybrid algorithm shows robustness to realistic
noise, and we find evidence that classical optimization can be used to train
around both coherent and incoherent imperfections.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Malicious URL Detection using Machine Learning: A Survey,"  Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)']","['cs.LG', 'cs.CR']"
Quantum embeddings for machine learning,"  Quantum classifiers are trainable quantum circuits used as machine learning
models. The first part of the circuit implements a quantum feature map that
encodes classical inputs into quantum states, embedding the data in a
high-dimensional Hilbert space; the second part of the circuit executes a
quantum measurement interpreted as the output of the model. Usually, the
measurement is trained to distinguish quantum-embedded data. We propose to
instead train the first part of the circuit -- the embedding -- with the
objective of maximally separating data classes in Hilbert space, a strategy we
call quantum metric learning. As a result, the measurement minimizing a linear
classification loss is already known and depends on the metric used: for
embeddings separating data using the l1 or trace distance, this is the Helstrom
measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple
overlap measurement. This approach provides a powerful analytic framework for
quantum machine learning and eliminates a major component in current models,
freeing up more precious resources to best leverage the capabilities of
near-term quantum information processors.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
"A machine learning methodology for real-time forecasting of the 2019-2020 COVID-19 outbreak using Internet searches, news alerts, and estimates from mechanistic models","  We present a timely and novel methodology that combines disease estimates
from mechanistic models with digital traces, via interpretable machine-learning
methodologies, to reliably forecast COVID-19 activity in Chinese provinces in
real-time. Specifically, our method is able to produce stable and accurate
forecasts 2 days ahead of current time, and uses as inputs (a) official health
reports from Chinese Center Disease for Control and Prevention (China CDC), (b)
COVID-19-related internet search activity from Baidu, (c) news media activity
reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from
GLEAM, an agent-based mechanistic model. Our machine-learning methodology uses
a clustering technique that enables the exploitation of geo-spatial
synchronicities of COVID-19 activity across Chinese provinces, and a data
augmentation technique to deal with the small number of historical disease
activity observations, characteristic of emerging outbreaks. Our model's
predictive power outperforms a collection of baseline models in 27 out of the
32 Chinese provinces, and could be easily extended to other geographies
currently affected by the COVID-19 outbreak to help decision makers.

    ",Other Statistics (stat.OT),; Machine Learning (cs.LG); Populations and Evolution (q-bio.PE); Machine Learning (stat.ML),"['OtherStatistics(stat.OT)', 'MachineLearning(cs.LG)', 'PopulationsandEvolution(q-bio.PE)', 'MachineLearning(stat.ML)']","['stat.OT', 'cs.LG', 'q-bio.PE', 'stat.ML']"
A Differentiable Programming System to Bridge Machine Learning and Scientific Computing,"  Scientific computing is increasingly incorporating the advancements in
machine learning and the ability to work with large amounts of data. At the
same time, machine learning models are becoming increasingly sophisticated and
exhibit many features often seen in scientific computing, stressing the
capabilities of machine learning frameworks. Just as the disciplines of
scientific computing and machine learning have shared common underlying
infrastructure in the form of numerical linear algebra, we now have the
opportunity to further share new computational infrastructure, and thus ideas,
in the form of Differentiable Programming. We describe Zygote, a Differentiable
Programming system that is able to take gradients of general program
structures. We implement this system in the Julia programming language. Our
system supports almost all language constructs (control flow, recursion,
mutation, etc.) and compiles high-performance code without requiring any user
intervention or refactoring to stage computations. This enables an expressive
programming model for deep learning, but more importantly, it enables us to
incorporate a large ecosystem of libraries in our models in a straightforward
way. We discuss our approach to automatic differentiation, including its
support for advanced techniques such as mixed-mode, complex and checkpointed
differentiation, and present several examples of differentiating programs.

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG),"['ProgrammingLanguages(cs.PL)', 'MachineLearning(cs.LG)']","['cs.PL', 'cs.LG']"
Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,"  Deep learning models with convolutional and recurrent networks are now
ubiquitous and analyze massive amounts of audio, image, video, text and graph
data, with applications in automatic translation, speech-to-text, scene
understanding, ranking user preferences, ad placement, etc. Competing
frameworks for building these networks such as TensorFlow, Chainer, CNTK,
Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between
usability and expressiveness, research or production orientation and supported
hardware. They operate on a DAG of computational operators, wrapping
high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for
various CPUs), and automate memory allocation, synchronization, distribution.
Custom operators are needed where the computation does not fit existing
high-performance library calls, usually at a high engineering cost. This is
frequently required when new operators are invented by researchers: such
operators suffer a severe performance penalty, which limits the pace of
innovation. Furthermore, even if there is an existing runtime call these
frameworks can use, it often doesn't offer optimal performance for a user's
particular network architecture and dataset, missing optimizations between
operators as well as optimizations that can be done knowing the size and shape
of data. Our contributions include (1) a language close to the mathematics of
deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time
compiler to convert a mathematical description of a deep learning DAG into a
CUDA kernel with delegated memory management and synchronization, also
providing optimizations such as operator fusion and specialization for specific
sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG),"['ProgrammingLanguages(cs.PL)', 'MachineLearning(cs.LG)']","['cs.PL', 'cs.LG']"
6G White Paper on Machine Learning in Wireless Communication Networks,"  The focus of this white paper is on machine learning (ML) in wireless
communications. 6G wireless communication networks will be the backbone of the
digital transformation of societies by providing ubiquitous, reliable, and
near-instant wireless connectivity for humans and machines. Recent advances in
ML research has led enable a wide range of novel technologies such as
self-driving vehicles and voice assistants. Such innovation is possible as a
result of the availability of advanced ML models, large datasets, and high
computational power. On the other hand, the ever-increasing demand for
connectivity will require a lot of innovation in 6G wireless networks, and ML
tools will play a major role in solving problems in the wireless domain. In
this paper, we provide an overview of the vision of how ML will impact the
wireless communication systems. We first give an overview of the ML methods
that have the highest potential to be used in wireless networks. Then, we
discuss the problems that can be solved by using ML in various layers of the
network such as the physical layer, medium access layer, and application layer.
Zero-touch optimization of wireless networks using ML is another interesting
aspect that is discussed in this paper. Finally, at the end of each section,
important research questions that the section aims to answer are presented.

    ",Information Theory (cs.IT),; Signal Processing (eess.SP),"['InformationTheory(cs.IT)', 'SignalProcessing(eess.SP)']","['cs.IT', 'eess.SP']"
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"  Benchmark datasets have a significant impact on accelerating research in
programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark
dataset to foster machine learning research for program understanding and
generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and
a platform for model evaluation and comparison. CodeXGLUE also features three
baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder
models, to make it easy for researchers to use the platform. The availability
of such data and baselines can help the development and validation of new
methods that can be applied to various program understanding and generation
problems.

    ",Software Engineering (cs.SE),; Computation and Language (cs.CL),"['SoftwareEngineering(cs.SE)', 'ComputationandLanguage(cs.CL)']","['cs.SE', 'cs.CL']"
"Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination","  This paper covers the two approaches for sentiment analysis: i) lexicon based
method; ii) machine learning method. We describe several techniques to
implement these approaches and discuss how they can be adopted for sentiment
classification of Twitter messages. We present a comparative study of different
lexicon combinations and show that enhancing sentiment lexicons with emoticons,
abbreviations and social-media slang expressions increases the accuracy of
lexicon-based classification for Twitter. We discuss the importance of feature
generation and feature selection processes for machine learning sentiment
classification. To quantify the performance of the main sentiment analysis
methods over Twitter we run these algorithms on a benchmark Twitter dataset
from the SemEval-2013 competition, task 2-B. The results show that machine
learning method based on SVM and Naive Bayes classifiers outperforms the
lexicon method. We present a new ensemble method that uses a lexicon based
sentiment score as input feature for the machine learning approach. The
combined method proved to produce more precise classifications. We also show
that employing a cost-sensitive classifier for highly unbalanced datasets
yields an improvement of sentiment classification performance up to 7%.

    ",Computation and Language (cs.CL),; Information Retrieval (cs.IR); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML),"['ComputationandLanguage(cs.CL)', 'InformationRetrieval(cs.IR)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)', 'MachineLearning(stat.ML)']","['cs.CL', 'cs.IR', 'cs.LG', 'stat.ME', 'stat.ML']"
sktime: A Unified Interface for Machine Learning with Time Series,"  We present sktime -- a new scikit-learn compatible Python library with a
unified interface for machine learning with time series. Time series data gives
rise to various distinct but closely related learning tasks, such as
forecasting and time series classification, many of which can be solved by
reducing them to related simpler tasks. We discuss the main rationale for
creating a unified interface, including reduction, as well as the design of
sktime's core API, supported by a clear overview of common time series tasks
and reduction approaches.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Troubling Trends in Machine Learning Scholarship,"  Collectively, machine learning (ML) researchers are engaged in the creation
and dissemination of knowledge about data-driven algorithms. In a given paper,
researchers might aspire to any subset of the following goals, among others: to
theoretically characterize what is learnable, to obtain understanding through
empirically rigorous experiments, or to build a working system that has high
predictive accuracy. While determining which knowledge warrants inquiry may be
subjective, once the topic is fixed, papers are most valuable to the community
when they act in service of the reader, creating foundational knowledge and
communicating as clearly as possible.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.AI', 'cs.LG']"
Introduction to Tensor Decompositions and their Applications in Machine Learning,"  Tensors are multidimensional arrays of numerical values and therefore
generalize matrices to multiple dimensions. While tensors first emerged in the
psychometrics community in the $20^{\text{th}}$ century, they have since then
spread to numerous other disciplines, including machine learning. Tensors and
their decompositions are especially beneficial in unsupervised learning
settings, but are gaining popularity in other sub-disciplines like temporal and
multi-relational data analysis, too.
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Comparing BERT against traditional machine learning text classification,"  The BERT model has arisen as a popular state-of-the-art machine learning
model in the recent years that is able to cope with multiple NLP tasks such as
supervised text classification without human supervision. Its flexibility to
cope with any type of corpus delivering great results has make this approach
very popular not only in academia but also in the industry. Although, there are
lots of different approaches that have been used throughout the years with
success. In this work, we first present BERT and include a little review on
classical NLP approaches. Then, we empirically test with a suite of experiments
dealing different scenarios the behaviour of BERT against the traditional
TF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work
is to add empirical evidence to support or refuse the use of BERT as a default
on NLP tasks. Experiments show the superiority of BERT and its independence of
features of the NLP problem such as the language of the text adding empirical
evidence to use BERT as a default technique to be used in NLP problems.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CL', 'cs.LG', 'stat.ML']"
Machine Teaching: A New Paradigm for Building Machine Learning Systems,"  The current processes for building machine learning systems require
practitioners with deep knowledge of machine learning. This significantly
limits the number of machine learning systems that can be created and has led
to a mismatch between the demand for machine learning systems and the ability
for organizations to build them. We believe that in order to meet this growing
demand for machine learning systems we must significantly increase the number
of individuals that can teach machines. We postulate that we can achieve this
goal by making the process of teaching machines easy, fast and above all,
universally accessible.
",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'Human-ComputerInteraction(cs.HC)', 'SoftwareEngineering(cs.SE)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.HC', 'cs.SE', 'stat.ML']"
Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning,"  Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'stat.ML']"
Detection of Unauthorized IoT Devices Using Machine Learning Techniques,"  Security experts have demonstrated numerous risks imposed by Internet of
Things (IoT) devices on organizations. Due to the widespread adoption of such
devices, their diversity, standardization obstacles, and inherent mobility,
organizations require an intelligent mechanism capable of automatically
detecting suspicious IoT devices connected to their networks. In particular,
devices not included in a white list of trustworthy IoT device types (allowed
to be used within the organizational premises) should be detected. In this
research, Random Forest, a supervised machine learning algorithm, was applied
to features extracted from network traffic data with the aim of accurately
identifying IoT device types from the white list. To train and evaluate
multi-class classifiers, we collected and manually labeled network traffic data
from 17 distinct IoT devices, representing nine types of IoT devices. Based on
the classification of 20 consecutive sessions and the use of majority rule, IoT
device types that are not on the white list were correctly detected as unknown
in 96% of test cases (on average), and white listed device types were correctly
classified by their actual types in 99% of cases. Some IoT device types were
identified quicker than others (e.g., sockets and thermostats were successfully
detected within five TCP sessions of connecting to the network). Perfect
detection of unauthorized IoT device types was achieved upon analyzing 110
consecutive sessions; perfect classification of white listed types required 346
consecutive sessions, 110 of which resulted in 99.49% accuracy. Further
experiments demonstrated the successful applicability of classifiers trained in
one location and tested on another. In addition, a discussion is provided
regarding the resilience of our machine learning-based IoT white listing method
to adversarial attacks.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV),"['CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.CR', 'cs.CV']"
OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"  Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems,"  Several researchers have argued that a machine learning system's
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent's role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
On Formalizing Fairness in Prediction with Machine Learning,"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
bartMachine: Machine Learning with Bayesian Additive Regression Trees,"  We present a new package in R implementing Bayesian additive regression trees
(BART). The package introduces many new features for data analysis using BART
such as variable selection, interaction detection, model diagnostic plots,
incorporation of missing data and the ability to save trees for future
prediction. It is significantly faster than the current R implementation,
parallelized, and capable of handling both large sample sizes and
high-dimensional data.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Machine Learning for Precipitation Nowcasting from Radar Images,"  High-resolution nowcasting is an essential tool needed for effective
adaptation to climate change, particularly for extreme weather. As Deep
Learning (DL) techniques have shown dramatic promise in many domains, including
the geosciences, we present an application of DL to the problem of
precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1
hour) predictions of precipitation. We treat forecasting as an image-to-image
translation problem and leverage the power of the ubiquitous UNET convolutional
neural network. We find this performs favorably when compared to three commonly
used models: optical flow, persistence and NOAA's numerical one-hour HRRR
nowcasting prediction.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'stat.ML']"
Supervised quantum machine learning models are kernel methods,"  With near-term quantum devices available and the race for fault-tolerant
quantum computers in full swing, researchers became interested in the question
of what happens if we replace a supervised machine learning model with a
quantum circuit. While such ""quantum models"" are sometimes called ""quantum
neural networks"", it has been repeatedly noted that their mathematical
structure is actually much more closely related to kernel methods: they analyse
data in high-dimensional Hilbert spaces to which we only have access through
inner products revealed by measurements. This technical manuscript summarises
and extends the idea of systematically rephrasing supervised quantum models as
a kernel method. With this, a lot of near-term and fault-tolerant quantum
models can be replaced by a general support vector machine whose kernel
computes distances between data-encoding quantum states. Kernel-based training
is then guaranteed to find better or equally good quantum models than
variational circuit training. Overall, the kernel perspective of quantum
machine learning tells us that the way that data is encoded into quantum states
is the main ingredient that can potentially set quantum models apart from
classical machine learning models.

    ",Quantum Physics (quant-ph),; Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(stat.ML)']","['quant-ph', 'stat.ML']"
Unsupervised machine learning and band topology,"  The study of topological bandstructures is an active area of research in
condensed matter physics and beyond. Here, we combine recent progress in this
field with developments in machine-learning, another rising topic of interest.
Specifically, we introduce an unsupervised machine-learning approach that
searches for and retrieves paths of adiabatic deformations between
Hamiltonians, thereby clustering them according to their topological
properties. The algorithm is general as it does not rely on a specific
parameterization of the Hamiltonian and is readily applicable to any symmetry
class. We demonstrate the approach using several different models in both one
and two spatial dimensions and for different symmetry classes with and without
crystalline symmetries. Accordingly, it is also shown how trivial and
topological phases can be diagnosed upon comparing with a generally designated
set of trivial atomic insulators.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),; Strongly Correlated Electrons (cond-mat.str-el); Computational Physics (physics.comp-ph),"['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'ComputationalPhysics(physics.comp-ph)']","['cond-mat.mes-hall', 'cond-mat.str-el', 'physics.comp-ph']"
Certified Data Removal from Machine Learning Models,"  Good data stewardship requires removal of data at the request of the data's
owner. This raises the question if and how a trained machine-learning model,
which implicitly stores information about its training data, should be affected
by such a removal request. Is it possible to ""remove"" data from a
machine-learning model? We study this problem by defining certified removal: a
very strong theoretical guarantee that a model from which data is removed
cannot be distinguished from a model that never observed the data to begin
with. We develop a certified-removal mechanism for linear classifiers and
empirically study learning settings in which this mechanism is practical.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation,"  Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable in the specific context
of verification. Suppose we have a machine learning system that predicts X, and
we provide rationale for this prediction X. Given an input, an explanation, and
an output, is the output consistent with the input and the supposed rationale?
Via a series of user-studies, we identify what kinds of increases in complexity
have the greatest effect on the time it takes for humans to verify the
rationale, and which seem relatively insensitive.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Automated Machine Learning: State-of-The-Art and Open Challenges,"  With the continuous and vast increase in the amount of data in our digital
world, it has been acknowledged that the number of knowledgeable data
scientists can not scale to address these challenges. Thus, there was a crucial
need for automating the process of building good machine learning models. In
the last few years, several techniques and frameworks have been introduced to
tackle the challenge of automating the process of Combined Algorithm Selection
and Hyper-parameter tuning (CASH) in the machine learning domain. The main aim
of these techniques is to reduce the role of the human in the loop and fill the
gap for non-expert machine learning users by playing the role of the domain
expert.
",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Chiron: Privacy-preserving Machine Learning as a Service,"  Major cloud operators offer machine learning (ML) as a service, enabling
customers who have the data but not ML expertise or infrastructure to train
predictive models on this data. Existing ML-as-a-service platforms require
users to reveal all training data to the service operator. We design,
implement, and evaluate Chiron, a system for privacy-preserving machine
learning as a service. First, Chiron conceals the training data from the
service operator. Second, in keeping with how many existing ML-as-a-service
platforms work, Chiron reveals neither the training algorithm nor the model
structure to the user, providing only black-box access to the trained model.
Chiron is implemented using SGX enclaves, but SGX alone does not achieve the
dual goals of data privacy and model confidentiality. Chiron runs the standard
ML training toolchain (including the popular Theano framework and C compiler)
in an enclave, but the untrusted model-creation code from the service operator
is further confined in a Ryoan sandbox to prevent it from leaking the training
data outside the enclave. To support distributed training, Chiron executes
multiple concurrent enclaves that exchange model parameters via a parameter
server. We evaluate Chiron on popular deep learning models, focusing on
benchmark image classification tasks such as CIFAR and ImageNet, and show that
its training performance and accuracy of the resulting models are practical for
common uses of ML-as-a-service.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning,"  The incidence of malignant melanoma continues to increase worldwide. This
cancer can strike at any age; it is one of the leading causes of loss of life
in young persons. Since this cancer is visible on the skin, it is potentially
detectable at a very early stage when it is curable. New developments have
converged to make fully automatic early melanoma detection a real possibility.
First, the advent of dermoscopy has enabled a dramatic boost in clinical
diagnostic ability to the point that melanoma can be detected in the clinic at
the very earliest stages. The global adoption of this technology has allowed
accumulation of large collections of dermoscopy images of melanomas and benign
lesions validated by histopathology. The development of advanced technologies
in the areas of image processing and machine learning have given us the ability
to allow distinction of malignant melanoma from the many benign mimics that
require no biopsy. These new technologies should allow not only earlier
detection of melanoma, but also reduction of the large number of needless and
costly biopsy procedures. Although some of the new systems reported for these
technologies have shown promise in preliminary trials, widespread
implementation must await further technical progress in accuracy and
reproducibility. In this paper, we provide an overview of computerized
detection of melanoma in dermoscopy images. First, we discuss the various
aspects of lesion segmentation. Then, we provide a brief overview of clinical
feature segmentation. Finally, we discuss the classification stage where
machine learning algorithms are applied to the attributes generated from the
segmented features to predict the existence of melanoma.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(stat.ML)']","['cs.CV', 'stat.ML']"
Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning,"  In this paper, we propose an automated computer platform for the purpose of
classifying Electroencephalography (EEG) signals associated with left and right
hand movements using a hybrid system that uses advanced feature extraction
techniques and machine learning algorithms. It is known that EEG represents the
brain activity by the electrical voltage fluctuations along the scalp, and
Brain-Computer Interface (BCI) is a device that enables the use of the brain
neural activity to communicate with others or to control machines, artificial
limbs, or robots without direct physical movements. In our research work, we
aspired to find the best feature extraction method that enables the
differentiation between left and right executed fist movements through various
classification algorithms. The EEG dataset used in this research was created
and contributed to PhysioNet by the developers of the BCI2000 instrumentation
system. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts
removal was done using AAR. Data was epoched on the basis of Event-Related (De)
Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)
features. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta
rhythms were isolated for the MRCP analysis. The Independent Component Analysis
(ICA) spatial filter was applied on related channels for noise reduction and
isolation of both artifactually and neutrally generated EEG sources. The final
feature vector included the ERD, ERS, and MRCP features in addition to the
mean, power and energy of the activations of the resulting independent
components of the epoched feature datasets. The datasets were inputted into two
machine-learning algorithms: Neural Networks (NNs) and Support Vector Machines
(SVMs). Intensive experiments were carried out and optimum classification
performances of 89.8 and 97.1 were obtained using NN and SVM, respectively.

    ",Neural and Evolutionary Computing (cs.NE),; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC),"['NeuralandEvolutionaryComputing(cs.NE)', 'ComputerVisionandPatternRecognition(cs.CV)', 'Human-ComputerInteraction(cs.HC)']","['cs.NE', 'cs.CV', 'cs.HC']"
Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning,"  Machine learning is a popular approach to signatureless malware detection
because it can generalize to never-before-seen malware families and polymorphic
strains. This has resulted in its practical use for either primary detection
engines or for supplementary heuristic detection by anti-malware vendors.
Recent work in adversarial machine learning has shown that deep learning models
are susceptible to gradient-based attacks, whereas non-differentiable models
that report a score can be attacked by genetic algorithms that aim to
systematically reduce the score. We propose a more general framework based on
reinforcement learning (RL) for attacking static portable executable (PE)
anti-malware engines. The general framework does not require a differentiable
model nor does it require the engine to produce a score. Instead, an RL agent
is equipped with a set of functionality-preserving operations that it may
perform on the PE file. Through a series of games played against the
anti-malware engine, it learns which sequences of operations are likely to
result in evading the detector for any given malware sample. This enables
completely black-box attacks against static PE anti-malware, and produces
functional evasive malware samples as a direct result. We show in experiments
that our method can attack a gradient-boosted machine learning model with
evasion rates that are substantial and appear to be strongly dependent on the
dataset. We demonstrate that attacks against this model appear to also evade
components of publicly hosted antivirus engines. Adversarial training results
are also presented: by retraining the model on evasive ransomware samples, a
subsequent attack is 33% less effective. However, there are overfitting dangers
when adversarial training, which we note. We release code to allow researchers
to reproduce and improve this approach.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
BLAZE: Blazing Fast Privacy-Preserving Machine Learning,"  Machine learning tools have illustrated their potential in many significant
sectors such as healthcare and finance, to aide in deriving useful inferences.
The sensitive and confidential nature of the data, in such sectors, raise
natural concerns for the privacy of data. This motivated the area of
Privacy-preserving Machine Learning (PPML) where privacy of the data is
guaranteed. Typically, ML techniques require large computing power, which leads
clients with limited infrastructure to rely on the method of Secure Outsourced
Computation (SOC). In SOC setting, the computation is outsourced to a set of
specialized and powerful cloud servers and the service is availed on a
pay-per-use basis. In this work, we explore PPML techniques in the SOC setting
for widely used ML algorithms-- Linear Regression, Logistic Regression, and
Neural Networks.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"  In response to algorithmic unfairness embedded in sociotechnical systems,
significant attention has been focused on the contents of machine learning
datasets which have revealed biases towards white, cisgender, male, and Western
data subjects. In contrast, comparatively less attention has been paid to the
histories, values, and norms embedded in such datasets. In this work, we
outline a research program - a genealogy of machine learning data - for
investigating how and why these datasets have been created, what and whose
values influence the choices of data to collect, the contextual and contingent
conditions of their creation. We describe the ways in which benchmark datasets
in machine learning operate as infrastructure and pose four research questions
for these datasets. This interrogation forces us to ""bring the people back in""
by aiding us in understanding the labor embedded in dataset construction, and
thereby presenting new avenues of contestation for other researchers
encountering the data.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach,"  Toxic online content has become a major issue in today's world due to an
exponential increase in the use of internet by people of different cultures and
educational background. Differentiating hate speech and offensive language is a
key challenge in automatic detection of toxic text content. In this paper, we
propose an approach to automatically classify tweets on Twitter into three
classes: hateful, offensive and clean. Using Twitter dataset, we perform
experiments considering n-grams as features and passing their term
frequency-inverse document frequency (TFIDF) values to multiple machine
learning models. We perform comparative analysis of the models considering
several values of n in n-grams and TFIDF normalization methods. After tuning
the model giving the best results, we achieve 95.6% accuracy upon evaluating it
on test data. We also create a module which serves as an intermediate between
user and Twitter.

    ",Computation and Language (cs.CL),,['ComputationandLanguage(cs.CL)'],['cs.CL']
Participation is not a Design Fix for Machine Learning,"  This paper critically examines existing modes of participation in design
practice and machine learning. Cautioning against 'participation-washing', it
suggests that the ML community must become attuned to possibly exploitative and
extractive forms of community involvement and shift away from the prerogatives
of context-independent scalability.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
A Time Series Analysis-Based Stock Price Prediction Using Machine Learning and Deep Learning Models,"  Prediction of future movement of stock prices has always been a challenging
task for the researchers. While the advocates of the efficient market
hypothesis (EMH) believe that it is impossible to design any predictive
framework that can accurately predict the movement of stock prices, there are
seminal work in the literature that have clearly demonstrated that the
seemingly random movement patterns in the time series of a stock price can be
predicted with a high level of accuracy. Design of such predictive models
requires choice of appropriate variables, right transformation methods of the
variables, and tuning of the parameters of the models. In this work, we present
a very robust and accurate framework of stock price prediction that consists of
an agglomeration of statistical, machine learning and deep learning models. We
use the daily stock price data, collected at five minutes interval of time, of
a very well known company that is listed in the National Stock Exchange (NSE)
of India. The granular data is aggregated into three slots in a day, and the
aggregated data is used for building and training the forecasting models. We
contend that the agglomerative approach of model building that uses a
combination of statistical, machine learning, and deep learning approaches, can
very effectively learn from the volatile and random movement patterns in a
stock price data. We build eight classification and eight regression models
based on statistical and machine learning approaches. In addition to these
models, a deep learning regression model using a long-and-short-term memory
(LSTM) network is also built. Extensive results have been presented on the
performance of these models, and the results are critically analyzed.

    ",Statistical Finance (q-fin.ST),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['StatisticalFinance(q-fin.ST)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['q-fin.ST', 'cs.LG', 'stat.ML']"
TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games,"  We present TorchCraft, a library that enables deep learning research on
Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it
easier to control these games from a machine learning framework, here Torch.
This white paper argues for using RTS games as a benchmark for AI research, and
describes the design and components of TorchCraft.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Analysis of the COVID-19 pandemic by SIR model and machine learning technics for forecasting,"  This work is a trial in which we propose SIR model and machine learning tools
to analyze the coronavirus pandemic in the real world. Based on the public data
from \cite{datahub}, we estimate main key pandemic parameters and make
predictions on the inflection point and possible ending time for the real world
and specifically for Senegal. The coronavirus disease 2019, by World Health
Organization, rapidly spread out in the whole China and then in the whole
world. Under optimistic estimation, the pandemic in some countries will end
soon, while for most part of countries in the world (US, Italy, etc.), the hit
of anti-pandemic will be no later than the end of April.

    ",Populations and Evolution (q-bio.PE),; Optimization and Control (math.OC); Machine Learning (stat.ML),"['PopulationsandEvolution(q-bio.PE)', 'OptimizationandControl(math.OC)', 'MachineLearning(stat.ML)']","['q-bio.PE', 'math.OC', 'stat.ML']"
Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development,"  Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
",Machine Learning (cs.LG),; Computers and Society (cs.CY); Biomolecules (q-bio.BM); Quantitative Methods (q-bio.QM),"['MachineLearning(cs.LG)', 'ComputersandSociety(cs.CY)', 'Biomolecules(q-bio.BM)', 'QuantitativeMethods(q-bio.QM)']","['cs.LG', 'cs.CY', 'q-bio.BM', 'q-bio.QM']"
A survey on measuring indirect discrimination in machine learning,"  Nowadays, many decisions are made using predictive models built on historical
data.Predictive models may systematically discriminate groups of people even if
the computing process is fair and well-intentioned. Discrimination-aware data
mining studies how to make predictive models free from discrimination, when
historical data, on which they are built, may be biased, incomplete, or even
contain past discriminatory decisions. Discrimination refers to disadvantageous
treatment of a person based on belonging to a category rather than on
individual merit. In this survey we review and organize various discrimination
measures that have been used for measuring discrimination in data, as well as
in evaluating performance of discrimination-aware predictive models. We also
discuss related measures from other disciplines, which have not been used for
measuring discrimination, but potentially could be suitable for this purpose.
We computationally analyze properties of selected measures. We also review and
discuss measuring procedures, and present recommendations for practitioners.
The primary target audience is data mining, machine learning, pattern
recognition, statistical modeling researchers developing new methods for
non-discriminatory predictive modeling. In addition, practitioners and policy
makers would use the survey for diagnosing potential discrimination by
predictive models.

    ",Computers and Society (cs.CY),; Applications (stat.AP),"['ComputersandSociety(cs.CY)', 'Applications(stat.AP)']","['cs.CY', 'stat.AP']"
Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers,"  To construct interpretable explanations that are consistent with the original
ML model, counterfactual examples---showing how the model's output changes with
small perturbations to the input---have been proposed. This paper extends the
work in counterfactual explanations by addressing the challenge of feasibility
of such examples. For explanations of ML models in critical domains such as
healthcare and finance, counterfactual examples are useful for an end-user only
to the extent that perturbation of feature inputs is feasible in the real
world. We formulate the problem of feasibility as preserving causal
relationships among input features and present a method that uses (partial)
structural causal models to generate actionable counterfactuals. When
feasibility constraints cannot be easily expressed, we consider an alternative
mechanism where people can label generated CF examples on feasibility: whether
it is feasible to intervene and realize the candidate CF example from the
original input. To learn from this labelled feasibility data, we propose a
modified variational auto encoder loss for generating CF examples that
optimizes for feasibility as people interact with its output. Our experiments
on Bayesian networks and the widely used ''Adult-Income'' dataset show that our
proposed methods can generate counterfactual explanations that better satisfy
feasibility constraints than existing methods.. Code repository can be accessed
here: \textit{",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
A Survey of Privacy Attacks in Machine Learning,"  As machine learning becomes more widely used, the need to study its
implications in security and privacy becomes more urgent. Although the body of
work in privacy has been steadily growing over the past few years, research on
the privacy aspects of machine learning has received less focus than the
security aspects. Our contribution in this research is an analysis of more than
40 papers related to privacy attacks against machine learning that have been
published during the past seven years. We propose an attack taxonomy, together
with a threat model that allows the categorization of different attacks based
on the adversarial knowledge, and the assets under attack. An initial
exploration of the causes of privacy leaks is presented, as well as a detailed
analysis of the different attacks. Finally, we present an overview of the most
commonly proposed defenses and a discussion of the open problems and future
directions identified during our analysis.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"  Bioinformatics research is characterized by voluminous and incremental
datasets and complex data analytics methods. The machine learning methods used
in bioinformatics are iterative and parallel. These methods can be scaled to
handle big data using the distributed and parallel computing technologies.
","Computational Engineering, Finance, and Science (cs.CE)",; Machine Learning (cs.LG),"['ComputationalEngineering,Finance,andScience(cs.CE)', 'MachineLearning(cs.LG)']","['cs.CE', 'cs.LG']"
Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning,"  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.

    ",Computation and Language (cs.CL),,['ComputationandLanguage(cs.CL)'],['cs.CL']
A Living Review of Machine Learning for Particle Physics,"  Modern machine learning techniques, including deep learning, are rapidly
being applied, adapted, and developed for high energy physics. Given the fast
pace of this research, we have created a living review with the goal of
providing a nearly comprehensive list of citations for those developing and
applying these approaches to experimental, phenomenological, or theoretical
analyses. As a living document, it will be updated as often as possible to
incorporate the latest developments. A list of proper (unchanging) reviews can
be found within. Papers are grouped into a small set of topics to be as useful
as possible. Suggestions and contributions are most welcome, and we provide
instructions for participating.

    ",High Energy Physics - Phenomenology (hep-ph),"; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)","['HighEnergyPhysics-Phenomenology(hep-ph)', 'MachineLearning(cs.LG)', 'HighEnergyPhysics-Experiment(hep-ex)', 'DataAnalysis,StatisticsandProbability(physics.data-an)', 'MachineLearning(stat.ML)']","['hep-ph', 'cs.LG', 'hep-ex', 'physics.data-an', 'stat.ML']"
A Primer on the Signature Method in Machine Learning,"  In these notes, we wish to provide an introduction to the signature method,
focusing on its basic theoretical properties and recent numerical applications.
",Machine Learning (stat.ML),; Machine Learning (cs.LG); Methodology (stat.ME),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)']","['stat.ML', 'cs.LG', 'stat.ME']"
Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"  Autonomous vehicles rely on machine learning to solve challenging tasks in
perception and motion planning. However, automotive software safety standards
have not fully evolved to address the challenges of machine learning safety
such as interpretability, verification, and performance limitations. In this
paper, we review and organize practical machine learning safety techniques that
can complement engineering safety for machine learning based software in
autonomous vehicles. Our organization maps safety strategies to
state-of-the-art machine learning techniques in order to enhance dependability
and safety of machine learning algorithms. We also discuss security limitations
and user experience aspects of machine learning components in autonomous
vehicles.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Unified Representation of Molecules and Crystals for Machine Learning,"  Accurate simulations of atomistic systems from first principles are limited
by computational cost. In high-throughput settings, machine learning can reduce
these costs significantly by accurately interpolating between reference
calculations. For this, kernel learning approaches crucially require a
representation that accommodates arbitrary atomistic systems. We introduce a
many-body tensor representation that is invariant to translations, rotations,
and nuclear permutations of same elements, unique, differentiable, can
represent molecules and crystals, and is fast to compute. Empirical evidence
for competitive energy and force prediction errors is presented for changes in
molecular structure, crystal chemistry, and molecular dynamics using kernel
regression and symmetric gradient-domain machine learning as models.
Applicability is demonstrated for phase diagrams of Pt-group/transition-metal
binary systems.

    ",Chemical Physics (physics.chem-ph),; Materials Science (cond-mat.mtrl-sci),"['ChemicalPhysics(physics.chem-ph)', 'MaterialsScience(cond-mat.mtrl-sci)']","['physics.chem-ph', 'cond-mat.mtrl-sci']"
AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling,"  Interpretability of the underlying AI representations is a key raison
d'tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of
learners' cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.

    ",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CY']"
Safely Entering the Deep: A Review of Verification and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry,"  Deep Neural Networks (DNN) will emerge as a cornerstone in automotive
software engineering. However, developing systems with DNNs introduces novel
challenges for safety assessments. This paper reviews the state-of-the-art in
verification and validation of safety-critical systems that rely on machine
learning. Furthermore, we report from a workshop series on DNNs for perception
with automotive experts in Sweden, confirming that ISO 26262 largely
contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge
transfer and systems-based safety approaches, e.g., safety cage architectures
and simulated system test cases.

    ",Software Engineering (cs.SE),,['SoftwareEngineering(cs.SE)'],['cs.SE']
Machine Learning Techniques for Intrusion Detection,"  An Intrusion Detection System (IDS) is a software that monitors a single or a
network of computers for malicious activities (attacks) that are aimed at
stealing or censoring information or corrupting network protocols. Most
techniques used in today's IDS are not able to deal with the dynamic and
complex nature of cyber attacks on computer networks. Hence, efficient adaptive
methods like various techniques of machine learning can result in higher
detection rates, lower false alarm rates and reasonable computation and
communication costs. In this paper, we study several such schemes and compare
their performance. We divide the schemes into methods based on classical
artificial intelligence (AI) and methods based on computational intelligence
(CI). We explain how various characteristics of CI techniques can be used to
build efficient IDS.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)']","['cs.CR', 'cs.LG', 'cs.NI']"
Trustless Machine Learning Contracts; Evaluating and Exchanging Machine Learning Models on the Ethereum Blockchain,"  Using blockchain technology, it is possible to create contracts that offer a
reward in exchange for a trained machine learning model for a particular data
set. This would allow users to train machine learning models for a reward in a
trustless manner. The smart contract will use the blockchain to automatically
validate the solution, so there would be no debate about whether the solution
was correct or not. Users who submit the solutions won't have counterparty risk
that they won't get paid for their work. Contracts can be created easily by
anyone with a dataset, even programmatically by software agents. This creates a
market where parties who are good at solving machine learning problems can
directly monetize their skillset, and where any organization or software agent
that has a problem to solve with AI can solicit solutions from all over the
world. This will incentivize the creation of better machine learning models,
and make AI more accessible to companies and software agents.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
Machine learning based hyperspectral image analysis: A survey,"  Hyperspectral sensors enable the study of the chemical properties of scene
materials remotely for the purpose of identification, detection, and chemical
composition analysis of objects in the environment. Hence, hyperspectral images
captured from earth observing satellites and aircraft have been increasingly
important in agriculture, environmental monitoring, urban planning, mining, and
defense. Machine learning algorithms due to their outstanding predictive power
have become a key tool for modern hyperspectral image analysis. Therefore, a
solid understanding of machine learning techniques have become essential for
remote sensing researchers and practitioners. This paper reviews and compares
recent machine learning-based hyperspectral image analysis methods published in
literature. We organize the methods by the image analysis task and by the type
of machine learning algorithm, and present a two-way mapping between the image
analysis tasks and the types of machine learning algorithms that can be applied
to them. The paper is comprehensive in coverage of both hyperspectral image
analysis tasks and machine learning algorithms. The image analysis tasks
considered are land cover classification, target detection, unmixing, and
physical parameter estimation. The machine learning algorithms covered are
Gaussian models, linear regression, logistic regression, support vector
machines, Gaussian mixture model, latent linear models, sparse linear models,
Gaussian mixture models, ensemble learning, directed graphical models,
undirected graphical models, clustering, Gaussian processes, Dirichlet
processes, and deep learning. We also discuss the open challenges in the field
of hyperspectral image analysis and explore possible future directions.

    ",Computer Vision and Pattern Recognition (cs.CV),; Image and Video Processing (eess.IV),"['ComputerVisionandPatternRecognition(cs.CV)', 'ImageandVideoProcessing(eess.IV)']","['cs.CV', 'eess.IV']"
Automated software vulnerability detection with machine learning,"  Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.

    ",Software Engineering (cs.SE),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['SoftwareEngineering(cs.SE)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.SE', 'cs.LG', 'stat.ML']"
Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters,"  Time series forecasting is one of the most active research topics. Machine
learning methods have been increasingly adopted to solve these predictive
tasks. However, in a recent work, these were shown to systematically present a
lower predictive performance relative to simple statistical methods. In this
work, we counter these results. We show that these are only valid under an
extremely low sample size. Using a learning curve method, our results suggest
that machine learning methods improve their relative predictive performance as
the sample size grows. The code to reproduce the experiments is available at
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
An Analysis of ISO 26262: Using Machine Learning Safely in Automotive Software,"  Machine learning (ML) plays an ever-increasing role in advanced automotive
functionality for driver assistance and autonomous operation; however, its
adequacy from the perspective of safety certification remains controversial. In
this paper, we analyze the impacts that the use of ML as an implementation
approach has on ISO 26262 safety lifecycle and ask what could be done to
address them. We then provide a set of recommendations on how to adapt the
standard to accommodate ML.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Software Engineering (cs.SE); Systems and Control (eess.SY),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'SoftwareEngineering(cs.SE)', 'SystemsandControl(eess.SY)']","['cs.AI', 'cs.LG', 'cs.SE', 'eess.SY']"
Quantifying Interpretability and Trust in Machine Learning Systems,"  Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Adversarial Examples in Modern Machine Learning: A Review,"  Recent research has found that many families of machine learning models are
vulnerable to adversarial examples: inputs that are specifically designed to
cause the target model to produce erroneous outputs. In this survey, we focus
on machine learning models in the visual domain, where methods for generating
and detecting such examples have been most extensively studied. We explore a
variety of adversarial attack methods that apply to image-space content, real
world adversarial attacks, adversarial defenses, and the transferability
property of adversarial examples. We also discuss strengths and weaknesses of
various methods of adversarial attack and defense. Our aim is to provide an
extensive coverage of the field, furnishing the reader with an intuitive
understanding of the mechanics of adversarial attack and defense mechanisms and
enlarging the community of researchers studying this fundamental set of
problems.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']"
A glass-box interactive machine learning approach for solving NP-hard problems with the human-in-the-loop,"  The goal of Machine Learning to automatically learn from data, extract
knowledge and to make decisions without any human intervention. Such automatic
(aML) approaches show impressive success. Recent results even demonstrate
intriguingly that deep learning applied for automatic classification of skin
lesions is on par with the performance of dermatologists, yet outperforms the
average. As human perception is inherently limited, such approaches can
discover patterns, e.g. that two objects are similar, in arbitrarily
high-dimensional spaces what no human is able to do. Humans can deal only with
limited amounts of data, whilst big data is beneficial for aML; however, in
health informatics, we are often confronted with a small number of data sets,
where aML suffer of insufficient training samples and many problems are
computationally hard. Here, interactive machine learning (iML) may be of help,
where a human-in-the-loop contributes to reduce the complexity of NP-hard
problems. A further motivation for iML is that standard black-box approaches
lack transparency, hence do not foster trust and acceptance of ML among
end-users. Rising legal and privacy aspects, e.g. with the new European General
Data Protection Regulations, make black-box approaches difficult to use,
because they often are not able to explain why a decision has been made. In
this paper, we present some experiments to demonstrate the effectiveness of the
human-in-the-loop approach, particularly in opening the black-box to a
glass-box and thus enabling a human directly to interact with an learning
algorithm. We selected the Ant Colony Optimization framework, and applied it on
the Traveling Salesman Problem, which is a good example, due to its relevance
for health informatics, e.g. for the study of protein folding. From studies of
how humans extract so much from so little data, fundamental ML-research also
may benefit.

    ",Artificial Intelligence (cs.AI),; Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.AI', 'stat.ML']"
Practical Coreset Constructions for Machine Learning,"  We investigate coresets - succinct, small summaries of large data sets - so
that solutions found on the summary are provably competitive with solution
found on the full data set. We provide an overview over the state-of-the-art in
coreset construction for machine learning. In Section 2, we present both the
intuition behind and a theoretically sound framework to construct coresets for
general problems and apply it to $k$-means clustering. In Section 3 we
summarize existing coreset construction algorithms for a variety of machine
learning problems such as maximum likelihood estimation of mixture models,
Bayesian non-parametric models, principal component analysis, regression and
general empirical risk minimization.

    ",Machine Learning (stat.ML),,['MachineLearning(stat.ML)'],['stat.ML']
Machine Learning with Multi-Site Imaging Data: An Empirical Study on the Impact of Scanner Effects,"  This is an empirical study to investigate the impact of scanner effects when
using machine learning on multi-site neuroimaging data. We utilize structural
T1-weighted brain MRI obtained from two different studies, Cam-CAN and UK
Biobank. For the purpose of our investigation, we construct a dataset
consisting of brain scans from 592 age- and sex-matched individuals, 296
subjects from each original study. Our results demonstrate that even after
careful pre-processing with state-of-the-art neuroimaging pipelines a
classifier can easily distinguish between the origin of the data with very high
accuracy. Our analysis on the example application of sex classification
suggests that current approaches to harmonize data are unable to remove
scanner-specific bias leading to overly optimistic performance estimates and
poor generalization. We conclude that multi-site data harmonization remains an
open challenge and particular care needs to be taken when using such data with
advanced machine learning methods for predictive modelling.

    ",Image and Video Processing (eess.IV),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC),"['ImageandVideoProcessing(eess.IV)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'NeuronsandCognition(q-bio.NC)']","['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.NC']"
Machine Learning for Spatiotemporal Sequence Forecasting: A Survey,"  Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
mlpy: Machine Learning Python,"  mlpy is a Python Open Source Machine Learning library built on top of
NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of
state-of-the-art machine learning methods for supervised and unsupervised
problems and it is aimed at finding a reasonable compromise among modularity,
maintainability, reproducibility, usability and efficiency. mlpy is
multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at
the website ",Mathematical Software (cs.MS),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['MathematicalSoftware(cs.MS)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.MS', 'cs.LG', 'stat.ML']"
Interpretability and Explainability: A Machine Learning Zoo Mini-tour,"  In this review, we examine the problem of designing interpretable and
explainable machine learning models. Interpretability and explainability lie at
the core of many machine learning and statistical applications in medicine,
economics, law, and natural sciences. Although interpretability and
explainability have escaped a clear universal definition, many techniques
motivated by these properties have been developed over the recent 30 years with
the focus currently shifting towards deep learning methods. In this review, we
emphasise the divide between interpretability and explainability and illustrate
these two different research directions with concrete examples of the
state-of-the-art. The review is intended for a general machine learning
audience with interest in exploring the problems of interpretation and
explanation beyond logistic regression or random forest variable importance.
This work is not an exhaustive literature survey, but rather a primer focusing
selectively on certain lines of research which the authors found interesting or
informative.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
AlphaD3M: Machine Learning Pipeline Synthesis,"  We introduce AlphaD3M, an automatic machine learning (AutoML) system based on
meta reinforcement learning using sequence models with self play. AlphaD3M is
based on edit operations performed over machine learning pipeline primitives
providing explainability. We compare AlphaD3M with state-of-the-art AutoML
systems: Autosklearn, Autostacker, and TPOT, on OpenML datasets. AlphaD3M
achieves competitive performance while being an order of magnitude faster,
reducing computation time from hours to minutes, and is explainable by design.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Tunability: Importance of Hyperparameters of Machine Learning Algorithms,"  Modern supervised machine learning algorithms involve hyperparameters that
have to be set before running them. Options for setting hyperparameters are
default values from the software package, manual configuration by the user or
configuring them for optimal predictive performance by a tuning procedure. The
goal of this paper is two-fold. Firstly, we formalize the problem of tuning
from a statistical point of view, define data-based defaults and suggest
general measures quantifying the tunability of hyperparameters of algorithms.
Secondly, we conduct a large-scale benchmarking study based on 38 datasets from
the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield
default values for hyperparameters and enable users to decide whether it is
worth conducting a possibly time consuming tuning strategy, to focus on the
most important hyperparameters and to chose adequate hyperparameter spaces for
tuning.

    ",Machine Learning (stat.ML),,['MachineLearning(stat.ML)'],['stat.ML']
Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions,"  Low-dimensional embedding, manifold learning, clustering, classification, and
anomaly detection are among the most important problems in machine learning.
The existing methods usually consider the case when each instance has a fixed,
finite-dimensional feature representation. Here we consider a different
setting. We assume that each instance corresponds to a continuous probability
distribution. These distributions are unknown, but we are given some i.i.d.
samples from each distribution. Our goal is to estimate the distances between
these distributions and use these distances to perform low-dimensional
embedding, clustering/classification, or anomaly detection for the
distributions. We present estimation algorithms, describe how to apply them for
machine learning tasks on distributions, and show empirical results on
synthetic data, real word images, and astronomical data sets.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Rafiki: Machine Learning as an Analytics Service System,"  Big data analytics is gaining massive momentum in the last few years.
Applying machine learning models to big data has become an implicit requirement
or an expectation for most analysis tasks, especially on high-stakes
applications.Typical applications include sentiment analysis against reviews
for analyzing on-line products, image classification in food logging
applications for monitoring user's daily intake and stock movement prediction.
Extending traditional database systems to support the above analysis is
intriguing but challenging. First, it is almost impossible to implement all
machine learning models in the database engines. Second, expertise knowledge is
required to optimize the training and inference procedures in terms of
efficiency and effectiveness, which imposes heavy burden on the system users.
In this paper, we develop and present a system, called Rafiki, to provide the
training and inference service of machine learning models, and facilitate
complex analytics on top of cloud platforms. Rafiki provides distributed
hyper-parameter tuning for the training service, and online ensemble modeling
for the inference service which trades off between latency and accuracy.
Experimental results confirm the efficiency, effectiveness, scalability and
usability of Rafiki.

    ",Databases (cs.DB),"; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)","['Databases(cs.DB)', 'ArtificialIntelligence(cs.AI)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.DB', 'cs.AI', 'cs.DC']"
Automated Machine Learning on Graphs: A Survey,"  Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Enhancing Computational Fluid Dynamics with Machine Learning,"  Machine learning is rapidly becoming a core technology for scientific
computing, with numerous opportunities to advance the field of computational
fluid dynamics. In this Perspective, we highlight some of the areas of highest
potential impact, including to accelerate direct numerical simulations, to
improve turbulence closure modeling, and to develop enhanced reduced-order
models. We also discuss emerging areas of machine learning that are promising
for computational fluid dynamics, as well as some potential limitations that
should be taken into account.

    ",Fluid Dynamics (physics.flu-dyn),; Machine Learning (cs.LG); Computational Physics (physics.comp-ph),"['FluidDynamics(physics.flu-dyn)', 'MachineLearning(cs.LG)', 'ComputationalPhysics(physics.comp-ph)']","['physics.flu-dyn', 'cs.LG', 'physics.comp-ph']"
Private Machine Learning in TensorFlow using Secure Computation,"  We present a framework for experimenting with secure multi-party computation
directly in TensorFlow. By doing so we benefit from several properties valuable
to both researchers and practitioners, including tight integration with
ordinary machine learning processes, existing optimizations for distributed
computation in TensorFlow, high-level abstractions for expressing complex
algorithms and protocols, and an expanded set of familiar tooling. We give an
open source implementation of a state-of-the-art protocol and report on
concrete benchmarks using typical models from private machine learning.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
The Values Encoded in Machine Learning Research,"  Machine learning currently exerts an outsized influence on the world,
increasingly affecting institutional practices and impacted communities. It is
therefore critical that we question vague conceptions of the field as
value-neutral or universally beneficial, and investigate what specific values
the field is advancing. In this paper, we first introduce a method and
annotation scheme for studying the values encoded in documents such as research
papers. Applying the scheme, we analyze 100 highly cited machine learning
papers published at premier machine learning conferences, ICML and NeurIPS. We
annotate key features of papers which reveal their values: their justification
for their choice of project, which attributes of their project they uplift,
their consideration of potential negative consequences, and their institutional
affiliations and funding sources. We find that few of the papers justify how
their project connects to a societal need (15\%) and far fewer discuss negative
potential (1\%). Through line-by-line content analysis, we identify 59 values
that are uplifted in ML research, and, of these, we find that the papers most
frequently justify and assess themselves based on Performance, Generalization,
Quantitative evidence, Efficiency, Building on past work, and Novelty. We
present extensive textual evidence and identify key themes in the definitions
and operationalization of these values. Notably, we find systematic textual
evidence that these top values are being defined and applied with assumptions
and implications generally supporting the centralization of power.Finally, we
find increasingly close ties between these highly cited papers and tech
companies and elite universities.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computers and Society (cs.CY),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.LG', 'cs.AI', 'cs.CY']"
"Verification for Machine Learning, Autonomy, and Neural Networks Survey","  This survey presents an overview of verification techniques for autonomous
systems, with a focus on safety-critical autonomous cyber-physical systems
(CPS) and subcomponents thereof. Autonomy in CPS is enabling by recent advances
in artificial intelligence (AI) and machine learning (ML) through approaches
such as deep neural networks (DNNs), embedded in so-called learning enabled
components (LECs) that accomplish tasks from classification to control.
Recently, the formal methods and formal verification community has developed
methods to characterize behaviors in these LECs with eventual goals of formally
verifying specifications for LECs, and this article presents a survey of many
of these recent approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter,"  In February 2016, World Health Organization declared the Zika outbreak a
Public Health Emergency of International Concern. With developing evidence it
can cause birth defects, and the Summer Olympics coming up in the worst
affected country, Brazil, the virus caught fire on social media. In this work,
use Zika as a case study in building a tool for tracking the misinformation
around health concerns on Twitter. We collect more than 13 million tweets --
spanning the initial reports in February 2016 and the Summer Olympics --
regarding the Zika outbreak and track rumors outlined by the World Health
Organization and Snopes fact checking website. The tool pipeline, which
incorporates health professionals, crowdsourcing, and machine learning, allows
us to capture health-related rumors around the world, as well as clarification
campaigns by reputable health organizations. In the case of Zika, we discover
an extremely bursty behavior of rumor-related topics, and show that, once the
questionable topic is detected, it is possible to identify rumor-bearing tweets
using automated techniques. Thus, we illustrate insights the proposed tools
provide into potentially harmful information on social media, allowing public
health researchers and practitioners to respond with a targeted and timely
action.

    ",Social and Information Networks (cs.SI),; Computers and Society (cs.CY),"['SocialandInformationNetworks(cs.SI)', 'ComputersandSociety(cs.CY)']","['cs.SI', 'cs.CY']"
Quantum Neuron: an elementary building block for machine learning on quantum computers,"  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the ""quantum neuron"", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.

    ",Quantum Physics (quant-ph),; Neural and Evolutionary Computing (cs.NE),"['QuantumPhysics(quant-ph)', 'NeuralandEvolutionaryComputing(cs.NE)']","['quant-ph', 'cs.NE']"
"Bayesian inference of chaotic dynamics by merging data assimilation, machine learning and expectation-maximization","  The reconstruction from observations of high-dimensional chaotic dynamics
such as geophysical flows is hampered by (i) the partial and noisy observations
that can realistically be obtained, (ii) the need to learn from long time
series of data, and (iii) the unstable nature of the dynamics. To achieve such
inference from the observations over long time series, it has been suggested to
combine data assimilation and machine learning in several ways. We show how to
unify these approaches from a Bayesian perspective using
expectation-maximization and coordinate descents. In doing so, the model, the
state trajectory and model error statistics are estimated all together.
Implementations and approximations of these methods are discussed. Finally, we
numerically and successfully test the approach on two relevant low-order
chaotic models with distinct identifiability.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'AtmosphericandOceanicPhysics(physics.ao-ph)']","['stat.ML', 'cs.LG', 'physics.ao-ph']"
Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't,"  The purpose of this article is to review the achievements made in the last
few years towards the understanding of the reasons behind the success and
subtleties of neural network-based machine learning. In the tradition of good
old applied mathematics, we will not only give attention to rigorous
mathematical results, but also the insight we have gained from careful
numerical experiments as well as the analysis of simplified models. Along the
way, we also list the open problems which we believe to be the most important
topics for further study. This is not a complete overview over this quickly
moving field, but we hope to provide a perspective which may be helpful
especially to new researchers in the area.

    ",Machine Learning (cs.LG),; Numerical Analysis (math.NA); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'NumericalAnalysis(math.NA)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.NA', 'stat.ML']"
Personalized explanation in machine learning: A conceptualization,"  Explanation in machine learning and related fields such as artificial
intelligence aims at making machine learning models and their decisions
understandable to humans. Existing work suggests that personalizing
explanations might help to improve understandability. In this work, we derive a
conceptualization of personalized explanation by defining and structuring the
problem based on prior work on machine learning explanation, personalization
(in machine learning) and concepts and techniques from other domains such as
privacy and knowledge elicitation. We perform a categorization of explainee
data used in the process of personalization as well as describing means to
collect this data. We also identify three key explanation properties that are
amendable to personalization: complexity, decision information and
presentation. We also enhance existing work on explanation by introducing
additional desiderata and measures to quantify the quality of personalized
explanations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
TensorNetwork: A Library for Physics and Machine Learning,"  TensorNetwork is an open source library for implementing tensor network
algorithms. Tensor networks are sparse data structures originally designed for
simulating quantum many-body physics, but are currently also applied in a
number of other research areas, including machine learning. We demonstrate the
use of the API with applications both physics and machine learning, with
details appearing in companion papers.

    ",Computational Physics (physics.comp-ph),; Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); High Energy Physics - Theory (hep-th); Machine Learning (stat.ML),"['ComputationalPhysics(physics.comp-ph)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'MachineLearning(cs.LG)', 'HighEnergyPhysics-Theory(hep-th)', 'MachineLearning(stat.ML)']","['physics.comp-ph', 'cond-mat.str-el', 'cs.LG', 'hep-th', 'stat.ML']"
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,"  Large computer-understandable proofs consist of millions of intermediate
logical steps. The vast majority of such steps originate from manually selected
and manually guided heuristics applied to intermediate goals. So far, machine
learning has generally not been used to filter or generate these steps. In this
paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for
the purpose of developing new machine learning-based theorem-proving
strategies. We make this dataset publicly available under the BSD license. We
propose various machine learning tasks that can be performed on this dataset,
and discuss their significance for theorem proving. We also benchmark a set of
simple baseline machine learning models suited for the tasks (including
logistic regression, convolutional neural networks and recurrent neural
networks). The results of our baseline models show the promise of applying
machine learning to HOL theorem proving.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Convergence Analysis of Machine Learning Algorithms for the Numerical Solution of Mean Field Control and Games: II -- The Finite Horizon Case,"  We propose two numerical methods for the optimal control of McKean-Vlasov
dynamics in finite time horizon. Both methods are based on the introduction of
a suitable loss function defined over the parameters of a neural network. This
allows the use of machine learning tools, and efficient implementations of
stochastic gradient descent in order to perform the optimization. In the first
method, the loss function stems directly from the optimal control problem. The
second method tackles a generic forward-backward stochastic differential
equation system (FBSDE) of McKean-Vlasov type, and relies on suitable
reformulation as a mean field control problem. To provide a guarantee on how
our numerical schemes approximate the solution of the original mean field
control problem, we introduce a new optimization problem, directly amenable to
numerical computation, and for which we rigorously provide an error rate.
Several numerical examples are provided. Both methods can easily be applied to
certain problems with common noise, which is not the case with the existing
technology. Furthermore, although the first approach is designed for mean field
control problems, the second is more general and can also be applied to the
FBSDE arising in the theory of mean field games.

    ",Optimization and Control (math.OC),; Machine Learning (cs.LG); Numerical Analysis (math.NA),"['OptimizationandControl(math.OC)', 'MachineLearning(cs.LG)', 'NumericalAnalysis(math.NA)']","['math.OC', 'cs.LG', 'math.NA']"
Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules,"  Predicting the relationship between a molecule's structure and its odor
remains a difficult, decades-old task. This problem, termed quantitative
structure-odor relationship (QSOR) modeling, is an important challenge in
chemistry, impacting human nutrition, manufacture of synthetic fragrance, the
environment, and sensory neuroscience. We propose the use of graph neural
networks for QSOR, and show they significantly out-perform prior methods on a
novel data set labeled by olfactory experts. Additional analysis shows that the
learned embeddings from graph neural networks capture a meaningful odor space
representation of the underlying relationship between structure and odor, as
demonstrated by strong performance on two challenging transfer learning tasks.
Machine learning has already had a large impact on the senses of sight and
sound. Based on these early results with graph neural networks for molecular
properties, we hope machine learning can eventually do for olfaction what it
has already done for vision and hearing.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'ChemicalPhysics(physics.chem-ph)']","['stat.ML', 'cs.LG', 'physics.chem-ph']"
A Machine Learning Approach For Opinion Holder Extraction In Arabic Language,"  Opinion mining aims at extracting useful subjective information from reliable
amounts of text. Opinion mining holder recognition is a task that has not been
considered yet in Arabic Language. This task essentially requires deep
understanding of clauses structures. Unfortunately, the lack of a robust,
publicly available, Arabic parser further complicates the research. This paper
presents a leading research for the opinion holder extraction in Arabic news
independent from any lexical parsers. We investigate constructing a
comprehensive feature set to compensate the lack of parsing structural
outcomes. The proposed feature set is tuned from English previous works coupled
with our proposed semantic field and named entities features. Our feature
analysis is based on Conditional Random Fields (CRF) and semi-supervised
pattern recognition techniques. Different research models are evaluated via
cross-validation experiments achieving 54.03 F-measure. We publicly release our
own research outcome corpus and lexicon for opinion mining community to
encourage further research.

    ",Information Retrieval (cs.IR),; Machine Learning (cs.LG),"['InformationRetrieval(cs.IR)', 'MachineLearning(cs.LG)']","['cs.IR', 'cs.LG']"
An Overview of Privacy in Machine Learning,"  Over the past few years, providers such as Google, Microsoft, and Amazon have
started to provide customers with access to software interfaces allowing them
to easily embed machine learning tasks into their applications. Overall,
organizations can now use Machine Learning as a Service (MLaaS) engines to
outsource complex tasks, e.g., training classifiers, performing predictions,
clustering, etc. They can also let others query models trained on their data.
Naturally, this approach can also be used (and is often advocated) in other
contexts, including government collaborations, citizen science projects, and
business-to-business partnerships. However, if malicious users were able to
recover data used to train these models, the resulting information leakage
would create serious issues. Likewise, if the inner parameters of the model are
considered proprietary information, then access to the model should not allow
an adversary to learn such parameters. In this document, we set to review
privacy challenges in this space, providing a systematic review of the relevant
research literature, also exploring possible countermeasures. More
specifically, we provide ample background information on relevant concepts
around machine learning and privacy. Then, we discuss possible adversarial
models and settings, cover a wide range of attacks that relate to private
and/or sensitive information leakage, and review recent results attempting to
defend against such attacks. Finally, we conclude with a list of open problems
that require more work, including the need for better evaluations, more
targeted defenses, and the study of the relation to policy and data protection
efforts.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computers and Society (cs.CY); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)', 'ComputersandSociety(cs.CY)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.CR', 'cs.CY', 'stat.ML']"
Privacy-preserving Machine Learning through Data Obfuscation,"  As machine learning becomes a practice and commodity, numerous cloud-based
services and frameworks are provided to help customers develop and deploy
machine learning applications. While it is prevalent to outsource model
training and serving tasks in the cloud, it is important to protect the privacy
of sensitive samples in the training dataset and prevent information leakage to
untrusted third parties. Past work have shown that a malicious machine learning
service provider or end user can easily extract critical information about the
training samples, from the model parameters or even just model outputs.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning,"  We present Placeto, a reinforcement learning (RL) approach to efficiently
find device placements for distributed neural network training. Unlike prior
approaches that only find a device placement for a specific computation graph,
Placeto can learn generalizable device placement policies that can be applied
to any graph. We propose two key ideas in our approach: (1) we represent the
policy as performing iterative placement improvements, rather than outputting a
placement in one shot; (2) we use graph embeddings to capture relevant
information about the structure of the computation graph, without relying on
node labels for indexing. These ideas allow Placeto to train efficiently and
generalize to unseen graphs. Our experiments show that Placeto requires up to
6.1x fewer training steps to find placements that are on par with or better
than the best placements found by prior approaches. Moreover, Placeto is able
to learn a generalizable placement policy for any given family of graphs, which
can then be used without any retraining to predict optimized placements for
unseen graphs from the same family. This eliminates the large overhead incurred
by prior RL approaches whose lack of generalizability necessitates re-training
from scratch every time a new graph is to be placed.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DC', 'stat.ML']"
hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices,"  Accessible machine learning algorithms, software, and diagnostic tools for
energy-efficient devices and systems are extremely valuable across a broad
range of application domains. In scientific domains, real-time near-sensor
processing can drastically improve experimental design and accelerate
scientific discoveries. To support domain scientists, we have developed hls4ml,
an open-source software-hardware codesign workflow to interpret and translate
machine learning algorithms for implementation with both FPGA and ASIC
technologies. We expand on previous hls4ml work by extending capabilities and
techniques towards low-power implementations and increased usability: new
Python APIs, quantization-aware pruning, end-to-end FPGA workflows, long
pipeline kernels for low power, and new device backends include an ASIC
workflow. Taken together, these and continued efforts in hls4ml will arm a new
generation of domain scientists with accessible, efficient, and powerful tools
for machine-learning-accelerated discovery.

    ",Machine Learning (cs.LG),; Hardware Architecture (cs.AR); Instrumentation and Detectors (physics.ins-det),"['MachineLearning(cs.LG)', 'HardwareArchitecture(cs.AR)', 'InstrumentationandDetectors(physics.ins-det)']","['cs.LG', 'cs.AR', 'physics.ins-det']"
Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems,"  Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV),"['CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.CR', 'cs.CV']"
TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning,"  TF.Learn is a high-level Python module for distributed machine learning
inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to
simplify the process of creating, configuring, training, evaluating, and
experimenting a machine learning model. TF.Learn integrates a wide range of
state-of-art machine learning algorithms built on top of TensorFlow's low level
APIs for small to large-scale supervised and unsupervised problems. This module
focuses on bringing machine learning to non-specialists using a general-purpose
high-level language as well as researchers who want to implement, benchmark,
and compare their new methods in a structured environment. Emphasis is put on
ease of use, performance, documentation, and API consistency.

    ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Benchmarking Automatic Machine Learning Frameworks,"  AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Benchmarking Machine Learning Technologies for Software Defect Detection,"  Machine Learning approaches are good in solving problems that have less
information. In most cases, the software domain problems characterize as a
process of learning that depend on the various circumstances and changes
accordingly. A predictive model is constructed by using machine learning
approaches and classified them into defective and non-defective modules.
Machine learning techniques help developers to retrieve useful information
after the classification and enable them to analyse data from different
perspectives. Machine learning techniques are proven to be useful in terms of
software bug prediction. This study used public available data sets of software
modules and provides comparative performance analysis of different machine
learning techniques for software bug prediction. Results showed most of the
machine learning methods performed well on software bug datasets.

    ",Software Engineering (cs.SE),,['SoftwareEngineering(cs.SE)'],['cs.SE']
Co-Creative Level Design via Machine Learning,"  Procedural Level Generation via Machine Learning (PLGML), the study of
generating game levels with machine learning, has received a large amount of
recent academic attention. For certain measures these approaches have shown
success at replicating the quality of existing game levels. However, it is
unclear the extent to which they might benefit human designers. In this paper
we present a framework for co-creative level design with a PLGML agent. In
support of this framework we present results from a user study and results from
a comparative study of PLGML approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey,"  Machine learning models have made many decision support systems to be faster,
more accurate and more efficient. However, applications of machine learning in
network security face more disproportionate threat of active adversarial
attacks compared to other domains. This is because machine learning
applications in network security such as malware detection, intrusion
detection, and spam filtering are by themselves adversarial in nature. In what
could be considered an arms race between attackers and defenders, adversaries
constantly probe machine learning systems with inputs which are explicitly
designed to bypass the system and induce a wrong prediction. In this survey, we
first provide a taxonomy of machine learning techniques, styles, and
algorithms. We then introduce a classification of machine learning in network
security applications. Next, we examine various adversarial attacks against
machine learning in network security and introduce two classification
approaches for adversarial attacks in network security. First, we classify
adversarial attacks in network security based on a taxonomy of network security
applications. Secondly, we categorize adversarial attacks in network security
into a problem space vs. feature space dimensional classification model. We
then analyze the various defenses against adversarial attacks on machine
learning-based network security applications. We conclude by introducing an
adversarial risk model and evaluate several existing adversarial attacks
against machine learning in network security using the risk model. We also
identify where each attack classification resides within the adversarial risk
model

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)']","['cs.CR', 'cs.LG', 'cs.NI']"
A Hierarchy of Limitations in Machine Learning,"  ""All models are wrong, but some are useful"", wrote George E. P. Box (1979).
Machine learning has focused on the usefulness of probability models for
prediction in social systems, but is only now coming to grips with the ways in
which these models are wrong---and the consequences of those shortcomings. This
paper attempts a comprehensive, structured overview of the specific conceptual,
procedural, and statistical limitations of models in machine learning when
applied to society. Machine learning modelers themselves can use the described
hierarchy to identify possible failure points and think through how to address
them, and consumers of machine learning models can know what to question when
confronted with the decision about if, where, and how to apply machine
learning. The limitations go from commitments inherent in quantification
itself, through to showing how unmodeled dependencies can lead to
cross-validation being overly optimistic as a way of assessing model
performance.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)', 'Econometrics(econ.EM)', 'StatisticsTheory(math.ST)', 'MachineLearning(stat.ML)']","['cs.CY', 'cs.LG', 'econ.EM', 'math.ST', 'stat.ML']"
Detecting Fake News Using Machine Learning : A Systematic Literature Review,"  Internet is one of the important inventions and a large number of persons are
its users. These persons use this for different purposes. There are different
social media platforms that are accessible to these users. Any user can make a
post or spread the news through the online platforms. These platforms do not
verify the users or their posts. So some of the users try to spread fake news
through these platforms. These news can be propaganda against an individual,
society, organization or political party. A human being is unable to detect all
these fake news. So there is a need for machine learning classifiers that can
detect these fake news automatically. Use of machine learning classifiers for
detecting fake news is described in this systematic literature review.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
ARDA: Automatic Relational Data Augmentation for Machine Learning,"  Automatic machine learning (\AML) is a family of techniques to automate the
process of training predictive models, aiming to both improve performance and
make machine learning more accessible. While many recent works have focused on
aspects of the machine learning pipeline like model selection, hyperparameter
tuning, and feature selection, relatively few works have focused on automatic
data augmentation. Automatic data augmentation involves finding new features
relevant to the user's predictive task with minimal ``human-in-the-loop''
involvement.
",Machine Learning (cs.LG),; Databases (cs.DB); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'Databases(cs.DB)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DB', 'stat.ML']"
Advances in quantum machine learning,"  Here we discuss advances in the field of quantum machine learning. The
following document offers a hybrid discussion; both reviewing the field as it
is currently, and suggesting directions for further research. We include both
algorithms and experimental implementations in the discussion. The field's
outlook is generally positive, showing significant promise. However, we believe
there are appreciable hurdles to overcome before one can claim that it is a
primary application of quantum computation.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
AGL: a Scalable System for Industrial-purpose Graph Machine Learning,"  Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
",Social and Information Networks (cs.SI),,['SocialandInformationNetworks(cs.SI)'],['cs.SI']
Secure Computation for Machine Learning With SPDZ,"  Secure Multi-Party Computation (MPC) is an area of cryptography that enables
computation on sensitive data from multiple sources while maintaining privacy
guarantees. However, theoretical MPC protocols often do not scale efficiently
to real-world data. This project investigates the efficiency of the SPDZ
framework, which provides an implementation of an MPC protocol with malicious
security, in the context of popular machine learning (ML) algorithms. In
particular, we chose applications such as linear regression and logistic
regression, which have been implemented and evaluated using semi-honest MPC
techniques. We demonstrate that the SPDZ framework outperforms these previous
implementations while providing stronger security.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
A review of homomorphic encryption and software tools for encrypted statistical machine learning,"  Recent advances in cryptography promise to enable secure statistical
computation on encrypted data, whereby a limited set of operations can be
carried out without the need to first decrypt. We review these homomorphic
encryption schemes in a manner accessible to statisticians and machine
learners, focusing on pertinent limitations inherent in the current state of
the art. These limitations restrict the kind of statistics and machine learning
algorithms which can be implemented and we review those which have been
successfully applied in the literature. Finally, we document a high performance
R package implementing a recent homomorphic scheme in a general framework.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.CR', 'cs.LG']"
Learning perturbation sets for robust machine learning,"  Although much progress has been made towards robust deep learning, a
significant gap in robustness remains between real-world perturbations and more
narrowly defined sets typically studied in adversarial defenses. In this paper,
we aim to bridge this gap by learning perturbation sets from data, in order to
characterize real-world effects for robust training and evaluation.
Specifically, we use a conditional generator that defines the perturbation set
over a constrained region of the latent space. We formulate desirable
properties that measure the quality of a learned perturbation set, and
theoretically prove that a conditional variational autoencoder naturally
satisfies these criteria. Using this framework, our approach can generate a
variety of perturbations at different complexities and scales, ranging from
baseline spatial transformations, through common image corruptions, to lighting
variations. We measure the quality of our learned perturbation sets both
quantitatively and qualitatively, finding that our models are capable of
producing a diverse set of meaningful perturbations beyond the limited data
seen during training. Finally, we leverage our learned perturbation sets to
train models which are empirically and certifiably robust to adversarial image
corruptions and adversarial lighting variations, while improving generalization
on non-adversarial data. All code and configuration files for reproducing the
experiments as well as pretrained model weights can be found at
",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Music Genre Classification using Machine Learning Techniques,"  Categorizing music files according to their genre is a challenging task in
the area of music information retrieval (MIR). In this study, we compare the
performance of two classes of models. The first is a deep learning approach
wherein a CNN model is trained end-to-end, to predict the genre label of an
audio signal, solely using its spectrogram. The second approach utilizes
hand-crafted features, both from the time domain and the frequency domain. We
train four traditional machine learning classifiers with these features and
compare their performance. The features that contribute the most towards this
multi-class classification task are identified. The experiments are conducted
on the Audio set data set and we report an AUC value of 0.894 for an ensemble
classifier which combines the two proposed approaches.

    ",Sound (cs.SD),; Audio and Speech Processing (eess.AS),"['Sound(cs.SD)', 'AudioandSpeechProcessing(eess.AS)']","['cs.SD', 'eess.AS']"
The Bach Doodle: Approachable music composition with machine learning at scale,"  To make music composition more approachable, we designed the first AI-powered
Google Doodle, the Bach Doodle, where users can create their own melody and
have it harmonized by a machine learning model Coconet (Huang et al., 2017) in
the style of Bach. For users to input melodies, we designed a simplified
sheet-music based interface. To support an interactive experience at scale, we
re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the
browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise
separable convolutions and fusing operations. We also reduced the model
download size to approximately 400KB through post-training weight quantization.
We calibrated a speed test based on partial model evaluation time to determine
if the harmonization request should be performed locally or sent to remote TPU
servers. In three days, people spent 350 years worth of time playing with the
Bach Doodle, and Coconet received more than 55 million queries. Users could
choose to rate their compositions and contribute them to a public dataset,
which we are releasing with this paper. We hope that the community finds this
dataset useful for applications ranging from ethnomusicological studies, to
music education, to improving machine learning models.

    ",Sound (cs.SD),; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML),"['Sound(cs.SD)', 'Human-ComputerInteraction(cs.HC)', 'MachineLearning(cs.LG)', 'AudioandSpeechProcessing(eess.AS)', 'MachineLearning(stat.ML)']","['cs.SD', 'cs.HC', 'cs.LG', 'eess.AS', 'stat.ML']"
Development of a Machine-Learning System to Classify Lung CT Scan Images into Normal/COVID-19 Class,"  Recently, the lung infection due to Coronavirus Disease (COVID-19) affected a
large human group worldwide and the assessment of the infection rate in the
lung is essential for treatment planning. This research aims to propose a
Machine-Learning-System (MLS) to detect the COVID-19 infection using the CT
scan Slices (CTS). This MLS implements a sequence of methods, such as
multi-thresholding, image separation using threshold filter,
feature-extraction, feature-selection, feature-fusion and classification. The
initial part implements the Chaotic-Bat-Algorithm and Kapur's Entropy (CBA+KE)
thresholding to enhance the CTS. The threshold filter separates the image into
two segments based on a chosen threshold 'Th'. The texture features of these
images are extracted, refined and selected using the chosen procedures.
Finally, a two-class classifier system is implemented to categorize the chosen
CTS (n=500 with a pixel dimension of 512x512x1) into normal/COVID-19 group. In
this work, the classifiers, such as Naive Bayes (NB), k-Nearest Neighbors
(KNN), Decision Tree (DT), Random Forest (RF) and Support Vector Machine with
linear kernel (SVM) are implemented and the classification task is performed
using various feature vectors. The experimental outcome of the SVM with
Fused-Feature-Vector (FFV) helped to attain a detection accuracy of 89.80%.

    ",Image and Video Processing (eess.IV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ImageandVideoProcessing(eess.IV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['eess.IV', 'cs.LG', 'stat.ML']"
Importance of Tuning Hyperparameters of Machine Learning Algorithms,"  The performance of many machine learning algorithms depends on their
hyperparameter settings. The goal of this study is to determine whether it is
important to tune a hyperparameter or whether it can be safely set to a default
value. We present a methodology to determine the importance of tuning a
hyperparameter based on a non-inferiority test and tuning risk: the performance
loss that is incurred when a hyperparameter is not tuned, but set to a default
value. Because our methods require the notion of a default parameter, we
present a simple procedure that can be used to determine reasonable default
parameters. We apply our methods in a benchmark study using 59 datasets from
OpenML. Our results show that leaving particular hyperparameters at their
default value is non-inferior to tuning these hyperparameters. In some cases,
leaving the hyperparameter at its default value even outperforms tuning it
using a search procedure with a limited number of iterations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
What's Sex Got To Do With Fair Machine Learning?,"  Debate about fairness in machine learning has largely centered around
competing definitions of what fairness or nondiscrimination between groups
requires. However, little attention has been paid to what precisely a group is.
Many recent approaches to ""fairness"" require one to specify a causal model of
the data generating process. These exercises make an implicit ontological
assumption that a racial or sex group is simply a collection of individuals who
share a given trait. We show this by exploring the formal assumption of
modularity in causal models, which holds that the dependencies captured by one
causal pathway are invariant to interventions on any other pathways. Causal
models of sex propose two substantive claims: 1) There exists a feature,
sex-on-its-own, that is an inherent trait of an individual that causally brings
about social phenomena external to it in the world; and 2) the relations
between sex and its effects can be modified in whichever ways and the former
feature would still retain the meaning that sex has in our world. We argue that
this ontological picture is false. Many of the ""effects"" that sex purportedly
""causes"" are in fact constitutive features of sex as a social status. They give
the social meaning of sex features, meanings that are precisely what make sex
discrimination a distinctively morally problematic type of action. Correcting
this conceptual error has a number of implications for how models can be used
to detect discrimination. Formal diagrams of constitutive relations present an
entirely different path toward reasoning about discrimination. Whereas causal
diagrams guide the construction of sophisticated modular counterfactuals,
constitutive diagrams identify a different kind of counterfactual as central to
an inquiry on discrimination: one that asks how the social meaning of a group
would be changed if its non-modular features were altered.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.AI', 'cs.LG']"
Machine Learning in Astronomy: a practical overview,"  Astronomy is experiencing a rapid growth in data size and complexity. This
change fosters the development of data-driven science as a useful companion to
the common model-driven data analysis paradigm, where astronomers develop
automatic tools to mine datasets and extract novel information from them. In
recent years, machine learning algorithms have become increasingly popular
among astronomers, and are now used for a wide variety of tasks. In light of
these developments, and the promise and challenges associated with them, the
IAC Winter School 2018 focused on big data in Astronomy, with a particular
emphasis on machine learning and deep learning techniques. This document
summarizes the topics of supervised and unsupervised learning algorithms
presented during the school, and provides practical information on the
application of such tools to astronomical datasets. In this document I cover
basic topics in supervised machine learning, including selection and
preprocessing of the input dataset, evaluation methods, and three popular
supervised learning algorithms, Support Vector Machines, Random Forests, and
shallow Artificial Neural Networks. My main focus is on unsupervised machine
learning algorithms, that are used to perform cluster analysis, dimensionality
reduction, visualization, and outlier detection. Unsupervised learning
algorithms are of particular importance to scientific research, since they can
be used to extract new knowledge from existing datasets, and can facilitate new
discoveries.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),,['InstrumentationandMethodsforAstrophysics(astro-ph.IM)'],['astro-ph.IM']
Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"  Injecting adversarial examples during training, known as adversarial
training, can improve robustness against one-step attacks, but not for unknown
iterative attacks. To address this challenge, we first show iteratively
generated adversarial images easily transfer between networks trained with the
same strategy. Inspired by this observation, we propose cascade adversarial
training, which transfers the knowledge of the end results of adversarial
training. We train a network from scratch by injecting iteratively generated
adversarial images crafted from already defended networks in addition to
one-step adversarial images from the network being trained. We also propose to
utilize embedding space for both classification and low-level (pixel-level)
similarity learning to ignore unknown pixel level perturbation. During
training, we inject adversarial images without replacing their corresponding
clean images and penalize the distance between the two embeddings (clean and
adversarial). Experimental results show that cascade adversarial training
together with our proposed low-level similarity learning efficiently enhances
the robustness against iterative attacks, but at the expense of decreased
robustness against one-step attacks. We show that combining those two
techniques can also improve robustness under the worst case black box attack
scenario.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Persistent-Homology-based Machine Learning and its Applications -- A Survey,"  A suitable feature representation that can both preserve the data intrinsic
information and reduce data complexity and dimensionality is key to the
performance of machine learning models. Deeply rooted in algebraic topology,
persistent homology (PH) provides a delicate balance between data
simplification and intrinsic structure characterization, and has been applied
to various areas successfully. However, the combination of PH and machine
learning has been hindered greatly by three challenges, namely topological
representation of data, PH-based distance measurements or metrics, and PH-based
feature representation. With the development of topological data analysis,
progresses have been made on all these three problems, but widely scattered in
different literatures. In this paper, we provide a systematical review of PH
and PH-based supervised and unsupervised models from a computational
perspective. Our emphasizes are the recent development of mathematical models
and tools, including PH softwares and PH-based functions, feature
representations, kernels, and similarity models. Essentially, this paper can
work as a roadmap for the practical application of PH-based machine learning
tools. Further, we consider different topological feature representations in
different machine learning models, and investigate their impacts on the protein
secondary structure classification.

    ",Algebraic Topology (math.AT),,['AlgebraicTopology(math.AT)'],['math.AT']
"Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus","  Recent literature has underscored the importance of dataset documentation
work for machine learning, and part of this work involves addressing
""documentation debt"" for datasets that have been used widely but documented
sparsely. This paper aims to help address documentation debt for BookCorpus, a
popular text dataset for training large language models. Notably, researchers
have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,
even though little to no documentation exists about the dataset's motivation,
composition, collection process, etc. We offer a preliminary datasheet that
provides key context and information about BookCorpus, highlighting several
notable deficiencies. In particular, we find evidence that (1) BookCorpus
likely violates copyright restrictions for many books, (2) BookCorpus contains
thousands of duplicated books, and (3) BookCorpus exhibits significant skews in
genre representation. We also find hints of other potential deficiencies that
call for future research, including problematic content, potential skews in
religious representation, and lopsided author contributions. While more work
remains, this initial effort to provide a datasheet for BookCorpus adds to
growing literature that urges more careful and systematic documentation for
machine learning datasets.

    ",Computation and Language (cs.CL),; Computers and Society (cs.CY); Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.CY', 'cs.LG']"
"DALI: a large Dataset of synchronized Audio, LyrIcs and notes, automatically created using teacher-student machine learning paradigm","  The goal of this paper is twofold. First, we introduce DALI, a large and rich
multimodal dataset containing 5358 audio tracks with their time-aligned vocal
melody notes and lyrics at four levels of granularity. The second goal is to
explain our methodology where dataset creation and learning models interact
using a teacher-student machine learning paradigm that benefits each other. We
start with a set of manual annotations of draft time-aligned lyrics and notes
made by non-expert users of Karaoke games. This set comes without audio.
Therefore, we need to find the corresponding audio and adapt the annotations to
it. To that end, we retrieve audio candidates from the Web. Each candidate is
then turned into a singing-voice probability over time using a teacher, a deep
convolutional neural network singing-voice detection system (SVD), trained on
cleaned data. Comparing the time-aligned lyrics and the singing-voice
probability, we detect matches and update the time-alignment lyrics
accordingly. From this, we obtain new audio sets. They are then used to train
new SVD students used to perform again the above comparison. The process could
be repeated iteratively. We show that this allows to progressively improve the
performances of our SVD and get better audio-matching and alignment.

    ",Audio and Speech Processing (eess.AS),; Databases (cs.DB); Machine Learning (cs.LG); Sound (cs.SD),"['AudioandSpeechProcessing(eess.AS)', 'Databases(cs.DB)', 'MachineLearning(cs.LG)', 'Sound(cs.SD)']","['eess.AS', 'cs.DB', 'cs.LG', 'cs.SD']"
Machine Learning Based Student Grade Prediction: A Case Study,"  In higher educational institutes, many students have to struggle hard to
complete different courses since there is no dedicated support offered to
students who need special attention in the registered courses. Machine learning
techniques can be utilized for students' grades prediction in different
courses. Such techniques would help students to improve their performance based
on predicted grades and would enable instructors to identify such individuals
who might need assistance in the courses. In this paper, we use Collaborative
Filtering (CF), Matrix Factorization (MF), and Restricted Boltzmann Machines
(RBM) techniques to systematically analyze a real-world data collected from
Information Technology University (ITU), Lahore, Pakistan. We evaluate the
academic performance of ITU students who got admission in the bachelor's degree
program in ITU's Electrical Engineering department. The RBM technique is found
to be better than the other techniques used in predicting the students'
performance in the particular course.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Equity forecast: Predicting long term stock price movement using machine learning,"  Long term investment is one of the major investment strategies. However,
calculating intrinsic value of some company and evaluating shares for long term
investment is not easy, since analyst have to care about a large number of
financial indicators and evaluate them in a right manner. So far, little help
in predicting the direction of the company value over the longer period of time
has been provided from the machines. In this paper we present a machine
learning aided approach to evaluate the equity's future price over the long
time. Our method is able to correctly predict whether some company's value will
be 10% higher or not over the period of one year in 76.5% of cases.

    ",Machine Learning (cs.LG),; General Finance (q-fin.GN),"['MachineLearning(cs.LG)', 'GeneralFinance(q-fin.GN)']","['cs.LG', 'q-fin.GN']"
Insights into Performance Fitness and Error Metrics for Machine Learning,"  Machine learning (ML) is the field of training machines to achieve high level
of cognition and perform human-like analysis. Since ML is a data-driven
approach, it seemingly fits into our daily lives and operations as well as
complex and interdisciplinary fields. With the rise of commercial, open-source
and user-catered ML tools, a key question often arises whenever ML is applied
to explore a phenomenon or a scenario: what constitutes a good ML model?
Keeping in mind that a proper answer to this question depends on a variety of
factors, this work presumes that a good ML model is one that optimally performs
and best describes the phenomenon on hand. From this perspective, identifying
proper assessment metrics to evaluate performance of ML models is not only
necessary but is also warranted. As such, this paper examines a number of the
most commonly-used performance fitness and error metrics for regression and
classification algorithms, with emphasis on engineering applications.

    ",Machine Learning (cs.LG),"; Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'DataAnalysis,StatisticsandProbability(physics.data-an)', 'MachineLearning(stat.ML)']","['cs.LG', 'physics.data-an', 'stat.ML']"
NSML: A Machine Learning Platform That Enables You to Focus on Your Models,"  Machine learning libraries such as TensorFlow and PyTorch simplify model
implementation. However, researchers are still required to perform a
non-trivial amount of manual tasks such as GPU allocation, training status
tracking, and comparison of models with different hyperparameter settings. We
propose a system to handle these tasks and help researchers focus on models. We
present the requirements of the system based on a collection of discussions
from an online study group comprising 25k members. These include automatic GPU
allocation, learning status visualization, handling model parameter snapshots
as well as hyperparameter modification during learning, and comparison of
performance metrics between models via a leaderboard. We describe the system
architecture that fulfills these requirements and present a proof-of-concept
implementation, NAVER Smart Machine Learning (NSML). We test the system and
confirm substantial efficiency improvements for model development.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.LG', 'cs.DC']"
CausalML: Python Package for Causal Machine Learning,"  CausalML is a Python implementation of algorithms related to causal inference
and machine learning. Algorithms combining causal inference and machine
learning have been a trending topic in recent years. This package tries to
bridge the gap between theoretical work on methodology and practical
applications by making a collection of methods in this field available in
Python. This paper introduces the key concepts, scope, and use cases of this
package.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)', 'Computation(stat.CO)', 'MachineLearning(stat.ML)']","['cs.CY', 'cs.LG', 'stat.CO', 'stat.ML']"
Double Debiased Machine Learning Nonparametric Inference with Continuous Treatments,"  We propose a nonparametric inference method for causal effects of continuous
treatment variables, under unconfoundedness and nonparametric or
high-dimensional nuisance parameters. Our double debiased machine learning
(DML) estimators for the average dose-response function (or the average
structural function) and the partial effects are asymptotically normal with
nonparametric convergence rates. The nuisance estimators for the conditional
expectation function and the conditional density can be nonparametric or ML
methods. Utilizing a kernel-based doubly robust moment function and
cross-fitting, we give high-level conditions under which the nuisance
estimators do not affect the first-order large sample distribution of the DML
estimators. We further provide sufficient low-level conditions for kernel,
series, and deep neural networks. We propose a data-driven bandwidth to
consistently estimate the optimal bandwidth that minimizes the asymptotic mean
squared error. We justify the use of kernel to localize the continuous
treatment at a given value by the Gateaux derivative. We implement various ML
methods in Monte Carlo simulations and an empirical application on a job
training program evaluation.

    ",Econometrics (econ.EM),,['Econometrics(econ.EM)'],['econ.EM']
Using Visual Analytics to Interpret Predictive Machine Learning Models,"  It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,"  The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
A Topology Layer for Machine Learning,"  Topology applied to real world data using persistent homology has started to
find applications within machine learning, including deep learning. We present
a differentiable topology layer that computes persistent homology based on
level set filtrations and edge-based filtrations. We present three novel
applications: the topological layer can (i) regularize data reconstruction or
the weights of machine learning models, (ii) construct a loss on the output of
a deep generative network to incorporate topological priors, and (iii) perform
topological adversarial attacks on deep networks trained with persistence
features. The code (",Machine Learning (cs.LG),; Algebraic Topology (math.AT); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'AlgebraicTopology(math.AT)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.AT', 'stat.ML']"
Machine Learning Explainability for External Stakeholders,"  As machine learning is increasingly deployed in high-stakes contexts
affecting people's livelihoods, there have been growing calls to open the black
box and to make machine learning algorithms more explainable. Providing useful
explanations requires careful consideration of the needs of stakeholders,
including end-users, regulators, and domain experts. Despite this need, little
work has been done to facilitate inter-stakeholder conversation around
explainable machine learning. To help address this gap, we conducted a
closed-door, day-long workshop between academics, industry experts, legal
scholars, and policymakers to develop a shared language around explainability
and to understand the current shortcomings of and potential solutions for
deploying explainable machine learning in service of transparency goals. We
also asked participants to share case studies in deploying explainable machine
learning at scale. In this paper, we provide a short summary of various case
studies of explainable machine learning, lessons from those studies, and
discuss open challenges.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)']","['cs.CY', 'cs.AI']"
On the Existence of Simpler Machine Learning Models,"  It is almost always easier to find an accurate-but-complex model than an
accurate-yet-simple model. Finding optimal, sparse, accurate models of various
forms (linear models with integer coefficients, decision sets, rule lists,
decision trees) is generally NP-hard. We often do not know whether the search
for a simpler model will be worthwhile, and thus we do not go to the trouble of
searching for one. In this work, we ask an important practical question: can
accurate-yet-simple models be proven to exist, or shown likely to exist, before
explicitly searching for them? We hypothesize that there is an important reason
that simple-yet-accurate models often do exist. This hypothesis is that the
size of the Rashomon set is often large, where the Rashomon set is the set of
almost-equally-accurate models from a function class. If the Rashomon set is
large, it contains numerous accurate models, and perhaps at least one of them
is the simple model we desire. In this work, we formally present the Rashomon
ratio as a new gauge of simplicity for a learning problem, depending on a
function class and a data set. The Rashomon ratio is the ratio of the volume of
the set of accurate models to the volume of the hypothesis space, and it is
different from standard complexity measures from statistical learning theory.
Insight from studying the Rashomon ratio provides an easy way to check whether
a simpler model might exist for a problem before finding it, namely whether
several different machine learning methods achieve similar performance on the
data. In that sense, the Rashomon ratio is a powerful tool for understanding
why and when an accurate-yet-simple model might exist. If, as we hypothesize in
this work, many real-world data sets admit large Rashomon sets, the
implications are vast: it means that simple or interpretable models may often
be used for high-stakes decisions without losing accuracy.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Revealing the Autonomous System Taxonomy: The Machine Learning Approach,"  Although the Internet AS-level topology has been extensively studied over the
past few years, little is known about the details of the AS taxonomy. An AS
""node"" can represent a wide variety of organizations, e.g., large ISP, or small
private business, university, with vastly different network characteristics,
external connectivity patterns, network growth tendencies, and other properties
that we can hardly neglect while working on veracious Internet representations
in simulation environments. In this paper, we introduce a radically new
approach based on machine learning techniques to map all the ASes in the
Internet into a natural AS taxonomy. We successfully classify 95.3% of ASes
with expected accuracy of 78.1%. We release to the community the AS-level
topology dataset augmented with: 1) the AS taxonomy information and 2) the set
of AS attributes we used to classify ASes. We believe that this dataset will
serve as an invaluable addition to further understanding of the structure and
evolution of the Internet.

    ",Networking and Internet Architecture (cs.NI),; Machine Learning (cs.LG),"['NetworkingandInternetArchitecture(cs.NI)', 'MachineLearning(cs.LG)']","['cs.NI', 'cs.LG']"
Encrypted statistical machine learning: new privacy preserving methods,"  We present two new statistical machine learning methods designed to learn on
fully homomorphic encrypted (FHE) data. The introduction of FHE schemes
following Gentry (2009) opens up the prospect of privacy preserving statistical
machine learning analysis and modelling of encrypted data without compromising
security constraints. We propose tailored algorithms for applying extremely
random forests, involving a new cryptographic stochastic fraction estimator,
and nave Bayes, involving a semi-parametric model for the class decision
boundary, and show how they can be used to learn and predict from encrypted
data. We demonstrate that these techniques perform competitively on a variety
of classification data sets and provide detailed information about the
computational practicalities of these and other FHE methods.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME),"['MachineLearning(stat.ML)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)']","['stat.ML', 'cs.CR', 'cs.LG', 'stat.ME']"
Machine Learning in Artificial Intelligence: Towards a Common Understanding,"  The application of ""machine learning"" and ""artificial intelligence"" has
become popular within the last decade. Both terms are frequently used in
science and media, sometimes interchangeably, sometimes with different
meanings. In this work, we aim to clarify the relationship between these terms
and, in particular, to specify the contribution of machine learning to
artificial intelligence. We review relevant literature and present a conceptual
framework which clarifies the role of machine learning to build (artificial)
intelligent agents. Hence, we seek to provide more terminological clarity and a
starting point for (interdisciplinary) discussions and future research.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Motion Planning and Control for Mobile Robot Navigation Using Machine Learning: a Survey,"  Moving in complex environments is an essential capability of intelligent
mobile robots. Decades of research and engineering have been dedicated to
developing sophisticated navigation systems to move mobile robots from one
point to another. Despite their overall success, a recently emerging research
thrust is devoted to developing machine learning techniques to address the same
problem, based in large part on the success of deep learning. However, to date,
there has not been much direct comparison between the classical and emerging
paradigms to this problem. In this article, we survey recent works that apply
machine learning for motion planning and control in mobile robot navigation,
within the context of classical navigation systems. The surveyed works are
classified into different categories, which delineate the relationship of the
learning approaches to classical methods. Based on this classification, we
identify common challenges and promising future directions.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
"Declarative Recursive Computation on an RDBMS, or, Why You Should Use a Database For Distributed Machine Learning","  A number of popular systems, most notably Google's TensorFlow, have been
implemented from the ground up to support machine learning tasks. We consider
how to make a very small set of changes to a modern relational database
management system (RDBMS) to make it suitable for distributed learning
computations. Changes include adding better support for recursion, and
optimization and execution of very large compute plans. We also show that there
are key advantages to using an RDBMS as a machine learning platform. In
particular, learning based on a database management system allows for trivial
scaling to large data sets and especially large models, where different
computational units operate on different parts of a model that may be too large
to fit into RAM.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
A Review of Machine Learning based Anomaly Detection Techniques,"  Intrusion detection is so much popular since the last two decades where
intrusion is attempted to break into or misuse the system. It is mainly of two
types based on the intrusions, first is Misuse or signature based detection and
the other is Anomaly detection. In this paper Machine learning based methods
which are one of the types of Anomaly detection techniques is discussed.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)']","['cs.LG', 'cs.CR']"
Value-laden Disciplinary Shifts in Machine Learning,"  As machine learning models are increasingly used for high-stakes decision
making, scholars have sought to intervene to ensure that such models do not
encode undesirable social and political values. However, little attention thus
far has been given to how values influence the machine learning discipline as a
whole. How do values influence what the discipline focuses on and the way it
develops? If undesirable values are at play at the level of the discipline,
then intervening on particular models will not suffice to address the problem.
Instead, interventions at the disciplinary-level are required. This paper
analyzes the discipline of machine learning through the lens of philosophy of
science. We develop a conceptual framework to evaluate the process through
which types of machine learning models (e.g. neural networks, support vector
machines, graphical models) become predominant. The rise and fall of
model-types is often framed as objective progress. However, such disciplinary
shifts are more nuanced. First, we argue that the rise of a model-type is
self-reinforcing--it influences the way model-types are evaluated. For example,
the rise of deep learning was entangled with a greater focus on evaluations in
compute-rich and data-rich environments. Second, the way model-types are
evaluated encodes loaded social and political values. For example, a greater
focus on evaluations in compute-rich and data-rich environments encodes values
about centralization of power, privacy, and environmental concerns.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning,"  SpiNNaker is an ARM-based processor platform optimized for the simulation of
spiking neural networks. This brief describes the roadmap in going from the
current SPINNaker1 system, a 1 Million core machine in 130nm CMOS, to
SpiNNaker2, a 10 Million core machine in 22nm FDSOI. Apart from pure scaling,
we will take advantage of specific technology features, such as runtime
adaptive body biasing, to deliver cutting-edge power consumption. Power
management of the cores allows a wide range of workload adaptivity, i.e.
processor power scales with the complexity and activity of the spiking network.
Additional numerical accelerators will enhance the utility of SpiNNaker2 for
simulation of spiking neural networks as well as for executing conventional
deep neural networks. These measures should increase the simulation capacity of
the machine by a factor $>$50. The interplay between the two domains, i.e.
spiking and rate based, will provide an interesting field for algorithm
exploration on SpiNNaker2. Apart from the platforms' traditional usage as a
neuroscience exploration tool, the extended functionality opens up new
application areas such as automotive AI, tactile internet, industry 4.0 and
biomedical processing.

    ",Emerging Technologies (cs.ET),,['EmergingTechnologies(cs.ET)'],['cs.ET']
Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients,"  The nature of clinical data makes it difficult to quickly select, tune and
apply machine learning algorithms to clinical prognosis. As a result, a lot of
time is spent searching for the most appropriate machine learning algorithms
applicable in clinical prognosis that contains either binary-valued or
multi-valued attributes. The study set out to identify and evaluate the
performance of machine learning classification schemes applied in clinical
prognosis of post-operative life expectancy in the lung cancer patients.
Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train
and test models on Thoracic Surgery datasets obtained from the University of
California Irvine machine learning repository. Stratified 10-fold
cross-validation was used to evaluate baseline performance accuracy of the
classifiers. The comparative analysis shows that multilayer perceptron
performed best with classification accuracy of 82.3%, J48 came out second with
classification accuracy of 81.8%, and Naive Bayes came out the worst with
classification accuracy of 74.4%. The quality and outcome of the chosen machine
learning algorithms depends on the ingenuity of the clinical miner.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning,"  Federated Learning is the current state of the art in supporting secure
multi-party machine learning (ML): data is maintained on the owner's device and
the updates to the model are aggregated through a secure protocol. However,
this process assumes a trusted centralized infrastructure for coordination, and
clients must trust that the central service does not use the byproducts of
client data. In addition to this, a group of malicious clients could also harm
the performance of the model by carrying out a poisoning attack.
",Machine Learning (cs.LG),"; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'cs.DC', 'stat.ML']"
Generalization in quantum machine learning from few training data,"  Modern quantum machine learning (QML) methods involve variationally
optimizing a parameterized quantum circuit on a training data set, and
subsequently making predictions on a testing data set (i.e., generalizing). In
this work, we provide a comprehensive study of generalization performance in
QML after training on a limited number $N$ of training data points. We show
that the generalization error of a quantum machine learning model with $T$
trainable gates scales at worst as $\sqrt{T/N}$. When only $K \ll T$ gates have
undergone substantial change in the optimization process, we prove that the
generalization error improves to $\sqrt{K / N}$. Our results imply that the
compiling of unitaries into a polynomial number of native gates, a crucial
application for the quantum computing industry that typically uses
exponential-size training data, can be sped up significantly. We also show that
classification of quantum states across a phase transition with a quantum
convolutional neural network requires only a very small training data set.
Other potential applications include learning quantum error correcting codes or
quantum dynamical simulation. Our work injects new hope into the field of QML,
as good generalization is guaranteed from few training data.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['quant-ph', 'cs.LG', 'stat.ML']"
BoostClean: Automated Error Detection and Repair for Machine Learning,"  Predictive models based on machine learning can be highly sensitive to data
error. Training data are often combined with a variety of different sources,
each susceptible to different types of inconsistencies, and new data streams
during prediction time, the model may encounter previously unseen
inconsistencies. An important class of such inconsistencies is domain value
violations that occur when an attribute value is outside of an allowed domain.
We explore automatically detecting and repairing such violations by leveraging
the often available clean test labels to determine whether a given detection
and repair combination will improve model accuracy. We present BoostClean which
automatically selects an ensemble of error detection and repair combinations
using statistical boosting. BoostClean selects this ensemble from an extensible
library that is pre-populated general detection functions, including a novel
detector based on the Word2Vec deep learning model, which detects errors across
a diverse set of domains. Our evaluation on a collection of 12 datasets from
Kaggle, the UCI repository, real-world data analyses, and production datasets
that show that Boost- Clean can increase absolute prediction accuracy by up to
9% over the best non-ensembled alternatives. Our optimizations including
parallelism, materialization, and indexing techniques show a 22.2x end-to-end
speedup on a 16-core machine.

    ",Databases (cs.DB),,['Databases(cs.DB)'],['cs.DB']
The Role of Machine Learning in the Next Decade of Cosmology,"  In recent years, machine learning (ML) methods have remarkably improved how
cosmologists can interpret data. The next decade will bring new opportunities
for data-driven cosmological discovery, but will also present new challenges
for adopting ML methodologies and understanding the results. ML could transform
our field, but this transformation will require the astronomy community to both
foster and promote interdisciplinary research endeavors.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),; Cosmology and Nongalactic Astrophysics (astro-ph.CO),"['InstrumentationandMethodsforAstrophysics(astro-ph.IM)', 'CosmologyandNongalacticAstrophysics(astro-ph.CO)']","['astro-ph.IM', 'astro-ph.CO']"
Machine Learning for AC Optimal Power Flow,"  We explore machine learning methods for AC Optimal Powerflow (ACOPF) - the
task of optimizing power generation in a transmission network according while
respecting physical and engineering constraints. We present two formulations of
ACOPF as a machine learning problem: 1) an end-to-end prediction task where we
directly predict the optimal generator settings, and 2) a constraint prediction
task where we predict the set of active constraints in the optimal solution. We
validate these approaches on two benchmark grids.

    ",Machine Learning (cs.LG),; Signal Processing (eess.SP); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'SignalProcessing(eess.SP)', 'MachineLearning(stat.ML)']","['cs.LG', 'eess.SP', 'stat.ML']"
Scaling Datalog for Machine Learning on Big Data,"  In this paper, we present the case for a declarative foundation for
data-intensive machine learning systems. Instead of creating a new system for
each specific flavor of machine learning task, or hardcoding new optimizations,
we argue for the use of recursive queries to program a variety of machine
learning systems. By taking this approach, database query optimization
techniques can be utilized to identify effective execution plans, and the
resulting runtime plans can be executed on a single unified data-parallel query
processing engine. As a proof of concept, we consider two programming
models--Pregel and Iterative Map-Reduce-Update---from the machine learning
domain, and show how they can be captured in Datalog, tuned for a specific
task, and then compiled into an optimized physical plan. Experiments performed
on a large computing cluster with real data demonstrate that this declarative
approach can provide very good performance while offering both increased
generality and programming ease.

    ",Databases (cs.DB),; Machine Learning (cs.LG); Performance (cs.PF),"['Databases(cs.DB)', 'MachineLearning(cs.LG)', 'Performance(cs.PF)']","['cs.DB', 'cs.LG', 'cs.PF']"
tf.data: A Machine Learning Data Processing Framework,"  Training machine learning models requires feeding input data for models to
ingest. Input pipelines for machine learning jobs are often challenging to
implement efficiently as they require reading large volumes of data, applying
complex transformations, and transferring data to hardware accelerators while
overlapping computation and communication to achieve optimal performance. We
present tf.data, a framework for building and executing efficient input
pipelines for machine learning jobs. The tf.data API provides operators which
can be parameterized with user-defined computation, composed, and reused across
different machine learning domains. These abstractions allow users to focus on
the application logic of data processing, while tf.data's runtime ensures that
pipelines run efficiently.
",Machine Learning (cs.LG),; Mathematical Software (cs.MS),"['MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['cs.LG', 'cs.MS']"
Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions,"  Applying popular machine learning algorithms to large amounts of data raised
new challenges for the ML practitioners. Traditional ML libraries does not
support well processing of huge datasets, so that new approaches were needed.
Parallelization using modern parallel computing frameworks, such as MapReduce,
CUDA, or Dryad gained in popularity and acceptance, resulting in new ML
libraries developed on top of these frameworks. We will briefly introduce the
most prominent industrial and academic outcomes, such as Apache Mahout,
GraphLab or Jubatus.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Opportunities in Machine Learning for Particle Accelerators,"  Machine learning (ML) is a subfield of artificial intelligence. The term
applies broadly to a collection of computational algorithms and techniques that
train systems from raw data rather than a priori models. ML techniques are now
technologically mature enough to be applied to particle accelerators, and we
expect that ML will become an increasingly valuable tool to meet new demands
for beam energy, brightness, and stability. The intent of this white paper is
to provide a high-level introduction to problems in accelerator science and
operation where incorporating ML-based approaches may provide significant
benefit. We review ML techniques currently being investigated at particle
accelerator facilities, and we place specific emphasis on active research
efforts and promising exploratory results. We also identify new applications
and discuss their feasibility, along with the required data and infrastructure
strategies. We conclude with a set of guidelines and recommendations for
laboratory managers and administrators, emphasizing the logistical and
technological requirements for successfully adopting this technology. This
white paper also serves as a summary of the discussion from a recent workshop
held at SLAC on ML for particle accelerators.

    ",Accelerator Physics (physics.acc-ph),,['AcceleratorPhysics(physics.acc-ph)'],['physics.acc-ph']
Predicting human decisions with behavioral theories and machine learning,"  Behavioral decision theories aim to explain human behavior. Can they help
predict it? An open tournament for prediction of human choices in fundamental
economic decision tasks is presented. The results suggest that integration of
certain behavioral theories as features in machine learning systems provides
the best predictions. Surprisingly, the most useful theories for prediction
build on basic properties of human and animal learning and are very different
from mainstream decision theories that focus on deviations from rational
choice. Moreover, we find that theoretical features should be based not only on
qualitative behavioral insights (e.g. loss aversion), but also on quantitative
behavioral foresights generated by functional descriptive models (e.g. Prospect
Theory). Our analysis prescribes a recipe for derivation of explainable, useful
predictions of human decisions.

    ",Artificial Intelligence (cs.AI),; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'ComputerScienceandGameTheory(cs.GT)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.GT', 'cs.LG']"
Tutorial: Safe and Reliable Machine Learning,"  This document serves as a brief overview of the ""Safe and Reliable Machine
Learning"" tutorial given at the 2019 ACM Conference on Fairness,
Accountability, and Transparency (FAT* 2019). The talk slides can be found
here: ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
TensorNetwork for Machine Learning,"  We demonstrate the use of tensor networks for image classification with the
TensorNetwork open source library. We explain in detail the encoding of image
data into a matrix product state form, and describe how to contract the network
in a way that is parallelizable and well-suited to automatic gradients for
optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
find out-of-the-box performance of 98% and 88% accuracy, respectively, using
the same tensor network architecture. The TensorNetwork library allows us to
seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
improvement in computational speed using a GPU.

    ",Machine Learning (cs.LG),; Strongly Correlated Electrons (cond-mat.str-el); Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'ComputerVisionandPatternRecognition(cs.CV)', 'ComputationalPhysics(physics.comp-ph)', 'MachineLearning(stat.ML)']","['cs.LG', 'cond-mat.str-el', 'cs.CV', 'physics.comp-ph', 'stat.ML']"
A semi-agnostic ansatz with variable structure for quantum machine learning,"  Quantum machine learning (QML) offers a powerful, flexible paradigm for
programming near-term quantum computers, with applications in chemistry,
metrology, materials science, data science, and mathematics. Here, one trains
an ansatz, in the form of a parameterized quantum circuit, to accomplish a task
of interest. However, challenges have recently emerged suggesting that deep
ansatzes are difficult to train, due to flat training landscapes caused by
randomness or by hardware noise. This motivates our work, where we present a
variable structure approach to build ansatzes for QML. Our approach, called
VAns (Variable Ansatz), applies a set of rules to both grow and (crucially)
remove quantum gates in an informed manner during the optimization.
Consequently, VAns is ideally suited to mitigate trainability and noise-related
issues by keeping the ansatz shallow. We employ VAns in the variational quantum
eigensolver for condensed matter and quantum chemistry applications and also in
the quantum autoencoder for data compression, showing successful results in all
cases.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['quant-ph', 'cs.LG', 'stat.ML']"
Classification with Quantum Machine Learning: A Survey,"  Due to the superiority and noteworthy progress of Quantum Computing (QC) in a
lot of applications such as cryptography, chemistry, Big data, machine
learning, optimization, Internet of Things (IoT), Blockchain, communication,
and many more. Fully towards to combine classical machine learning (ML) with
Quantum Information Processing (QIP) to build a new field in the quantum world
is called Quantum Machine Learning (QML) to solve and improve problems that
displayed in classical machine learning (e.g. time and energy consumption,
kernel estimation). The aim of this paper presents and summarizes a
comprehensive survey of the state-of-the-art advances in Quantum Machine
Learning (QML). Especially, recent QML classification works. Also, we cover
about 30 publications that are published lately in Quantum Machine Learning
(QML). we propose a classification scheme in the quantum world and discuss
encoding methods for mapping classical data to quantum data. Then, we provide
quantum subroutines and some methods of Quantum Computing (QC) in improving
performance and speed up of classical Machine Learning (ML). And also some of
QML applications in various fields, challenges, and future vision will be
presented.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)']","['quant-ph', 'cs.LG']"
Helix: Holistic Optimization for Accelerating Iterative Machine Learning,"  Machine learning workflow development is a process of trial-and-error:
developers iterate on workflows by testing out small modifications until the
desired accuracy is achieved. Unfortunately, existing machine learning systems
focus narrowly on model training---a small fraction of the overall development
time---and neglect to address iterative development. We propose Helix, a
machine learning system that optimizes the execution across
iterations---intelligently caching and reusing, or recomputing intermediates as
appropriate. Helix captures a wide variety of application needs within its
Scala DSL, with succinct syntax defining unified processes for data
preprocessing, model specification, and learning. We demonstrate that the reuse
problem can be cast as a Max-Flow problem, while the caching problem is
NP-Hard. We develop effective lightweight heuristics for the latter. Empirical
evaluation shows that Helix is not only able to handle a wide variety of use
cases in one unified workflow but also much faster, providing run time
reductions of up to 19x over state-of-the-art systems, such as DeepDive or
KeystoneML, on four real-world applications in natural language processing,
computer vision, social and natural sciences.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
A Comprehensive Physics-Informed Machine Learning Framework for Predictive Turbulence Modeling,"  Although an increased availability of computational resources has enabled
high-fidelity simulations of turbulent flows, the RANS models are still the
dominant tools for industrial applications. However, the predictive
capabilities of RANS models are limited by potential inaccuracy driven by
hypotheses in the Reynolds stress closure. Recently, a Physics-Informed Machine
Learning (PIML) approach has been proposed to learn the functional form of
Reynolds stress discrepancy in RANS simulations based on available data. It has
been demonstrated that the learned discrepancy function can be used to improve
Reynolds stresses in different flows where data are not available. However,
owing to a number of challenges, the improvements have been demonstrated only
in the Reynolds stress prediction but not in the corresponding propagated
quantities of interest. In this work, we introduce the procedures toward a
complete PIML framework for predictive turbulence modeling, including learning
Reynolds stress discrepancy function, predicting Reynolds stresses in different
flows, and propagating to mean flow fields. The process of Reynolds stress
propagation and predictive accuracy of the propagated velocity field are
investigated. To improve the learning-prediction performance, the input
features are enriched based on an integrity basis of invariants. The fully
developed turbulent flow in a square duct is used as the test case. The
discrepancy model is trained on flow fields obtained from several Reynolds
numbers and evaluated on a duct flow at a Reynolds number higher than any of
the training cases. The predicted Reynolds stresses are propagated to velocity
field through RANS equations. Numerical results show excellent predictive
performances in both Reynolds stresses and their propagated velocities,
demonstrating the merits of the PIML approach in predictive turbulence
modeling.

    ",Fluid Dynamics (physics.flu-dyn),,['FluidDynamics(physics.flu-dyn)'],['physics.flu-dyn']
A machine learning framework for data driven acceleration of computations of differential equations,"  We propose a machine learning framework to accelerate numerical computations
of time-dependent ODEs and PDEs. Our method is based on recasting
(generalizations of) existing numerical methods as artificial neural networks,
with a set of trainable parameters. These parameters are determined in an
offline training process by (approximately) minimizing suitable (possibly
non-convex) loss functions by (stochastic) gradient descent methods. The
proposed algorithm is designed to be always consistent with the underlying
differential equation. Numerical experiments involving both linear and
non-linear ODE and PDE model problems demonstrate a significant gain in
computational efficiency over standard numerical methods.

    ",Numerical Analysis (math.NA),; Machine Learning (cs.LG),"['NumericalAnalysis(math.NA)', 'MachineLearning(cs.LG)']","['math.NA', 'cs.LG']"
Boosting Combinatorial Problem Modeling with Machine Learning,"  In the past few years, the area of Machine Learning (ML) has witnessed
tremendous advancements, becoming a pervasive technology in a wide range of
applications. One area that can significantly benefit from the use of ML is
Combinatorial Optimization. The three pillars of constraint satisfaction and
optimization problem solving, i.e., modeling, search, and optimization, can
exploit ML techniques to boost their accuracy, efficiency and effectiveness. In
this survey we focus on the modeling component, whose effectiveness is crucial
for solving the problem. The modeling activity has been traditionally shaped by
optimization and domain experts, interacting to provide realistic results.
Machine Learning techniques can tremendously ease the process, and exploit the
available data to either create models or refine expert-designed ones. In this
survey we cover approaches that have been recently proposed to enhance the
modeling process by learning either single constraints, objective functions, or
the whole model. We highlight common themes to multiple approaches and draw
connections with related fields of research.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Malware Detection Module using Machine Learning Algorithms to Assist in Centralized Security in Enterprise Networks,"  Malicious software is abundant in a world of innumerable computer users, who
are constantly faced with these threats from various sources like the internet,
local networks and portable drives. Malware is potentially low to high risk and
can cause systems to function incorrectly, steal data and even crash. Malware
may be executable or system library files in the form of viruses, worms,
Trojans, all aimed at breaching the security of the system and compromising
user privacy. Typically, anti-virus software is based on a signature definition
system which keeps updating from the internet and thus keeping track of known
viruses. While this may be sufficient for home-users, a security risk from a
new virus could threaten an entire enterprise network. This paper proposes a
new and more sophisticated antivirus engine that can not only scan files, but
also build knowledge and detect files as potential viruses. This is done by
extracting system API calls made by various normal and harmful executable, and
using machine learning algorithms to classify and hence, rank files on a scale
of security risk. While such a system is processor heavy, it is very effective
when used centrally to protect an enterprise network which maybe more prone to
such threats.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Enhanced Membership Inference Attacks against Machine Learning Models,"  How much does a machine learning algorithm leak about its training data, and
why? Membership inference attacks are used as an auditing tool to quantify this
leakage. In this paper, we present a comprehensive \textit{hypothesis testing
framework} that enables us not only to formally express the prior work in a
consistent way, but also to design new membership inference attacks that use
reference models to achieve a significantly higher power (true positive rate)
for any (false positive rate) error. More importantly, we explain \textit{why}
different attacks perform differently. We present a template for
indistinguishability games, and provide an interpretation of attack success
rate across different instances of the game. We discuss various uncertainties
of attackers that arise from the formulation of the problem, and show how our
approach tries to minimize the attack uncertainty to the one bit secret about
the presence or absence of a data point in the training set. We perform a
\textit{differential analysis} between all types of attacks, explain the gap
between them, and show what causes data points to be vulnerable to an attack
(as the reasons vary due to different granularities of memorization, from
overfitting to conditional memorization). Our auditing framework is openly
accessible as part of the \textit{Privacy Meter} software tool.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'stat.ML']"
Survey on Causal-based Machine Learning Fairness Notions,"  Addressing the problem of fairness is crucial to safely use machine learning
algorithms to support decisions with a critical impact on people's lives such
as job hiring, child maltreatment, disease diagnosis, loan granting, etc.
Several notions of fairness have been defined and examined in the past decade,
such as statistical parity and equalized odds. The most recent fairness
notions, however, are causal-based and reflect the now widely accepted idea
that using causality is necessary to appropriately address the problem of
fairness. This paper examines an exhaustive list of causal-based fairness
notions and study their applicability in real-world scenarios. As the majority
of causal-based fairness notions are defined in terms of non-observable
quantities (e.g., interventions and counterfactuals), their deployment in
practice requires to compute or estimate those quantities using observational
data. This paper offers a comprehensive report of the different approaches to
infer causal quantities from observational data including identifiability
(Pearl's SCM framework) and estimation (potential outcome framework). The main
contributions of this survey paper are (1) a guideline to help selecting a
suitable fairness notion given a specific real-world scenario, and (2) a
ranking of the fairness notions according to Pearl's causation ladder
indicating how difficult it is to deploy each notion in practice.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Thumbs up? Sentiment Classification using Machine Learning Techniques,"  We consider the problem of classifying documents not by topic, but by overall
sentiment, e.g., determining whether a review is positive or negative. Using
movie reviews as data, we find that standard machine learning techniques
definitively outperform human-produced baselines. However, the three machine
learning methods we employed (Naive Bayes, maximum entropy classification, and
support vector machines) do not perform as well on sentiment classification as
on traditional topic-based categorization. We conclude by examining factors
that make the sentiment classification problem more challenging.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.LG']"
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"  MXNet is a multi-language machine learning (ML) library to ease the
development of ML algorithms, especially for deep neural networks. Embedded in
the host language, it blends declarative symbolic expression with imperative
tensor computation. It offers auto differentiation to derive gradients. MXNet
is computation and memory efficient and runs on various heterogeneous systems,
ranging from mobile devices to distributed GPU clusters.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG); Mathematical Software (cs.MS); Neural and Evolutionary Computing (cs.NE),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)', 'NeuralandEvolutionaryComputing(cs.NE)']","['cs.DC', 'cs.LG', 'cs.MS', 'cs.NE']"
API design for machine learning software: experiences from the scikit-learn project,"  Scikit-learn is an increasingly popular machine learning li- brary. Written
in Python, it is designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this paper, we present and
discuss our design choices for the application programming interface (API) of
the project. In particular, we describe the simple and elegant interface shared
by all learning and processing units in the library and then discuss its
advantages in terms of composition and reusability. The paper also comments on
implementation details specific to the Python ecosystem and analyzes obstacles
faced by users and developers of the library.

    ",Machine Learning (cs.LG),; Mathematical Software (cs.MS),"['MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['cs.LG', 'cs.MS']"
Foolbox: A Python toolbox to benchmark the robustness of machine learning models,"  Even todays most advanced machine learning models are easily fooled by almost
imperceptible perturbations of their inputs. Foolbox is a new Python package to
generate such adversarial perturbations and to quantify and compare the
robustness of machine learning models. It is build around the idea that the
most comparable robustness measure is the minimum perturbation needed to craft
an adversarial example. To this end, Foolbox provides reference implementations
of most published adversarial attack methods alongside some new ones, all of
which perform internal hyperparameter tuning to find the minimum adversarial
perturbation. Additionally, Foolbox interfaces with most popular deep learning
frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows
different adversarial criteria such as targeted misclassification and top-k
misclassification as well as different distance measures. The code is licensed
under the MIT license and is openly available at
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"  TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands of
computational devices such as GPU cards. The system is flexible and can be used
to express a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been used for conducting
research and for deploying machine learning systems into production across more
than a dozen areas of computer science and other fields, including speech
recognition, computer vision, robotics, information retrieval, natural language
processing, geographic information extraction, and computational drug
discovery. This paper describes the TensorFlow interface and an implementation
of that interface that we have built at Google. The TensorFlow API and a
reference implementation were released as an open-source package under the
Apache 2.0 license in November, 2015 and are available at ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Taking Human out of Learning Applications: A Survey on Automated Machine Learning,"  Machine learning techniques have deeply rooted in our everyday life. However,
since it is knowledge- and labor-intensive to pursue good learning performance,
human experts are heavily involved in every aspect of machine learning. In
order to make machine learning techniques easier to apply and reduce the demand
for experienced human experts, automated machine learning (AutoML) has emerged
as a hot topic with both industrial and academic interest. In this paper, we
provide an up to date survey on AutoML. First, we introduce and define the
AutoML problem, with inspiration from both realms of automation and machine
learning. Then, we propose a general AutoML framework that not only covers most
existing approaches to date but also can guide the design for new methods.
Subsequently, we categorize and review the existing works from two aspects,
i.e., the problem setup and the employed techniques. Finally, we provide a
detailed analysis of AutoML approaches and explain the reasons underneath their
successful applications. We hope this survey can serve as not only an
insightful guideline for AutoML beginners but also an inspiration for future
research.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.AI', 'cs.LG', 'stat.ML']"
Universal Differential Equations for Scientific Machine Learning,"  In the context of science, the well-known adage ""a picture is worth a
thousand words"" might well be ""a model is worth a thousand datasets."" In this
manuscript we introduce the SciML software ecosystem as a tool for mixing the
information of physical laws and scientific models with data-driven machine
learning approaches. We describe a mathematical object, which we denote
universal differential equations (UDEs), as the unifying framework connecting
the ecosystem. We show how a wide variety of applications, from automatically
discovering biological mechanisms to solving high-dimensional
Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled
through the UDE formalism and its tooling. We demonstrate the generality of the
software tooling to handle stochasticity, delays, and implicit constraints.
This funnels the wide variety of SciML applications into a core set of training
mechanisms which are highly optimized, stabilized for stiff equations, and
compatible with distributed parallelism and GPU accelerators.

    ",Machine Learning (cs.LG),; Dynamical Systems (math.DS); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'DynamicalSystems(math.DS)', 'QuantitativeMethods(q-bio.QM)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.DS', 'q-bio.QM', 'stat.ML']"
Distributed GraphLab: A Framework for Machine Learning in the Cloud,"  While high-level data parallel frameworks, like MapReduce, simplify the
design and implementation of large-scale data processing systems, they do not
naturally or efficiently support many important data mining and machine
learning algorithms and can lead to inefficient learning systems. To help fill
this critical void, we introduced the GraphLab abstraction which naturally
expresses asynchronous, dynamic, graph-parallel computation while ensuring data
consistency and achieving a high degree of parallel performance in the
shared-memory setting. In this paper, we extend the GraphLab framework to the
substantially more challenging distributed setting while preserving strong data
consistency guarantees. We develop graph based extensions to pipelined locking
and data versioning to reduce network congestion and mitigate the effect of
network latency. We also introduce fault tolerance to the GraphLab abstraction
using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can
be easily implemented by exploiting the GraphLab abstraction itself. Finally,
we evaluate our distributed implementation of the GraphLab abstraction on a
large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains
over Hadoop-based implementations.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning,"  The nascent field of fair machine learning aims to ensure that decisions
guided by algorithms are equitable. Over the last several years, three formal
definitions of fairness have gained prominence: (1) anti-classification,
meaning that protected attributes---like race, gender, and their proxies---are
not explicitly used to make decisions; (2) classification parity, meaning that
common measures of predictive performance (e.g., false positive and false
negative rates) are equal across groups defined by the protected attributes;
and (3) calibration, meaning that conditional on risk estimates, outcomes are
independent of protected attributes. Here we show that all three of these
fairness definitions suffer from significant statistical limitations. Requiring
anti-classification or classification parity can, perversely, harm the very
groups they were designed to protect; and calibration, though generally
desirable, provides little guarantee that decisions are equitable. In contrast
to these formal fairness criteria, we argue that it is often preferable to
treat similarly risky people similarly, based on the most statistically
accurate estimates of risk that one can produce. Such a strategy, while not
universally applicable, often aligns well with policy objectives; notably, this
strategy will typically violate both anti-classification and classification
parity. In practice, it requires significant effort to construct suitable risk
estimates. One must carefully define and measure the targets of prediction to
avoid retrenching biases in the data. But, importantly, one cannot generally
address these difficulties by requiring that algorithms satisfy popular
mathematical formalizations of fairness. By highlighting these challenges in
the foundation of fair machine learning, we hope to help researchers and
practitioners productively advance the area.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning","  The correct use of model evaluation, model selection, and algorithm selection
techniques is vital in academic machine learning research as well as in many
industrial settings. This article reviews different techniques that can be used
for each of these three subtasks and discusses the main advantages and
disadvantages of each technique with references to theoretical and empirical
studies. Further, recommendations are given to encourage best yet feasible
practices in research and applications of machine learning. Common methods such
as the holdout method for model evaluation and selection are covered, which are
not recommended when working with small datasets. Different flavors of the
bootstrap technique are introduced for estimating the uncertainty of
performance estimates, as an alternative to confidence intervals via normal
approximation if bootstrapping is computationally feasible. Common
cross-validation techniques such as leave-one-out cross-validation and k-fold
cross-validation are reviewed, the bias-variance trade-off for choosing k is
discussed, and practical tips for the optimal choice of k are given based on
empirical evidence. Different statistical tests for algorithm comparisons are
presented, and strategies for dealing with multiple comparisons such as omnibus
tests and multiple-comparison corrections are discussed. Finally, alternative
methods for algorithm selection, such as the combined F-test 5x2
cross-validation and nested cross-validation, are recommended for comparing
machine learning algorithms when datasets are small.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Coronavirus (COVID-19) Classification using CT Images by Machine Learning Methods,"  This study presents early phase detection of Coronavirus (COVID-19), which is
named by World Health Organization (WHO), by machine learning methods. The
detection process was implemented on abdominal Computed Tomography (CT) images.
The expert radiologists detected from CT images that COVID-19 shows different
behaviours from other viral pneumonia. Therefore, the clinical experts specify
that COVD-19 virus needs to be diagnosed in early phase. For detection of
the COVID-19, four different datasets were formed by taking patches sized as
16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process
was applied to patches to increase the classification performance. Grey Level
Co-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run
Length Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete
Wavelet Transform (DWT) algorithms were used as feature extraction methods.
Support Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold
and 10-fold cross-validations were implemented during the classification
process. Sensitivity, specificity, accuracy, precision, and F-score metrics
were used to evaluate the classification performance. The best classification
accuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature
extraction method.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'ImageandVideoProcessing(eess.IV)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
InterpretML: A Unified Framework for Machine Learning Interpretability,"  InterpretML is an open-source Python package which exposes machine learning
interpretability algorithms to practitioners and researchers. InterpretML
exposes two types of interpretability - glassbox models, which are machine
learning models designed for interpretability (ex: linear models, rule lists,
generalized additive models), and blackbox explainability techniques for
explaining existing systems (ex: Partial Dependence, LIME). The package enables
practitioners to easily compare interpretability algorithms by exposing
multiple methods under a unified API, and by having a built-in, extensible
visualization platform. InterpretML also includes the first implementation of
the Explainable Boosting Machine, a powerful, interpretable, glassbox model
that can be as accurate as many blackbox models. The MIT licensed source code
can be downloaded from ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
GraphLab: A New Framework For Parallel Machine Learning,"  Designing and implementing efficient, provably correct parallel machine
learning (ML) algorithms is challenging. Existing high-level parallel
abstractions like MapReduce are insufficiently expressive while low-level tools
like MPI and Pthreads leave ML experts repeatedly solving the same design
challenges. By targeting common patterns in ML, we developed GraphLab, which
improves upon abstractions like MapReduce by compactly expressing asynchronous
iterative algorithms with sparse computational dependencies while ensuring data
consistency and achieving a high degree of parallel performance. We demonstrate
the expressiveness of the GraphLab framework by designing and implementing
parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and
Compressed Sensing. We show that using GraphLab we can achieve excellent
parallel performance on large scale real-world problems.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.LG', 'cs.DC']"
Adversarial Machine Learning at Scale,"  Adversarial examples are malicious inputs designed to fool machine learning
models. They often transfer from one model to another, allowing attackers to
mount black box attacks without knowledge of the target model's parameters.
Adversarial training is the process of explicitly training a model on
adversarial examples, in order to make it more robust to attack or to reduce
its test error on clean inputs. So far, adversarial training has primarily been
applied to small problems. In this research, we apply adversarial training to
ImageNet. Our contributions include: (1) recommendations for how to succesfully
scale adversarial training to large models and datasets, (2) the observation
that adversarial training confers robustness to single-step attack methods, (3)
the finding that multi-step attack methods are somewhat less transferable than
single-step attack methods, so single-step attacks are the best for mounting
black-box attacks, and (4) resolution of a ""label leaking"" effect that causes
adversarially trained models to perform better on adversarial examples than on
clean examples, because the adversarial example construction process uses the
true label and the model can learn to exploit regularities in the construction
process.

    ",Computer Vision and Pattern Recognition (cs.CV),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.CR', 'cs.LG', 'stat.ML']"
"Interpretable machine learning: definitions, methods, and applications","  Machine-learning models have demonstrated great success in learning complex
patterns that enable them to make predictions about unobserved data. In
addition to using models for prediction, the ability to interpret what a model
has learned is receiving an increasing amount of attention. However, this
increased focus has led to considerable confusion about the notion of
interpretability. In particular, it is unclear how the wide array of proposed
interpretation methods are related, and what common concepts can be used to
evaluate them.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'Applications(stat.AP)']","['stat.ML', 'cs.AI', 'cs.LG', 'stat.AP']"
Pylearn2: a machine learning research library,"  Pylearn2 is a machine learning research library. This does not just mean that
it is a collection of machine learning algorithms that share a common API; it
means that it has been designed for flexibility and extensibility in order to
facilitate research projects that involve new or unusual use cases. In this
paper we give a brief history of the library, an overview of its basic
philosophy, a summary of the library's architecture, and a description of how
the Pylearn2 community functions socially.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Mathematical Software (cs.MS),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['stat.ML', 'cs.LG', 'cs.MS']"
The Frontiers of Fairness in Machine Learning,"  The last few years have seen an explosion of academic and popular interest in
algorithmic fairness. Despite this interest and the volume and velocity of work
that has been produced recently, the fundamental science of fairness in machine
learning is still in a nascent state. In March 2018, we convened a group of
experts as part of a CCC visioning workshop to assess the state of the field,
and distill the most promising research directions going forward. This report
summarizes the findings of that workshop. Along the way, it surveys recent
theoretical work in the field and points towards promising directions for
research.

    ",Machine Learning (cs.LG),; Data Structures and Algorithms (cs.DS); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'DataStructuresandAlgorithms(cs.DS)', 'ComputerScienceandGameTheory(cs.GT)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DS', 'cs.GT', 'stat.ML']"
Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data,"  On-device machine learning (ML) enables the training process to exploit a
massive amount of user-generated private data samples. To enjoy this benefit,
inter-device communication overhead should be minimized. With this end, we
propose federated distillation (FD), a distributed model training algorithm
whose communication payload size is much smaller than a benchmark scheme,
federated learning (FL), particularly when the model size is large. Moreover,
user-generated data samples are likely to become non-IID across devices, which
commonly degrades the performance compared to the case with an IID dataset. To
cope with this, we propose federated augmentation (FAug), where each device
collectively trains a generative model, and thereby augments its local data
towards yielding an IID dataset. Empirical studies demonstrate that FD with
FAug yields around 26x less communication overhead while achieving 95-98% test
accuracy compared to FL.

    ",Machine Learning (cs.LG),; Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.NI', 'stat.ML']"
ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models,"  Machine learning (ML) has become a core component of many real-world
applications and training data is a key factor that drives current progress.
This huge success has led Internet companies to deploy machine learning as a
service (MLaaS). Recently, the first membership inference attack has shown that
extraction of information on the training set is possible in such MLaaS
settings, which has severe security and privacy implications.
",Cryptography and Security (cs.CR),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.AI', 'cs.LG']"
BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,"  Deep learning-based techniques have achieved state-of-the-art performance on
a wide variety of recognition and classification tasks. However, these networks
are typically computationally expensive to train, requiring weeks of
computation on many GPUs; as a result, many users outsource the training
procedure to the cloud or rely on pre-trained models that are then fine-tuned
for a specific task. In this paper we show that outsourced training introduces
new security risks: an adversary can create a maliciously trained network (a
backdoored neural network, or a \emph{BadNet}) that has state-of-the-art
performance on the user's training and validation samples, but behaves badly on
specific attacker-chosen inputs. We first explore the properties of BadNets in
a toy example, by creating a backdoored handwritten digit classifier. Next, we
demonstrate backdoors in a more realistic scenario by creating a U.S. street
sign classifier that identifies stop signs as speed limits when a special
sticker is added to the stop sign; we then show in addition that the backdoor
in our US street sign detector can persist even if the network is later
retrained for another task and cause a drop in accuracy of {25}\% on average
when the backdoor trigger is present. These results demonstrate that backdoors
in neural networks are both powerful and---because the behavior of neural
networks is difficult to explicate---stealthy. This work provides motivation
for further research into techniques for verifying and inspecting neural
networks, just as we have developed tools for verifying and debugging software.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"  We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.AI', 'cs.LG']"
Federated Optimization: Distributed Machine Learning for On-Device Intelligence,"  We introduce a new and increasingly relevant setting for distributed
optimization in machine learning, where the data defining the optimization are
unevenly distributed over an extremely large number of nodes. The goal is to
train a high-quality centralized model. We refer to this setting as Federated
Optimization. In this setting, communication efficiency is of the utmost
importance and minimizing the number of rounds of communication is the
principal goal.
",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Machine Learning that Matters,"  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution,"  Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
"Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge","  Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic
core, active and non-enhancing core. This intrinsic heterogeneity is also
portrayed in their radio-phenotype, as their sub-regions are depicted by
varying intensity profiles disseminated across multi-parametric magnetic
resonance imaging (mpMRI) scans, reflecting varying biological properties.
Their heterogeneous shape, extent, and location are some of the factors that
make these tumors difficult to resect, and in some cases inoperable. The amount
of resected tumor is a factor also considered in longitudinal scans, when
evaluating the apparent tumor for potential diagnosis of progression.
Furthermore, there is mounting evidence that accurate segmentation of the
various tumor sub-regions can offer the basis for quantitative image analysis
towards prediction of patient overall survival. This study assesses the
state-of-the-art machine learning (ML) methods used for brain tumor image
analysis in mpMRI scans, during the last seven instances of the International
Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we
focus on i) evaluating segmentations of the various glioma sub-regions in
pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue
of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO
criteria, and iii) predicting the overall survival from pre-operative mpMRI
scans of patients that underwent gross total resection. Finally, we investigate
the challenge of identifying the best ML algorithms for each of these tasks,
considering that apart from being diverse on each instance of the challenge,
the multi-institutional mpMRI BraTS dataset has also been a continuously
evolving/growing dataset.

    ",Computer Vision and Pattern Recognition (cs.CV),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']"
Quantifying the Carbon Emissions of Machine Learning,"  From an environmental standpoint, there are a few crucial aspects of training
a neural network that have a major impact on the quantity of carbon that it
emits. These factors include: the location of the server used for training and
the energy grid that it uses, the length of the training procedure, and even
the make and model of hardware on which the training takes place. In order to
approximate these emissions, we present our Machine Learning Emissions
Calculator, a tool for our community to better understand the environmental
impact of training ML models. We accompany this tool with an explanation of the
factors cited above, as well as concrete actions that individual practitioners
and organizations can take to mitigate their carbon emissions.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
Fairness in Machine Learning: A Survey,"  As Machine Learning technologies become increasingly used in contexts that
affect citizens, companies as well as researchers need to be confident that
their application of these methods will not have unexpected social
implications, such as bias towards gender, ethnicity, and/or people with
disabilities. There is significant literature on approaches to mitigate bias
and promote fairness, yet the area is complex and hard to penetrate for
newcomers to the domain. This article seeks to provide an overview of the
different schools of thought and approaches to mitigating (social) biases and
increase fairness in the Machine Learning literature. It organises approaches
into the widely accepted framework of pre-processing, in-processing, and
post-processing methods, subcategorizing into a further 11 method areas.
Although much of the literature emphasizes binary classification, a discussion
of fairness in regression, recommender systems, unsupervised learning, and
natural language processing is also provided along with a selection of
currently available open source libraries. The article concludes by summarising
open challenges articulated as four dilemmas for fairness research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning,"  Current advances in Artificial Intelligence and machine learning in general,
and deep learning in particular have reached unprecedented impact not only
across research communities, but also over popular media channels. However,
concerns about interpretability and accountability of AI have been raised by
influential thinkers. In spite of the recent impact of AI, several works have
identified the need for principled knowledge representation and reasoning
mechanisms integrated with deep learning-based systems to provide sound and
explainable models for such systems. Neural-symbolic computing aims at
integrating, as foreseen by Valiant, two most fundamental cognitive abilities:
the ability to learn from the environment, and the ability to reason from what
has been learned. Neural-symbolic computing has been an active topic of
research for many years, reconciling the advantages of robust learning in
neural networks and reasoning and interpretability of symbolic representation.
In this paper, we survey recent accomplishments of neural-symbolic computing as
a principled methodology for integrated machine learning and reasoning. We
illustrate the effectiveness of the approach by outlining the main
characteristics of the methodology: principled integration of neural learning
with symbolic knowledge representation and reasoning allowing for the
construction of explainable AI systems. The insights provided by
neural-symbolic computing shed new light on the increasingly prominent need for
interpretable and accountable AI systems.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Towards the Science of Security and Privacy in Machine Learning,"  Advances in machine learning (ML) in recent years have enabled a dizzying
array of applications such as data analytics, autonomous systems, and security
diagnostics. ML is now pervasive---new systems and models are being deployed in
every domain imaginable, leading to rapid and widespread deployment of software
based inference and decision making. There is growing recognition that ML
exposes new vulnerabilities in software systems, yet the technical community's
understanding of the nature and extent of these vulnerabilities remains
limited. We systematize recent findings on ML security and privacy, focusing on
attacks identified on these systems and defenses crafted to date. We articulate
a comprehensive threat model for ML, and categorize attacks and defenses within
an adversarial framework. Key insights resulting from works both in the ML and
security communities are identified and the effectiveness of approaches are
related to structural elements of ML algorithms and the data used to train
them. We conclude by formally exploring the opposing relationship between model
accuracy and resilience to adversarial manipulation. Through these
explorations, we show that there are (possibly unavoidable) tensions between
model complexity, accuracy, and resilience that must be calibrated for the
environments in which they will be used.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
"Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology","  Performance metrics (error measures) are vital components of the evaluation
frameworks in various fields. The intention of this study was to overview of a
variety of performance metrics and approaches to their classification. The main
goal of the study was to develop a typology that will help to improve our
knowledge and understanding of metrics and facilitate their selection in
machine learning regression, forecasting and prognostics. Based on the analysis
of the structure of numerous performance metrics, we propose a framework of
metrics which includes four (4) categories: primary metrics, extended metrics,
composite metrics, and hybrid sets of metrics. The paper identified three (3)
key components (dimensions) that determine the structure and properties of
primary metrics: method of determining point distance, method of normalization,
method of aggregation of point distances over a data set.

    ",Methodology (stat.ME),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['Methodology(stat.ME)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['stat.ME', 'cs.LG', 'stat.ML']"
Opportunities and Challenges for Machine Learning in Materials Science,"  Advances in machine learning have impacted myriad areas of materials science,
ranging from the discovery of novel materials to the improvement of molecular
simulations, with likely many more important developments to come. Given the
rapid changes in this field, it is challenging to understand both the breadth
of opportunities as well as best practices for their use. In this review, we
address aspects of both problems by providing an overview of the areas where
machine learning has recently had significant impact in materials science, and
then provide a more detailed discussion on determining the accuracy and domain
of applicability of some common types of machine learning models. Finally, we
discuss some opportunities and challenges for the materials community to fully
utilize the capabilities of machine learning.

    ",Materials Science (cond-mat.mtrl-sci),; Computational Physics (physics.comp-ph),"['MaterialsScience(cond-mat.mtrl-sci)', 'ComputationalPhysics(physics.comp-ph)']","['cond-mat.mtrl-sci', 'physics.comp-ph']"
TensorFlow Quantum: A Software Framework for Quantum Machine Learning,"  We introduce TensorFlow Quantum (TFQ), an open source library for the rapid
prototyping of hybrid quantum-classical models for classical or quantum data.
This framework offers high-level abstractions for the design and training of
both discriminative and generative quantum models under TensorFlow and supports
high-performance quantum circuit simulators. We provide an overview of the
software architecture and building blocks through several examples and review
the theory of hybrid quantum-classical neural networks. We illustrate TFQ
functionalities via several basic applications including supervised learning
for quantum classification, quantum control, simulating noisy quantum circuits,
and quantum approximate optimization. Moreover, we demonstrate how one can
apply TFQ to tackle advanced quantum learning tasks including meta-learning,
layerwise learning, Hamiltonian learning, sampling thermal states, variational
quantum eigensolvers, classification of quantum phase transitions, generative
adversarial networks, and reinforcement learning. We hope this framework
provides the necessary tools for the quantum computing and machine learning
research communities to explore models of both natural and artificial quantum
systems, and ultimately discover new quantum algorithms which could potentially
yield a quantum advantage.

    ",Quantum Physics (quant-ph),; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Programming Languages (cs.PL),"['QuantumPhysics(quant-ph)', 'DisorderedSystemsandNeuralNetworks(cond-mat.dis-nn)', 'MachineLearning(cs.LG)', 'ProgrammingLanguages(cs.PL)']","['quant-ph', 'cond-mat.dis-nn', 'cs.LG', 'cs.PL']"
Augmentor: An Image Augmentation Library for Machine Learning,"  The generation of artificial data based on existing observations, known as
data augmentation, is a technique used in machine learning to improve model
accuracy, generalisation, and to control overfitting. Augmentor is a software
package, available in both Python and Julia versions, that provides a high
level API for the expansion of image data using a stochastic, pipeline-based
approach which effectively allows for images to be sampled from a distribution
of augmented images at runtime. Augmentor provides methods for most standard
augmentation practices as well as several advanced features such as
label-preserving, randomised elastic distortions, and provides many helper
functions for typical augmentation tasks used in machine learning.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'stat.ML']"
FedML: A Research Library and Benchmark for Federated Machine Learning,"  Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models,"  This paper describes EMBER: a labeled benchmark dataset for training machine
learning models to statically detect malicious Windows portable executable
files. The dataset includes features extracted from 1.1M binary files: 900K
training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test
samples (100K malicious, 100K benign). To accompany the dataset, we also
release open source code for extracting features from additional binaries so
that additional sample features can be appended to the dataset. This dataset
fills a void in the information security machine learning community: a
benign/malicious dataset that is large, open and general enough to cover
several interesting use cases. We enumerate several use cases that we
considered when structuring the dataset. Additionally, we demonstrate one use
case wherein we compare a baseline gradient boosted decision tree model trained
using LightGBM with default settings to MalConv, a recently published
end-to-end (featureless) deep learning model for malware detection. Results
show that even without hyper-parameter optimization, the baseline EMBER model
outperforms MalConv. The authors hope that the dataset, code and baseline model
provided by EMBER will help invigorate machine learning research for malware
detection, in much the same way that benchmark datasets have advanced computer
vision research.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
Differential Privacy and Machine Learning: a Survey and Review,"  The objective of machine learning is to extract useful information from data,
while privacy is preserved by concealing information. Thus it seems hard to
reconcile these competing interests. However, they frequently must be balanced
when mining sensitive data. For example, medical research represents an
important application where it is necessary both to extract useful information
and protect patient privacy. One way to resolve the conflict is to extract
general characteristics of whole populations without disclosing the private
information of individuals.
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Databases (cs.DB),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'Databases(cs.DB)']","['cs.LG', 'cs.CR', 'cs.DB']"
Unsupervised Machine Learning on a Hybrid Quantum Computer,"  Machine learning techniques have led to broad adoption of a statistical model
of computing. The statistical distributions natively available on quantum
processors are a superset of those available classically. Harnessing this
attribute has the potential to accelerate or otherwise improve machine learning
relative to purely classical performance. A key challenge toward that goal is
learning to hybridize classical computing resources and traditional learning
techniques with the emerging capabilities of general purpose quantum
processors. Here, we demonstrate such hybridization by training a 19-qubit gate
model processor to solve a clustering problem, a foundational challenge in
unsupervised learning. We use the quantum approximate optimization algorithm in
conjunction with a gradient-free Bayesian optimization to train the quantum
machine. This quantum/classical hybrid algorithm shows robustness to realistic
noise, and we find evidence that classical optimization can be used to train
around both coherent and incoherent imperfections.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Malicious URL Detection using Machine Learning: A Survey,"  Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)']","['cs.LG', 'cs.CR']"
Quantum embeddings for machine learning,"  Quantum classifiers are trainable quantum circuits used as machine learning
models. The first part of the circuit implements a quantum feature map that
encodes classical inputs into quantum states, embedding the data in a
high-dimensional Hilbert space; the second part of the circuit executes a
quantum measurement interpreted as the output of the model. Usually, the
measurement is trained to distinguish quantum-embedded data. We propose to
instead train the first part of the circuit -- the embedding -- with the
objective of maximally separating data classes in Hilbert space, a strategy we
call quantum metric learning. As a result, the measurement minimizing a linear
classification loss is already known and depends on the metric used: for
embeddings separating data using the l1 or trace distance, this is the Helstrom
measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple
overlap measurement. This approach provides a powerful analytic framework for
quantum machine learning and eliminates a major component in current models,
freeing up more precious resources to best leverage the capabilities of
near-term quantum information processors.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
"A machine learning methodology for real-time forecasting of the 2019-2020 COVID-19 outbreak using Internet searches, news alerts, and estimates from mechanistic models","  We present a timely and novel methodology that combines disease estimates
from mechanistic models with digital traces, via interpretable machine-learning
methodologies, to reliably forecast COVID-19 activity in Chinese provinces in
real-time. Specifically, our method is able to produce stable and accurate
forecasts 2 days ahead of current time, and uses as inputs (a) official health
reports from Chinese Center Disease for Control and Prevention (China CDC), (b)
COVID-19-related internet search activity from Baidu, (c) news media activity
reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from
GLEAM, an agent-based mechanistic model. Our machine-learning methodology uses
a clustering technique that enables the exploitation of geo-spatial
synchronicities of COVID-19 activity across Chinese provinces, and a data
augmentation technique to deal with the small number of historical disease
activity observations, characteristic of emerging outbreaks. Our model's
predictive power outperforms a collection of baseline models in 27 out of the
32 Chinese provinces, and could be easily extended to other geographies
currently affected by the COVID-19 outbreak to help decision makers.

    ",Other Statistics (stat.OT),; Machine Learning (cs.LG); Populations and Evolution (q-bio.PE); Machine Learning (stat.ML),"['OtherStatistics(stat.OT)', 'MachineLearning(cs.LG)', 'PopulationsandEvolution(q-bio.PE)', 'MachineLearning(stat.ML)']","['stat.OT', 'cs.LG', 'q-bio.PE', 'stat.ML']"
A Differentiable Programming System to Bridge Machine Learning and Scientific Computing,"  Scientific computing is increasingly incorporating the advancements in
machine learning and the ability to work with large amounts of data. At the
same time, machine learning models are becoming increasingly sophisticated and
exhibit many features often seen in scientific computing, stressing the
capabilities of machine learning frameworks. Just as the disciplines of
scientific computing and machine learning have shared common underlying
infrastructure in the form of numerical linear algebra, we now have the
opportunity to further share new computational infrastructure, and thus ideas,
in the form of Differentiable Programming. We describe Zygote, a Differentiable
Programming system that is able to take gradients of general program
structures. We implement this system in the Julia programming language. Our
system supports almost all language constructs (control flow, recursion,
mutation, etc.) and compiles high-performance code without requiring any user
intervention or refactoring to stage computations. This enables an expressive
programming model for deep learning, but more importantly, it enables us to
incorporate a large ecosystem of libraries in our models in a straightforward
way. We discuss our approach to automatic differentiation, including its
support for advanced techniques such as mixed-mode, complex and checkpointed
differentiation, and present several examples of differentiating programs.

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG),"['ProgrammingLanguages(cs.PL)', 'MachineLearning(cs.LG)']","['cs.PL', 'cs.LG']"
Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,"  Deep learning models with convolutional and recurrent networks are now
ubiquitous and analyze massive amounts of audio, image, video, text and graph
data, with applications in automatic translation, speech-to-text, scene
understanding, ranking user preferences, ad placement, etc. Competing
frameworks for building these networks such as TensorFlow, Chainer, CNTK,
Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between
usability and expressiveness, research or production orientation and supported
hardware. They operate on a DAG of computational operators, wrapping
high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for
various CPUs), and automate memory allocation, synchronization, distribution.
Custom operators are needed where the computation does not fit existing
high-performance library calls, usually at a high engineering cost. This is
frequently required when new operators are invented by researchers: such
operators suffer a severe performance penalty, which limits the pace of
innovation. Furthermore, even if there is an existing runtime call these
frameworks can use, it often doesn't offer optimal performance for a user's
particular network architecture and dataset, missing optimizations between
operators as well as optimizations that can be done knowing the size and shape
of data. Our contributions include (1) a language close to the mathematics of
deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time
compiler to convert a mathematical description of a deep learning DAG into a
CUDA kernel with delegated memory management and synchronization, also
providing optimizations such as operator fusion and specialization for specific
sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG),"['ProgrammingLanguages(cs.PL)', 'MachineLearning(cs.LG)']","['cs.PL', 'cs.LG']"
6G White Paper on Machine Learning in Wireless Communication Networks,"  The focus of this white paper is on machine learning (ML) in wireless
communications. 6G wireless communication networks will be the backbone of the
digital transformation of societies by providing ubiquitous, reliable, and
near-instant wireless connectivity for humans and machines. Recent advances in
ML research has led enable a wide range of novel technologies such as
self-driving vehicles and voice assistants. Such innovation is possible as a
result of the availability of advanced ML models, large datasets, and high
computational power. On the other hand, the ever-increasing demand for
connectivity will require a lot of innovation in 6G wireless networks, and ML
tools will play a major role in solving problems in the wireless domain. In
this paper, we provide an overview of the vision of how ML will impact the
wireless communication systems. We first give an overview of the ML methods
that have the highest potential to be used in wireless networks. Then, we
discuss the problems that can be solved by using ML in various layers of the
network such as the physical layer, medium access layer, and application layer.
Zero-touch optimization of wireless networks using ML is another interesting
aspect that is discussed in this paper. Finally, at the end of each section,
important research questions that the section aims to answer are presented.

    ",Information Theory (cs.IT),; Signal Processing (eess.SP),"['InformationTheory(cs.IT)', 'SignalProcessing(eess.SP)']","['cs.IT', 'eess.SP']"
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"  Benchmark datasets have a significant impact on accelerating research in
programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark
dataset to foster machine learning research for program understanding and
generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and
a platform for model evaluation and comparison. CodeXGLUE also features three
baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder
models, to make it easy for researchers to use the platform. The availability
of such data and baselines can help the development and validation of new
methods that can be applied to various program understanding and generation
problems.

    ",Software Engineering (cs.SE),; Computation and Language (cs.CL),"['SoftwareEngineering(cs.SE)', 'ComputationandLanguage(cs.CL)']","['cs.SE', 'cs.CL']"
"Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination","  This paper covers the two approaches for sentiment analysis: i) lexicon based
method; ii) machine learning method. We describe several techniques to
implement these approaches and discuss how they can be adopted for sentiment
classification of Twitter messages. We present a comparative study of different
lexicon combinations and show that enhancing sentiment lexicons with emoticons,
abbreviations and social-media slang expressions increases the accuracy of
lexicon-based classification for Twitter. We discuss the importance of feature
generation and feature selection processes for machine learning sentiment
classification. To quantify the performance of the main sentiment analysis
methods over Twitter we run these algorithms on a benchmark Twitter dataset
from the SemEval-2013 competition, task 2-B. The results show that machine
learning method based on SVM and Naive Bayes classifiers outperforms the
lexicon method. We present a new ensemble method that uses a lexicon based
sentiment score as input feature for the machine learning approach. The
combined method proved to produce more precise classifications. We also show
that employing a cost-sensitive classifier for highly unbalanced datasets
yields an improvement of sentiment classification performance up to 7%.

    ",Computation and Language (cs.CL),; Information Retrieval (cs.IR); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML),"['ComputationandLanguage(cs.CL)', 'InformationRetrieval(cs.IR)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)', 'MachineLearning(stat.ML)']","['cs.CL', 'cs.IR', 'cs.LG', 'stat.ME', 'stat.ML']"
sktime: A Unified Interface for Machine Learning with Time Series,"  We present sktime -- a new scikit-learn compatible Python library with a
unified interface for machine learning with time series. Time series data gives
rise to various distinct but closely related learning tasks, such as
forecasting and time series classification, many of which can be solved by
reducing them to related simpler tasks. We discuss the main rationale for
creating a unified interface, including reduction, as well as the design of
sktime's core API, supported by a clear overview of common time series tasks
and reduction approaches.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Troubling Trends in Machine Learning Scholarship,"  Collectively, machine learning (ML) researchers are engaged in the creation
and dissemination of knowledge about data-driven algorithms. In a given paper,
researchers might aspire to any subset of the following goals, among others: to
theoretically characterize what is learnable, to obtain understanding through
empirically rigorous experiments, or to build a working system that has high
predictive accuracy. While determining which knowledge warrants inquiry may be
subjective, once the topic is fixed, papers are most valuable to the community
when they act in service of the reader, creating foundational knowledge and
communicating as clearly as possible.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.AI', 'cs.LG']"
Introduction to Tensor Decompositions and their Applications in Machine Learning,"  Tensors are multidimensional arrays of numerical values and therefore
generalize matrices to multiple dimensions. While tensors first emerged in the
psychometrics community in the $20^{\text{th}}$ century, they have since then
spread to numerous other disciplines, including machine learning. Tensors and
their decompositions are especially beneficial in unsupervised learning
settings, but are gaining popularity in other sub-disciplines like temporal and
multi-relational data analysis, too.
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Comparing BERT against traditional machine learning text classification,"  The BERT model has arisen as a popular state-of-the-art machine learning
model in the recent years that is able to cope with multiple NLP tasks such as
supervised text classification without human supervision. Its flexibility to
cope with any type of corpus delivering great results has make this approach
very popular not only in academia but also in the industry. Although, there are
lots of different approaches that have been used throughout the years with
success. In this work, we first present BERT and include a little review on
classical NLP approaches. Then, we empirically test with a suite of experiments
dealing different scenarios the behaviour of BERT against the traditional
TF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work
is to add empirical evidence to support or refuse the use of BERT as a default
on NLP tasks. Experiments show the superiority of BERT and its independence of
features of the NLP problem such as the language of the text adding empirical
evidence to use BERT as a default technique to be used in NLP problems.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CL', 'cs.LG', 'stat.ML']"
Machine Teaching: A New Paradigm for Building Machine Learning Systems,"  The current processes for building machine learning systems require
practitioners with deep knowledge of machine learning. This significantly
limits the number of machine learning systems that can be created and has led
to a mismatch between the demand for machine learning systems and the ability
for organizations to build them. We believe that in order to meet this growing
demand for machine learning systems we must significantly increase the number
of individuals that can teach machines. We postulate that we can achieve this
goal by making the process of teaching machines easy, fast and above all,
universally accessible.
",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'Human-ComputerInteraction(cs.HC)', 'SoftwareEngineering(cs.SE)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.HC', 'cs.SE', 'stat.ML']"
Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning,"  Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'stat.ML']"
Detection of Unauthorized IoT Devices Using Machine Learning Techniques,"  Security experts have demonstrated numerous risks imposed by Internet of
Things (IoT) devices on organizations. Due to the widespread adoption of such
devices, their diversity, standardization obstacles, and inherent mobility,
organizations require an intelligent mechanism capable of automatically
detecting suspicious IoT devices connected to their networks. In particular,
devices not included in a white list of trustworthy IoT device types (allowed
to be used within the organizational premises) should be detected. In this
research, Random Forest, a supervised machine learning algorithm, was applied
to features extracted from network traffic data with the aim of accurately
identifying IoT device types from the white list. To train and evaluate
multi-class classifiers, we collected and manually labeled network traffic data
from 17 distinct IoT devices, representing nine types of IoT devices. Based on
the classification of 20 consecutive sessions and the use of majority rule, IoT
device types that are not on the white list were correctly detected as unknown
in 96% of test cases (on average), and white listed device types were correctly
classified by their actual types in 99% of cases. Some IoT device types were
identified quicker than others (e.g., sockets and thermostats were successfully
detected within five TCP sessions of connecting to the network). Perfect
detection of unauthorized IoT device types was achieved upon analyzing 110
consecutive sessions; perfect classification of white listed types required 346
consecutive sessions, 110 of which resulted in 99.49% accuracy. Further
experiments demonstrated the successful applicability of classifiers trained in
one location and tested on another. In addition, a discussion is provided
regarding the resilience of our machine learning-based IoT white listing method
to adversarial attacks.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV),"['CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.CR', 'cs.CV']"
OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"  Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems,"  Several researchers have argued that a machine learning system's
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent's role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
On Formalizing Fairness in Prediction with Machine Learning,"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
bartMachine: Machine Learning with Bayesian Additive Regression Trees,"  We present a new package in R implementing Bayesian additive regression trees
(BART). The package introduces many new features for data analysis using BART
such as variable selection, interaction detection, model diagnostic plots,
incorporation of missing data and the ability to save trees for future
prediction. It is significantly faster than the current R implementation,
parallelized, and capable of handling both large sample sizes and
high-dimensional data.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Machine Learning for Precipitation Nowcasting from Radar Images,"  High-resolution nowcasting is an essential tool needed for effective
adaptation to climate change, particularly for extreme weather. As Deep
Learning (DL) techniques have shown dramatic promise in many domains, including
the geosciences, we present an application of DL to the problem of
precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1
hour) predictions of precipitation. We treat forecasting as an image-to-image
translation problem and leverage the power of the ubiquitous UNET convolutional
neural network. We find this performs favorably when compared to three commonly
used models: optical flow, persistence and NOAA's numerical one-hour HRRR
nowcasting prediction.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.CV', 'cs.LG', 'stat.ML']"
Supervised quantum machine learning models are kernel methods,"  With near-term quantum devices available and the race for fault-tolerant
quantum computers in full swing, researchers became interested in the question
of what happens if we replace a supervised machine learning model with a
quantum circuit. While such ""quantum models"" are sometimes called ""quantum
neural networks"", it has been repeatedly noted that their mathematical
structure is actually much more closely related to kernel methods: they analyse
data in high-dimensional Hilbert spaces to which we only have access through
inner products revealed by measurements. This technical manuscript summarises
and extends the idea of systematically rephrasing supervised quantum models as
a kernel method. With this, a lot of near-term and fault-tolerant quantum
models can be replaced by a general support vector machine whose kernel
computes distances between data-encoding quantum states. Kernel-based training
is then guaranteed to find better or equally good quantum models than
variational circuit training. Overall, the kernel perspective of quantum
machine learning tells us that the way that data is encoded into quantum states
is the main ingredient that can potentially set quantum models apart from
classical machine learning models.

    ",Quantum Physics (quant-ph),; Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(stat.ML)']","['quant-ph', 'stat.ML']"
Unsupervised machine learning and band topology,"  The study of topological bandstructures is an active area of research in
condensed matter physics and beyond. Here, we combine recent progress in this
field with developments in machine-learning, another rising topic of interest.
Specifically, we introduce an unsupervised machine-learning approach that
searches for and retrieves paths of adiabatic deformations between
Hamiltonians, thereby clustering them according to their topological
properties. The algorithm is general as it does not rely on a specific
parameterization of the Hamiltonian and is readily applicable to any symmetry
class. We demonstrate the approach using several different models in both one
and two spatial dimensions and for different symmetry classes with and without
crystalline symmetries. Accordingly, it is also shown how trivial and
topological phases can be diagnosed upon comparing with a generally designated
set of trivial atomic insulators.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),; Strongly Correlated Electrons (cond-mat.str-el); Computational Physics (physics.comp-ph),"['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'ComputationalPhysics(physics.comp-ph)']","['cond-mat.mes-hall', 'cond-mat.str-el', 'physics.comp-ph']"
Certified Data Removal from Machine Learning Models,"  Good data stewardship requires removal of data at the request of the data's
owner. This raises the question if and how a trained machine-learning model,
which implicitly stores information about its training data, should be affected
by such a removal request. Is it possible to ""remove"" data from a
machine-learning model? We study this problem by defining certified removal: a
very strong theoretical guarantee that a model from which data is removed
cannot be distinguished from a model that never observed the data to begin
with. We develop a certified-removal mechanism for linear classifiers and
empirically study learning settings in which this mechanism is practical.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation,"  Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable in the specific context
of verification. Suppose we have a machine learning system that predicts X, and
we provide rationale for this prediction X. Given an input, an explanation, and
an output, is the output consistent with the input and the supposed rationale?
Via a series of user-studies, we identify what kinds of increases in complexity
have the greatest effect on the time it takes for humans to verify the
rationale, and which seem relatively insensitive.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Automated Machine Learning: State-of-The-Art and Open Challenges,"  With the continuous and vast increase in the amount of data in our digital
world, it has been acknowledged that the number of knowledgeable data
scientists can not scale to address these challenges. Thus, there was a crucial
need for automating the process of building good machine learning models. In
the last few years, several techniques and frameworks have been introduced to
tackle the challenge of automating the process of Combined Algorithm Selection
and Hyper-parameter tuning (CASH) in the machine learning domain. The main aim
of these techniques is to reduce the role of the human in the loop and fill the
gap for non-expert machine learning users by playing the role of the domain
expert.
",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Chiron: Privacy-preserving Machine Learning as a Service,"  Major cloud operators offer machine learning (ML) as a service, enabling
customers who have the data but not ML expertise or infrastructure to train
predictive models on this data. Existing ML-as-a-service platforms require
users to reveal all training data to the service operator. We design,
implement, and evaluate Chiron, a system for privacy-preserving machine
learning as a service. First, Chiron conceals the training data from the
service operator. Second, in keeping with how many existing ML-as-a-service
platforms work, Chiron reveals neither the training algorithm nor the model
structure to the user, providing only black-box access to the trained model.
Chiron is implemented using SGX enclaves, but SGX alone does not achieve the
dual goals of data privacy and model confidentiality. Chiron runs the standard
ML training toolchain (including the popular Theano framework and C compiler)
in an enclave, but the untrusted model-creation code from the service operator
is further confined in a Ryoan sandbox to prevent it from leaking the training
data outside the enclave. To support distributed training, Chiron executes
multiple concurrent enclaves that exchange model parameters via a parameter
server. We evaluate Chiron on popular deep learning models, focusing on
benchmark image classification tasks such as CIFAR and ImageNet, and show that
its training performance and accuracy of the resulting models are practical for
common uses of ML-as-a-service.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning,"  The incidence of malignant melanoma continues to increase worldwide. This
cancer can strike at any age; it is one of the leading causes of loss of life
in young persons. Since this cancer is visible on the skin, it is potentially
detectable at a very early stage when it is curable. New developments have
converged to make fully automatic early melanoma detection a real possibility.
First, the advent of dermoscopy has enabled a dramatic boost in clinical
diagnostic ability to the point that melanoma can be detected in the clinic at
the very earliest stages. The global adoption of this technology has allowed
accumulation of large collections of dermoscopy images of melanomas and benign
lesions validated by histopathology. The development of advanced technologies
in the areas of image processing and machine learning have given us the ability
to allow distinction of malignant melanoma from the many benign mimics that
require no biopsy. These new technologies should allow not only earlier
detection of melanoma, but also reduction of the large number of needless and
costly biopsy procedures. Although some of the new systems reported for these
technologies have shown promise in preliminary trials, widespread
implementation must await further technical progress in accuracy and
reproducibility. In this paper, we provide an overview of computerized
detection of melanoma in dermoscopy images. First, we discuss the various
aspects of lesion segmentation. Then, we provide a brief overview of clinical
feature segmentation. Finally, we discuss the classification stage where
machine learning algorithms are applied to the attributes generated from the
segmented features to predict the existence of melanoma.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (stat.ML),"['ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(stat.ML)']","['cs.CV', 'stat.ML']"
Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning,"  In this paper, we propose an automated computer platform for the purpose of
classifying Electroencephalography (EEG) signals associated with left and right
hand movements using a hybrid system that uses advanced feature extraction
techniques and machine learning algorithms. It is known that EEG represents the
brain activity by the electrical voltage fluctuations along the scalp, and
Brain-Computer Interface (BCI) is a device that enables the use of the brain
neural activity to communicate with others or to control machines, artificial
limbs, or robots without direct physical movements. In our research work, we
aspired to find the best feature extraction method that enables the
differentiation between left and right executed fist movements through various
classification algorithms. The EEG dataset used in this research was created
and contributed to PhysioNet by the developers of the BCI2000 instrumentation
system. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts
removal was done using AAR. Data was epoched on the basis of Event-Related (De)
Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)
features. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta
rhythms were isolated for the MRCP analysis. The Independent Component Analysis
(ICA) spatial filter was applied on related channels for noise reduction and
isolation of both artifactually and neutrally generated EEG sources. The final
feature vector included the ERD, ERS, and MRCP features in addition to the
mean, power and energy of the activations of the resulting independent
components of the epoched feature datasets. The datasets were inputted into two
machine-learning algorithms: Neural Networks (NNs) and Support Vector Machines
(SVMs). Intensive experiments were carried out and optimum classification
performances of 89.8 and 97.1 were obtained using NN and SVM, respectively.

    ",Neural and Evolutionary Computing (cs.NE),; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC),"['NeuralandEvolutionaryComputing(cs.NE)', 'ComputerVisionandPatternRecognition(cs.CV)', 'Human-ComputerInteraction(cs.HC)']","['cs.NE', 'cs.CV', 'cs.HC']"
Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning,"  Machine learning is a popular approach to signatureless malware detection
because it can generalize to never-before-seen malware families and polymorphic
strains. This has resulted in its practical use for either primary detection
engines or for supplementary heuristic detection by anti-malware vendors.
Recent work in adversarial machine learning has shown that deep learning models
are susceptible to gradient-based attacks, whereas non-differentiable models
that report a score can be attacked by genetic algorithms that aim to
systematically reduce the score. We propose a more general framework based on
reinforcement learning (RL) for attacking static portable executable (PE)
anti-malware engines. The general framework does not require a differentiable
model nor does it require the engine to produce a score. Instead, an RL agent
is equipped with a set of functionality-preserving operations that it may
perform on the PE file. Through a series of games played against the
anti-malware engine, it learns which sequences of operations are likely to
result in evading the detector for any given malware sample. This enables
completely black-box attacks against static PE anti-malware, and produces
functional evasive malware samples as a direct result. We show in experiments
that our method can attack a gradient-boosted machine learning model with
evasion rates that are substantial and appear to be strongly dependent on the
dataset. We demonstrate that attacks against this model appear to also evade
components of publicly hosted antivirus engines. Adversarial training results
are also presented: by retraining the model on evasive ransomware samples, a
subsequent attack is 33% less effective. However, there are overfitting dangers
when adversarial training, which we note. We release code to allow researchers
to reproduce and improve this approach.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
BLAZE: Blazing Fast Privacy-Preserving Machine Learning,"  Machine learning tools have illustrated their potential in many significant
sectors such as healthcare and finance, to aide in deriving useful inferences.
The sensitive and confidential nature of the data, in such sectors, raise
natural concerns for the privacy of data. This motivated the area of
Privacy-preserving Machine Learning (PPML) where privacy of the data is
guaranteed. Typically, ML techniques require large computing power, which leads
clients with limited infrastructure to rely on the method of Secure Outsourced
Computation (SOC). In SOC setting, the computation is outsourced to a set of
specialized and powerful cloud servers and the service is availed on a
pay-per-use basis. In this work, we explore PPML techniques in the SOC setting
for widely used ML algorithms-- Linear Regression, Logistic Regression, and
Neural Networks.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"  In response to algorithmic unfairness embedded in sociotechnical systems,
significant attention has been focused on the contents of machine learning
datasets which have revealed biases towards white, cisgender, male, and Western
data subjects. In contrast, comparatively less attention has been paid to the
histories, values, and norms embedded in such datasets. In this work, we
outline a research program - a genealogy of machine learning data - for
investigating how and why these datasets have been created, what and whose
values influence the choices of data to collect, the contextual and contingent
conditions of their creation. We describe the ways in which benchmark datasets
in machine learning operate as infrastructure and pose four research questions
for these datasets. This interrogation forces us to ""bring the people back in""
by aiding us in understanding the labor embedded in dataset construction, and
thereby presenting new avenues of contestation for other researchers
encountering the data.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach,"  Toxic online content has become a major issue in today's world due to an
exponential increase in the use of internet by people of different cultures and
educational background. Differentiating hate speech and offensive language is a
key challenge in automatic detection of toxic text content. In this paper, we
propose an approach to automatically classify tweets on Twitter into three
classes: hateful, offensive and clean. Using Twitter dataset, we perform
experiments considering n-grams as features and passing their term
frequency-inverse document frequency (TFIDF) values to multiple machine
learning models. We perform comparative analysis of the models considering
several values of n in n-grams and TFIDF normalization methods. After tuning
the model giving the best results, we achieve 95.6% accuracy upon evaluating it
on test data. We also create a module which serves as an intermediate between
user and Twitter.

    ",Computation and Language (cs.CL),,['ComputationandLanguage(cs.CL)'],['cs.CL']
Participation is not a Design Fix for Machine Learning,"  This paper critically examines existing modes of participation in design
practice and machine learning. Cautioning against 'participation-washing', it
suggests that the ML community must become attuned to possibly exploitative and
extractive forms of community involvement and shift away from the prerogatives
of context-independent scalability.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
A Time Series Analysis-Based Stock Price Prediction Using Machine Learning and Deep Learning Models,"  Prediction of future movement of stock prices has always been a challenging
task for the researchers. While the advocates of the efficient market
hypothesis (EMH) believe that it is impossible to design any predictive
framework that can accurately predict the movement of stock prices, there are
seminal work in the literature that have clearly demonstrated that the
seemingly random movement patterns in the time series of a stock price can be
predicted with a high level of accuracy. Design of such predictive models
requires choice of appropriate variables, right transformation methods of the
variables, and tuning of the parameters of the models. In this work, we present
a very robust and accurate framework of stock price prediction that consists of
an agglomeration of statistical, machine learning and deep learning models. We
use the daily stock price data, collected at five minutes interval of time, of
a very well known company that is listed in the National Stock Exchange (NSE)
of India. The granular data is aggregated into three slots in a day, and the
aggregated data is used for building and training the forecasting models. We
contend that the agglomerative approach of model building that uses a
combination of statistical, machine learning, and deep learning approaches, can
very effectively learn from the volatile and random movement patterns in a
stock price data. We build eight classification and eight regression models
based on statistical and machine learning approaches. In addition to these
models, a deep learning regression model using a long-and-short-term memory
(LSTM) network is also built. Extensive results have been presented on the
performance of these models, and the results are critically analyzed.

    ",Statistical Finance (q-fin.ST),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['StatisticalFinance(q-fin.ST)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['q-fin.ST', 'cs.LG', 'stat.ML']"
TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games,"  We present TorchCraft, a library that enables deep learning research on
Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it
easier to control these games from a machine learning framework, here Torch.
This white paper argues for using RTS games as a benchmark for AI research, and
describes the design and components of TorchCraft.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Analysis of the COVID-19 pandemic by SIR model and machine learning technics for forecasting,"  This work is a trial in which we propose SIR model and machine learning tools
to analyze the coronavirus pandemic in the real world. Based on the public data
from \cite{datahub}, we estimate main key pandemic parameters and make
predictions on the inflection point and possible ending time for the real world
and specifically for Senegal. The coronavirus disease 2019, by World Health
Organization, rapidly spread out in the whole China and then in the whole
world. Under optimistic estimation, the pandemic in some countries will end
soon, while for most part of countries in the world (US, Italy, etc.), the hit
of anti-pandemic will be no later than the end of April.

    ",Populations and Evolution (q-bio.PE),; Optimization and Control (math.OC); Machine Learning (stat.ML),"['PopulationsandEvolution(q-bio.PE)', 'OptimizationandControl(math.OC)', 'MachineLearning(stat.ML)']","['q-bio.PE', 'math.OC', 'stat.ML']"
Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development,"  Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
",Machine Learning (cs.LG),; Computers and Society (cs.CY); Biomolecules (q-bio.BM); Quantitative Methods (q-bio.QM),"['MachineLearning(cs.LG)', 'ComputersandSociety(cs.CY)', 'Biomolecules(q-bio.BM)', 'QuantitativeMethods(q-bio.QM)']","['cs.LG', 'cs.CY', 'q-bio.BM', 'q-bio.QM']"
A survey on measuring indirect discrimination in machine learning,"  Nowadays, many decisions are made using predictive models built on historical
data.Predictive models may systematically discriminate groups of people even if
the computing process is fair and well-intentioned. Discrimination-aware data
mining studies how to make predictive models free from discrimination, when
historical data, on which they are built, may be biased, incomplete, or even
contain past discriminatory decisions. Discrimination refers to disadvantageous
treatment of a person based on belonging to a category rather than on
individual merit. In this survey we review and organize various discrimination
measures that have been used for measuring discrimination in data, as well as
in evaluating performance of discrimination-aware predictive models. We also
discuss related measures from other disciplines, which have not been used for
measuring discrimination, but potentially could be suitable for this purpose.
We computationally analyze properties of selected measures. We also review and
discuss measuring procedures, and present recommendations for practitioners.
The primary target audience is data mining, machine learning, pattern
recognition, statistical modeling researchers developing new methods for
non-discriminatory predictive modeling. In addition, practitioners and policy
makers would use the survey for diagnosing potential discrimination by
predictive models.

    ",Computers and Society (cs.CY),; Applications (stat.AP),"['ComputersandSociety(cs.CY)', 'Applications(stat.AP)']","['cs.CY', 'stat.AP']"
Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers,"  To construct interpretable explanations that are consistent with the original
ML model, counterfactual examples---showing how the model's output changes with
small perturbations to the input---have been proposed. This paper extends the
work in counterfactual explanations by addressing the challenge of feasibility
of such examples. For explanations of ML models in critical domains such as
healthcare and finance, counterfactual examples are useful for an end-user only
to the extent that perturbation of feature inputs is feasible in the real
world. We formulate the problem of feasibility as preserving causal
relationships among input features and present a method that uses (partial)
structural causal models to generate actionable counterfactuals. When
feasibility constraints cannot be easily expressed, we consider an alternative
mechanism where people can label generated CF examples on feasibility: whether
it is feasible to intervene and realize the candidate CF example from the
original input. To learn from this labelled feasibility data, we propose a
modified variational auto encoder loss for generating CF examples that
optimizes for feasibility as people interact with its output. Our experiments
on Bayesian networks and the widely used ''Adult-Income'' dataset show that our
proposed methods can generate counterfactual explanations that better satisfy
feasibility constraints than existing methods.. Code repository can be accessed
here: \textit{",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
A Survey of Privacy Attacks in Machine Learning,"  As machine learning becomes more widely used, the need to study its
implications in security and privacy becomes more urgent. Although the body of
work in privacy has been steadily growing over the past few years, research on
the privacy aspects of machine learning has received less focus than the
security aspects. Our contribution in this research is an analysis of more than
40 papers related to privacy attacks against machine learning that have been
published during the past seven years. We propose an attack taxonomy, together
with a threat model that allows the categorization of different attacks based
on the adversarial knowledge, and the assets under attack. An initial
exploration of the causes of privacy leaks is presented, as well as a detailed
analysis of the different attacks. Finally, we present an overview of the most
commonly proposed defenses and a discussion of the open problems and future
directions identified during our analysis.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"  Bioinformatics research is characterized by voluminous and incremental
datasets and complex data analytics methods. The machine learning methods used
in bioinformatics are iterative and parallel. These methods can be scaled to
handle big data using the distributed and parallel computing technologies.
","Computational Engineering, Finance, and Science (cs.CE)",; Machine Learning (cs.LG),"['ComputationalEngineering,Finance,andScience(cs.CE)', 'MachineLearning(cs.LG)']","['cs.CE', 'cs.LG']"
Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning,"  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.

    ",Computation and Language (cs.CL),,['ComputationandLanguage(cs.CL)'],['cs.CL']
A Living Review of Machine Learning for Particle Physics,"  Modern machine learning techniques, including deep learning, are rapidly
being applied, adapted, and developed for high energy physics. Given the fast
pace of this research, we have created a living review with the goal of
providing a nearly comprehensive list of citations for those developing and
applying these approaches to experimental, phenomenological, or theoretical
analyses. As a living document, it will be updated as often as possible to
incorporate the latest developments. A list of proper (unchanging) reviews can
be found within. Papers are grouped into a small set of topics to be as useful
as possible. Suggestions and contributions are most welcome, and we provide
instructions for participating.

    ",High Energy Physics - Phenomenology (hep-ph),"; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)","['HighEnergyPhysics-Phenomenology(hep-ph)', 'MachineLearning(cs.LG)', 'HighEnergyPhysics-Experiment(hep-ex)', 'DataAnalysis,StatisticsandProbability(physics.data-an)', 'MachineLearning(stat.ML)']","['hep-ph', 'cs.LG', 'hep-ex', 'physics.data-an', 'stat.ML']"
A Primer on the Signature Method in Machine Learning,"  In these notes, we wish to provide an introduction to the signature method,
focusing on its basic theoretical properties and recent numerical applications.
",Machine Learning (stat.ML),; Machine Learning (cs.LG); Methodology (stat.ME),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)']","['stat.ML', 'cs.LG', 'stat.ME']"
Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"  Autonomous vehicles rely on machine learning to solve challenging tasks in
perception and motion planning. However, automotive software safety standards
have not fully evolved to address the challenges of machine learning safety
such as interpretability, verification, and performance limitations. In this
paper, we review and organize practical machine learning safety techniques that
can complement engineering safety for machine learning based software in
autonomous vehicles. Our organization maps safety strategies to
state-of-the-art machine learning techniques in order to enhance dependability
and safety of machine learning algorithms. We also discuss security limitations
and user experience aspects of machine learning components in autonomous
vehicles.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Unified Representation of Molecules and Crystals for Machine Learning,"  Accurate simulations of atomistic systems from first principles are limited
by computational cost. In high-throughput settings, machine learning can reduce
these costs significantly by accurately interpolating between reference
calculations. For this, kernel learning approaches crucially require a
representation that accommodates arbitrary atomistic systems. We introduce a
many-body tensor representation that is invariant to translations, rotations,
and nuclear permutations of same elements, unique, differentiable, can
represent molecules and crystals, and is fast to compute. Empirical evidence
for competitive energy and force prediction errors is presented for changes in
molecular structure, crystal chemistry, and molecular dynamics using kernel
regression and symmetric gradient-domain machine learning as models.
Applicability is demonstrated for phase diagrams of Pt-group/transition-metal
binary systems.

    ",Chemical Physics (physics.chem-ph),; Materials Science (cond-mat.mtrl-sci),"['ChemicalPhysics(physics.chem-ph)', 'MaterialsScience(cond-mat.mtrl-sci)']","['physics.chem-ph', 'cond-mat.mtrl-sci']"
AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling,"  Interpretability of the underlying AI representations is a key raison
d'tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of
learners' cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.

    ",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY),"['ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.AI', 'cs.CY']"
Safely Entering the Deep: A Review of Verification and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry,"  Deep Neural Networks (DNN) will emerge as a cornerstone in automotive
software engineering. However, developing systems with DNNs introduces novel
challenges for safety assessments. This paper reviews the state-of-the-art in
verification and validation of safety-critical systems that rely on machine
learning. Furthermore, we report from a workshop series on DNNs for perception
with automotive experts in Sweden, confirming that ISO 26262 largely
contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge
transfer and systems-based safety approaches, e.g., safety cage architectures
and simulated system test cases.

    ",Software Engineering (cs.SE),,['SoftwareEngineering(cs.SE)'],['cs.SE']
Machine Learning Techniques for Intrusion Detection,"  An Intrusion Detection System (IDS) is a software that monitors a single or a
network of computers for malicious activities (attacks) that are aimed at
stealing or censoring information or corrupting network protocols. Most
techniques used in today's IDS are not able to deal with the dynamic and
complex nature of cyber attacks on computer networks. Hence, efficient adaptive
methods like various techniques of machine learning can result in higher
detection rates, lower false alarm rates and reasonable computation and
communication costs. In this paper, we study several such schemes and compare
their performance. We divide the schemes into methods based on classical
artificial intelligence (AI) and methods based on computational intelligence
(CI). We explain how various characteristics of CI techniques can be used to
build efficient IDS.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)']","['cs.CR', 'cs.LG', 'cs.NI']"
Trustless Machine Learning Contracts; Evaluating and Exchanging Machine Learning Models on the Ethereum Blockchain,"  Using blockchain technology, it is possible to create contracts that offer a
reward in exchange for a trained machine learning model for a particular data
set. This would allow users to train machine learning models for a reward in a
trustless manner. The smart contract will use the blockchain to automatically
validate the solution, so there would be no debate about whether the solution
was correct or not. Users who submit the solutions won't have counterparty risk
that they won't get paid for their work. Contracts can be created easily by
anyone with a dataset, even programmatically by software agents. This creates a
market where parties who are good at solving machine learning problems can
directly monetize their skillset, and where any organization or software agent
that has a problem to solve with AI can solicit solutions from all over the
world. This will incentivize the creation of better machine learning models,
and make AI more accessible to companies and software agents.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
Machine learning based hyperspectral image analysis: A survey,"  Hyperspectral sensors enable the study of the chemical properties of scene
materials remotely for the purpose of identification, detection, and chemical
composition analysis of objects in the environment. Hence, hyperspectral images
captured from earth observing satellites and aircraft have been increasingly
important in agriculture, environmental monitoring, urban planning, mining, and
defense. Machine learning algorithms due to their outstanding predictive power
have become a key tool for modern hyperspectral image analysis. Therefore, a
solid understanding of machine learning techniques have become essential for
remote sensing researchers and practitioners. This paper reviews and compares
recent machine learning-based hyperspectral image analysis methods published in
literature. We organize the methods by the image analysis task and by the type
of machine learning algorithm, and present a two-way mapping between the image
analysis tasks and the types of machine learning algorithms that can be applied
to them. The paper is comprehensive in coverage of both hyperspectral image
analysis tasks and machine learning algorithms. The image analysis tasks
considered are land cover classification, target detection, unmixing, and
physical parameter estimation. The machine learning algorithms covered are
Gaussian models, linear regression, logistic regression, support vector
machines, Gaussian mixture model, latent linear models, sparse linear models,
Gaussian mixture models, ensemble learning, directed graphical models,
undirected graphical models, clustering, Gaussian processes, Dirichlet
processes, and deep learning. We also discuss the open challenges in the field
of hyperspectral image analysis and explore possible future directions.

    ",Computer Vision and Pattern Recognition (cs.CV),; Image and Video Processing (eess.IV),"['ComputerVisionandPatternRecognition(cs.CV)', 'ImageandVideoProcessing(eess.IV)']","['cs.CV', 'eess.IV']"
Automated software vulnerability detection with machine learning,"  Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.

    ",Software Engineering (cs.SE),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['SoftwareEngineering(cs.SE)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.SE', 'cs.LG', 'stat.ML']"
Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters,"  Time series forecasting is one of the most active research topics. Machine
learning methods have been increasingly adopted to solve these predictive
tasks. However, in a recent work, these were shown to systematically present a
lower predictive performance relative to simple statistical methods. In this
work, we counter these results. We show that these are only valid under an
extremely low sample size. Using a learning curve method, our results suggest
that machine learning methods improve their relative predictive performance as
the sample size grows. The code to reproduce the experiments is available at
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
An Analysis of ISO 26262: Using Machine Learning Safely in Automotive Software,"  Machine learning (ML) plays an ever-increasing role in advanced automotive
functionality for driver assistance and autonomous operation; however, its
adequacy from the perspective of safety certification remains controversial. In
this paper, we analyze the impacts that the use of ML as an implementation
approach has on ISO 26262 safety lifecycle and ask what could be done to
address them. We then provide a set of recommendations on how to adapt the
standard to accommodate ML.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Software Engineering (cs.SE); Systems and Control (eess.SY),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'SoftwareEngineering(cs.SE)', 'SystemsandControl(eess.SY)']","['cs.AI', 'cs.LG', 'cs.SE', 'eess.SY']"
Quantifying Interpretability and Trust in Machine Learning Systems,"  Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Adversarial Examples in Modern Machine Learning: A Review,"  Recent research has found that many families of machine learning models are
vulnerable to adversarial examples: inputs that are specifically designed to
cause the target model to produce erroneous outputs. In this survey, we focus
on machine learning models in the visual domain, where methods for generating
and detecting such examples have been most extensively studied. We explore a
variety of adversarial attack methods that apply to image-space content, real
world adversarial attacks, adversarial defenses, and the transferability
property of adversarial examples. We also discuss strengths and weaknesses of
various methods of adversarial attack and defense. Our aim is to provide an
extensive coverage of the field, furnishing the reader with an intuitive
understanding of the mechanics of adversarial attack and defense mechanisms and
enlarging the community of researchers studying this fundamental set of
problems.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML']"
A glass-box interactive machine learning approach for solving NP-hard problems with the human-in-the-loop,"  The goal of Machine Learning to automatically learn from data, extract
knowledge and to make decisions without any human intervention. Such automatic
(aML) approaches show impressive success. Recent results even demonstrate
intriguingly that deep learning applied for automatic classification of skin
lesions is on par with the performance of dermatologists, yet outperforms the
average. As human perception is inherently limited, such approaches can
discover patterns, e.g. that two objects are similar, in arbitrarily
high-dimensional spaces what no human is able to do. Humans can deal only with
limited amounts of data, whilst big data is beneficial for aML; however, in
health informatics, we are often confronted with a small number of data sets,
where aML suffer of insufficient training samples and many problems are
computationally hard. Here, interactive machine learning (iML) may be of help,
where a human-in-the-loop contributes to reduce the complexity of NP-hard
problems. A further motivation for iML is that standard black-box approaches
lack transparency, hence do not foster trust and acceptance of ML among
end-users. Rising legal and privacy aspects, e.g. with the new European General
Data Protection Regulations, make black-box approaches difficult to use,
because they often are not able to explain why a decision has been made. In
this paper, we present some experiments to demonstrate the effectiveness of the
human-in-the-loop approach, particularly in opening the black-box to a
glass-box and thus enabling a human directly to interact with an learning
algorithm. We selected the Ant Colony Optimization framework, and applied it on
the Traveling Salesman Problem, which is a good example, due to its relevance
for health informatics, e.g. for the study of protein folding. From studies of
how humans extract so much from so little data, fundamental ML-research also
may benefit.

    ",Artificial Intelligence (cs.AI),; Machine Learning (stat.ML),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.AI', 'stat.ML']"
Practical Coreset Constructions for Machine Learning,"  We investigate coresets - succinct, small summaries of large data sets - so
that solutions found on the summary are provably competitive with solution
found on the full data set. We provide an overview over the state-of-the-art in
coreset construction for machine learning. In Section 2, we present both the
intuition behind and a theoretically sound framework to construct coresets for
general problems and apply it to $k$-means clustering. In Section 3 we
summarize existing coreset construction algorithms for a variety of machine
learning problems such as maximum likelihood estimation of mixture models,
Bayesian non-parametric models, principal component analysis, regression and
general empirical risk minimization.

    ",Machine Learning (stat.ML),,['MachineLearning(stat.ML)'],['stat.ML']
Machine Learning with Multi-Site Imaging Data: An Empirical Study on the Impact of Scanner Effects,"  This is an empirical study to investigate the impact of scanner effects when
using machine learning on multi-site neuroimaging data. We utilize structural
T1-weighted brain MRI obtained from two different studies, Cam-CAN and UK
Biobank. For the purpose of our investigation, we construct a dataset
consisting of brain scans from 592 age- and sex-matched individuals, 296
subjects from each original study. Our results demonstrate that even after
careful pre-processing with state-of-the-art neuroimaging pipelines a
classifier can easily distinguish between the origin of the data with very high
accuracy. Our analysis on the example application of sex classification
suggests that current approaches to harmonize data are unable to remove
scanner-specific bias leading to overly optimistic performance estimates and
poor generalization. We conclude that multi-site data harmonization remains an
open challenge and particular care needs to be taken when using such data with
advanced machine learning methods for predictive modelling.

    ",Image and Video Processing (eess.IV),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC),"['ImageandVideoProcessing(eess.IV)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)', 'NeuronsandCognition(q-bio.NC)']","['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.NC']"
Machine Learning for Spatiotemporal Sequence Forecasting: A Survey,"  Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
mlpy: Machine Learning Python,"  mlpy is a Python Open Source Machine Learning library built on top of
NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of
state-of-the-art machine learning methods for supervised and unsupervised
problems and it is aimed at finding a reasonable compromise among modularity,
maintainability, reproducibility, usability and efficiency. mlpy is
multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at
the website ",Mathematical Software (cs.MS),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['MathematicalSoftware(cs.MS)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.MS', 'cs.LG', 'stat.ML']"
Interpretability and Explainability: A Machine Learning Zoo Mini-tour,"  In this review, we examine the problem of designing interpretable and
explainable machine learning models. Interpretability and explainability lie at
the core of many machine learning and statistical applications in medicine,
economics, law, and natural sciences. Although interpretability and
explainability have escaped a clear universal definition, many techniques
motivated by these properties have been developed over the recent 30 years with
the focus currently shifting towards deep learning methods. In this review, we
emphasise the divide between interpretability and explainability and illustrate
these two different research directions with concrete examples of the
state-of-the-art. The review is intended for a general machine learning
audience with interest in exploring the problems of interpretation and
explanation beyond logistic regression or random forest variable importance.
This work is not an exhaustive literature survey, but rather a primer focusing
selectively on certain lines of research which the authors found interesting or
informative.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
AlphaD3M: Machine Learning Pipeline Synthesis,"  We introduce AlphaD3M, an automatic machine learning (AutoML) system based on
meta reinforcement learning using sequence models with self play. AlphaD3M is
based on edit operations performed over machine learning pipeline primitives
providing explainability. We compare AlphaD3M with state-of-the-art AutoML
systems: Autosklearn, Autostacker, and TPOT, on OpenML datasets. AlphaD3M
achieves competitive performance while being an order of magnitude faster,
reducing computation time from hours to minutes, and is explainable by design.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Tunability: Importance of Hyperparameters of Machine Learning Algorithms,"  Modern supervised machine learning algorithms involve hyperparameters that
have to be set before running them. Options for setting hyperparameters are
default values from the software package, manual configuration by the user or
configuring them for optimal predictive performance by a tuning procedure. The
goal of this paper is two-fold. Firstly, we formalize the problem of tuning
from a statistical point of view, define data-based defaults and suggest
general measures quantifying the tunability of hyperparameters of algorithms.
Secondly, we conduct a large-scale benchmarking study based on 38 datasets from
the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield
default values for hyperparameters and enable users to decide whether it is
worth conducting a possibly time consuming tuning strategy, to focus on the
most important hyperparameters and to chose adequate hyperparameter spaces for
tuning.

    ",Machine Learning (stat.ML),,['MachineLearning(stat.ML)'],['stat.ML']
Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions,"  Low-dimensional embedding, manifold learning, clustering, classification, and
anomaly detection are among the most important problems in machine learning.
The existing methods usually consider the case when each instance has a fixed,
finite-dimensional feature representation. Here we consider a different
setting. We assume that each instance corresponds to a continuous probability
distribution. These distributions are unknown, but we are given some i.i.d.
samples from each distribution. Our goal is to estimate the distances between
these distributions and use these distances to perform low-dimensional
embedding, clustering/classification, or anomaly detection for the
distributions. We present estimation algorithms, describe how to apply them for
machine learning tasks on distributions, and show empirical results on
synthetic data, real word images, and astronomical data sets.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Rafiki: Machine Learning as an Analytics Service System,"  Big data analytics is gaining massive momentum in the last few years.
Applying machine learning models to big data has become an implicit requirement
or an expectation for most analysis tasks, especially on high-stakes
applications.Typical applications include sentiment analysis against reviews
for analyzing on-line products, image classification in food logging
applications for monitoring user's daily intake and stock movement prediction.
Extending traditional database systems to support the above analysis is
intriguing but challenging. First, it is almost impossible to implement all
machine learning models in the database engines. Second, expertise knowledge is
required to optimize the training and inference procedures in terms of
efficiency and effectiveness, which imposes heavy burden on the system users.
In this paper, we develop and present a system, called Rafiki, to provide the
training and inference service of machine learning models, and facilitate
complex analytics on top of cloud platforms. Rafiki provides distributed
hyper-parameter tuning for the training service, and online ensemble modeling
for the inference service which trades off between latency and accuracy.
Experimental results confirm the efficiency, effectiveness, scalability and
usability of Rafiki.

    ",Databases (cs.DB),"; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)","['Databases(cs.DB)', 'ArtificialIntelligence(cs.AI)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.DB', 'cs.AI', 'cs.DC']"
Automated Machine Learning on Graphs: A Survey,"  Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Enhancing Computational Fluid Dynamics with Machine Learning,"  Machine learning is rapidly becoming a core technology for scientific
computing, with numerous opportunities to advance the field of computational
fluid dynamics. In this Perspective, we highlight some of the areas of highest
potential impact, including to accelerate direct numerical simulations, to
improve turbulence closure modeling, and to develop enhanced reduced-order
models. We also discuss emerging areas of machine learning that are promising
for computational fluid dynamics, as well as some potential limitations that
should be taken into account.

    ",Fluid Dynamics (physics.flu-dyn),; Machine Learning (cs.LG); Computational Physics (physics.comp-ph),"['FluidDynamics(physics.flu-dyn)', 'MachineLearning(cs.LG)', 'ComputationalPhysics(physics.comp-ph)']","['physics.flu-dyn', 'cs.LG', 'physics.comp-ph']"
Private Machine Learning in TensorFlow using Secure Computation,"  We present a framework for experimenting with secure multi-party computation
directly in TensorFlow. By doing so we benefit from several properties valuable
to both researchers and practitioners, including tight integration with
ordinary machine learning processes, existing optimizations for distributed
computation in TensorFlow, high-level abstractions for expressing complex
algorithms and protocols, and an expanded set of familiar tooling. We give an
open source implementation of a state-of-the-art protocol and report on
concrete benchmarks using typical models from private machine learning.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
The Values Encoded in Machine Learning Research,"  Machine learning currently exerts an outsized influence on the world,
increasingly affecting institutional practices and impacted communities. It is
therefore critical that we question vague conceptions of the field as
value-neutral or universally beneficial, and investigate what specific values
the field is advancing. In this paper, we first introduce a method and
annotation scheme for studying the values encoded in documents such as research
papers. Applying the scheme, we analyze 100 highly cited machine learning
papers published at premier machine learning conferences, ICML and NeurIPS. We
annotate key features of papers which reveal their values: their justification
for their choice of project, which attributes of their project they uplift,
their consideration of potential negative consequences, and their institutional
affiliations and funding sources. We find that few of the papers justify how
their project connects to a societal need (15\%) and far fewer discuss negative
potential (1\%). Through line-by-line content analysis, we identify 59 values
that are uplifted in ML research, and, of these, we find that the papers most
frequently justify and assess themselves based on Performance, Generalization,
Quantitative evidence, Efficiency, Building on past work, and Novelty. We
present extensive textual evidence and identify key themes in the definitions
and operationalization of these values. Notably, we find systematic textual
evidence that these top values are being defined and applied with assumptions
and implications generally supporting the centralization of power.Finally, we
find increasingly close ties between these highly cited papers and tech
companies and elite universities.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computers and Society (cs.CY),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'ComputersandSociety(cs.CY)']","['cs.LG', 'cs.AI', 'cs.CY']"
"Verification for Machine Learning, Autonomy, and Neural Networks Survey","  This survey presents an overview of verification techniques for autonomous
systems, with a focus on safety-critical autonomous cyber-physical systems
(CPS) and subcomponents thereof. Autonomy in CPS is enabling by recent advances
in artificial intelligence (AI) and machine learning (ML) through approaches
such as deep neural networks (DNNs), embedded in so-called learning enabled
components (LECs) that accomplish tasks from classification to control.
Recently, the formal methods and formal verification community has developed
methods to characterize behaviors in these LECs with eventual goals of formally
verifying specifications for LECs, and this article presents a survey of many
of these recent approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter,"  In February 2016, World Health Organization declared the Zika outbreak a
Public Health Emergency of International Concern. With developing evidence it
can cause birth defects, and the Summer Olympics coming up in the worst
affected country, Brazil, the virus caught fire on social media. In this work,
use Zika as a case study in building a tool for tracking the misinformation
around health concerns on Twitter. We collect more than 13 million tweets --
spanning the initial reports in February 2016 and the Summer Olympics --
regarding the Zika outbreak and track rumors outlined by the World Health
Organization and Snopes fact checking website. The tool pipeline, which
incorporates health professionals, crowdsourcing, and machine learning, allows
us to capture health-related rumors around the world, as well as clarification
campaigns by reputable health organizations. In the case of Zika, we discover
an extremely bursty behavior of rumor-related topics, and show that, once the
questionable topic is detected, it is possible to identify rumor-bearing tweets
using automated techniques. Thus, we illustrate insights the proposed tools
provide into potentially harmful information on social media, allowing public
health researchers and practitioners to respond with a targeted and timely
action.

    ",Social and Information Networks (cs.SI),; Computers and Society (cs.CY),"['SocialandInformationNetworks(cs.SI)', 'ComputersandSociety(cs.CY)']","['cs.SI', 'cs.CY']"
Quantum Neuron: an elementary building block for machine learning on quantum computers,"  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the ""quantum neuron"", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.

    ",Quantum Physics (quant-ph),; Neural and Evolutionary Computing (cs.NE),"['QuantumPhysics(quant-ph)', 'NeuralandEvolutionaryComputing(cs.NE)']","['quant-ph', 'cs.NE']"
"Bayesian inference of chaotic dynamics by merging data assimilation, machine learning and expectation-maximization","  The reconstruction from observations of high-dimensional chaotic dynamics
such as geophysical flows is hampered by (i) the partial and noisy observations
that can realistically be obtained, (ii) the need to learn from long time
series of data, and (iii) the unstable nature of the dynamics. To achieve such
inference from the observations over long time series, it has been suggested to
combine data assimilation and machine learning in several ways. We show how to
unify these approaches from a Bayesian perspective using
expectation-maximization and coordinate descents. In doing so, the model, the
state trajectory and model error statistics are estimated all together.
Implementations and approximations of these methods are discussed. Finally, we
numerically and successfully test the approach on two relevant low-order
chaotic models with distinct identifiability.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'AtmosphericandOceanicPhysics(physics.ao-ph)']","['stat.ML', 'cs.LG', 'physics.ao-ph']"
Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't,"  The purpose of this article is to review the achievements made in the last
few years towards the understanding of the reasons behind the success and
subtleties of neural network-based machine learning. In the tradition of good
old applied mathematics, we will not only give attention to rigorous
mathematical results, but also the insight we have gained from careful
numerical experiments as well as the analysis of simplified models. Along the
way, we also list the open problems which we believe to be the most important
topics for further study. This is not a complete overview over this quickly
moving field, but we hope to provide a perspective which may be helpful
especially to new researchers in the area.

    ",Machine Learning (cs.LG),; Numerical Analysis (math.NA); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'NumericalAnalysis(math.NA)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.NA', 'stat.ML']"
Personalized explanation in machine learning: A conceptualization,"  Explanation in machine learning and related fields such as artificial
intelligence aims at making machine learning models and their decisions
understandable to humans. Existing work suggests that personalizing
explanations might help to improve understandability. In this work, we derive a
conceptualization of personalized explanation by defining and structuring the
problem based on prior work on machine learning explanation, personalization
(in machine learning) and concepts and techniques from other domains such as
privacy and knowledge elicitation. We perform a categorization of explainee
data used in the process of personalization as well as describing means to
collect this data. We also identify three key explanation properties that are
amendable to personalization: complexity, decision information and
presentation. We also enhance existing work on explanation by introducing
additional desiderata and measures to quantify the quality of personalized
explanations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
TensorNetwork: A Library for Physics and Machine Learning,"  TensorNetwork is an open source library for implementing tensor network
algorithms. Tensor networks are sparse data structures originally designed for
simulating quantum many-body physics, but are currently also applied in a
number of other research areas, including machine learning. We demonstrate the
use of the API with applications both physics and machine learning, with
details appearing in companion papers.

    ",Computational Physics (physics.comp-ph),; Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); High Energy Physics - Theory (hep-th); Machine Learning (stat.ML),"['ComputationalPhysics(physics.comp-ph)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'MachineLearning(cs.LG)', 'HighEnergyPhysics-Theory(hep-th)', 'MachineLearning(stat.ML)']","['physics.comp-ph', 'cond-mat.str-el', 'cs.LG', 'hep-th', 'stat.ML']"
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,"  Large computer-understandable proofs consist of millions of intermediate
logical steps. The vast majority of such steps originate from manually selected
and manually guided heuristics applied to intermediate goals. So far, machine
learning has generally not been used to filter or generate these steps. In this
paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for
the purpose of developing new machine learning-based theorem-proving
strategies. We make this dataset publicly available under the BSD license. We
propose various machine learning tasks that can be performed on this dataset,
and discuss their significance for theorem proving. We also benchmark a set of
simple baseline machine learning models suited for the tasks (including
logistic regression, convolutional neural networks and recurrent neural
networks). The results of our baseline models show the promise of applying
machine learning to HOL theorem proving.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Convergence Analysis of Machine Learning Algorithms for the Numerical Solution of Mean Field Control and Games: II -- The Finite Horizon Case,"  We propose two numerical methods for the optimal control of McKean-Vlasov
dynamics in finite time horizon. Both methods are based on the introduction of
a suitable loss function defined over the parameters of a neural network. This
allows the use of machine learning tools, and efficient implementations of
stochastic gradient descent in order to perform the optimization. In the first
method, the loss function stems directly from the optimal control problem. The
second method tackles a generic forward-backward stochastic differential
equation system (FBSDE) of McKean-Vlasov type, and relies on suitable
reformulation as a mean field control problem. To provide a guarantee on how
our numerical schemes approximate the solution of the original mean field
control problem, we introduce a new optimization problem, directly amenable to
numerical computation, and for which we rigorously provide an error rate.
Several numerical examples are provided. Both methods can easily be applied to
certain problems with common noise, which is not the case with the existing
technology. Furthermore, although the first approach is designed for mean field
control problems, the second is more general and can also be applied to the
FBSDE arising in the theory of mean field games.

    ",Optimization and Control (math.OC),; Machine Learning (cs.LG); Numerical Analysis (math.NA),"['OptimizationandControl(math.OC)', 'MachineLearning(cs.LG)', 'NumericalAnalysis(math.NA)']","['math.OC', 'cs.LG', 'math.NA']"
Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules,"  Predicting the relationship between a molecule's structure and its odor
remains a difficult, decades-old task. This problem, termed quantitative
structure-odor relationship (QSOR) modeling, is an important challenge in
chemistry, impacting human nutrition, manufacture of synthetic fragrance, the
environment, and sensory neuroscience. We propose the use of graph neural
networks for QSOR, and show they significantly out-perform prior methods on a
novel data set labeled by olfactory experts. Additional analysis shows that the
learned embeddings from graph neural networks capture a meaningful odor space
representation of the underlying relationship between structure and odor, as
demonstrated by strong performance on two challenging transfer learning tasks.
Machine learning has already had a large impact on the senses of sight and
sound. Based on these early results with graph neural networks for molecular
properties, we hope machine learning can eventually do for olfaction what it
has already done for vision and hearing.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)', 'ChemicalPhysics(physics.chem-ph)']","['stat.ML', 'cs.LG', 'physics.chem-ph']"
A Machine Learning Approach For Opinion Holder Extraction In Arabic Language,"  Opinion mining aims at extracting useful subjective information from reliable
amounts of text. Opinion mining holder recognition is a task that has not been
considered yet in Arabic Language. This task essentially requires deep
understanding of clauses structures. Unfortunately, the lack of a robust,
publicly available, Arabic parser further complicates the research. This paper
presents a leading research for the opinion holder extraction in Arabic news
independent from any lexical parsers. We investigate constructing a
comprehensive feature set to compensate the lack of parsing structural
outcomes. The proposed feature set is tuned from English previous works coupled
with our proposed semantic field and named entities features. Our feature
analysis is based on Conditional Random Fields (CRF) and semi-supervised
pattern recognition techniques. Different research models are evaluated via
cross-validation experiments achieving 54.03 F-measure. We publicly release our
own research outcome corpus and lexicon for opinion mining community to
encourage further research.

    ",Information Retrieval (cs.IR),; Machine Learning (cs.LG),"['InformationRetrieval(cs.IR)', 'MachineLearning(cs.LG)']","['cs.IR', 'cs.LG']"
An Overview of Privacy in Machine Learning,"  Over the past few years, providers such as Google, Microsoft, and Amazon have
started to provide customers with access to software interfaces allowing them
to easily embed machine learning tasks into their applications. Overall,
organizations can now use Machine Learning as a Service (MLaaS) engines to
outsource complex tasks, e.g., training classifiers, performing predictions,
clustering, etc. They can also let others query models trained on their data.
Naturally, this approach can also be used (and is often advocated) in other
contexts, including government collaborations, citizen science projects, and
business-to-business partnerships. However, if malicious users were able to
recover data used to train these models, the resulting information leakage
would create serious issues. Likewise, if the inner parameters of the model are
considered proprietary information, then access to the model should not allow
an adversary to learn such parameters. In this document, we set to review
privacy challenges in this space, providing a systematic review of the relevant
research literature, also exploring possible countermeasures. More
specifically, we provide ample background information on relevant concepts
around machine learning and privacy. Then, we discuss possible adversarial
models and settings, cover a wide range of attacks that relate to private
and/or sensitive information leakage, and review recent results attempting to
defend against such attacks. Finally, we conclude with a list of open problems
that require more work, including the need for better evaluations, more
targeted defenses, and the study of the relation to policy and data protection
efforts.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computers and Society (cs.CY); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)', 'ComputersandSociety(cs.CY)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'cs.CR', 'cs.CY', 'stat.ML']"
Privacy-preserving Machine Learning through Data Obfuscation,"  As machine learning becomes a practice and commodity, numerous cloud-based
services and frameworks are provided to help customers develop and deploy
machine learning applications. While it is prevalent to outsource model
training and serving tasks in the cloud, it is important to protect the privacy
of sensitive samples in the training dataset and prevent information leakage to
untrusted third parties. Past work have shown that a malicious machine learning
service provider or end user can easily extract critical information about the
training samples, from the model parameters or even just model outputs.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning,"  We present Placeto, a reinforcement learning (RL) approach to efficiently
find device placements for distributed neural network training. Unlike prior
approaches that only find a device placement for a specific computation graph,
Placeto can learn generalizable device placement policies that can be applied
to any graph. We propose two key ideas in our approach: (1) we represent the
policy as performing iterative placement improvements, rather than outputting a
placement in one shot; (2) we use graph embeddings to capture relevant
information about the structure of the computation graph, without relying on
node labels for indexing. These ideas allow Placeto to train efficiently and
generalize to unseen graphs. Our experiments show that Placeto requires up to
6.1x fewer training steps to find placements that are on par with or better
than the best placements found by prior approaches. Moreover, Placeto is able
to learn a generalizable placement policy for any given family of graphs, which
can then be used without any retraining to predict optimized placements for
unseen graphs from the same family. This eliminates the large overhead incurred
by prior RL approaches whose lack of generalizability necessitates re-training
from scratch every time a new graph is to be placed.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DC', 'stat.ML']"
hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices,"  Accessible machine learning algorithms, software, and diagnostic tools for
energy-efficient devices and systems are extremely valuable across a broad
range of application domains. In scientific domains, real-time near-sensor
processing can drastically improve experimental design and accelerate
scientific discoveries. To support domain scientists, we have developed hls4ml,
an open-source software-hardware codesign workflow to interpret and translate
machine learning algorithms for implementation with both FPGA and ASIC
technologies. We expand on previous hls4ml work by extending capabilities and
techniques towards low-power implementations and increased usability: new
Python APIs, quantization-aware pruning, end-to-end FPGA workflows, long
pipeline kernels for low power, and new device backends include an ASIC
workflow. Taken together, these and continued efforts in hls4ml will arm a new
generation of domain scientists with accessible, efficient, and powerful tools
for machine-learning-accelerated discovery.

    ",Machine Learning (cs.LG),; Hardware Architecture (cs.AR); Instrumentation and Detectors (physics.ins-det),"['MachineLearning(cs.LG)', 'HardwareArchitecture(cs.AR)', 'InstrumentationandDetectors(physics.ins-det)']","['cs.LG', 'cs.AR', 'physics.ins-det']"
Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems,"  Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV),"['CryptographyandSecurity(cs.CR)', 'ComputerVisionandPatternRecognition(cs.CV)']","['cs.CR', 'cs.CV']"
TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning,"  TF.Learn is a high-level Python module for distributed machine learning
inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to
simplify the process of creating, configuring, training, evaluating, and
experimenting a machine learning model. TF.Learn integrates a wide range of
state-of-art machine learning algorithms built on top of TensorFlow's low level
APIs for small to large-scale supervised and unsupervised problems. This module
focuses on bringing machine learning to non-specialists using a general-purpose
high-level language as well as researchers who want to implement, benchmark,
and compare their new methods in a structured environment. Emphasis is put on
ease of use, performance, documentation, and API consistency.

    ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Benchmarking Automatic Machine Learning Frameworks,"  AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
Benchmarking Machine Learning Technologies for Software Defect Detection,"  Machine Learning approaches are good in solving problems that have less
information. In most cases, the software domain problems characterize as a
process of learning that depend on the various circumstances and changes
accordingly. A predictive model is constructed by using machine learning
approaches and classified them into defective and non-defective modules.
Machine learning techniques help developers to retrieve useful information
after the classification and enable them to analyse data from different
perspectives. Machine learning techniques are proven to be useful in terms of
software bug prediction. This study used public available data sets of software
modules and provides comparative performance analysis of different machine
learning techniques for software bug prediction. Results showed most of the
machine learning methods performed well on software bug datasets.

    ",Software Engineering (cs.SE),,['SoftwareEngineering(cs.SE)'],['cs.SE']
Co-Creative Level Design via Machine Learning,"  Procedural Level Generation via Machine Learning (PLGML), the study of
generating game levels with machine learning, has received a large amount of
recent academic attention. For certain measures these approaches have shown
success at replicating the quality of existing game levels. However, it is
unclear the extent to which they might benefit human designers. In this paper
we present a framework for co-creative level design with a PLGML agent. In
support of this framework we present results from a user study and results from
a comparative study of PLGML approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.LG']"
The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey,"  Machine learning models have made many decision support systems to be faster,
more accurate and more efficient. However, applications of machine learning in
network security face more disproportionate threat of active adversarial
attacks compared to other domains. This is because machine learning
applications in network security such as malware detection, intrusion
detection, and spam filtering are by themselves adversarial in nature. In what
could be considered an arms race between attackers and defenders, adversaries
constantly probe machine learning systems with inputs which are explicitly
designed to bypass the system and induce a wrong prediction. In this survey, we
first provide a taxonomy of machine learning techniques, styles, and
algorithms. We then introduce a classification of machine learning in network
security applications. Next, we examine various adversarial attacks against
machine learning in network security and introduce two classification
approaches for adversarial attacks in network security. First, we classify
adversarial attacks in network security based on a taxonomy of network security
applications. Secondly, we categorize adversarial attacks in network security
into a problem space vs. feature space dimensional classification model. We
then analyze the various defenses against adversarial attacks on machine
learning-based network security applications. We conclude by introducing an
adversarial risk model and evaluate several existing adversarial attacks
against machine learning in network security using the risk model. We also
identify where each attack classification resides within the adversarial risk
model

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'NetworkingandInternetArchitecture(cs.NI)']","['cs.CR', 'cs.LG', 'cs.NI']"
A Hierarchy of Limitations in Machine Learning,"  ""All models are wrong, but some are useful"", wrote George E. P. Box (1979).
Machine learning has focused on the usefulness of probability models for
prediction in social systems, but is only now coming to grips with the ways in
which these models are wrong---and the consequences of those shortcomings. This
paper attempts a comprehensive, structured overview of the specific conceptual,
procedural, and statistical limitations of models in machine learning when
applied to society. Machine learning modelers themselves can use the described
hierarchy to identify possible failure points and think through how to address
them, and consumers of machine learning models can know what to question when
confronted with the decision about if, where, and how to apply machine
learning. The limitations go from commitments inherent in quantification
itself, through to showing how unmodeled dependencies can lead to
cross-validation being overly optimistic as a way of assessing model
performance.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)', 'Econometrics(econ.EM)', 'StatisticsTheory(math.ST)', 'MachineLearning(stat.ML)']","['cs.CY', 'cs.LG', 'econ.EM', 'math.ST', 'stat.ML']"
Detecting Fake News Using Machine Learning : A Systematic Literature Review,"  Internet is one of the important inventions and a large number of persons are
its users. These persons use this for different purposes. There are different
social media platforms that are accessible to these users. Any user can make a
post or spread the news through the online platforms. These platforms do not
verify the users or their posts. So some of the users try to spread fake news
through these platforms. These news can be propaganda against an individual,
society, organization or political party. A human being is unable to detect all
these fake news. So there is a need for machine learning classifiers that can
detect these fake news automatically. Use of machine learning classifiers for
detecting fake news is described in this systematic literature review.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.LG']"
ARDA: Automatic Relational Data Augmentation for Machine Learning,"  Automatic machine learning (\AML) is a family of techniques to automate the
process of training predictive models, aiming to both improve performance and
make machine learning more accessible. While many recent works have focused on
aspects of the machine learning pipeline like model selection, hyperparameter
tuning, and feature selection, relatively few works have focused on automatic
data augmentation. Automatic data augmentation involves finding new features
relevant to the user's predictive task with minimal ``human-in-the-loop''
involvement.
",Machine Learning (cs.LG),; Databases (cs.DB); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'Databases(cs.DB)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.DB', 'stat.ML']"
Advances in quantum machine learning,"  Here we discuss advances in the field of quantum machine learning. The
following document offers a hybrid discussion; both reviewing the field as it
is currently, and suggesting directions for further research. We include both
algorithms and experimental implementations in the discussion. The field's
outlook is generally positive, showing significant promise. However, we believe
there are appreciable hurdles to overcome before one can claim that it is a
primary application of quantum computation.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
AGL: a Scalable System for Industrial-purpose Graph Machine Learning,"  Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
",Social and Information Networks (cs.SI),,['SocialandInformationNetworks(cs.SI)'],['cs.SI']
Secure Computation for Machine Learning With SPDZ,"  Secure Multi-Party Computation (MPC) is an area of cryptography that enables
computation on sensitive data from multiple sources while maintaining privacy
guarantees. However, theoretical MPC protocols often do not scale efficiently
to real-world data. This project investigates the efficiency of the SPDZ
framework, which provides an implementation of an MPC protocol with malicious
security, in the context of popular machine learning (ML) algorithms. In
particular, we chose applications such as linear regression and logistic
regression, which have been implemented and evaluated using semi-honest MPC
techniques. We demonstrate that the SPDZ framework outperforms these previous
implementations while providing stronger security.

    ",Cryptography and Security (cs.CR),,['CryptographyandSecurity(cs.CR)'],['cs.CR']
A review of homomorphic encryption and software tools for encrypted statistical machine learning,"  Recent advances in cryptography promise to enable secure statistical
computation on encrypted data, whereby a limited set of operations can be
carried out without the need to first decrypt. We review these homomorphic
encryption schemes in a manner accessible to statisticians and machine
learners, focusing on pertinent limitations inherent in the current state of
the art. These limitations restrict the kind of statistics and machine learning
algorithms which can be implemented and we review those which have been
successfully applied in the literature. Finally, we document a high performance
R package implementing a recent homomorphic scheme in a general framework.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.CR', 'cs.LG']"
Learning perturbation sets for robust machine learning,"  Although much progress has been made towards robust deep learning, a
significant gap in robustness remains between real-world perturbations and more
narrowly defined sets typically studied in adversarial defenses. In this paper,
we aim to bridge this gap by learning perturbation sets from data, in order to
characterize real-world effects for robust training and evaluation.
Specifically, we use a conditional generator that defines the perturbation set
over a constrained region of the latent space. We formulate desirable
properties that measure the quality of a learned perturbation set, and
theoretically prove that a conditional variational autoencoder naturally
satisfies these criteria. Using this framework, our approach can generate a
variety of perturbations at different complexities and scales, ranging from
baseline spatial transformations, through common image corruptions, to lighting
variations. We measure the quality of our learned perturbation sets both
quantitatively and qualitatively, finding that our models are capable of
producing a diverse set of meaningful perturbations beyond the limited data
seen during training. Finally, we leverage our learned perturbation sets to
train models which are empirically and certifiably robust to adversarial image
corruptions and adversarial lighting variations, while improving generalization
on non-adversarial data. All code and configuration files for reproducing the
experiments as well as pretrained model weights can be found at
",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Music Genre Classification using Machine Learning Techniques,"  Categorizing music files according to their genre is a challenging task in
the area of music information retrieval (MIR). In this study, we compare the
performance of two classes of models. The first is a deep learning approach
wherein a CNN model is trained end-to-end, to predict the genre label of an
audio signal, solely using its spectrogram. The second approach utilizes
hand-crafted features, both from the time domain and the frequency domain. We
train four traditional machine learning classifiers with these features and
compare their performance. The features that contribute the most towards this
multi-class classification task are identified. The experiments are conducted
on the Audio set data set and we report an AUC value of 0.894 for an ensemble
classifier which combines the two proposed approaches.

    ",Sound (cs.SD),; Audio and Speech Processing (eess.AS),"['Sound(cs.SD)', 'AudioandSpeechProcessing(eess.AS)']","['cs.SD', 'eess.AS']"
The Bach Doodle: Approachable music composition with machine learning at scale,"  To make music composition more approachable, we designed the first AI-powered
Google Doodle, the Bach Doodle, where users can create their own melody and
have it harmonized by a machine learning model Coconet (Huang et al., 2017) in
the style of Bach. For users to input melodies, we designed a simplified
sheet-music based interface. To support an interactive experience at scale, we
re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the
browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise
separable convolutions and fusing operations. We also reduced the model
download size to approximately 400KB through post-training weight quantization.
We calibrated a speed test based on partial model evaluation time to determine
if the harmonization request should be performed locally or sent to remote TPU
servers. In three days, people spent 350 years worth of time playing with the
Bach Doodle, and Coconet received more than 55 million queries. Users could
choose to rate their compositions and contribute them to a public dataset,
which we are releasing with this paper. We hope that the community finds this
dataset useful for applications ranging from ethnomusicological studies, to
music education, to improving machine learning models.

    ",Sound (cs.SD),; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML),"['Sound(cs.SD)', 'Human-ComputerInteraction(cs.HC)', 'MachineLearning(cs.LG)', 'AudioandSpeechProcessing(eess.AS)', 'MachineLearning(stat.ML)']","['cs.SD', 'cs.HC', 'cs.LG', 'eess.AS', 'stat.ML']"
Development of a Machine-Learning System to Classify Lung CT Scan Images into Normal/COVID-19 Class,"  Recently, the lung infection due to Coronavirus Disease (COVID-19) affected a
large human group worldwide and the assessment of the infection rate in the
lung is essential for treatment planning. This research aims to propose a
Machine-Learning-System (MLS) to detect the COVID-19 infection using the CT
scan Slices (CTS). This MLS implements a sequence of methods, such as
multi-thresholding, image separation using threshold filter,
feature-extraction, feature-selection, feature-fusion and classification. The
initial part implements the Chaotic-Bat-Algorithm and Kapur's Entropy (CBA+KE)
thresholding to enhance the CTS. The threshold filter separates the image into
two segments based on a chosen threshold 'Th'. The texture features of these
images are extracted, refined and selected using the chosen procedures.
Finally, a two-class classifier system is implemented to categorize the chosen
CTS (n=500 with a pixel dimension of 512x512x1) into normal/COVID-19 group. In
this work, the classifiers, such as Naive Bayes (NB), k-Nearest Neighbors
(KNN), Decision Tree (DT), Random Forest (RF) and Support Vector Machine with
linear kernel (SVM) are implemented and the classification task is performed
using various feature vectors. The experimental outcome of the SVM with
Fused-Feature-Vector (FFV) helped to attain a detection accuracy of 89.80%.

    ",Image and Video Processing (eess.IV),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['ImageandVideoProcessing(eess.IV)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['eess.IV', 'cs.LG', 'stat.ML']"
Importance of Tuning Hyperparameters of Machine Learning Algorithms,"  The performance of many machine learning algorithms depends on their
hyperparameter settings. The goal of this study is to determine whether it is
important to tune a hyperparameter or whether it can be safely set to a default
value. We present a methodology to determine the importance of tuning a
hyperparameter based on a non-inferiority test and tuning risk: the performance
loss that is incurred when a hyperparameter is not tuned, but set to a default
value. Because our methods require the notion of a default parameter, we
present a simple procedure that can be used to determine reasonable default
parameters. We apply our methods in a benchmark study using 59 datasets from
OpenML. Our results show that leaving particular hyperparameters at their
default value is non-inferior to tuning these hyperparameters. In some cases,
leaving the hyperparameter at its default value even outperforms tuning it
using a search procedure with a limited number of iterations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
What's Sex Got To Do With Fair Machine Learning?,"  Debate about fairness in machine learning has largely centered around
competing definitions of what fairness or nondiscrimination between groups
requires. However, little attention has been paid to what precisely a group is.
Many recent approaches to ""fairness"" require one to specify a causal model of
the data generating process. These exercises make an implicit ontological
assumption that a racial or sex group is simply a collection of individuals who
share a given trait. We show this by exploring the formal assumption of
modularity in causal models, which holds that the dependencies captured by one
causal pathway are invariant to interventions on any other pathways. Causal
models of sex propose two substantive claims: 1) There exists a feature,
sex-on-its-own, that is an inherent trait of an individual that causally brings
about social phenomena external to it in the world; and 2) the relations
between sex and its effects can be modified in whichever ways and the former
feature would still retain the meaning that sex has in our world. We argue that
this ontological picture is false. Many of the ""effects"" that sex purportedly
""causes"" are in fact constitutive features of sex as a social status. They give
the social meaning of sex features, meanings that are precisely what make sex
discrimination a distinctively morally problematic type of action. Correcting
this conceptual error has a number of implications for how models can be used
to detect discrimination. Formal diagrams of constitutive relations present an
entirely different path toward reasoning about discrimination. Whereas causal
diagrams guide the construction of sophisticated modular counterfactuals,
constitutive diagrams identify a different kind of counterfactual as central to
an inquiry on discrimination: one that asks how the social meaning of a group
would be changed if its non-modular features were altered.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)']","['cs.CY', 'cs.AI', 'cs.LG']"
Machine Learning in Astronomy: a practical overview,"  Astronomy is experiencing a rapid growth in data size and complexity. This
change fosters the development of data-driven science as a useful companion to
the common model-driven data analysis paradigm, where astronomers develop
automatic tools to mine datasets and extract novel information from them. In
recent years, machine learning algorithms have become increasingly popular
among astronomers, and are now used for a wide variety of tasks. In light of
these developments, and the promise and challenges associated with them, the
IAC Winter School 2018 focused on big data in Astronomy, with a particular
emphasis on machine learning and deep learning techniques. This document
summarizes the topics of supervised and unsupervised learning algorithms
presented during the school, and provides practical information on the
application of such tools to astronomical datasets. In this document I cover
basic topics in supervised machine learning, including selection and
preprocessing of the input dataset, evaluation methods, and three popular
supervised learning algorithms, Support Vector Machines, Random Forests, and
shallow Artificial Neural Networks. My main focus is on unsupervised machine
learning algorithms, that are used to perform cluster analysis, dimensionality
reduction, visualization, and outlier detection. Unsupervised learning
algorithms are of particular importance to scientific research, since they can
be used to extract new knowledge from existing datasets, and can facilitate new
discoveries.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),,['InstrumentationandMethodsforAstrophysics(astro-ph.IM)'],['astro-ph.IM']
Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"  Injecting adversarial examples during training, known as adversarial
training, can improve robustness against one-step attacks, but not for unknown
iterative attacks. To address this challenge, we first show iteratively
generated adversarial images easily transfer between networks trained with the
same strategy. Inspired by this observation, we propose cascade adversarial
training, which transfers the knowledge of the end results of adversarial
training. We train a network from scratch by injecting iteratively generated
adversarial images crafted from already defended networks in addition to
one-step adversarial images from the network being trained. We also propose to
utilize embedding space for both classification and low-level (pixel-level)
similarity learning to ignore unknown pixel level perturbation. During
training, we inject adversarial images without replacing their corresponding
clean images and penalize the distance between the two embeddings (clean and
adversarial). Experimental results show that cascade adversarial training
together with our proposed low-level similarity learning efficiently enhances
the robustness against iterative attacks, but at the expense of decreased
robustness against one-step attacks. We show that combining those two
techniques can also improve robustness under the worst case black box attack
scenario.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
Persistent-Homology-based Machine Learning and its Applications -- A Survey,"  A suitable feature representation that can both preserve the data intrinsic
information and reduce data complexity and dimensionality is key to the
performance of machine learning models. Deeply rooted in algebraic topology,
persistent homology (PH) provides a delicate balance between data
simplification and intrinsic structure characterization, and has been applied
to various areas successfully. However, the combination of PH and machine
learning has been hindered greatly by three challenges, namely topological
representation of data, PH-based distance measurements or metrics, and PH-based
feature representation. With the development of topological data analysis,
progresses have been made on all these three problems, but widely scattered in
different literatures. In this paper, we provide a systematical review of PH
and PH-based supervised and unsupervised models from a computational
perspective. Our emphasizes are the recent development of mathematical models
and tools, including PH softwares and PH-based functions, feature
representations, kernels, and similarity models. Essentially, this paper can
work as a roadmap for the practical application of PH-based machine learning
tools. Further, we consider different topological feature representations in
different machine learning models, and investigate their impacts on the protein
secondary structure classification.

    ",Algebraic Topology (math.AT),,['AlgebraicTopology(math.AT)'],['math.AT']
"Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus","  Recent literature has underscored the importance of dataset documentation
work for machine learning, and part of this work involves addressing
""documentation debt"" for datasets that have been used widely but documented
sparsely. This paper aims to help address documentation debt for BookCorpus, a
popular text dataset for training large language models. Notably, researchers
have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,
even though little to no documentation exists about the dataset's motivation,
composition, collection process, etc. We offer a preliminary datasheet that
provides key context and information about BookCorpus, highlighting several
notable deficiencies. In particular, we find evidence that (1) BookCorpus
likely violates copyright restrictions for many books, (2) BookCorpus contains
thousands of duplicated books, and (3) BookCorpus exhibits significant skews in
genre representation. We also find hints of other potential deficiencies that
call for future research, including problematic content, potential skews in
religious representation, and lopsided author contributions. While more work
remains, this initial effort to provide a datasheet for BookCorpus adds to
growing literature that urges more careful and systematic documentation for
machine learning datasets.

    ",Computation and Language (cs.CL),; Computers and Society (cs.CY); Machine Learning (cs.LG),"['ComputationandLanguage(cs.CL)', 'ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)']","['cs.CL', 'cs.CY', 'cs.LG']"
"DALI: a large Dataset of synchronized Audio, LyrIcs and notes, automatically created using teacher-student machine learning paradigm","  The goal of this paper is twofold. First, we introduce DALI, a large and rich
multimodal dataset containing 5358 audio tracks with their time-aligned vocal
melody notes and lyrics at four levels of granularity. The second goal is to
explain our methodology where dataset creation and learning models interact
using a teacher-student machine learning paradigm that benefits each other. We
start with a set of manual annotations of draft time-aligned lyrics and notes
made by non-expert users of Karaoke games. This set comes without audio.
Therefore, we need to find the corresponding audio and adapt the annotations to
it. To that end, we retrieve audio candidates from the Web. Each candidate is
then turned into a singing-voice probability over time using a teacher, a deep
convolutional neural network singing-voice detection system (SVD), trained on
cleaned data. Comparing the time-aligned lyrics and the singing-voice
probability, we detect matches and update the time-alignment lyrics
accordingly. From this, we obtain new audio sets. They are then used to train
new SVD students used to perform again the above comparison. The process could
be repeated iteratively. We show that this allows to progressively improve the
performances of our SVD and get better audio-matching and alignment.

    ",Audio and Speech Processing (eess.AS),; Databases (cs.DB); Machine Learning (cs.LG); Sound (cs.SD),"['AudioandSpeechProcessing(eess.AS)', 'Databases(cs.DB)', 'MachineLearning(cs.LG)', 'Sound(cs.SD)']","['eess.AS', 'cs.DB', 'cs.LG', 'cs.SD']"
Machine Learning Based Student Grade Prediction: A Case Study,"  In higher educational institutes, many students have to struggle hard to
complete different courses since there is no dedicated support offered to
students who need special attention in the registered courses. Machine learning
techniques can be utilized for students' grades prediction in different
courses. Such techniques would help students to improve their performance based
on predicted grades and would enable instructors to identify such individuals
who might need assistance in the courses. In this paper, we use Collaborative
Filtering (CF), Matrix Factorization (MF), and Restricted Boltzmann Machines
(RBM) techniques to systematically analyze a real-world data collected from
Information Technology University (ITU), Lahore, Pakistan. We evaluate the
academic performance of ITU students who got admission in the bachelor's degree
program in ITU's Electrical Engineering department. The RBM technique is found
to be better than the other techniques used in predicting the students'
performance in the particular course.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
Equity forecast: Predicting long term stock price movement using machine learning,"  Long term investment is one of the major investment strategies. However,
calculating intrinsic value of some company and evaluating shares for long term
investment is not easy, since analyst have to care about a large number of
financial indicators and evaluate them in a right manner. So far, little help
in predicting the direction of the company value over the longer period of time
has been provided from the machines. In this paper we present a machine
learning aided approach to evaluate the equity's future price over the long
time. Our method is able to correctly predict whether some company's value will
be 10% higher or not over the period of one year in 76.5% of cases.

    ",Machine Learning (cs.LG),; General Finance (q-fin.GN),"['MachineLearning(cs.LG)', 'GeneralFinance(q-fin.GN)']","['cs.LG', 'q-fin.GN']"
Insights into Performance Fitness and Error Metrics for Machine Learning,"  Machine learning (ML) is the field of training machines to achieve high level
of cognition and perform human-like analysis. Since ML is a data-driven
approach, it seemingly fits into our daily lives and operations as well as
complex and interdisciplinary fields. With the rise of commercial, open-source
and user-catered ML tools, a key question often arises whenever ML is applied
to explore a phenomenon or a scenario: what constitutes a good ML model?
Keeping in mind that a proper answer to this question depends on a variety of
factors, this work presumes that a good ML model is one that optimally performs
and best describes the phenomenon on hand. From this perspective, identifying
proper assessment metrics to evaluate performance of ML models is not only
necessary but is also warranted. As such, this paper examines a number of the
most commonly-used performance fitness and error metrics for regression and
classification algorithms, with emphasis on engineering applications.

    ",Machine Learning (cs.LG),"; Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'DataAnalysis,StatisticsandProbability(physics.data-an)', 'MachineLearning(stat.ML)']","['cs.LG', 'physics.data-an', 'stat.ML']"
NSML: A Machine Learning Platform That Enables You to Focus on Your Models,"  Machine learning libraries such as TensorFlow and PyTorch simplify model
implementation. However, researchers are still required to perform a
non-trivial amount of manual tasks such as GPU allocation, training status
tracking, and comparison of models with different hyperparameter settings. We
propose a system to handle these tasks and help researchers focus on models. We
present the requirements of the system based on a collection of discussions
from an online study group comprising 25k members. These include automatic GPU
allocation, learning status visualization, handling model parameter snapshots
as well as hyperparameter modification during learning, and comparison of
performance metrics between models via a leaderboard. We describe the system
architecture that fulfills these requirements and present a proof-of-concept
implementation, NAVER Smart Machine Learning (NSML). We test the system and
confirm substantial efficiency improvements for model development.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)","['MachineLearning(cs.LG)', 'Distributed,Parallel,andClusterComputing(cs.DC)']","['cs.LG', 'cs.DC']"
CausalML: Python Package for Causal Machine Learning,"  CausalML is a Python implementation of algorithms related to causal inference
and machine learning. Algorithms combining causal inference and machine
learning have been a trending topic in recent years. This package tries to
bridge the gap between theoretical work on methodology and practical
applications by making a collection of methods in this field available in
Python. This paper introduces the key concepts, scope, and use cases of this
package.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML),"['ComputersandSociety(cs.CY)', 'MachineLearning(cs.LG)', 'Computation(stat.CO)', 'MachineLearning(stat.ML)']","['cs.CY', 'cs.LG', 'stat.CO', 'stat.ML']"
Double Debiased Machine Learning Nonparametric Inference with Continuous Treatments,"  We propose a nonparametric inference method for causal effects of continuous
treatment variables, under unconfoundedness and nonparametric or
high-dimensional nuisance parameters. Our double debiased machine learning
(DML) estimators for the average dose-response function (or the average
structural function) and the partial effects are asymptotically normal with
nonparametric convergence rates. The nuisance estimators for the conditional
expectation function and the conditional density can be nonparametric or ML
methods. Utilizing a kernel-based doubly robust moment function and
cross-fitting, we give high-level conditions under which the nuisance
estimators do not affect the first-order large sample distribution of the DML
estimators. We further provide sufficient low-level conditions for kernel,
series, and deep neural networks. We propose a data-driven bandwidth to
consistently estimate the optimal bandwidth that minimizes the asymptotic mean
squared error. We justify the use of kernel to localize the continuous
treatment at a given value by the Gateaux derivative. We implement various ML
methods in Monte Carlo simulations and an empirical application on a job
training program evaluation.

    ",Econometrics (econ.EM),,['Econometrics(econ.EM)'],['econ.EM']
Using Visual Analytics to Interpret Predictive Machine Learning Models,"  It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,"  The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.
",Machine Learning (stat.ML),; Machine Learning (cs.LG),"['MachineLearning(stat.ML)', 'MachineLearning(cs.LG)']","['stat.ML', 'cs.LG']"
A Topology Layer for Machine Learning,"  Topology applied to real world data using persistent homology has started to
find applications within machine learning, including deep learning. We present
a differentiable topology layer that computes persistent homology based on
level set filtrations and edge-based filtrations. We present three novel
applications: the topological layer can (i) regularize data reconstruction or
the weights of machine learning models, (ii) construct a loss on the output of
a deep generative network to incorporate topological priors, and (iii) perform
topological adversarial attacks on deep networks trained with persistence
features. The code (",Machine Learning (cs.LG),; Algebraic Topology (math.AT); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'AlgebraicTopology(math.AT)', 'MachineLearning(stat.ML)']","['cs.LG', 'math.AT', 'stat.ML']"
Machine Learning Explainability for External Stakeholders,"  As machine learning is increasingly deployed in high-stakes contexts
affecting people's livelihoods, there have been growing calls to open the black
box and to make machine learning algorithms more explainable. Providing useful
explanations requires careful consideration of the needs of stakeholders,
including end-users, regulators, and domain experts. Despite this need, little
work has been done to facilitate inter-stakeholder conversation around
explainable machine learning. To help address this gap, we conducted a
closed-door, day-long workshop between academics, industry experts, legal
scholars, and policymakers to develop a shared language around explainability
and to understand the current shortcomings of and potential solutions for
deploying explainable machine learning in service of transparency goals. We
also asked participants to share case studies in deploying explainable machine
learning at scale. In this paper, we provide a short summary of various case
studies of explainable machine learning, lessons from those studies, and
discuss open challenges.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI),"['ComputersandSociety(cs.CY)', 'ArtificialIntelligence(cs.AI)']","['cs.CY', 'cs.AI']"
On the Existence of Simpler Machine Learning Models,"  It is almost always easier to find an accurate-but-complex model than an
accurate-yet-simple model. Finding optimal, sparse, accurate models of various
forms (linear models with integer coefficients, decision sets, rule lists,
decision trees) is generally NP-hard. We often do not know whether the search
for a simpler model will be worthwhile, and thus we do not go to the trouble of
searching for one. In this work, we ask an important practical question: can
accurate-yet-simple models be proven to exist, or shown likely to exist, before
explicitly searching for them? We hypothesize that there is an important reason
that simple-yet-accurate models often do exist. This hypothesis is that the
size of the Rashomon set is often large, where the Rashomon set is the set of
almost-equally-accurate models from a function class. If the Rashomon set is
large, it contains numerous accurate models, and perhaps at least one of them
is the simple model we desire. In this work, we formally present the Rashomon
ratio as a new gauge of simplicity for a learning problem, depending on a
function class and a data set. The Rashomon ratio is the ratio of the volume of
the set of accurate models to the volume of the hypothesis space, and it is
different from standard complexity measures from statistical learning theory.
Insight from studying the Rashomon ratio provides an easy way to check whether
a simpler model might exist for a problem before finding it, namely whether
several different machine learning methods achieve similar performance on the
data. In that sense, the Rashomon ratio is a powerful tool for understanding
why and when an accurate-yet-simple model might exist. If, as we hypothesize in
this work, many real-world data sets admit large Rashomon sets, the
implications are vast: it means that simple or interpretable models may often
be used for high-stakes decisions without losing accuracy.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.LG', 'stat.ML']"
Revealing the Autonomous System Taxonomy: The Machine Learning Approach,"  Although the Internet AS-level topology has been extensively studied over the
past few years, little is known about the details of the AS taxonomy. An AS
""node"" can represent a wide variety of organizations, e.g., large ISP, or small
private business, university, with vastly different network characteristics,
external connectivity patterns, network growth tendencies, and other properties
that we can hardly neglect while working on veracious Internet representations
in simulation environments. In this paper, we introduce a radically new
approach based on machine learning techniques to map all the ASes in the
Internet into a natural AS taxonomy. We successfully classify 95.3% of ASes
with expected accuracy of 78.1%. We release to the community the AS-level
topology dataset augmented with: 1) the AS taxonomy information and 2) the set
of AS attributes we used to classify ASes. We believe that this dataset will
serve as an invaluable addition to further understanding of the structure and
evolution of the Internet.

    ",Networking and Internet Architecture (cs.NI),; Machine Learning (cs.LG),"['NetworkingandInternetArchitecture(cs.NI)', 'MachineLearning(cs.LG)']","['cs.NI', 'cs.LG']"
Encrypted statistical machine learning: new privacy preserving methods,"  We present two new statistical machine learning methods designed to learn on
fully homomorphic encrypted (FHE) data. The introduction of FHE schemes
following Gentry (2009) opens up the prospect of privacy preserving statistical
machine learning analysis and modelling of encrypted data without compromising
security constraints. We propose tailored algorithms for applying extremely
random forests, involving a new cryptographic stochastic fraction estimator,
and nave Bayes, involving a semi-parametric model for the class decision
boundary, and show how they can be used to learn and predict from encrypted
data. We demonstrate that these techniques perform competitively on a variety
of classification data sets and provide detailed information about the
computational practicalities of these and other FHE methods.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME),"['MachineLearning(stat.ML)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)', 'Methodology(stat.ME)']","['stat.ML', 'cs.CR', 'cs.LG', 'stat.ME']"
Machine Learning in Artificial Intelligence: Towards a Common Understanding,"  The application of ""machine learning"" and ""artificial intelligence"" has
become popular within the last decade. Both terms are frequently used in
science and media, sometimes interchangeably, sometimes with different
meanings. In this work, we aim to clarify the relationship between these terms
and, in particular, to specify the contribution of machine learning to
artificial intelligence. We review relevant literature and present a conceptual
framework which clarifies the role of machine learning to build (artificial)
intelligent agents. Hence, we seek to provide more terminological clarity and a
starting point for (interdisciplinary) discussions and future research.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
Motion Planning and Control for Mobile Robot Navigation Using Machine Learning: a Survey,"  Moving in complex environments is an essential capability of intelligent
mobile robots. Decades of research and engineering have been dedicated to
developing sophisticated navigation systems to move mobile robots from one
point to another. Despite their overall success, a recently emerging research
thrust is devoted to developing machine learning techniques to address the same
problem, based in large part on the success of deep learning. However, to date,
there has not been much direct comparison between the classical and emerging
paradigms to this problem. In this article, we survey recent works that apply
machine learning for motion planning and control in mobile robot navigation,
within the context of classical navigation systems. The surveyed works are
classified into different categories, which delineate the relationship of the
learning approaches to classical methods. Based on this classification, we
identify common challenges and promising future directions.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
"Declarative Recursive Computation on an RDBMS, or, Why You Should Use a Database For Distributed Machine Learning","  A number of popular systems, most notably Google's TensorFlow, have been
implemented from the ground up to support machine learning tasks. We consider
how to make a very small set of changes to a modern relational database
management system (RDBMS) to make it suitable for distributed learning
computations. Changes include adding better support for recursion, and
optimization and execution of very large compute plans. We also show that there
are key advantages to using an RDBMS as a machine learning platform. In
particular, learning based on a database management system allows for trivial
scaling to large data sets and especially large models, where different
computational units operate on different parts of a model that may be too large
to fit into RAM.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
A Review of Machine Learning based Anomaly Detection Techniques,"  Intrusion detection is so much popular since the last two decades where
intrusion is attempted to break into or misuse the system. It is mainly of two
types based on the intrusions, first is Misuse or signature based detection and
the other is Anomaly detection. In this paper Machine learning based methods
which are one of the types of Anomaly detection techniques is discussed.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)']","['cs.LG', 'cs.CR']"
Value-laden Disciplinary Shifts in Machine Learning,"  As machine learning models are increasingly used for high-stakes decision
making, scholars have sought to intervene to ensure that such models do not
encode undesirable social and political values. However, little attention thus
far has been given to how values influence the machine learning discipline as a
whole. How do values influence what the discipline focuses on and the way it
develops? If undesirable values are at play at the level of the discipline,
then intervening on particular models will not suffice to address the problem.
Instead, interventions at the disciplinary-level are required. This paper
analyzes the discipline of machine learning through the lens of philosophy of
science. We develop a conceptual framework to evaluate the process through
which types of machine learning models (e.g. neural networks, support vector
machines, graphical models) become predominant. The rise and fall of
model-types is often framed as objective progress. However, such disciplinary
shifts are more nuanced. First, we argue that the rise of a model-type is
self-reinforcing--it influences the way model-types are evaluated. For example,
the rise of deep learning was entangled with a greater focus on evaluations in
compute-rich and data-rich environments. Second, the way model-types are
evaluated encodes loaded social and political values. For example, a greater
focus on evaluations in compute-rich and data-rich environments encodes values
about centralization of power, privacy, and environmental concerns.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.AI', 'stat.ML']"
SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning,"  SpiNNaker is an ARM-based processor platform optimized for the simulation of
spiking neural networks. This brief describes the roadmap in going from the
current SPINNaker1 system, a 1 Million core machine in 130nm CMOS, to
SpiNNaker2, a 10 Million core machine in 22nm FDSOI. Apart from pure scaling,
we will take advantage of specific technology features, such as runtime
adaptive body biasing, to deliver cutting-edge power consumption. Power
management of the cores allows a wide range of workload adaptivity, i.e.
processor power scales with the complexity and activity of the spiking network.
Additional numerical accelerators will enhance the utility of SpiNNaker2 for
simulation of spiking neural networks as well as for executing conventional
deep neural networks. These measures should increase the simulation capacity of
the machine by a factor $>$50. The interplay between the two domains, i.e.
spiking and rate based, will provide an interesting field for algorithm
exploration on SpiNNaker2. Apart from the platforms' traditional usage as a
neuroscience exploration tool, the extended functionality opens up new
application areas such as automotive AI, tactile internet, industry 4.0 and
biomedical processing.

    ",Emerging Technologies (cs.ET),,['EmergingTechnologies(cs.ET)'],['cs.ET']
Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients,"  The nature of clinical data makes it difficult to quickly select, tune and
apply machine learning algorithms to clinical prognosis. As a result, a lot of
time is spent searching for the most appropriate machine learning algorithms
applicable in clinical prognosis that contains either binary-valued or
multi-valued attributes. The study set out to identify and evaluate the
performance of machine learning classification schemes applied in clinical
prognosis of post-operative life expectancy in the lung cancer patients.
Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train
and test models on Thoracic Surgery datasets obtained from the University of
California Irvine machine learning repository. Stratified 10-fold
cross-validation was used to evaluate baseline performance accuracy of the
classifiers. The comparative analysis shows that multilayer perceptron
performed best with classification accuracy of 82.3%, J48 came out second with
classification accuracy of 81.8%, and Naive Bayes came out the worst with
classification accuracy of 74.4%. The quality and outcome of the chosen machine
learning algorithms depends on the ingenuity of the clinical miner.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning,"  Federated Learning is the current state of the art in supporting secure
multi-party machine learning (ML): data is maintained on the owner's device and
the updates to the model are aggregated through a secure protocol. However,
this process assumes a trusted centralized infrastructure for coordination, and
clients must trust that the central service does not use the byproducts of
client data. In addition to this, a group of malicious clients could also harm
the performance of the model by carrying out a poisoning attack.
",Machine Learning (cs.LG),"; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)","['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'cs.DC', 'stat.ML']"
Generalization in quantum machine learning from few training data,"  Modern quantum machine learning (QML) methods involve variationally
optimizing a parameterized quantum circuit on a training data set, and
subsequently making predictions on a testing data set (i.e., generalizing). In
this work, we provide a comprehensive study of generalization performance in
QML after training on a limited number $N$ of training data points. We show
that the generalization error of a quantum machine learning model with $T$
trainable gates scales at worst as $\sqrt{T/N}$. When only $K \ll T$ gates have
undergone substantial change in the optimization process, we prove that the
generalization error improves to $\sqrt{K / N}$. Our results imply that the
compiling of unitaries into a polynomial number of native gates, a crucial
application for the quantum computing industry that typically uses
exponential-size training data, can be sped up significantly. We also show that
classification of quantum states across a phase transition with a quantum
convolutional neural network requires only a very small training data set.
Other potential applications include learning quantum error correcting codes or
quantum dynamical simulation. Our work injects new hope into the field of QML,
as good generalization is guaranteed from few training data.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['quant-ph', 'cs.LG', 'stat.ML']"
BoostClean: Automated Error Detection and Repair for Machine Learning,"  Predictive models based on machine learning can be highly sensitive to data
error. Training data are often combined with a variety of different sources,
each susceptible to different types of inconsistencies, and new data streams
during prediction time, the model may encounter previously unseen
inconsistencies. An important class of such inconsistencies is domain value
violations that occur when an attribute value is outside of an allowed domain.
We explore automatically detecting and repairing such violations by leveraging
the often available clean test labels to determine whether a given detection
and repair combination will improve model accuracy. We present BoostClean which
automatically selects an ensemble of error detection and repair combinations
using statistical boosting. BoostClean selects this ensemble from an extensible
library that is pre-populated general detection functions, including a novel
detector based on the Word2Vec deep learning model, which detects errors across
a diverse set of domains. Our evaluation on a collection of 12 datasets from
Kaggle, the UCI repository, real-world data analyses, and production datasets
that show that Boost- Clean can increase absolute prediction accuracy by up to
9% over the best non-ensembled alternatives. Our optimizations including
parallelism, materialization, and indexing techniques show a 22.2x end-to-end
speedup on a 16-core machine.

    ",Databases (cs.DB),,['Databases(cs.DB)'],['cs.DB']
The Role of Machine Learning in the Next Decade of Cosmology,"  In recent years, machine learning (ML) methods have remarkably improved how
cosmologists can interpret data. The next decade will bring new opportunities
for data-driven cosmological discovery, but will also present new challenges
for adopting ML methodologies and understanding the results. ML could transform
our field, but this transformation will require the astronomy community to both
foster and promote interdisciplinary research endeavors.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),; Cosmology and Nongalactic Astrophysics (astro-ph.CO),"['InstrumentationandMethodsforAstrophysics(astro-ph.IM)', 'CosmologyandNongalacticAstrophysics(astro-ph.CO)']","['astro-ph.IM', 'astro-ph.CO']"
Machine Learning for AC Optimal Power Flow,"  We explore machine learning methods for AC Optimal Powerflow (ACOPF) - the
task of optimizing power generation in a transmission network according while
respecting physical and engineering constraints. We present two formulations of
ACOPF as a machine learning problem: 1) an end-to-end prediction task where we
directly predict the optimal generator settings, and 2) a constraint prediction
task where we predict the set of active constraints in the optimal solution. We
validate these approaches on two benchmark grids.

    ",Machine Learning (cs.LG),; Signal Processing (eess.SP); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'SignalProcessing(eess.SP)', 'MachineLearning(stat.ML)']","['cs.LG', 'eess.SP', 'stat.ML']"
Scaling Datalog for Machine Learning on Big Data,"  In this paper, we present the case for a declarative foundation for
data-intensive machine learning systems. Instead of creating a new system for
each specific flavor of machine learning task, or hardcoding new optimizations,
we argue for the use of recursive queries to program a variety of machine
learning systems. By taking this approach, database query optimization
techniques can be utilized to identify effective execution plans, and the
resulting runtime plans can be executed on a single unified data-parallel query
processing engine. As a proof of concept, we consider two programming
models--Pregel and Iterative Map-Reduce-Update---from the machine learning
domain, and show how they can be captured in Datalog, tuned for a specific
task, and then compiled into an optimized physical plan. Experiments performed
on a large computing cluster with real data demonstrate that this declarative
approach can provide very good performance while offering both increased
generality and programming ease.

    ",Databases (cs.DB),; Machine Learning (cs.LG); Performance (cs.PF),"['Databases(cs.DB)', 'MachineLearning(cs.LG)', 'Performance(cs.PF)']","['cs.DB', 'cs.LG', 'cs.PF']"
tf.data: A Machine Learning Data Processing Framework,"  Training machine learning models requires feeding input data for models to
ingest. Input pipelines for machine learning jobs are often challenging to
implement efficiently as they require reading large volumes of data, applying
complex transformations, and transferring data to hardware accelerators while
overlapping computation and communication to achieve optimal performance. We
present tf.data, a framework for building and executing efficient input
pipelines for machine learning jobs. The tf.data API provides operators which
can be parameterized with user-defined computation, composed, and reused across
different machine learning domains. These abstractions allow users to focus on
the application logic of data processing, while tf.data's runtime ensures that
pipelines run efficiently.
",Machine Learning (cs.LG),; Mathematical Software (cs.MS),"['MachineLearning(cs.LG)', 'MathematicalSoftware(cs.MS)']","['cs.LG', 'cs.MS']"
Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions,"  Applying popular machine learning algorithms to large amounts of data raised
new challenges for the ML practitioners. Traditional ML libraries does not
support well processing of huge datasets, so that new approaches were needed.
Parallelization using modern parallel computing frameworks, such as MapReduce,
CUDA, or Dryad gained in popularity and acceptance, resulting in new ML
libraries developed on top of these frameworks. We will briefly introduce the
most prominent industrial and academic outcomes, such as Apache Mahout,
GraphLab or Jubatus.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG),"['Distributed,Parallel,andClusterComputing(cs.DC)', 'MachineLearning(cs.LG)']","['cs.DC', 'cs.LG']"
Opportunities in Machine Learning for Particle Accelerators,"  Machine learning (ML) is a subfield of artificial intelligence. The term
applies broadly to a collection of computational algorithms and techniques that
train systems from raw data rather than a priori models. ML techniques are now
technologically mature enough to be applied to particle accelerators, and we
expect that ML will become an increasingly valuable tool to meet new demands
for beam energy, brightness, and stability. The intent of this white paper is
to provide a high-level introduction to problems in accelerator science and
operation where incorporating ML-based approaches may provide significant
benefit. We review ML techniques currently being investigated at particle
accelerator facilities, and we place specific emphasis on active research
efforts and promising exploratory results. We also identify new applications
and discuss their feasibility, along with the required data and infrastructure
strategies. We conclude with a set of guidelines and recommendations for
laboratory managers and administrators, emphasizing the logistical and
technological requirements for successfully adopting this technology. This
white paper also serves as a summary of the discussion from a recent workshop
held at SLAC on ML for particle accelerators.

    ",Accelerator Physics (physics.acc-ph),,['AcceleratorPhysics(physics.acc-ph)'],['physics.acc-ph']
Predicting human decisions with behavioral theories and machine learning,"  Behavioral decision theories aim to explain human behavior. Can they help
predict it? An open tournament for prediction of human choices in fundamental
economic decision tasks is presented. The results suggest that integration of
certain behavioral theories as features in machine learning systems provides
the best predictions. Surprisingly, the most useful theories for prediction
build on basic properties of human and animal learning and are very different
from mainstream decision theories that focus on deviations from rational
choice. Moreover, we find that theoretical features should be based not only on
qualitative behavioral insights (e.g. loss aversion), but also on quantitative
behavioral foresights generated by functional descriptive models (e.g. Prospect
Theory). Our analysis prescribes a recipe for derivation of explainable, useful
predictions of human decisions.

    ",Artificial Intelligence (cs.AI),; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG),"['ArtificialIntelligence(cs.AI)', 'ComputerScienceandGameTheory(cs.GT)', 'MachineLearning(cs.LG)']","['cs.AI', 'cs.GT', 'cs.LG']"
Tutorial: Safe and Reliable Machine Learning,"  This document serves as a brief overview of the ""Safe and Reliable Machine
Learning"" tutorial given at the 2019 ACM Conference on Fairness,
Accountability, and Transparency (FAT* 2019). The talk slides can be found
here: ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)']","['cs.LG', 'cs.AI']"
TensorNetwork for Machine Learning,"  We demonstrate the use of tensor networks for image classification with the
TensorNetwork open source library. We explain in detail the encoding of image
data into a matrix product state form, and describe how to contract the network
in a way that is parallelizable and well-suited to automatic gradients for
optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
find out-of-the-box performance of 98% and 88% accuracy, respectively, using
the same tensor network architecture. The TensorNetwork library allows us to
seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
improvement in computational speed using a GPU.

    ",Machine Learning (cs.LG),; Strongly Correlated Electrons (cond-mat.str-el); Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'ComputerVisionandPatternRecognition(cs.CV)', 'ComputationalPhysics(physics.comp-ph)', 'MachineLearning(stat.ML)']","['cs.LG', 'cond-mat.str-el', 'cs.CV', 'physics.comp-ph', 'stat.ML']"
A semi-agnostic ansatz with variable structure for quantum machine learning,"  Quantum machine learning (QML) offers a powerful, flexible paradigm for
programming near-term quantum computers, with applications in chemistry,
metrology, materials science, data science, and mathematics. Here, one trains
an ansatz, in the form of a parameterized quantum circuit, to accomplish a task
of interest. However, challenges have recently emerged suggesting that deep
ansatzes are difficult to train, due to flat training landscapes caused by
randomness or by hardware noise. This motivates our work, where we present a
variable structure approach to build ansatzes for QML. Our approach, called
VAns (Variable Ansatz), applies a set of rules to both grow and (crucially)
remove quantum gates in an informed manner during the optimization.
Consequently, VAns is ideally suited to mitigate trainability and noise-related
issues by keeping the ansatz shallow. We employ VAns in the variational quantum
eigensolver for condensed matter and quantum chemistry applications and also in
the quantum autoencoder for data compression, showing successful results in all
cases.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['quant-ph', 'cs.LG', 'stat.ML']"
Classification with Quantum Machine Learning: A Survey,"  Due to the superiority and noteworthy progress of Quantum Computing (QC) in a
lot of applications such as cryptography, chemistry, Big data, machine
learning, optimization, Internet of Things (IoT), Blockchain, communication,
and many more. Fully towards to combine classical machine learning (ML) with
Quantum Information Processing (QIP) to build a new field in the quantum world
is called Quantum Machine Learning (QML) to solve and improve problems that
displayed in classical machine learning (e.g. time and energy consumption,
kernel estimation). The aim of this paper presents and summarizes a
comprehensive survey of the state-of-the-art advances in Quantum Machine
Learning (QML). Especially, recent QML classification works. Also, we cover
about 30 publications that are published lately in Quantum Machine Learning
(QML). we propose a classification scheme in the quantum world and discuss
encoding methods for mapping classical data to quantum data. Then, we provide
quantum subroutines and some methods of Quantum Computing (QC) in improving
performance and speed up of classical Machine Learning (ML). And also some of
QML applications in various fields, challenges, and future vision will be
presented.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG),"['QuantumPhysics(quant-ph)', 'MachineLearning(cs.LG)']","['quant-ph', 'cs.LG']"
Helix: Holistic Optimization for Accelerating Iterative Machine Learning,"  Machine learning workflow development is a process of trial-and-error:
developers iterate on workflows by testing out small modifications until the
desired accuracy is achieved. Unfortunately, existing machine learning systems
focus narrowly on model training---a small fraction of the overall development
time---and neglect to address iterative development. We propose Helix, a
machine learning system that optimizes the execution across
iterations---intelligently caching and reusing, or recomputing intermediates as
appropriate. Helix captures a wide variety of application needs within its
Scala DSL, with succinct syntax defining unified processes for data
preprocessing, model specification, and learning. We demonstrate that the reuse
problem can be cast as a Max-Flow problem, while the caching problem is
NP-Hard. We develop effective lightweight heuristics for the latter. Empirical
evaluation shows that Helix is not only able to handle a wide variety of use
cases in one unified workflow but also much faster, providing run time
reductions of up to 19x over state-of-the-art systems, such as DeepDive or
KeystoneML, on four real-world applications in natural language processing,
computer vision, social and natural sciences.

    ",Databases (cs.DB),; Machine Learning (cs.LG),"['Databases(cs.DB)', 'MachineLearning(cs.LG)']","['cs.DB', 'cs.LG']"
A Comprehensive Physics-Informed Machine Learning Framework for Predictive Turbulence Modeling,"  Although an increased availability of computational resources has enabled
high-fidelity simulations of turbulent flows, the RANS models are still the
dominant tools for industrial applications. However, the predictive
capabilities of RANS models are limited by potential inaccuracy driven by
hypotheses in the Reynolds stress closure. Recently, a Physics-Informed Machine
Learning (PIML) approach has been proposed to learn the functional form of
Reynolds stress discrepancy in RANS simulations based on available data. It has
been demonstrated that the learned discrepancy function can be used to improve
Reynolds stresses in different flows where data are not available. However,
owing to a number of challenges, the improvements have been demonstrated only
in the Reynolds stress prediction but not in the corresponding propagated
quantities of interest. In this work, we introduce the procedures toward a
complete PIML framework for predictive turbulence modeling, including learning
Reynolds stress discrepancy function, predicting Reynolds stresses in different
flows, and propagating to mean flow fields. The process of Reynolds stress
propagation and predictive accuracy of the propagated velocity field are
investigated. To improve the learning-prediction performance, the input
features are enriched based on an integrity basis of invariants. The fully
developed turbulent flow in a square duct is used as the test case. The
discrepancy model is trained on flow fields obtained from several Reynolds
numbers and evaluated on a duct flow at a Reynolds number higher than any of
the training cases. The predicted Reynolds stresses are propagated to velocity
field through RANS equations. Numerical results show excellent predictive
performances in both Reynolds stresses and their propagated velocities,
demonstrating the merits of the PIML approach in predictive turbulence
modeling.

    ",Fluid Dynamics (physics.flu-dyn),,['FluidDynamics(physics.flu-dyn)'],['physics.flu-dyn']
A machine learning framework for data driven acceleration of computations of differential equations,"  We propose a machine learning framework to accelerate numerical computations
of time-dependent ODEs and PDEs. Our method is based on recasting
(generalizations of) existing numerical methods as artificial neural networks,
with a set of trainable parameters. These parameters are determined in an
offline training process by (approximately) minimizing suitable (possibly
non-convex) loss functions by (stochastic) gradient descent methods. The
proposed algorithm is designed to be always consistent with the underlying
differential equation. Numerical experiments involving both linear and
non-linear ODE and PDE model problems demonstrate a significant gain in
computational efficiency over standard numerical methods.

    ",Numerical Analysis (math.NA),; Machine Learning (cs.LG),"['NumericalAnalysis(math.NA)', 'MachineLearning(cs.LG)']","['math.NA', 'cs.LG']"
Boosting Combinatorial Problem Modeling with Machine Learning,"  In the past few years, the area of Machine Learning (ML) has witnessed
tremendous advancements, becoming a pervasive technology in a wide range of
applications. One area that can significantly benefit from the use of ML is
Combinatorial Optimization. The three pillars of constraint satisfaction and
optimization problem solving, i.e., modeling, search, and optimization, can
exploit ML techniques to boost their accuracy, efficiency and effectiveness. In
this survey we focus on the modeling component, whose effectiveness is crucial
for solving the problem. The modeling activity has been traditionally shaped by
optimization and domain experts, interacting to provide realistic results.
Machine Learning techniques can tremendously ease the process, and exploit the
available data to either create models or refine expert-designed ones. In this
survey we cover approaches that have been recently proposed to enhance the
modeling process by learning either single constraints, objective functions, or
the whole model. We highlight common themes to multiple approaches and draw
connections with related fields of research.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
Malware Detection Module using Machine Learning Algorithms to Assist in Centralized Security in Enterprise Networks,"  Malicious software is abundant in a world of innumerable computer users, who
are constantly faced with these threats from various sources like the internet,
local networks and portable drives. Malware is potentially low to high risk and
can cause systems to function incorrectly, steal data and even crash. Malware
may be executable or system library files in the form of viruses, worms,
Trojans, all aimed at breaching the security of the system and compromising
user privacy. Typically, anti-virus software is based on a signature definition
system which keeps updating from the internet and thus keeping track of known
viruses. While this may be sufficient for home-users, a security risk from a
new virus could threaten an entire enterprise network. This paper proposes a
new and more sophisticated antivirus engine that can not only scan files, but
also build knowledge and detect files as potential viruses. This is done by
extracting system API calls made by various normal and harmful executable, and
using machine learning algorithms to classify and hence, rank files on a scale
of security risk. While such a system is processor heavy, it is very effective
when used centrally to protect an enterprise network which maybe more prone to
such threats.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG),"['CryptographyandSecurity(cs.CR)', 'MachineLearning(cs.LG)']","['cs.CR', 'cs.LG']"
Enhanced Membership Inference Attacks against Machine Learning Models,"  How much does a machine learning algorithm leak about its training data, and
why? Membership inference attacks are used as an auditing tool to quantify this
leakage. In this paper, we present a comprehensive \textit{hypothesis testing
framework} that enables us not only to formally express the prior work in a
consistent way, but also to design new membership inference attacks that use
reference models to achieve a significantly higher power (true positive rate)
for any (false positive rate) error. More importantly, we explain \textit{why}
different attacks perform differently. We present a template for
indistinguishability games, and provide an interpretation of attack success
rate across different instances of the game. We discuss various uncertainties
of attackers that arise from the formulation of the problem, and show how our
approach tries to minimize the attack uncertainty to the one bit secret about
the presence or absence of a data point in the training set. We perform a
\textit{differential analysis} between all types of attacks, explain the gap
between them, and show what causes data points to be vulnerable to an attack
(as the reasons vary due to different granularities of memorization, from
overfitting to conditional memorization). Our auditing framework is openly
accessible as part of the \textit{Privacy Meter} software tool.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'CryptographyandSecurity(cs.CR)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.CR', 'stat.ML']"
Survey on Causal-based Machine Learning Fairness Notions,"  Addressing the problem of fairness is crucial to safely use machine learning
algorithms to support decisions with a critical impact on people's lives such
as job hiring, child maltreatment, disease diagnosis, loan granting, etc.
Several notions of fairness have been defined and examined in the past decade,
such as statistical parity and equalized odds. The most recent fairness
notions, however, are causal-based and reflect the now widely accepted idea
that using causality is necessary to appropriately address the problem of
fairness. This paper examines an exhaustive list of causal-based fairness
notions and study their applicability in real-world scenarios. As the majority
of causal-based fairness notions are defined in terms of non-observable
quantities (e.g., interventions and counterfactuals), their deployment in
practice requires to compute or estimate those quantities using observational
data. This paper offers a comprehensive report of the different approaches to
infer causal quantities from observational data including identifiability
(Pearl's SCM framework) and estimation (potential outcome framework). The main
contributions of this survey paper are (1) a guideline to help selecting a
suitable fairness notion given a specific real-world scenario, and (2) a
ranking of the fairness notions according to Pearl's causation ladder
indicating how difficult it is to deploy each notion in practice.

    ",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Floquet engineering of interlayer couplings: Tuning the magic angle of twisted bilayer graphene at the exit of a waveguide,"  We introduce a new approach that allows one complete control over the
modulation of the effective twist angle change in few-layer van der Waals
heterostructures by irradiating them with longitudinal waves of light at the
end of a waveguide. As a specific application, we consider twisted bilayer
graphene and show that one can tune the magic angles to be either larger or
smaller, allowing in-situ experimental control of the phase diagram of this and
other related materials. A waveguide allows one to circumvent the free-space
constraints on the absence of longitudinal electric field components of light.
We propose to place twisted bilayer graphene at a specific location at the exit
of a waveguide, such that it is subjected to purely longitudinal components of
a transverse magnetic modes (TM) wave.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),,['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)'],['cond-mat.mes-hall']
Tracing out Correlated Chern Insulators in Magic Angle Twisted Bilayer Graphene,"  Magic-angle twisted bilayer graphene (MATBG) exhibits a range of correlated
phenomena that originate from strong electron-electron interactions. These
interactions make the Fermi surface highly susceptible to reconstruction when $
\pm 1, \pm 2, \pm 3$ electrons occupy each moir\' e unit cell and lead to the
formation of correlated insulating, superconducting and ferromagnetic phases.
While some phases have been shown to carry a non-zero Chern number, the local
microscopic properties and topological character of many other phases remain
elusive. Here we introduce a set of novel techniques hinging on scanning
tunneling microscopy (STM) to map out topological phases in MATBG that emerge
in finite magnetic field. By following the evolution of the local density of
states (LDOS) at the Fermi level with electrostatic doping and magnetic field,
we visualize a local Landau fan diagram that enables us to directly assign
Chern numbers to all observed phases. We uncover the existence of six
topological phases emanating from integer fillings in finite fields and whose
origin relates to a cascade of symmetry-breaking transitions driven by
correlations. The spatially resolved and electron-density-tuned LDOS maps
further reveal that these topological phases can form only in a small range of
twist angles around the magic-angle value. Both the microscopic origin and
extreme sensitivity to twist angle differentiate these topological phases from
the Landau levels observed near charge neutrality. Moreover, we observe that
even the charge-neutrality Landau spectrum taken at low fields is considerably
modified by interactions and exhibits an unexpected splitting between zero
Landau levels that can be as large as ${\sim }\,3-5$ meV. Our results show how
strong electronic interactions affect the band structure of MATBG and lead to
the formation of correlation-enabled topological phases.

    ",Strongly Correlated Electrons (cond-mat.str-el),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)']","['cond-mat.str-el', 'cond-mat.mes-hall']"
A novel LIDAR-based Atmospheric Calibration Method for Improving the Data Analysis of MAGIC,"  A new method for analyzing the returns of the custom-made 'micro'-LIDAR
system, which is operated along with the two MAGIC telescopes, allows to apply
atmospheric corrections in the MAGIC data analysis chain. Such corrections make
it possible to extend the effective observation time of MAGIC under adverse
atmospheric conditions and reduce the systematic errors of energy and flux in
the data analysis. LIDAR provides a range-resolved atmospheric backscatter
profile from which the extinction of Cherenkov light from air shower events can
be estimated. Knowledge of the extinction can allow to reconstruct the true
image parameters, including energy and flux. Our final goal is to recover the
source-intrinsic energy spectrum also for data affected by atmospheric
extinction from aerosol layers, such as clouds.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),,['InstrumentationandMethodsforAstrophysics(astro-ph.IM)'],['astro-ph.IM']
Soft modes in magic angle twisted bilayer graphene,"  We present a systematic study of the low-energy collective modes for
different insulating states at integer fillings in twisted bilayer graphene. In
particular, we provide a simple counting rule for the total number of soft
modes, and analyze their energies and symmetry quantum numbers in detail. To
study the soft mode spectra, we employ time dependent Hartree-Fock whose
results are reproduced analytically via an effective sigma model description.
We find two different types of low-energy modes - (i) approximate Goldstone
modes associated with breaking an enlarged U(4)$\times$U(4) symmetry and,
surprisingly, a set of (ii) nematic modes with non-zero angular momentum under
three-fold rotation. The modes of type (i) include true gapless Goldstone modes
associated with exact symmetries in addition to gapped ""pseudo-Goldstone"" modes
associated with approximate symmetries. While the modes of type (ii) are always
gapped, we show that their gap decreases as the Berry curvature grows more
concentrated. For realistic parameter values, the gapped soft modes of both
types have comparable gaps of only a few meV, and lie completely inside the
mean-field bandgap. The entire set of soft modes emerge as Goldstone modes of a
different idealized model in which Berry flux is limited to a solenoid, which
enjoys an enlarged U(8) symmetry. Furthermore, we discuss the number of
Goldstone modes for each symmetry-broken state, distinguishing the linearly vs
quadratically dispersing modes. Finally, we present a general symmetry analysis
of the soft modes for all possible insulating Slater determinant states at
integer fillings that preserve translation symmetry, independent of the
energetic details. The resulting soft mode degeneracies and symmetry quantum
numbers provide a fingerprint of the different insulting states enabling their
experimental identification from a measurement of their soft modes.

    ",Strongly Correlated Electrons (cond-mat.str-el),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)']","['cond-mat.str-el', 'cond-mat.mes-hall']"
Measurement of the cosmic electron plus positron spectrum with the MAGIC telescopes,"  Cosmic electrons with energies in the TeV range lose their energy rapidly
through synchrotron radiation and inverse Compton processes, resulting in a
relatively short lifetime (~ 10^5 years). They are only visible from
comparatively nearby sources (<1 kpc). Unexpected features in their spectrum at
a few hundreds GeV, as measured by several experiments (ATIC, Fermi and
H.E.S.S. among others), might be caused by local sources such as pulsars or by
dark matter annihilation/decay. In order to investigate these possibilities,
new measurements in the TeV energy region are needed. Since the completion of
the stereo system, the MAGIC Cherenkov experiment is sensitive enough to
measure the cosmic electron flux between a few hundred GeV and few TeV. The
electron signal has to be extracted from the overwhelming background of
hadronic cosmic rays estimated through Monte Carlo simulations. Here we present
the first results of the cosmic electron spectrum measured with the MAGIC
telescopes.

    ",High Energy Astrophysical Phenomena (astro-ph.HE),,['HighEnergyAstrophysicalPhenomena(astro-ph.HE)'],['astro-ph.HE']
"Combined Dark Matter searches towards dwarf spheroidal galaxies with Fermi-LAT, HAWC, HESS, MAGIC and VERITAS","  The search for Dark Matter (DM) has great potential to reveal physics beyond
the Standard Model. As such, searches for evidence of DM particles are being
carried out using a wide range of techniques, such as direct searches for DM
particles, searches for DM produced with colliders, and indirect searches for
the Standard Model annihilation products of DM. Dwarf spheroidal galaxies
(dSphs) are excellent targets for indirect Dark Matter searches due to their
relatively high DM content and negligible expected astrophysical background. A
collaboration was formed to maximise the sensitivity of DM searches towards
dSphs by combining for the first time dSph data from three imaging air
Cherenkov telescope (IACT) arrays: HESS, MAGIC, and VERITAS; the Fermi-LAT
satellite, and the water Cherenkov detector HAWC. Due to the diverse nature of
the instruments involved, each experiment will analyse their individual
datasets from multiple targets and then the results will be combined at the
likelihood level. For consistency of the likelihoods across the five
experiments, a common approach is used to treat the astrophysical factor
(J-Factor) for each target and an agreed set of annihilation channels are
considered. We also agree on a common statistical approach and treatment of
instrumental systematic uncertainties. The results are presented in terms of
constraints on the velocity-weighted cross section for DM self-annihilation as
a function of the DM particle mass.

    ",High Energy Astrophysical Phenomena (astro-ph.HE),,['HighEnergyAstrophysicalPhenomena(astro-ph.HE)'],['astro-ph.HE']
Algebraic Combinatorics of Magic Squares,"  We describe how to construct and enumerate Magic squares, Franklin squares,
Magic cubes, and Magic graphs as lattice points inside polyhedral cones using
techniques from Algebraic Combinatorics. The main tools of our methods are the
Hilbert Poincare series to enumerate lattice points and the Hilbert bases to
generate lattice points. We define polytopes of magic labelings of graphs and
digraphs, and give a description of the faces of the Birkhoff polytope as
polytopes of magic labelings of digraphs.

    ",Combinatorics (math.CO),,['Combinatorics(math.CO)'],['math.CO']
Quantum critical behavior in magic-angle twisted bilayer graphene,"  The flat bands of magic-angle twisted bilayer graphene (MATBG) host
strongly-correlated electronic phases such as correlated insulators,
superconductors and a strange-metal state. The latter state, believed to be key
for understanding the electronic properties of MATBG, is obscured by various
phase transitions and thus could not be unequivocally differentiated from a
metal undergoing frequent electron-phonon collisions. Here, we report transport
measurements in superconducting MATBG in which the correlated insulator states
are suppressed by screening. The uninterrupted metallic ground state shows
resistivity that is linear in temperature over three decades and spans a broad
range of doping including those where a correlation-driven Fermi surface
reconstruction occurs. This strange-metal behavior is distinguished by
Planckian scattering rates and a linear magnetoresistivity. In contrast, near
charge neutrality or a fully-filled flat band, as well as for devices twisted
away from the magic angle, we observe the archetypal Fermi liquid behavior. Our
measurements demonstrate the existence of a quantum critical phase whose
fluctuations dominate the metallic ground state throughout a continuum of
doping. Further, we observe a transition to the strange metal upon suppression
of the superconducting order, suggesting a relationship between quantum
fluctuations and superconductivity in MATBG.

    ",Strongly Correlated Electrons (cond-mat.str-el),,['StronglyCorrelatedElectrons(cond-mat.str-el)'],['cond-mat.str-el']
Magic-Angle Multilayer Graphene: A Robust Family of Moir Superconductors,"  The discovery of correlated states and superconductivity in magic-angle
twisted bilayer graphene (MATBG) has established moir quantum matter as a
new platform to explore interaction-driven and topological quantum phenomena.
Multitudes of phases have been realized in moir systems, but surprisingly,
robust superconductivity has been one of the least common of all, initially
found in MATBG and only more recently also in magic-angle twisted trilayer
graphene (MATTG). While MATBG and MATTG share some similar characteristics,
they also exhibit substantial differences, such as in their response to
external electric and magnetic fields. This raises the question of whether they
are simply two separate unique systems, or whether they form part of a broader
family of superconducting materials. Here, we report the experimental
realization of magic-angle twisted 4-layer and 5-layer graphene (MAT4G and
MAT5G, respectively), which turn out to be superconductors, hence establishing
alternating-twist magic-angle multilayer graphene as a robust family of moir
superconductors. The members of this family have flat bands in their electronic
structure as a common feature, suggesting their central role in the observed
robust superconductivity. On the other hand, there are also important
variations across the family, such as different symmetries for members with
even and odd number of layers. However, our measurements in parallel magnetic
fields, in particular the investigation of Pauli limit violation and
spontaneous rotational symmetry breaking, reveal that the most pronounced
distinction is between the N=2 and N>2-layer structures. Our results expand the
emergent family of moir superconductors, providing new insight with
potential implications for the design of novel superconducting materials
platforms.

    ",Superconductivity (cond-mat.supr-con),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Strongly Correlated Electrons (cond-mat.str-el),"['Superconductivity(cond-mat.supr-con)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)']","['cond-mat.supr-con', 'cond-mat.mes-hall', 'cond-mat.str-el']"
Tunable Phase Boundaries and Ultra-Strong Coupling Superconductivity in Mirror Symmetric Magic-Angle Trilayer Graphene,"  Moir superlattices have recently emerged as a novel platform where
correlated physics and superconductivity can be studied with unprecedented
tunability. Although correlated effects have been observed in several other
moir systems, magic-angle twisted bilayer graphene (MATBG) remains the only
one where robust superconductivity has been reproducibly measured. Here we
realize a new moir superconductor, mirror symmetric magic-angle twisted
trilayer graphene (MATTG) with dramatically richer tunability in electronic
structure and superconducting properties. Hall effect and quantum oscillations
measurements as a function of density and electric field allow us to determine
the system's tunable phase boundaries in the normal state. Zero magnetic field
resistivity measurements then reveal that the existence of superconductivity is
intimately connected to the broken symmetry phase emerging from two carriers
per moir unit cell. Strikingly, we find that the superconducting phase gets
suppressed and bounded at the van Hove singularities (vHs) partially
surrounding the broken-symmetry phase, which is difficult to reconcile with
weak-coupling BCS theory. Moreover, the extensive in situ tunability of our
system allows us to achieve the ultra-strong coupling regime, characterized by
a Ginzburg-Landau coherence length reaching the average inter-particle distance
and very large $T_\mathrm{BKT}/T_{F}$ ratios in excess of 0.1, where
$T_\mathrm{BKT}$ and $T_F$ are the Berezinskii-Kosterlitz-Thouless transition
and Fermi temperatures, respectively. These observations suggest that MATTG can
be electrically tuned close to the two-dimensional BCS-BEC crossover. Our
results establish a new generation of tunable moir superconductors with the
potential to revolutionize our fundamental understanding and the applications
of strong coupling superconductivity.

    ",Superconductivity (cond-mat.supr-con),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Strongly Correlated Electrons (cond-mat.str-el),"['Superconductivity(cond-mat.supr-con)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)']","['cond-mat.supr-con', 'cond-mat.mes-hall', 'cond-mat.str-el']"
The Parallel-Repeated Magic Square Game is Rigid,"  We show that the $n$-round parallel repetition of the Magic Square game of
Mermin and Peres is rigid, in the sense that for any entangled strategy
succeeding with probability $1 -\varepsilon$, the players' shared state is
$O(\mathrm{poly}(n\varepsilon))$-close to $2n$ EPR pairs under a local
isometry. Furthermore, we show that, under local isometry, the players'
measurements in said entangled strategy must be
$O(\mathrm{poly}(n\varepsilon))$ close to the ""ideal"" strategy when acting on
the shared state.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Collective excitations in twisted bilayer graphene close to the magic angle,"  The electronic properties of twisted bilayer graphene (TBG) can be
dramatically different from those of a single graphene layer, in particular
when the two layers are rotated relative to each other by a small angle. TBG
has recently attracted a great deal of interest, sparked by the discovery of
correlated insulating and superconducting states, for twist angle $\theta$
close to a so-called 'magic angle' $\approx 1.1$. In this work, we
unveil, via near-field optical microscopy, a collective plasmon mode in
charge-neutral TBG near the magic angle, which is dramatically different from
the ordinary single-layer graphene intraband plasmon. In selected regions of
our samples, we find a gapped collective mode with linear dispersion, akin to
the bulk magnetoplasmons of a two-dimensional (2D) electron gas. We interpret
these as interband plasmons and associate those with the optical transitions
between quasi-localized states originating from the moir superlattice.
Surprisingly, we find a higher plasmon group velocity than expected, which
implies an enhanced strength of the corresponding optical transition. This
points to a weaker interlayer coupling in the AA regions. These intriguing
optical properties offer new insights, complementary to other techniques, on
the carrier dynamics in this novel quantum electron system.

    ",Strongly Correlated Electrons (cond-mat.str-el),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)']","['cond-mat.str-el', 'cond-mat.mes-hall']"
Field-tuned and zero-field fractional Chern insulators in magic angle graphene,"  In contrast to the fractional quantum Hall (FQH) effect, where electron
density fixes the applied magnetic field, fractional Chern insulators (FCIs)
can realize FQH states in comparatively weak or even zero magnetic fields.
Previous theoretical work highlighted magic angle graphene as a promising FCI
platform, satisfying the twin requirements of flat bands and
lowest-Landau-level-like quantum geometry. Indeed, recent experiments have
demonstrated FCIs in magic angle graphene with weak magnetic fields. Here we
conduct a detailed theoretical study of the most prominent FCI state observed,
and clarify the role of the magnetic field in stabilizing this state. We
introduce two new technical tools: first, we generalize the notion of ideal
quantum geometry to Hofstadter minibands and, second, we extend the
Hartree-Fock theory of magic-angle graphene to finite field, to account for the
interaction generated bandwidth. We show that magnetic field both dramatically
reduces the effective bandwidth and improves the quantum geometry for hosting
FCIs. Using density matrix renormalization group (DMRG) simulations of a
microscopic model of magic angle graphene, we establish the regime of bandwidth
and quantum geometry indicators where FCIs are stabilized. Further
characterizing the finite-field bands by the same quantities we show how a
zero-field charge density wave state gives way to an FCI state at a magnetic
flux consistent with experiment. We also speculate on the other FCIs seen in
the same experiments, including anomalous incompressible states and
even-denominator fractions which may host non-Abelian states. Finally, when
bandwidth is the limiting factor, we propose a range of experimental parameters
where FCIs should appear at zero magnetic field.

    ",Strongly Correlated Electrons (cond-mat.str-el),; Statistical Mechanics (cond-mat.stat-mech),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'StatisticalMechanics(cond-mat.stat-mech)']","['cond-mat.str-el', 'cond-mat.stat-mech']"
Extending and Characterizing Quantum Magic Games,"  The Mermin-Peres magic square game is a cooperative two-player nonlocal game
in which shared quantum entanglement allows the players to win with certainty,
while players limited to classical operations cannot do so, a phenomenon dubbed
""quantum pseudo-telepathy"". The game has a referee separately ask each player
to color a subset of a 3x3 grid. The referee checks that their colorings
satisfy certain parity constraints that can't all be simultaneously realized.
",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
The Active Mirror Control of the MAGIC Telescope,"  One of the main design goals of the MAGIC telescopes is the very fast
repositioning in case of Gamma Ray Burst (GRB) alarms, implying a low weight of
the telescope dish. This is accomplished by using a space frame made of carbon
fiber epoxy tubes, resulting in a strong but not very rigid support structure.
Therefore it is necessary to readjust the individual mirror tiles to correct
for deformations of the dish under varying gravitational load while tracking an
object. We present the concept of the Active Mirror Control (AMC) as
implemented in the MAGIC telescopes and the actual performance reached.
Additionally we show that also telescopes using a stiff structure can benefit
from using an AMC.

    ",Astrophysics (astro-ph),,['Astrophysics(astro-ph)'],['astro-ph']
Large Pauli Limit Violation and Reentrant Superconductivity in Magic-Angle Twisted Trilayer Graphene,"  Moir quantum matter has emerged as a novel materials platform where
correlated and topological phases can be explored with unprecedented control.
Among them, magic-angle systems constructed from two or three layers of
graphene have shown robust superconducting phases with unconventional
characteristics. However, direct evidence for unconventional pairing remains to
be experimentally demonstrated. Here, we show that magic-angle twisted trilayer
graphene (MATTG) exhibits superconductivity up to in-plane magnetic fields in
excess of 10 T, which represents a large ($2\sim3$ times) violation of the
Pauli limit for conventional spin-singlet superconductors. This observation is
surprising for a system which is not expected to have strong spin-orbit
coupling. Furthermore, the Pauli limit violation is observed over the entire
superconducting phase, indicating that it is not related to a possible
pseudogap phase with large superconducting amplitude pairing. More strikingly,
we observe reentrant superconductivity at large magnetic fields, which is
present in a narrower range of carrier density and displacement field. These
findings suggest that the superconductivity in MATTG is likely driven by a
mechanism resulting in non-spin-singlet Cooper pairs, where the external
magnetic field can cause transitions between phases with potentially different
order parameters. Our results showcase the richness of moir
superconductivity and may pave a new route towards designing next-generation
exotic quantum matter.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),; Strongly Correlated Electrons (cond-mat.str-el); Superconductivity (cond-mat.supr-con),"['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'Superconductivity(cond-mat.supr-con)']","['cond-mat.mes-hall', 'cond-mat.str-el', 'cond-mat.supr-con']"
Magnetic Josephson Junctions and Superconducting Diodes in Magic Angle Twisted Bilayer Graphene,"  The simultaneous co-existence and gate-tuneability of the superconducting
(SC), magnetic and topological orders in magic angle twisted bilayer graphene
(MATBG) open up entirely new possibilities for the creation of complex hybrid
Josephson junctions (JJ). Here we report on the creation of gate-defined,
magnetic Josephson junctions in MATBG, where the weak link is gate-tuned close
to the correlated state at a moir filling factor of {\nu}=-2. A highly
unconventional Fraunhofer pattern emerges, which is phase-shifted and
asymmetric with respect to the current and magnetic field directions, and shows
a pronounced magnetic hysteresis. Interestingly, our theoretical calculations
of the JJ with a valley polarized {\nu}=-2 with orbital magnetization as the
weak link explain most of these unconventional features without fine tuning the
parameters. While these unconventional Josephson effects persist up to the
critical temperature Tc ~ 3.5K of the superconducting state, at temperatures
below T < 800mK, we observed a pronounced magnetic hysteresis possibly due to
further spin-polarization of the {\nu}=-2 state. We demonstrate how the
combination of magnetization and its current induced magnetization switching in
the MATBG JJ allows us to realize a programmable zero field superconducting
diode, which represents a major building block for a new generation of
superconducting quantum electronics.

    ",Superconductivity (cond-mat.supr-con),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Strongly Correlated Electrons (cond-mat.str-el),"['Superconductivity(cond-mat.supr-con)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)']","['cond-mat.supr-con', 'cond-mat.mes-hall', 'cond-mat.str-el']"
"Combined dark matter searches towards dwarf spheroidal galaxies with Fermi-LAT, HAWC, H.E.S.S., MAGIC, and VERITAS","  Cosmological and astrophysical observations suggest that 85\% of the total
matter of the Universe is made of Dark Matter (DM). However, its nature remains
one of the most challenging and fundamental open questions of particle physics.
Assuming particle DM, this exotic form of matter cannot consist of Standard
Model (SM) particles. Many models have been developed to attempt unraveling the
nature of DM such as Weakly Interacting Massive Particles (WIMPs), the most
favored particle candidates. WIMP annihilations and decay could produce SM
particles which in turn hadronize and decay to give SM secondaries such as high
energy $\gamma$ rays. In the framework of indirect DM search, observations of
promising targets are used to search for signatures of DM annihilation. Among
these, the dwarf spheroidal galaxies (dSphs) are commonly favored owing to
their expected high DM content and negligible astrophysical background. In this
work, we present the very first combination of 20 dSph observations, performed
by the Fermi-LAT, HAWC, H.E.S.S., MAGIC, and VERITAS collaborations in order to
maximize the sensitivity of DM searches and improve the current results. We use
a joint maximum likelihood approach combining each experiment's individual
analysis to derive more constraining upper limits on the WIMP DM
self-annihilation cross-section as a function of DM particle mass. We present
new DM constraints over the widest mass range ever reported, extending from 5
GeV to 100 TeV thanks to the combination of these five different $\gamma$-ray
instruments.

    ",High Energy Physics - Experiment (hep-ex),; High Energy Astrophysical Phenomena (astro-ph.HE),"['HighEnergyPhysics-Experiment(hep-ex)', 'HighEnergyAstrophysicalPhenomena(astro-ph.HE)']","['hep-ex', 'astro-ph.HE']"
Twistons in a Sea of Magic,"  Magic angle twisted trilayer graphene (TTG) has recently emerged as a new
platform to engineer strongly correlated flat bands. Here, we reveal the
structural and electronic properties of TTG using low temperature scanning
tunneling microscopy at twist angles for which superconductivity has been
observed. Real trilayer samples deviate from their idealized structure due to a
strong reconstruction of the moir lattice, which locks layers into
near-magic angle, mirror symmetric domains comparable in size to the
superconducting coherence length. The price for this magic relaxation is the
introduction of an array of localized twist angle faults, termed twistons.
These novel, gate-tunable moir defects offer a natural explanation for the
superconducting dome observed in transport and provide an avenue to probe
superconducting pairing mechanisms through disorder tuning.

    ",Strongly Correlated Electrons (cond-mat.str-el),; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Superconductivity (cond-mat.supr-con),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'Superconductivity(cond-mat.supr-con)']","['cond-mat.str-el', 'cond-mat.mes-hall', 'cond-mat.supr-con']"
Black Magic in Deep Learning: How Human Skill Impacts Network Training,"  How does a user's prior experience with deep learning impact accuracy? We
present an initial study based on 31 participants with different levels of
experience. Their task is to perform hyperparameter optimization for a given
deep learning architecture. The results show a strong positive correlation
between the participant's experience and the final performance. They
additionally indicate that an experienced participant finds better solutions
using fewer resources on average. The data suggests furthermore that
participants with no prior experience follow random strategies in their pursuit
of optimal hyperparameters. Our study investigates the subjective human factor
in comparisons of state of the art results and scientific reproducibility in
deep learning.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
"MARS, the MAGIC Analysis and Reconstruction Software","  With the commissioning of the second MAGIC gamma-ray Cherenkov telescope
situated close to MAGIC-I, the standard analysis package of the MAGIC
collaboration, MARS, has been upgraded in order to perform the stereoscopic
reconstruction of the detected atmospheric showers. MARS is a ROOT-based code
written in C++, which includes all the necessary algorithms to transform the
raw data recorded by the telescopes into information about the physics
parameters of the observed targets. An overview of the methods for extracting
the basic shower parameters is presented, together with a description of the
tools used in the background discrimination and in the estimation of the
gamma-ray source spectra.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),; High Energy Astrophysical Phenomena (astro-ph.HE),"['InstrumentationandMethodsforAstrophysics(astro-ph.IM)', 'HighEnergyAstrophysicalPhenomena(astro-ph.HE)']","['astro-ph.IM', 'astro-ph.HE']"
Irreducible magic sets for $n$-qubit systems,"  Magic sets of observables are minimal structures that capture quantum
state-independent advantage for systems of $n\ge 2$ qubits and are, therefore,
fundamental tools for investigating the interface between classical and quantum
physics. A theorem by Arkhipov (",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Critical magnetic fields and electron-pairing in magic-angle twisted bilayer graphene,"  The velocities of the quasiparticles that form Cooper pairs in a
superconductor are revealed by the upper critical magnetic field. Here we use
this property to assess superconductivity in magic-angle twisted bilayer
graphene (MATBG), which has been observed over a range of moir band filling,
twist angle, and screening environment conditions. We find that for pairing
mechanisms that are unrelated to correlations within the MATBG flat bands,
minima in an average Fermi velocity $v_F^* \equiv k_B T_c \ell_c /\hbar $,
where $\ell_c$ is the magnetic length at the critical perpendicular magnetic
field, are always coincident with transition temperature maxima. Both extrema
occur near flat-band van Hove singularities. Since no such association is
present in MATBG experimental data, we conclude that electronic correlations
that yield a band-filling-dependent pairing glue must play a crucial role in
MATBG superconductivity.

    ",Superconductivity (cond-mat.supr-con),,['Superconductivity(cond-mat.supr-con)'],['cond-mat.supr-con']
Magic-angle graphene superlattices: a new platform for unconventional superconductivity,"  The understanding of strongly-correlated materials, and in particular
unconventional superconductors, has puzzled physicists for decades. Such
difficulties have stimulated new research paradigms, such as ultra-cold atom
lattices for simulating quantum materials. Here we report on the realization of
intrinsic unconventional superconductivity in a 2D superlattice created by
stacking two graphene sheets with a small twist angle. For angles near
$1.1^\circ$, the first `magic' angle, twisted bilayer graphene (TBG) exhibits
ultra-flat bands near charge neutrality, which lead to correlated insulating
states at half-filling. Upon electrostatic doping away from these correlated
insulating states, we observe tunable zero-resistance states with a critical
temperature $T_c$ up to 1.7 K. The temperature-density phase diagram shows
similarities with that of the cuprates, including superconducting domes.
Moreover, quantum oscillations indicate small Fermi surfaces near the
correlated insulating phase, in analogy with under-doped cuprates. The relative
high $T_c$, given such small Fermi surface (corresponding to a record-low 2D
carrier density of $10^{11} \textrm{cm}^{-2}$ , renders TBG among the strongest
coupling superconductors, in a regime close to the BCS-BEC crossover. These
novel results establish TBG as the first purely carbon-based 2D superconductor
and as a highly tunable platform to investigate strongly-correlated phenomena,
which could lead to insights into the physics of high-$T_c$ superconductors and
quantum spin liquids.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),; Strongly Correlated Electrons (cond-mat.str-el); Superconductivity (cond-mat.supr-con),"['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'Superconductivity(cond-mat.supr-con)']","['cond-mat.mes-hall', 'cond-mat.str-el', 'cond-mat.supr-con']"
Fine structure of flat bands in a chiral model of magic angles,"  We analyze symmetries of Bloch eigenfunctions at magic angles for the
Tarnopolsky--Kruchkov--Vishwanath chiral model of the twisted bilayer graphene
(TBG) following the framework introduced by Becker--Embree--Wittsten--Zworski.
In particular, we show that vanishing of the first Bloch eigenvalue away from
the Dirac points implies its vanishing at all momenta, that is the existence of
a flat band. We then study how the multiplicity of the flat band is related to
the nodal set of the Bloch eigenfunctions. We also demonstrate that for a
generic choice of tunneling potentials, obeying all translational and
rotational symmetries, the Hamiltonian only exhibits flat bands of minimal
multiplicity. We conclude with two numerical observations about the structure
of flat bands.

    ",Mathematical Physics (math-ph),; Materials Science (cond-mat.mtrl-sci); Strongly Correlated Electrons (cond-mat.str-el); Spectral Theory (math.SP); Quantum Physics (quant-ph),"['MathematicalPhysics(math-ph)', 'MaterialsScience(cond-mat.mtrl-sci)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'SpectralTheory(math.SP)', 'QuantumPhysics(quant-ph)']","['math-ph', 'cond-mat.mtrl-sci', 'cond-mat.str-el', 'math.SP', 'quant-ph']"
Advanced stereoscopic gamma-ray shower analysis with the MAGIC telescopes,"  The MAGIC experiment was upgraded to a two-telescope system in 2009. Unlike
other Imaging Air Cherenkov Telescope arrays, MAGIC has operated for five years
exclusively in monoscopic mode, and the single telescope analysis was optimized
throughout this time. To improve the analysis, we used techniques like the
random forest event classification method for different purposes, and
sophisticated image cleaning algorithms. The monoscopic performance was
optimized in the energy domain around and below 100 GeV, which is inaccessible
for the other arrays of Cherenkov telescopes. Still, with these analysis
techniques, we were competitive also in the TeV regime. In the recent
development of the stereoscopic analysis chain, the know-how of these single
telescope techniques was combined with the new possibilities of the
three-dimensional reconstruction, taking advantage both of the richness of
single images and their projections onto the sky. We present recent
advancements in the image cleaning and direction reconstruction algorithms, sky
mapping and other procedures currently used in the analysis of MAGIC stereo
data.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),,['InstrumentationandMethodsforAstrophysics(astro-ph.IM)'],['astro-ph.IM']
Isotopic shift and search of magic number in the superheavy region,"  The ground state bulk properties such as binding energy, root-mean-square
radius, pairing energy, nuclear density distributions, and single-particle
energies are calculated for the isotopic chain of Ca, Sn, Pb, and Z = 120
nuclei. The relativistic mean-field with recently developed G3, IOPB-1, and
Relativistic-Hartree-Bogoliubov with density-dependent DD-ME1 and DD-ME2
parameter sets are used in the present analysis. The respective shifts over the
isotopic chain for the structural observables and surface property like
symmetry energy are also estimated using a three-point method, which is crucial
for the systematic analysis of the shell/sub-shell closure. The calculated
results are compared with the available experimental data for various bulk
properties, wherever available. A multiple isotopic shifts leads to the
shell/sub-shell closure at N = (20 \& 28), (50 \& 82), and 126 for Ca, Sn, and
Pb isotopes, respectively, are observed. The analysis also supports the neutron
magic at N = 40 and 184 for highly neutron-rich $^{60}$Ca, and $^{304}$120,
predicted to be the next double magic beyond $^{208}$Pb, respectively.
Observing the occupancy number, we notice the higher neutron orbitals are
mostly occupied before the lower one, which causes the kinks at neutron magic
with an amalgam in the isotopic chain trend above nuclei. We also notice the
correlation between the occupation probabilities and the magicity of a nucleus
and vice-versa.

    ",Nuclear Theory (nucl-th),,['NuclearTheory(nucl-th)'],['nucl-th']
Magic ELF: Image Deraining Meets Association Learning and Transformer,"  Convolutional neural network (CNN) and Transformer have achieved great
success in multimedia applications. However, little effort has been made to
effectively and efficiently harmonize these two architectures to satisfy image
deraining. This paper aims to unify these two architectures to take advantage
of their learning merits for image deraining. In particular, the local
connectivity and translation equivariance of CNN and the global aggregation
ability of self-attention (SA) in Transformer are fully exploited for specific
local context and global structure representations. Based on the observation
that rain distribution reveals the degradation location and degree, we
introduce degradation prior to help background recovery and accordingly present
the association refinement deraining scheme. A novel multi-input attention
module (MAM) is proposed to associate rain perturbation removal and background
recovery. Moreover, we equip our model with effective depth-wise separable
convolutions to learn the specific feature representations and trade off
computational complexity. Extensive experiments show that our proposed method
(dubbed as ELF) outperforms the state-of-the-art approach (MPRNet) by 0.25 dB
on average, but only accounts for 11.7\% and 42.1\% of its computational cost
and parameters. The source code is available at
",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Complex instruction set computing architecture for performing accurate quantum $Z$ rotations with less magic,"  We present quantum protocols for executing arbitrarily accurate $\pi/2^k$
rotations of a qubit about its $Z$ axis. Reduced instruction set computing
(\textsc{risc}) architectures typically restrict the instruction set to
stabilizer operations and a single non-stabilizer operation, such as
preparation of a ""magic"" state from which $T = Z(\pi/4)$ gates can be
teleported. Although the overhead required to distill high-fidelity copies of
this magic state is high, the subsequent quantum compiling overhead to realize
$Z$ rotations in a \textsc{risc} architecture can be much greater. We develop a
complex instruction set computing (\textsc{cisc}) architecture whose
instruction set includes stabilizer operations and preparation of magic states
from which $Z(\pi/2^k)$ gates can be teleported, for $2 \leq k \leq
k_{\text{max}}$. This results in a substantial overall reduction in the number
of gates required to achieve a desired gate accuracy for $Z$ rotations. The key
to our construction is a family of shortened quantum Reed-Muller codes of
length $2^{k+2}-1$, whose magic-state distillation threshold shrinks with $k$
but is greater than 0.85% for $k \leq 6$.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
Integrability in the chiral model of magic angles,"  Magic angles in the chiral model of twisted bilayer graphene are parameters
for which the chiral version of the Bistritzer--MacDonald Hamiltonian exhibits
a flat band at energy zero. We compute the sums over powers of (complex) magic
angles and use that to show that the set of magic angles is infinite. We also
provide a new proof of the existence of the first real magic angle, showing
also that the corresponding flat band has minimal multiplicity for the simplest
possible choice of potentials satisfying all symmetries. These results indicate
(though not prove) a hidden integrability of the chiral model.

    ",Mathematical Physics (math-ph),; Materials Science (cond-mat.mtrl-sci); Strongly Correlated Electrons (cond-mat.str-el); Spectral Theory (math.SP); Quantum Physics (quant-ph),"['MathematicalPhysics(math-ph)', 'MaterialsScience(cond-mat.mtrl-sci)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'SpectralTheory(math.SP)', 'QuantumPhysics(quant-ph)']","['math-ph', 'cond-mat.mtrl-sci', 'cond-mat.str-el', 'math.SP', 'quant-ph']"
Stabilizer Rnyi entropy,"  We introduce a novel measure for the quantum property of nonstabilizerness -
commonly known as ""magic"" - by considering the Rnyi entropy of the
probability distribution associated to a pure quantum state given by the square
of the expectation value of Pauli strings in that state. We show that this is a
good measure of nonstabilizerness from the point of view of resource theory and
show bounds with other known measures. The stabilizer Rnyi entropy has the
advantage of being easily computable because it does not need a minimization
procedure. We present a protocol for an experimental measurement by randomized
measurements. We show that the nonstabilizerness is intimately connected to
out-of-time-order correlation functions and that maximal levels of
nonstabilizerness are necessary for quantum chaos.

    ",Quantum Physics (quant-ph),,['QuantumPhysics(quant-ph)'],['quant-ph']
A lower bound on intergalactic magnetic fields from time variability of 1ES 0229+200 from MAGIC and Fermi/LAT observations,"  Extended and delayed emission around distant TeV sources induced by the
effects of propagation of gamma rays through the intergalactic medium can be
used for the measurement of the intergalactic magnetic field (IGMF). We search
for delayed GeV emission from the hard-spectrum TeV blazar 1ES 0229+200 with
the goal to detect or constrain the IGMF-dependent secondary flux generated
during the propagation of TeV gamma rays through the intergalactic medium. We
analyze the most recent MAGIC observations over a 5 year time span and
complement them with historic data of the H.E.S.S. and VERITAS telescopes along
with a 12-year long exposure of the Fermi/LAT telescope. We use them to trace
source evolution in the GeV-TeV band over one-and-a-half decade in time. We use
Monte Carlo simulations to predict the delayed secondary gamma-ray flux,
modulated by the source variability, as revealed by TeV-band observations. We
then compare these predictions for various assumed IGMF strengths to all
available measurements of the gamma-ray flux evolution. We find that the source
flux in the energy range above 200 GeV experiences variations around its
average on the 14 years time span of observations. No evidence for the flux
variability is found in 1-100 GeV energy range accessible to Fermi/LAT.
Non-detection of variability due to delayed emission from electromagnetic
cascade developing in the intergalactic medium imposes a lower bound of
B>1.8e-17 G for long correlation length IGMF and B>1e-14 G for an IGMF of the
cosmological origin. Though weaker than the one previously derived from the
analysis of Fermi/LAT data, this bound is more robust, being based on a
conservative intrinsic source spectrum estimate and accounting for the details
of source variability in the TeV energy band. We discuss implications of this
bound for cosmological magnetic fields which might explain the baryon asymmetry
of the Universe.

    ",High Energy Astrophysical Phenomena (astro-ph.HE),; Cosmology and Nongalactic Astrophysics (astro-ph.CO),"['HighEnergyAstrophysicalPhenomena(astro-ph.HE)', 'CosmologyandNongalacticAstrophysics(astro-ph.CO)']","['astro-ph.HE', 'astro-ph.CO']"
Magic entanglement renormalization for quantum fields,"  Continuous tensor networks are variational wavefunctions proposed in recent
years to efficiently simulate quantum field theories (QFTs). Prominent examples
include the continuous matrix product state (cMPS) and the continuous
multi-scale entanglement renormalization ansatz (cMERA). While the cMPS can
approximate ground states of a class of QFT Hamiltonians that are both local
and interacting, cMERA is only well-understood for QFTs that are quasi-local
and non-interacting. In this paper we propose the magic cMERA, a concrete
realization of cMERA for a free boson QFT that simultaneously satisfies four
remarkable properties: (i) it is the exact ground state of a strictly local
Hamiltonian; (ii) in the massless case, its spectrum of scaling operators is
exactly soluble in real space; (iii) it has the short-distance structure of a
cMPS; (iv) it is generated by a quasi-local entangler that can be written as a
continuous matrix product operator. None of these properties is fulfilled by
previous cMERA proposals. Properties (iii)-(iv) establish a firm connection
between cMERA and cMPS wavefunctionals, opening the path to applying powerful
cMPS numerical techniques, valid for interacting QFTs, also to cMERA
calculations.

    ",Strongly Correlated Electrons (cond-mat.str-el),; High Energy Physics - Theory (hep-th); Quantum Physics (quant-ph),"['StronglyCorrelatedElectrons(cond-mat.str-el)', 'HighEnergyPhysics-Theory(hep-th)', 'QuantumPhysics(quant-ph)']","['cond-mat.str-el', 'hep-th', 'quant-ph']"
"MAGIC: a general, powerful and tractable method for selective inference","  Selective inference is a recent research topic that tries to perform valid
inference after using the data to select a reasonable statistical model. We
propose MAGIC, a new method for selective inference that is general, powerful
and tractable. MAGIC is a method for selective inference after solving a convex
optimization problem with smooth loss and $\ell_1$ penalty. Randomization is
incorporated into the optimization problem to boost statistical power. Through
reparametrization, MAGIC reduces the problem into a sampling problem with
simple constraints. MAGIC applies to many $\ell_1$ penalized optimization
problem including the Lasso, logistic Lasso and neighborhood selection in
graphical models, all of which we consider in this paper.

    ",Statistics Theory (math.ST),,['StatisticsTheory(math.ST)'],['math.ST']
Chaos by Magic,"  There is a property of a quantum state called magic. It measures how
difficult for a classical computer to simulate the state. In this paper, we
study magic of states in the integrable and chaotic regimes of the higher-spin
generalization of the Ising model through two quantities called ""Mana"" and
""Robustness of Magic"" (RoM). We find that in the chaotic regime, Mana increases
monotonically in time in the early-time region, and at late times these
quantities oscillate around some non-zero value that increases linearly with
respect to the system size. Our result also suggests that under chaotic
dynamics, any state evolves to a state whose Mana almost saturates the optimal
upper bound, i.e., the state becomes ""maximally magical."" We find that RoM also
shows similar behaviors. On the other hand, in the integrable regime, Mana and
RoM behave periodically in time in contrast to the chaotic case. In the anti-de
Sitter/conformal field theory correspondence (AdS/CFT correspondence),
classical spacetime emerges from the chaotic nature of the dual quantum system.
Our result suggests that magic of quantum states is strongly involved behind
the emergence of spacetime geometry.

    ",High Energy Physics - Theory (hep-th),; Strongly Correlated Electrons (cond-mat.str-el); Quantum Physics (quant-ph),"['HighEnergyPhysics-Theory(hep-th)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'QuantumPhysics(quant-ph)']","['hep-th', 'cond-mat.str-el', 'quant-ph']"
Magic angles and correlations in twisted nodal superconductors,"  Motivated by recent advances in the fabrication of twisted bilayers of 2D
materials, we consider the low-energy properties of a twisted pair of
two-dimensional nodal superconductors. We study both the cases of singlet and
triplet superconductors. It is demonstrated that the Bogoliubov-de Gennes (BdG)
quasiparticle dispersion undergoes dramatic reconstruction due to the twist. In
particular, the velocity of the neutral massless Dirac excitations near the gap
nodes is strongly renormalized by the interlayer hopping and vanishes at a
``magic angle'' where in the limit of a circular Fermi surface a quadratic band
touching is formed. In addition, it is shown that the BdG disperion can be
tuned with an interlayer displacement field, magnetic field, and current, which
can suppress the velocity renormalization, create finite BdG Fermi surfaces, or
open a gap, respectively. Finally, interactions between quasiparticles are
shown to lead to the emergence of a correlated superconducting state breaking
time-reversal symmetry in the vicinity of the magic angle. Estimates of the
magic angle in a variety of nodal superconductors are presented, ranging from
the cuprates to the organic and heavy fermion superconductors, all of which are
shown to be promising for the experimental realization of our proposal.

    ",Superconductivity (cond-mat.supr-con),; Strongly Correlated Electrons (cond-mat.str-el),"['Superconductivity(cond-mat.supr-con)', 'StronglyCorrelatedElectrons(cond-mat.str-el)']","['cond-mat.supr-con', 'cond-mat.str-el']"
Goodness of Generalized Seniority in Semi-magic Nuclei,"  Symmetry plays an important role in understanding the nuclear structure
properties from the rotation of a nucleus to the spin, parity and isospin of
nuclear states. This simplifies the complexity of the nuclear problems in one
way or the other. Seniority is also a well known quantum number which arises
due to the symmetry in the pairing interaction of nuclei. We present empirical
as well as theoretical evidences based on decay rates which support the
goodness of seniority at higher spins as well as in nrich or, n-deficient
nuclei. We find that the generalized seniority governs the identical trends of
high-spin isomers in different semi-magic chains, where different set of
nucleon orbitals from different valence spaces are involved.

    ",Nuclear Theory (nucl-th),,['NuclearTheory(nucl-th)'],['nucl-th']
Magic Fermions: Carroll and Flat Bands,"  The Carroll algebra is constructed as the $c\to0$ limit of the Poincare
algebra and is associated to symmetries on generic null surfaces. In this
paper, we begin investigations of Carrollian fermions or fermions defined on
generic null surfaces. Due to the availability of two different (degenerate)
metrics on Carroll spacetimes, there is the possibility of two different
versions of Carroll Clifford algebras. We consider both possibilities and
construct explicit representations of Carrollian gamma matrices and show how to
build higher spacetime dimensional representations out of lower ones. Actions
for Carroll fermions are constructed with these gamma matrices and the
properties of these actions are investigated.
",High Energy Physics - Theory (hep-th),; Strongly Correlated Electrons (cond-mat.str-el); Superconductivity (cond-mat.supr-con),"['HighEnergyPhysics-Theory(hep-th)', 'StronglyCorrelatedElectrons(cond-mat.str-el)', 'Superconductivity(cond-mat.supr-con)']","['hep-th', 'cond-mat.str-el', 'cond-mat.supr-con']"
"First Combined Study on LIV from Observations of Energy-dependent Time Delays from Multiple-type Gamma-ray Sources -- Part I. Motivation, Method Description and Validation through Simulations of H.E.S.S., MAGIC and VERITAS Datasets","  Gamma-ray astronomy has become one of the main experimental ways to test the
modified dispersion relations (MDRs) of photons in vacuum, obtained in some
attempts to formulate a theory of Quantum Gravity. The MDRs in use imply time
delays which depend on the energy, and which increase with distance following
some function of redshift. The use of transient, or variable, distant and
highly energetic sources, already allows us to set stringent limits on the
energy scale related to this phenomenon, usually thought to be of the order of
the Planck energy, but robust conclusions on the existence of MDR-related
propagation effects still require the analysis of a large population of
sources.
",High Energy Astrophysical Phenomena (astro-ph.HE),; General Relativity and Quantum Cosmology (gr-qc),"['HighEnergyAstrophysicalPhenomena(astro-ph.HE)', 'GeneralRelativityandQuantumCosmology(gr-qc)']","['astro-ph.HE', 'gr-qc']"
Moir Landau fans and magic zeros,"  We study the energy spectrum of moir systems under a uniform magnetic
field. The superlattice potential generally broadens Landau levels into Chern
bands with finite bandwidth. However, we find that these Chern bands become
flat at a discrete set of magnetic fields which we dub ""magic zeros"". The flat
band subspace is generally different from the Landau level subspace in the
absence of the moir superlattice. By developing a semiclassical quantization
method and taking account of superlattice induced Bragg reflection, we prove
that magic zeros arise from the simultaneous quantization of two distinct
$k$-space orbits. The flat bands at magic zeros provide a new setting for
exploring crystalline fractional quantum Hall physics.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),,['MesoscaleandNanoscalePhysics(cond-mat.mes-hall)'],['cond-mat.mes-hall']
Reducing the Barrier to Entry of Complex Robotic Software: a MoveIt! Case Study,"  Developing robot agnostic software frameworks involves synthesizing the
disparate fields of robotic theory and software engineering while
simultaneously accounting for a large variability in hardware designs and
control paradigms. As the capabilities of robotic software frameworks increase,
the setup difficulty and learning curve for new users also increase. If the
entry barriers for configuring and using the software on robots is too high,
even the most powerful of frameworks are useless. A growing need exists in
robotic software engineering to aid users in getting started with, and
customizing, the software framework as necessary for particular robotic
applications. In this paper a case study is presented for the best practices
found for lowering the barrier of entry in the MoveIt! framework, an
open-source tool for mobile manipulation in ROS, that allows users to 1)
quickly get basic motion planning functionality with minimal initial setup, 2)
automate its configuration and optimization, and 3) easily customize its
components. A graphical interface that assists the user in configuring MoveIt!
is the cornerstone of our approach, coupled with the use of an existing
standardized robot model for input, automatically generated robot-specific
configuration files, and a plugin-based architecture for extensibility. These
best practices are summarized into a set of barrier to entry design principles
applicable to other robotic software. The approaches for lowering the entry
barrier are evaluated by usage statistics, a user survey, and compared against
our design objectives for their effectiveness to users.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
The Ingredients of Real-World Robotic Reinforcement Learning,"  The success of reinforcement learning for real world robotics has been, in
many cases limited to instrumented laboratory scenarios, often requiring
arduous human effort and oversight to enable continuous learning. In this work,
we discuss the elements that are needed for a robotic learning system that can
continually and autonomously improve with data collected in the real world. We
propose a particular instantiation of such a system, using dexterous
manipulation as our case study. Subsequently, we investigate a number of
challenges that come up when learning without instrumentation. In such
settings, learning must be feasible without manually designed resets, using
only on-board perception, and without hand-engineered reward functions. We
propose simple and scalable solutions to these challenges, and then demonstrate
the efficacy of our proposed system on a set of dexterous robotic manipulation
tasks, providing an in-depth analysis of the challenges associated with this
learning paradigm. We demonstrate that our complete system can learn without
any human intervention, acquiring a variety of vision-based skills with a
real-world three-fingered hand. Results and videos can be found at
",Machine Learning (cs.LG),; Robotics (cs.RO); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'Robotics(cs.RO)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.RO', 'stat.ML']"
Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo,"  This paper presents an extension of the OpenAI Gym for robotics using the
Robot Operating System (ROS) and the Gazebo simulator. The content discusses
the software architecture proposed and the results obtained by using two
Reinforcement Learning techniques: Q-Learning and Sarsa. Ultimately, the output
of this work presents a benchmarking system for robotics that allows different
techniques and algorithms to be compared using the same virtual conditions.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
PyRobot: An Open-source Robotics Framework for Research and Benchmarking,"  This paper introduces PyRobot, an open-source robotics framework for research
and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS
that provides a consistent set of hardware independent mid-level APIs to
control different robots. PyRobot abstracts away details about low-level
controllers and inter-process communication, and allows non-robotics
researchers (ML, CV researchers) to focus on building high-level AI
applications. PyRobot aims to provide a research ecosystem with convenient
access to robotics datasets, algorithm implementations and models that can be
used to quickly create a state-of-the-art baseline. We believe PyRobot, when
paired up with low-cost robot platforms such as LoCoBot, will reduce the entry
barrier into robotics, and democratize robotics. PyRobot is open-source, and
can be accessed via ",Robotics (cs.RO),; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']"
BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators,"  We introduce BayesSim, a framework for robotics simulations allowing a full
Bayesian treatment for the parameters of the simulator. As simulators become
more sophisticated and able to represent the dynamics more accurately,
fundamental problems in robotics such as motion planning and perception can be
solved in simulation and solutions transferred to the physical robot. However,
even the most complex simulator might still not be able to represent reality in
all its details either due to inaccurate parametrization or simplistic
assumptions in the dynamic models. BayesSim provides a principled framework to
reason about the uncertainty of simulation parameters. Given a black box
simulator (or generative model) that outputs trajectories of state and action
pairs from unknown simulation parameters, followed by trajectories obtained
with a physical robot, we develop a likelihood-free inference method that
computes the posterior distribution of simulation parameters. This posterior
can then be used in problems where Sim2Real is critical, for example in policy
search. We compare the performance of BayesSim in obtaining accurate posteriors
in a number of classical control and robotics problems. Results show that the
posterior computed from BayesSim can be used for domain randomization
outperforming alternative methods that randomize based on uniform priors.

    ",Robotics (cs.RO),; Machine Learning (cs.LG),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.LG']"
CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning,"  Despite recent successes of reinforcement learning (RL), it remains a
challenge for agents to transfer learned skills to related environments. To
facilitate research addressing this problem, we propose CausalWorld, a
benchmark for causal structure and transfer learning in a robotic manipulation
environment. The environment is a simulation of an open-source robotic
platform, hence offering the possibility of sim-to-real transfer. Tasks consist
of constructing 3D shapes from a given set of blocks - inspired by how children
learn to build complex structures. The key strength of CausalWorld is that it
provides a combinatorial family of such tasks with common causal structure and
underlying factors (including, e.g., robot and object masses, colors, sizes).
The user (or the agent) may intervene on all causal variables, which allows for
fine-grained control over how similar different tasks (or task distributions)
are. One can thus easily define training and evaluation distributions of a
desired difficulty level, targeting a specific form of generalization (e.g.,
only changes in appearance or object mass). Further, this common
parametrization facilitates defining curricula by interpolating between an
initial and a target task. While users may define their own task distributions,
we present eight meaningful distributions as concrete benchmarks, ranging from
simple to very challenging, all of which require long-horizon planning as well
as precise low-level motor control. Finally, we provide baseline results for a
subset of these tasks on distinct training curricula and corresponding
evaluation protocols, verifying the feasibility of the tasks in this benchmark.

    ",Robotics (cs.RO),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.RO', 'cs.LG', 'stat.ML']"
2017 Robotic Instrument Segmentation Challenge,"  In mainstream computer vision and machine learning, public datasets such as
ImageNet, COCO and KITTI have helped drive enormous improvements by enabling
researchers to understand the strengths and limitations of different algorithms
via performance comparison. However, this type of approach has had limited
translation to problems in robotic assisted surgery as this field has never
established the same level of common datasets and benchmarking methods. In 2015
a sub-challenge was introduced at the EndoVis workshop where a set of robotic
images were provided with automatically generated annotations from robot
forward kinematics. However, there were issues with this dataset due to the
limited background variation, lack of complex motion and inaccuracies in the
annotation. In this work we present the results of the 2017 challenge on
robotic instrument segmentation which involved 10 teams participating in
binary, parts and type based segmentation of articulated da Vinci robotic
instruments.

    ",Computer Vision and Pattern Recognition (cs.CV),,['ComputerVisionandPatternRecognition(cs.CV)'],['cs.CV']
Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills,"  We consider the problem of learning useful robotic skills from previously
collected offline data without access to manually specified rewards or
additional online exploration, a setting that is becoming increasingly
important for scaling robot learning by reusing past robotic data. In
particular, we propose the objective of learning a functional understanding of
the environment by learning to reach any goal state in a given dataset. We
employ goal-conditioned Q-learning with hindsight relabeling and develop
several techniques that enable training in a particularly challenging offline
setting. We find that our method can operate on high-dimensional camera images
and learn a variety of skills on real robots that generalize to previously
unseen scenes and objects. We also show that our method can learn to reach
long-horizon goals across multiple episodes through goal chaining, and learn
rich representations that can help with downstream tasks through pre-training
or auxiliary objectives. The videos of our experiments can be found at
",Robotics (cs.RO),; Machine Learning (cs.LG),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.LG']"
Scaling data-driven robotics with reward sketching and batch reinforcement learning,"  We present a framework for data-driven robotics that makes use of a large
dataset of recorded robot experience and scales to several tasks using learned
reward functions. We show how to apply this framework to accomplish three
different object manipulation tasks on a real robot platform. Given
demonstrations of a task together with task-agnostic recorded experience, we
use a special form of human annotation as supervision to learn a reward
function, which enables us to deal with real-world tasks where the reward
signal cannot be acquired directly. Learned rewards are used in combination
with a large dataset of experience from different tasks to learn a robot policy
offline using batch RL. We show that using our approach it is possible to train
agents to perform a variety of challenging manipulation tasks including
stacking rigid objects and handling cloth.

    ",Robotics (cs.RO),; Machine Learning (cs.LG),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.LG']"
Open-Sourced Reinforcement Learning Environments for Surgical Robotics,"  Reinforcement Learning (RL) is a machine learning framework for artificially
intelligent systems to solve a variety of complex problems. Recent years has
seen a surge of successes solving challenging games and smaller domain
problems, including simple though non-specific robotic manipulation and
grasping tasks. Rapid successes in RL have come in part due to the strong
collaborative effort by the RL community to work on common, open-sourced
environment simulators such as OpenAI's Gym that allow for expedited
development and valid comparisons between different, state-of-art strategies.
In this paper, we aim to start the bridge between the RL and the surgical
robotics communities by presenting the first open-sourced reinforcement
learning environments for surgical robots, called dVRL[3]{dVRL available at
",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
PythonRobotics: a Python code collection of robotics algorithms,"  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation, and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project, the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
A micro Lie theory for state estimation in robotics,"  A Lie group is an old mathematical abstract object dating back to the XIX
century, when mathematician Sophus Lie laid the foundations of the theory of
continuous transformation groups. As it often happens, its usage has spread
over diverse areas of science and technology many years later. In robotics, we
are recently experiencing an important trend in its usage, at least in the
fields of estimation, and particularly in motion estimation for navigation. Yet
for a vast majority of roboticians, Lie groups are highly abstract
constructions and therefore difficult to understand and to use. This may be due
to the fact that most of the literature on Lie theory is written by and for
mathematicians and physicists, who might be more used than us to the deep
abstractions this theory deals with.
",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
Learning Robotic Manipulation through Visual Planning and Acting,"  Planning for robotic manipulation requires reasoning about the changes a
robot can affect on objects. When such interactions can be modelled
analytically, as in domains with rigid objects, efficient planning algorithms
exist. However, in both domestic and industrial domains, the objects of
interest can be soft, or deformable, and hard to model analytically. For such
cases, we posit that a data-driven modelling approach is more suitable. In
recent years, progress in deep generative models has produced methods that
learn to `imagine' plausible images from data. Building on the recent Causal
InfoGAN generative model, in this work we learn to imagine goal-directed object
manipulation directly from raw image data of self-supervised interaction of the
robot with the object. After learning, given a goal observation of the system,
our model can generate an imagined plan -- a sequence of images that transition
the object into the desired goal. To execute the plan, we use it as a reference
trajectory to track with a visual servoing controller, which we also learn from
the data as an inverse dynamics model. In a simulated manipulation task, we
show that separating the problem into visual planning and visual tracking
control is more sample efficient and more interpretable than alternative
data-driven approaches. We further demonstrate our approach on learning to
imagine and execute in 3 environments, the final of which is deformable rope
manipulation on a PR2 robot.

    ",Robotics (cs.RO),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ComputerVisionandPatternRecognition(cs.CV)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.CV', 'cs.LG']"
MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale,"  General-purpose robotic systems must master a large repertoire of diverse
skills to be useful in a range of daily tasks. While reinforcement learning
provides a powerful framework for acquiring individual behaviors, the time
needed to acquire each skill makes the prospect of a generalist robot trained
with RL daunting. In this paper, we study how a large-scale collective robotic
learning system can acquire a repertoire of behaviors simultaneously, sharing
exploration, experience, and representations across tasks. In this framework
new tasks can be continuously instantiated from previously learned tasks
improving overall performance and capabilities of the system. To instantiate
this system, we develop a scalable and intuitive framework for specifying new
tasks through user-provided examples of desired outcomes, devise a multi-robot
collective learning system for data collection that simultaneously collects
experience for multiple tasks, and develop a scalable and generalizable
multi-task deep reinforcement learning method, which we call MT-Opt. We
demonstrate how MT-Opt can learn a wide range of skills, including semantic
picking (i.e., picking an object from a particular category), placing into
various fixtures (e.g., placing a food item onto a plate), covering, aligning,
and rearranging. We train and evaluate our system on a set of 12 real-world
tasks with data collected from 7 robots, and demonstrate the performance of our
system both in terms of its ability to generalize to structurally similar new
tasks, and acquire distinct new tasks more quickly by leveraging past
experience. We recommend viewing the videos at
",Robotics (cs.RO),; Machine Learning (cs.LG),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.LG']"
Towards 5G Enabled Tactile Robotic Telesurgery,"  Robotic telesurgery has a potential to provide extreme and urgent health care
services and bring unprecedented opportunities to deliver highly specialized
skills globally. It has a significant societal impact and is regarded as one of
the appealing use cases of Tactile Internet and 5G applications. However, the
performance of robotic telesurgery largely depends on the network performance
in terms of latency, jitter and packet loss, especially when telesurgical
system is equipped with haptic feedback. This imposes significant challenges to
design a reliable and secure but cost-effective communication solution. This
article aims to give a better understanding of the characteristics of robotic
telesurgical system, and the limiting factors, the possible telesurgery
services and the communication quality of service (QoS) requirements of the
multi-modal sensory data. Based on this, a viable network architecture enabled
by the converged edge and core cloud is presented and the relevant research
challenges, open issues and enabling technologies in the 5G communication
system are discussed.

    ",Networking and Internet Architecture (cs.NI),,['NetworkingandInternetArchitecture(cs.NI)'],['cs.NI']
NeBula: Quest for Robotic Autonomy in Challenging Environments; TEAM CoSTAR at the DARPA Subterranean Challenge,"  This paper presents and discusses algorithms, hardware, and software
architecture developed by the TEAM CoSTAR (Collaborative SubTerranean
Autonomous Robots), competing in the DARPA Subterranean Challenge.
Specifically, it presents the techniques utilized within the Tunnel (2019) and
Urban (2020) competitions, where CoSTAR achieved 2nd and 1st place,
respectively. We also discuss CoSTAR's demonstrations in Martian-analog surface
and subsurface (lava tubes) exploration. The paper introduces our autonomy
solution, referred to as NeBula (Networked Belief-aware Perceptual Autonomy).
NeBula is an uncertainty-aware framework that aims at enabling resilient and
modular autonomy solutions by performing reasoning and decision making in the
belief space (space of probability distributions over the robot and world
states). We discuss various components of the NeBula framework, including: (i)
geometric and semantic environment mapping; (ii) a multi-modal positioning
system; (iii) traversability analysis and local planning; (iv) global motion
planning and exploration behavior; (i) risk-aware mission planning; (vi)
networking and decentralized reasoning; and (vii) learning-enabled adaptation.
We discuss the performance of NeBula on several robot types (e.g. wheeled,
legged, flying), in various environments. We discuss the specific results and
lessons learned from fielding this solution in the challenging courses of the
DARPA Subterranean Challenge competition.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)']","['cs.RO', 'cs.AI']"
DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting,"  Robotic cutting of soft materials is critical for applications such as food
processing, household automation, and surgical manipulation. As in other areas
of robotics, simulators can facilitate controller verification, policy
learning, and dataset generation. Moreover, differentiable simulators can
enable gradient-based optimization, which is invaluable for calibrating
simulation parameters and optimizing controllers. In this work, we present
DiSECt: the first differentiable simulator for cutting soft materials. The
simulator augments the finite element method (FEM) with a continuous contact
model based on signed distance fields (SDF), as well as a continuous damage
model that inserts springs on opposite sides of the cutting plane and allows
them to weaken until zero stiffness, enabling crack formation. Through various
experiments, we evaluate the performance of the simulator. We first show that
the simulator can be calibrated to match resultant forces and deformation
fields from a state-of-the-art commercial solver and real-world cutting
datasets, with generality across cutting velocities and object instances. We
then show that Bayesian inference can be performed efficiently by leveraging
the differentiability of the simulator, estimating posteriors over hundreds of
parameters in a fraction of the time of derivative-free methods. Finally, we
illustrate that control parameters in the simulation can be optimized to
minimize cutting forces via lateral slicing motions.
",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
Compare Contact Model-based Control and Contact Model-free Learning: A Survey of Robotic Peg-in-hole Assembly Strategies,"  In this paper, we present an overview of robotic peg-in-hole assembly and
analyze two main strategies: contact model-based and contact model-free
strategies. More specifically, we first introduce the contact model control
approaches, including contact state recognition and compliant control two
steps. Additionally, we focus on a comprehensive analysis of the whole robotic
assembly system. Second, without the contact state recognition process, we
decompose the contact model-free learning algorithms into two main subfields:
learning from demonstrations and learning from environments (mainly based on
reinforcement learning). For each subfield, we survey the landmark studies and
ongoing research to compare the different categories. We hope to strengthen the
relation between these two research communities by revealing the underlying
links. Ultimately, the remaining challenges and open questions in the field of
robotic peg-in-hole assembly community is discussed. The promising directions
and potential future work are also considered.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
Self-Supervised Siamese Learning on Stereo Image Pairs for Depth Estimation in Robotic Surgery,"  Robotic surgery has become a powerful tool for performing minimally invasive
procedures, providing advantages in dexterity, precision, and 3D vision, over
traditional surgery. One popular robotic system is the da Vinci surgical
platform, which allows preoperative information to be incorporated into live
procedures using Augmented Reality (AR). Scene depth estimation is a
prerequisite for AR, as accurate registration requires 3D correspondences
between preoperative and intraoperative organ models. In the past decade, there
has been much progress on depth estimation for surgical scenes, such as using
monocular or binocular laparoscopes [1,2]. More recently, advances in deep
learning have enabled depth estimation via Convolutional Neural Networks (CNNs)
[3], but training requires a large image dataset with ground truth depths.
Inspired by [4], we propose a deep learning framework for surgical scene depth
estimation using self-supervision for scalable data acquisition. Our framework
consists of an autoencoder for depth prediction, and a differentiable spatial
transformer for training the autoencoder on stereo image pairs without ground
truth depths. Validation was conducted on stereo videos collected in robotic
partial nephrectomy.

    ",Computer Vision and Pattern Recognition (cs.CV),; Robotics (cs.RO),"['ComputerVisionandPatternRecognition(cs.CV)', 'Robotics(cs.RO)']","['cs.CV', 'cs.RO']"
A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation,"  Deep learning techniques have been widely applied, achieving state-of-the-art
results in various fields of study. This survey focuses on deep learning
solutions that target learning control policies for robotics applications. We
carry out our discussions on the two main paradigms for learning control with
deep networks: deep reinforcement learning and imitation learning. For deep
reinforcement learning (DRL), we begin from traditional reinforcement learning
algorithms, showing how they are extended to the deep context and effective
mechanisms that could be added on top of the DRL algorithms. We then introduce
representative works that utilize DRL to solve navigation and manipulation
tasks in robotics. We continue our discussion on methods addressing the
challenge of the reality gap for transferring DRL policies trained in
simulation to real-world scenarios, and summarize robotics simulation platforms
for conducting DRL research. For imitation leaning, we go through its three
main categories, behavior cloning, inverse reinforcement learning and
generative adversarial imitation learning, by introducing their formulations
and their corresponding robotics applications. Finally, we discuss the open
challenges and research frontiers.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)', 'MachineLearning(cs.LG)', 'SystemsandControl(eess.SY)']","['cs.RO', 'cs.AI', 'cs.LG', 'eess.SY']"
Asymmetric self-play for automatic goal discovery in robotic manipulation,"  We train a single, goal-conditioned policy that can solve many robotic
manipulation tasks, including tasks with previously unseen goals and objects.
We rely on asymmetric self-play for goal discovery, where two agents, Alice and
Bob, play a game. Alice is asked to propose challenging goals and Bob aims to
solve them. We show that this method can discover highly diverse and complex
goals without any human priors. Bob can be trained with only sparse rewards,
because the interaction between Alice and Bob results in a natural curriculum
and Bob can learn from Alice's trajectory when relabeled as a goal-conditioned
demonstration. Finally, our method scales, resulting in a single policy that
can generalize to many unseen tasks such as setting a table, stacking blocks,
and solving simple puzzles. Videos of a learned policy is available at
",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO),"['MachineLearning(cs.LG)', 'ArtificialIntelligence(cs.AI)', 'ComputerVisionandPatternRecognition(cs.CV)', 'Robotics(cs.RO)']","['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO']"
"MuSHR: A Low-Cost, Open-Source Robotic Racecar for Education and Research","  We present MuSHR, the Multi-agent System for non-Holonomic Racing. MuSHR is a
low-cost, open-source robotic racecar platform for education and research,
developed by the Personal Robotics Lab in the Paul G. Allen School of Computer
Science & Engineering at the University of Washington. MuSHR aspires to
contribute towards democratizing the field of robotics as a low-cost platform
that can be built and deployed by following detailed, open documentation and
do-it-yourself tutorials. A set of demos and lab assignments developed for the
Mobile Robots course at the University of Washington provide guided, hands-on
experience with the platform, and milestones for further development. MuSHR is
a valuable asset for academic research labs, robotics instructors, and robotics
enthusiasts.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
An Overview of Blockchain Integration with Robotics and Artificial Intelligence,"  Blockchain technology is growing everyday at a fast-passed rhythm and it's
possible to integrate it with many systems, namely Robotics with AI services.
However, this is still a recent field and there isn't yet a clear understanding
of what it could potentially become. In this paper, we conduct an overview of
many different methods and platforms that try to leverage the power of
blockchain into robotic systems, to improve AI services or to solve problems
that are present in the major blockchains, which can lead to the ability of
creating robotic systems with increased capabilities and security. We present
an overview, discuss the methods and conclude the paper with our view on the
future of the integration of these technologies.

    ",Artificial Intelligence (cs.AI),; Cryptography and Security (cs.CR),"['ArtificialIntelligence(cs.AI)', 'CryptographyandSecurity(cs.CR)']","['cs.AI', 'cs.CR']"
Recent Developments in Aerial Robotics: A Survey and Prototypes Overview,"  In recent years, research and development in aerial robotics (i.e., unmanned
aerial vehicles, UAVs) has been growing at an unprecedented speed, and there is
a need to summarize the background, latest developments, and trends of UAV
research. Along with a general overview on the definition, types, categories,
and topics of UAV, this work describes a systematic way to identify 1,318
high-quality UAV papers from more than thirty thousand that have been appeared
in the top journals and conferences. On top of that, we provide a bird's-eye
view of UAV research since 2001 by summarizing various statistical information,
such as the year, type, and topic distribution of the UAV papers. We make our
survey list public and believe that the list can not only help researchers
identify, study, and compare their work, but is also useful for understanding
research trends in the field. From our survey results, we find there are many
types of UAV, and to the best of our knowledge, no literature has attempted to
summarize all types in one place. With our survey list, we explain the types
within our survey and outline the recent progress of each. We believe this
summary can enhance readers' understanding on the UAVs and inspire researchers
to propose new methods and new applications.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
A Workflow for Offline Model-Free Robotic Reinforcement Learning,"  Offline reinforcement learning (RL) enables learning control policies by
utilizing only prior experience, without any online interaction. This can allow
robots to acquire generalizable skills from large and diverse datasets, without
any costly or unsafe online data collection. Despite recent algorithmic
advances in offline RL, applying these methods to real-world problems has
proven challenging. Although offline RL methods can learn from prior data,
there is no clear and well-understood process for making various design
choices, from model architecture to algorithm hyperparameters, without actually
evaluating the learned policies online. In this paper, our aim is to develop a
practical workflow for using offline RL analogous to the relatively
well-understood workflows for supervised learning problems. To this end, we
devise a set of metrics and conditions that can be tracked over the course of
offline training, and can inform the practitioner about how the algorithm and
model architecture should be adjusted to improve final performance. Our
workflow is derived from a conceptual understanding of the behavior of
conservative offline RL algorithms and cross-validation in supervised learning.
We demonstrate the efficacy of this workflow in producing effective policies
without any online tuning, both in several simulated robotic learning scenarios
and for three tasks on two distinct real robots, focusing on learning
manipulation skills with raw image observations with sparse binary rewards.
Explanatory video and additional results can be found at
",Machine Learning (cs.LG),,['MachineLearning(cs.LG)'],['cs.LG']
Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics,"  Scaling end-to-end reinforcement learning to control real robots from vision
presents a series of challenges, in particular in terms of sample efficiency.
Against end-to-end learning, state representation learning can help learn a
compact, efficient and relevant representation of states that speeds up policy
learning, reducing the number of samples needed, and that is easier to
interpret. We evaluate several state representation learning methods on goal
based robotics tasks and propose a new unsupervised model that stacks
representations and combines strengths of several of these approaches. This
method encodes all the relevant features, performs on par or better than
end-to-end learning with better sample efficiency, and is robust to
hyper-parameters change.

    ",Machine Learning (cs.LG),; Robotics (cs.RO); Machine Learning (stat.ML),"['MachineLearning(cs.LG)', 'Robotics(cs.RO)', 'MachineLearning(stat.ML)']","['cs.LG', 'cs.RO', 'stat.ML']"
Deep Reinforcement Learning for Robotic Manipulation-The state of the art,"  The focus of this work is to enumerate the various approaches and algorithms
that center around application of reinforcement learning in robotic ma-
]]nipulation tasks. Earlier methods utilized specialized policy representations
and human demonstrations to constrict the policy. Such methods worked well with
continuous state and policy space of robots but failed to come up with
generalized policies. Subsequently, high dimensional non-linear function
approximators like neural networks have been used to learn policies from
scratch. Several novel and recent approaches have also embedded control policy
with efficient perceptual representation using deep learning. This has led to
the emergence of a new branch of dynamic robot control system called deep r
inforcement learning(DRL). This work embodies a survey of the most recent
algorithms, architectures and their implementations in simulations and real
world robotic platforms. The gamut of DRL architectures are partitioned into
two different branches namely, discrete action space algorithms(DAS) and
continuous action space algorithms(CAS). Further, the CAS algorithms are
divided into stochastic continuous action space(SCAS) and deterministic
continuous action space(DCAS) algorithms. Along with elucidating an organ-
isation of the DRL algorithms this work also manifests some of the state of the
art applications of these approaches in robotic manipulation tasks.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)']","['cs.RO', 'cs.AI']"
Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning,"  Reinforcement learning provides a general framework for learning robotic
skills while minimizing engineering effort. However, most reinforcement
learning algorithms assume that a well-designed reward function is provided,
and learn a single behavior for that single reward function. Such reward
functions can be difficult to design in practice. Can we instead develop
efficient reinforcement learning methods that acquire diverse skills without
any reward function, and then repurpose these skills for downstream tasks? In
this paper, we demonstrate that a recently proposed unsupervised skill
discovery algorithm can be extended into an efficient off-policy method, making
it suitable for performing unsupervised reinforcement learning in the real
world. Firstly, we show that our proposed algorithm provides substantial
improvement in learning efficiency, making reward-free real-world training
feasible. Secondly, we move beyond the simulation environments and evaluate the
algorithm on real physical hardware. On quadrupeds, we observe that locomotion
skills with diverse gaits and different orientations emerge without any rewards
or demonstrations. We also demonstrate that the learned skills can be composed
using model predictive control for goal-oriented navigation, without any
additional training.

    ",Robotics (cs.RO),; Machine Learning (cs.LG),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.LG']"
The implications of embodiment for behavior and cognition: animal and robotic case studies,"  In this paper, we will argue that if we want to understand the function of
the brain (or the control in the case of robots), we must understand how the
brain is embedded into the physical system, and how the organism interacts with
the real world. While embodiment has often been used in its trivial meaning,
i.e. 'intelligence requires a body', the concept has deeper and more important
implications, concerned with the relation between physical and information
(neural, control) processes. A number of case studies are presented to
illustrate the concept. These involve animals and robots and are concentrated
around locomotion, grasping, and visual perception. A theoretical scheme that
can be used to embed the diverse case studies will be presented. Finally, we
will establish a link between the low-level sensory-motor processes and
cognition. We will present an embodied view on categorization, and propose the
concepts of 'body schema' and 'forward models' as a natural extension of the
embodied approach toward first representations.

    ",Artificial Intelligence (cs.AI),,['ArtificialIntelligence(cs.AI)'],['cs.AI']
HATP: An HTN Planner for Robotics,"  Hierarchical Task Network (HTN) planning is a popular approach that cuts down
on the classical planning search space by relying on a given hierarchical
library of domain control knowledge. This provides an intuitive methodology for
specifying high-level instructions on how robots and agents should perform
tasks, while also giving the planner enough flexibility to choose the
lower-level steps and their ordering. In this paper we present the HATP
(Hierarchical Agent-based Task Planner) planning framework which extends the
traditional HTN planning domain representation and semantics by making them
more suitable for roboticists, and treating agents as ""first class"" entities in
the language. The former is achieved by allowing ""social rules"" to be defined
which specify what behaviour is acceptable/unacceptable by the agents/robots in
the domain, and interleaving planning with geometric reasoning in order to
validate online -with respect to a detailed geometric 3D world- the human/robot
actions currently being pursued by HATP.

    ",Robotics (cs.RO),; Artificial Intelligence (cs.AI),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)']","['cs.RO', 'cs.AI']"
The ORCA Hub: Explainable Offshore Robotics through Intelligent Interfaces,"  We present the UK Robotics and Artificial Intelligence Hub for Offshore
Robotics for Certification of Assets (ORCA Hub), a 3.5 year EPSRC funded,
multi-site project. The ORCA Hub vision is to use teams of robots and
autonomous intelligent systems (AIS) to work on offshore energy platforms to
enable cheaper, safer and more efficient working practices. The ORCA Hub will
research, integrate, validate and deploy remote AIS solutions that can operate
with existing and future offshore energy assets and sensors, interacting safely
in autonomous or semi-autonomous modes in complex and cluttered environments,
co-operating with remote operators. The goal is that through the use of such
robotic systems offshore, the need for personnel will decrease. To enable this
to happen, the remote operator will need a high level of situation awareness
and key to this is the transparency of what the autonomous systems are doing
and why. This increased transparency will facilitate a trusting relationship,
which is particularly key in high-stakes, hazardous situations.

    ",Artificial Intelligence (cs.AI),; Human-Computer Interaction (cs.HC); Robotics (cs.RO),"['ArtificialIntelligence(cs.AI)', 'Human-ComputerInteraction(cs.HC)', 'Robotics(cs.RO)']","['cs.AI', 'cs.HC', 'cs.RO']"
Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping,"  The distributional perspective on reinforcement learning (RL) has given rise
to a series of successful Q-learning algorithms, resulting in state-of-the-art
performance in arcade game environments. However, it has not yet been analyzed
how these findings from a discrete setting translate to complex practical
applications characterized by noisy, high dimensional and continuous
state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a
distributional variant of the recently introduced distributed Q-learning
algorithm for continuous domains, and examine its behaviour in a series of
simulated and real vision-based robotic grasping tasks. The absence of an actor
in Q2-Opt allows us to directly draw a parallel to the previous discrete
experiments in the literature without the additional complexities induced by an
actor-critic architecture. We demonstrate that Q2-Opt achieves a superior
vision-based object grasping success rate, while also being more sample
efficient. The distributional formulation also allows us to experiment with
various risk distortion metrics that give us an indication of how robots can
concretely manage risk in practice using a Deep RL control policy. As an
additional contribution, we perform batch RL experiments in our virtual
environment and compare them with the latest findings from discrete settings.
Surprisingly, we find that the previous batch RL findings from the literature
obtained on arcade game environments do not generalise to our setup.

    ",Robotics (cs.RO),; Machine Learning (cs.LG); Machine Learning (stat.ML),"['Robotics(cs.RO)', 'MachineLearning(cs.LG)', 'MachineLearning(stat.ML)']","['cs.RO', 'cs.LG', 'stat.ML']"
Dynamic Movement Primitives in Robotics: A Tutorial Survey,"  Biological systems, including human beings, have the innate ability to
perform complex tasks in versatile and agile manner. Researchers in
sensorimotor control have tried to understand and formally define this innate
property. The idea, supported by several experimental findings, that biological
systems are able to combine and adapt basic units of motion into complex tasks
finally lead to the formulation of the motor primitives theory. In this
respect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical
formulation of the motor primitives as stable dynamical systems, and are well
suited to generate motor commands for artificial systems like robots. In the
last decades, DMPs have inspired researchers in different robotic fields
including imitation and reinforcement learning, optimal control,physical
interaction, and human-robot co-working, resulting a considerable amount of
published papers. The goal of this tutorial survey is two-fold. On one side, we
present the existing DMPs formulations in rigorous mathematical terms,and
discuss advantages and limitations of each approach as well as practical
implementation details. In the tutorial vein, we also search for existing
implementations of presented approaches and release several others. On the
other side, we provide a systematic and comprehensive review of existing
literature and categorize state of the art work on DMP. The paper concludes
with a discussion on the limitations of DMPs and an outline of possible
research directions.

    ",Robotics (cs.RO),,['Robotics(cs.RO)'],['cs.RO']
"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","  Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's ""hands and eyes,"" while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at ",Robotics (cs.RO),; Computation and Language (cs.CL); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.CL', 'cs.LG']"
"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action","  Goal-conditioned policies for robotic navigation can be trained on large,
unannotated datasets, providing for good generalization to real-world settings.
However, particularly in vision-based settings where specifying goals requires
an image, this makes for an unnatural interface. Language provides a more
convenient modality for communication with robots, but contemporary methods
typically require expensive supervision, in the form of trajectories annotated
with language descriptions. We present a system, LM-Nav, for robotic navigation
that enjoys the benefits of training on unannotated large datasets of
trajectories, while still providing a high-level interface to the user. Instead
of utilizing a labeled instruction following dataset, we show that such a
system can be constructed entirely out of pre-trained models for navigation
(ViNG), image-language association (CLIP), and language modeling (GPT-3),
without requiring any fine-tuning or language-annotated robot data. We
instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon
navigation through complex, outdoor environments from natural language
instructions. For videos of our experiments, code release, and an interactive
Colab notebook that runs in your browser, please check out our project page
",Robotics (cs.RO),; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG),"['Robotics(cs.RO)', 'ArtificialIntelligence(cs.AI)', 'ComputationandLanguage(cs.CL)', 'MachineLearning(cs.LG)']","['cs.RO', 'cs.AI', 'cs.CL', 'cs.LG']"
"Beyond STEM, How Can Women Engage Big Data, Analytics, Robotics and Artificial Intelligence? An Exploratory Analysis of Confidence and Educational Factors in the Emerging Technology Waves Influencing the Role of, and Impact Upon, Women","  In spite of the rapidly advancing global technological environment, the
professional participation of women in technology, big data, analytics,
artificial intelligence and information systems related domains remains
proportionately low. Furthermore, it is of no less concern that the number of
women in leadership in these domains are in even lower proportions. In spite of
numerous initiatives to improve the participation of women in technological
domains, there is an increasing need to gain additional insights into this
phenomenon especially since it occurs in nations and geographies which have
seen a sharp rise in overall female education, without such increase
translating into a corresponding spurt in information systems and technological
roles for women. The present paper presents findings from an exploratory
analysis and outlines a framework to gain insights into educational factors in
the emerging technology waves influencing the role of, and impact upon, women.
We specifically identify ways for learning and self-efficacy as key factors,
which together lead us to the Advancement of Women in Technology (AWT) insights
framework. Based on the AWT framework, we also proposition principles that can
be used to encourage higher professional engagement of women in emerging and
advanced technologies. Key Words- Women's Education, Technology, Artificial
Intelligence, Knowing, Confidence, Self-Efficacy, Learning.

    ",Computers and Society (cs.CY),,['ComputersandSociety(cs.CY)'],['cs.CY']
2018 Robotic Scene Segmentation Challenge,"  In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich
using endoscope images of ex-vivo tissue with automatically generated
annotations from robot forward kinematics and instrument CAD models. However,
the limited background variation and simple motion rendered the dataset
uninformative in learning about which techniques would be suitable for
segmentation in real surgery. In 2017, at the same workshop in Quebec we
introduced the robotic instrument segmentation dataset with 10 teams
participating in the challenge to perform binary, articulating parts and type
segmentation of da Vinci instruments. This challenge included realistic
instrument motion and more complex porcine tissue as background and was widely
addressed with modifications on U-Nets and other popular CNN architectures. In
2018 we added to the complexity by introducing a set of anatomical objects and
medical devices to the segmented classes. To avoid over-complicating the
challenge, we continued with porcine data which is dramatically simpler than
human tissue due to the lack of fatty tissue occluding many organs.

    ",Computer Vision and Pattern Recognition (cs.CV),; Robotics (cs.RO),"['ComputerVisionandPatternRecognition(cs.CV)', 'Robotics(cs.RO)']","['cs.CV', 'cs.RO']"
