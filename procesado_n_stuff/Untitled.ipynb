{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff541d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yo\\.conda\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    SummarizationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,BigBirdPegasusForConditionalGeneration\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addcd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# *inicio de sesion en mongo y listado de colecciones.\n",
    "# *Otros user_name [Miguel, Lucas, Julieta, Guillermo]\n",
    "# def inicio_sesion(user_name, password):\n",
    "def inicio_sesion():\n",
    "    user_name = 'Guillermo'\n",
    "    password = 'aplicacionesytendenciasdelanalisisdedatos2022'\n",
    "    cluster = MongoClient(f'mongodb+srv://{user_name}:{password}@cluster0.rnxayot.mongodb.net/?retryWrites=true&w=majority')\n",
    "    # *seleccion de la coleccion de datos\n",
    "    db = cluster['papers']\n",
    "    db.list_collection_names()\n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97875b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Acceso a todas las colecciones. Devuelve una lista con las colecciones\n",
    "# nombres colecciones: papers   final_distances  tockenizador  refinded_paper  final_paper   papers_2\n",
    "def acceder_una_coleccion(nombre_col):\n",
    "    db = inicio_sesion()\n",
    "    collection = db[nombre_col]\n",
    "    return(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64ea494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Debe pasarsele un datos en formato json y una coleccion como acceder_coleccion[x] \n",
    "def save_mongo(data, collection):\n",
    "    # *guarda data en mongo db, si la coleccion esta vacia inserta el primero, \n",
    "    # *si no esta vacio compara si alguno de los documentos en la coleccion tiene \n",
    "    # *el mismo titulo que el que se va a insertar, si no es asi se inserta.\n",
    "    cantidad_inicial = collection.count_documents({})\n",
    "    try:\n",
    "        for i in range(len(data)):\n",
    "            if collection.count_documents({}) == 0:\n",
    "                collection.insert_one(data[i])\n",
    "            try:\n",
    "                collection.find({'titulo':data[i]['titulo']})[0]\n",
    "            except:\n",
    "                collection.insert_one(data[i])\n",
    "        print('Creación correcta') \n",
    "    except Exception as exception:\n",
    "        print(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3c1fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = acceder_una_coleccion(\"papers\")\n",
    "db2 = acceder_una_coleccion(\"papers_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f4371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creación correcta\n"
     ]
    }
   ],
   "source": [
    "save_mongo(list(db.find()),db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b140473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_doc = list(db2.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17fe3f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('639d9f0d6d67832e75895c46'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU'],\n",
       "  'autores': ['Mohammad Babaeizadeh',\n",
       "   'Iuri Frosio',\n",
       "   'Stephen Tyree',\n",
       "   'Jason Clemons',\n",
       "   'Jan Kautz'],\n",
       "  'abstract': ['\\n      ',\n",
       "   \"  We introduce a hybrid CPU/GPU version of the Asynchronous Advantage\\nActor-Critic (A3C) algorithm, currently the state-of-the-art method in\\nreinforcement learning for various gaming tasks. We analyze its computational\\ntraits and concentrate on aspects critical to leveraging the GPU's\\ncomputational power. We introduce a system of queues and a dynamic scheduling\\nstrategy, potentially helpful for other asynchronous algorithms as well. Our\\nhybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant\\nspeed up compared to a CPU implementation; we make it publicly available to\\nother researchers at \",\n",
       "   ' .\\n\\n    '],\n",
       "  'clase_primaria': ['Machine Learning (cs.LG)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 222'],\n",
       "  'citado': ['/scholar?cites=8757115672331028243&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0d6d67832e75895c47'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic'],\n",
       "  'autores': ['Behrad Toghi',\n",
       "   'Rodolfo Valiente',\n",
       "   'Dorsa Sadigh',\n",
       "   'Ramtin Pedarsani',\n",
       "   'Yaser P. Fallah'],\n",
       "  'abstract': ['\\n      ',\n",
       "   \"  With the adoption of autonomous vehicles on our roads, we will witness a\\nmixed-autonomy environment where autonomous and human-driven vehicles must\\nlearn to co-exist by sharing the same road infrastructure. To attain\\nsocially-desirable behaviors, autonomous vehicles must be instructed to\\nconsider the utility of other vehicles around them in their decision-making\\nprocess. Particularly, we study the maneuver planning problem for autonomous\\nvehicles and investigate how a decentralized reward structure can induce\\naltruism in their behavior and incentivize them to account for the interest of\\nother autonomous and human-driven vehicles. This is a challenging problem due\\nto the ambiguity of a human driver's willingness to cooperate with an\\nautonomous vehicle. Thus, in contrast with the existing works which rely on\\nbehavior models of human drivers, we take an end-to-end approach and let the\\nautonomous agents to implicitly learn the decision-making process of human\\ndrivers only from experience. We introduce a multi-agent variant of the\\nsynchronous Advantage Actor-Critic (A2C) algorithm and train agents that\\ncoordinate with each other and can affect the behavior of human drivers to\\nimprove traffic flow and safety.\\n\\n    \"],\n",
       "  'clase_primaria': ['Robotics (cs.RO)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 11'],\n",
       "  'citado': ['/scholar?cites=1543039800876013962&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0e6d67832e75895c48'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup'],\n",
       "  'autores': ['Han Shen', 'Kaiqing Zhang', 'Mingyi Hong', 'Tianyi Chen'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  Asynchronous and parallel implementation of standard reinforcement learning\\n(RL) algorithms is a key enabler of the tremendous success of modern RL. Among\\nmany asynchronous RL algorithms, arguably the most popular and effective one is\\nthe asynchronous advantage actor-critic (A3C) algorithm. Although A3C is\\nbecoming the workhorse of RL, its theoretical properties are still not\\nwell-understood, including its non-asymptotic analysis and the performance gain\\nof parallelism (a.k.a. linear speedup). This paper revisits the A3C algorithm\\nand establishes its non-asymptotic convergence guarantees. Under both i.i.d.\\nand Markovian sampling, we establish the local convergence guarantee for A3C in\\nthe general policy approximation case and the global convergence guarantee in\\nsoftmax policy parameterization. Under i.i.d. sampling, A3C obtains sample\\ncomplexity of $\\\\mathcal{O}(\\\\epsilon^{-2.5}/N)$ per worker to achieve $\\\\epsilon$\\naccuracy, where $N$ is the number of workers. Compared to the best-known sample\\ncomplexity of $\\\\mathcal{O}(\\\\epsilon^{-2.5})$ for two-timescale AC, A3C achieves\\n\\\\emph{linear speedup}, which justifies the advantage of parallelism and\\nasynchrony in AC algorithms theoretically for the first time. Numerical tests\\non synthetic environment, OpenAI Gym environments and Atari games have been\\nprovided to verify our theoretical analysis.\\n\\n    '],\n",
       "  'clase_primaria': ['Machine Learning (cs.LG)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 10'],\n",
       "  'citado': ['/scholar?cites=17816434878891582694&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0e6d67832e75895c49'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['The Advantage Regret-Matching Actor-Critic'],\n",
       "  'autores': ['Audrūnas Gruslys',\n",
       "   'Marc Lanctot',\n",
       "   'Rémi Munos',\n",
       "   'Finbarr Timbers',\n",
       "   'Martin Schmid',\n",
       "   'Julien Perolat',\n",
       "   'Dustin Morrill',\n",
       "   'Vinicius Zambaldi',\n",
       "   'Jean-Baptiste Lespiau',\n",
       "   'John Schultz',\n",
       "   'Mohammad Gheshlaghi Azar',\n",
       "   'Michael Bowling',\n",
       "   'Karl Tuyls'],\n",
       "  'abstract': ['\\n      ',\n",
       "   \"  Regret minimization has played a key role in online learning, equilibrium\\ncomputation in games, and reinforcement learning (RL). In this paper, we\\ndescribe a general model-free RL method for no-regret learning based on\\nrepeated reconsideration of past behavior. We propose a model-free RL\\nalgorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than\\nsaving past state-action data, ARMAC saves a buffer of past policies, replaying\\nthrough them to reconstruct hindsight assessments of past behavior. These\\nretrospective value estimates are used to predict conditional advantages which,\\ncombined with regret matching, produces a new policy. In particular, ARMAC\\nlearns from sampled trajectories in a centralized training setting, without\\nrequiring the application of importance sampling commonly used in Monte Carlo\\ncounterfactual regret (CFR) minimization; hence, it does not suffer from\\nexcessive variance in large environments. In the single-agent setting, ARMAC\\nshows an interesting form of exploration by keeping past policies intact. In\\nthe multiagent setting, ARMAC in self-play approaches Nash equilibria on some\\npartially-observable zero-sum benchmarks. We provide exploitability estimates\\nin the significantly larger game of betting-abstracted no-limit Texas Hold'em.\\n\\n    \"],\n",
       "  'clase_primaria': ['Artificial Intelligence (cs.AI)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 14'],\n",
       "  'citado': ['/scholar?cites=13622395690856603026&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0e6d67832e75895c4a'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Actor-Critic Sequence Training for Image Captioning'],\n",
       "  'autores': ['Li Zhang',\n",
       "   'Flood Sung',\n",
       "   'Feng Liu',\n",
       "   'Tao Xiang',\n",
       "   'Shaogang Gong',\n",
       "   'Yongxin Yang',\n",
       "   'Timothy M. Hospedales'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  Generating natural language descriptions of images is an important capability\\nfor a robot or other visual-intelligence driven AI agent that may need to\\ncommunicate with human users about what it is seeing. Such image captioning\\nmethods are typically trained by maximising the likelihood of ground-truth\\nannotated caption given the image. While simple and easy to implement, this\\napproach does not directly maximise the language quality metrics we care about\\nsuch as CIDEr. In this paper we investigate training image captioning methods\\nbased on actor-critic reinforcement learning in order to directly optimise\\nnon-differentiable quality metrics of interest. By formulating a per-token\\nadvantage and value computation strategy in this novel reinforcement learning\\nbased captioning model, we show that it is possible to achieve the state of the\\nart performance on the widely used MSCOCO benchmark.\\n\\n    '],\n",
       "  'clase_primaria': ['Computer Vision and Pattern Recognition (cs.CV)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 111'],\n",
       "  'citado': ['/scholar?cites=16998376404483706528&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0f6d67832e75895c4b'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Sample Efficient Actor-Critic with Experience Replay'],\n",
       "  'autores': ['Ziyu Wang',\n",
       "   'Victor Bapst',\n",
       "   'Nicolas Heess',\n",
       "   'Volodymyr Mnih',\n",
       "   'Remi Munos',\n",
       "   'Koray Kavukcuoglu',\n",
       "   'Nando de Freitas'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  This paper presents an actor-critic deep reinforcement learning agent with\\nexperience replay that is stable, sample efficient, and performs remarkably\\nwell on challenging environments, including the discrete 57-game Atari domain\\nand several continuous control problems. To achieve this, the paper introduces\\nseveral innovations, including truncated importance sampling with bias\\ncorrection, stochastic dueling network architectures, and a new trust region\\npolicy optimization method.\\n\\n    '],\n",
       "  'clase_primaria': ['Machine Learning (cs.LG)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 786'],\n",
       "  'citado': ['/scholar?cites=8369222693188103740&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0f6d67832e75895c4c'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)'],\n",
       "  'autores': ['Zhimin Hou',\n",
       "   'Kuangen Zhang',\n",
       "   'Yi Wan',\n",
       "   'Dongyu Li',\n",
       "   'Chenglong Fu',\n",
       "   'Haoyong Yu'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  The optimal policy of a reinforcement learning problem is often discontinuous\\nand non-smooth. I.e., for two states with similar representations, their\\noptimal policies can be significantly different. In this case, representing the\\nentire policy with a function approximator (FA) with shared parameters for all\\nstates maybe not desirable, as the generalization ability of parameters sharing\\nmakes representing discontinuous, non-smooth policies difficult. A common way\\nto solve this problem, known as Mixture-of-Experts, is to represent the policy\\nas the weighted sum of multiple components, where different components perform\\nwell on different parts of the state space. Following this idea and inspired by\\na recent work called advantage-weighted information maximization, we propose to\\nlearn for each state weights of these components, so that they entail the\\ninformation of the state itself and also the preferred action learned so far\\nfor the state. The action preference is characterized via the advantage\\nfunction. In this case, the weight of each component would only be large for\\ncertain groups of states whose representations are similar and preferred action\\nrepresentations are also similar. Therefore each component is easy to be\\nrepresented. We call a policy parameterized in this way an Advantage Weighted\\nMixture Policy (AWMP) and apply this idea to improve soft-actor-critic (SAC),\\none of the most competitive continuous control algorithm. Experimental results\\ndemonstrate that SAC with AWMP clearly outperforms SAC in four commonly used\\ncontinuous control tasks and achieve stable performance across different random\\nseeds.\\n\\n    '],\n",
       "  'clase_primaria': ['Machine Learning (cs.LG)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 10'],\n",
       "  'citado': ['/scholar?cites=9691472150620828666&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0f6d67832e75895c4d'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Asynchronous Advantage Actor-Critic Agent for Starcraft II'],\n",
       "  'autores': ['Basel Alghanem', 'Keerthana P G'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  Deep reinforcement learning, and especially the Asynchronous Advantage\\nActor-Critic algorithm, has been successfully used to achieve super-human\\nperformance in a variety of video games. Starcraft II is a new challenge for\\nthe reinforcement learning community with the release of pysc2 learning\\nenvironment proposed by Google Deepmind and Blizzard Entertainment. Despite\\nbeing a target for several AI developers, few have achieved human level\\nperformance. In this project we explain the complexities of this environment\\nand discuss the results from our experiments on the environment. We have\\ncompared various architectures and have proved that transfer learning can be an\\neffective paradigm in reinforcement learning research for complex scenarios\\nrequiring skill transfer.\\n\\n    '],\n",
       "  'clase_primaria': ['Artificial Intelligence (cs.AI)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 4'],\n",
       "  'citado': ['/scholar?cites=11859479095225713207&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f0f6d67832e75895c4e'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['An advantage actor-critic algorithm for robotic motion planning in dense and dynamic scenarios'],\n",
       "  'autores': ['Chengmin Zhou', 'Bingding Huang', 'Pasi Fränti'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  Intelligent robots provide a new insight into efficiency improvement in\\nindustrial and service scenarios to replace human labor. However, these\\nscenarios include dense and dynamic obstacles that make motion planning of\\nrobots challenging. Traditional algorithms like A* can plan collision-free\\ntrajectories in static environment, but their performance degrades and\\ncomputational cost increases steeply in dense and dynamic scenarios.\\nOptimal-value reinforcement learning algorithms (RL) can address these problems\\nbut suffer slow speed and instability in network convergence. Network of policy\\ngradient RL converge fast in Atari games where action is discrete and finite,\\nbut few works have been done to address problems where continuous actions and\\nlarge action space are required. In this paper, we modify existing advantage\\nactor-critic algorithm and suit it to complex motion planning, therefore\\noptimal speeds and directions of robot are generated. Experimental results\\ndemonstrate that our algorithm converges faster and stable than optimal-value\\nRL. It achieves higher success rate in motion planning with lesser processing\\ntime for robot to reach its goal.\\n\\n    '],\n",
       "  'clase_primaria': ['Robotics (cs.RO)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 2'],\n",
       "  'citado': ['/scholar?cites=16945176442469715803&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f106d67832e75895c4f'),\n",
       "  'fuente': 'arxiv',\n",
       "  'titulo': ['Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space'],\n",
       "  'autores': ['Zhou Fan', 'Rui Su', 'Weinan Zhang', 'Yong Yu'],\n",
       "  'abstract': ['\\n      ',\n",
       "   '  In this paper we propose a hybrid architecture of actor-critic algorithms for\\nreinforcement learning in parameterized action space, which consists of\\nmultiple parallel sub-actor networks to decompose the structured action space\\ninto simpler action spaces along with a critic network to guide the training of\\nall sub-actor networks. While this paper is mainly focused on parameterized\\naction space, the proposed architecture, which we call hybrid actor-critic, can\\nbe extended for more general action spaces which has a hierarchical structure.\\nWe present an instance of the hybrid actor-critic architecture based on\\nproximal policy optimization (PPO), which we refer to as hybrid proximal policy\\noptimization (H-PPO). Our experiments test H-PPO on a collection of tasks with\\nparameterized action space, where H-PPO demonstrates superior performance over\\nprevious methods of parameterized action reinforcement learning.\\n\\n    '],\n",
       "  'clase_primaria': ['Machine Learning (cs.LG)'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=',\n",
       "  'n_citaciones': ['Citado por 47'],\n",
       "  'citado': ['/scholar?cites=18098031806703639264&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f106d67832e75895c50'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Design and application of adaptive PID controller based on asynchronous advantage actor–critic learning method'],\n",
       "  'autores': ['Qifeng Sun', 'Chengze Du', 'Hongqiang Li'],\n",
       "  'abstract': ['To address the problems of the slow convergence and inefficiency in the existing adaptive PID controllers, we propose a new adaptive PID controller using the asynchronous advantage actor–critic (A3C) algorithm. Firstly, the controller can train the multiple agents of the actor–critic structures in parallel exploiting the multi-thread asynchronous learning characteristics of the A3C structure. Secondly, in order to achieve the best control effect, each agent uses a multilayer neural network to approach the strategy function and value function to search the best parameter-tuning strategy in continuous action space. The simulation results indicate that our proposed controller can achieve the fast convergence and strong adaptability compared with conventional controllers.'],\n",
       "  'clase_primaria': ['Reinforcement learning',\n",
       "   'Asynchronous advantage actor–critic',\n",
       "   'Adaptive PID control',\n",
       "   'Stepping motor'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 13'],\n",
       "  'citado': ['/scholar?cites=7313712638896097429&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f106d67832e75895c51'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Balance Control for the First-order Inverted Pendulum Based on the Advantage Actor-critic Algorithm'],\n",
       "  'autores': ['Yan Zheng', 'Xutong Li', 'Long Xu'],\n",
       "  'abstract': ['In this paper, a control algorithm based on Advantage Actor-Critic for the classical inverted pendulum system has been proposed. To enrich the observed states which are used to control, a CNN feature-based state is proposed. The direct control and the indirect control algorithms are introduced to address different control situations, such as the situation which only physical states like angle, velocity, etc. provided or the situation which only the indirect states provided like images, etc. A comparison experiment between the direct control and the indirect control algorithms based on the Advantage Actor-Critic has been evaluated. Besides, the comparison experiment with the Deep Q-Network algorithm has been performed. The experiment results show that the proposed method achieves comparable performance with the PID control algorithm and better than the Deep Q-Network based algorithm.'],\n",
       "  'clase_primaria': ['Actor critic',\n",
       "   'deep Q network(DQN)',\n",
       "   'inverted pendulum',\n",
       "   'PID',\n",
       "   'reinforcement learning'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 6'],\n",
       "  'citado': ['/scholar?cites=4536347427942814130&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c52'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Attention-based advantage actor-critic algorithm with prioritized experience replay for complex 2-D robotic motion planning'],\n",
       "  'autores': ['Chengmin Zhou', 'Bingding Huang', 'Pasi Fränti'],\n",
       "  'abstract': ['Robotic motion planning in dense and dynamic indoor scenarios constantly challenges the researchers because of the motion unpredictability of obstacles. Recent progress in reinforcement learning enables robots to better cope with the dense and unpredictable obstacles by encoding complex features of the robot and obstacles into the encoders like the ',\n",
       "   ' (LSTM). Then these features are learned by the robot using reinforcement learning algorithms, such as the deep Q network and asynchronous advantage actor critic algorithm. However, existing methods depend heavily on expert experiences to enhance the convergence speed of the networks by initializing them via imitation learning. Moreover, those approaches based on LSTM to encode the obstacle features are not always efficient and robust enough, therefore sometimes causing the network overfitting in training. This paper focuses on the advantage actor critic algorithm and introduces an ',\n",
       "   ' to improve the performance of existing algorithm from two perspectives. First, LSTM encoder is replaced by a robust encoder ',\n",
       "   ' to better interpret the complex features of the robot and obstacles. Second, the robot learns from its past prioritized experiences to initialize the networks of the advantage actor-critic algorithm. This is achieved by applying the ',\n",
       "   ' method, which makes the best of past useful experiences to improve the convergence speed. As results, the network based on our algorithm takes only around 15% and 30% experiences to get rid of the early-stage training without the expert experiences in cases with five and ten obstacles, respectively. Then it converges faster to a better reward with less experiences (near 45% and 65% of experiences in cases with ten and five obstacles respectively) when comparing with the baseline LSTM-based advantage actor critic algorithm. Our source code is freely available at the GitHub (',\n",
       "   ').'],\n",
       "  'clase_primaria': ['Motion planning',\n",
       "   'Path planning',\n",
       "   'Reinforcement learning',\n",
       "   'Intelligent robot',\n",
       "   'Deep learning'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c53'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Robustness Assessment of Asynchronous Advantage Actor-Critic Based on Dynamic Skewness and Sparseness Computation: A Parallel Computing View'],\n",
       "  'autores': ['Tong Chen', 'Ji-Qiang Liu', 'Gang Li'],\n",
       "  'abstract': [\"Reinforcement learning as autonomous learning is greatly driving artificial intelligence (AI) development to practical applications. Having demonstrated the potential to significantly improve synchronously parallel learning, the parallel computing based asynchronous advantage actor-critic (A3C) opens a new door for reinforcement learning. Unfortunately, the acceleration's inuence on A3C robustness has been largely overlooked. In this paper, we perform the first robustness assessment of A3C based on parallel computing. By perceiving the policy’s action, we construct a global matrix of action probability deviation and define two novel measures of skewness and sparseness to form an integral robustness measure. Based on such static assessment, we then develop a dynamic robustness assessing algorithm through situational whole-space state sampling of changing episodes. Extensive experiments with different combinations of agent number and learning rate are implemented on an A3C-based pathfinding application, demonstrating that our proposed robustness assessment can effectively measure the robustness of A3C, which can achieve an accuracy of 83.3%.\"],\n",
       "  'clase_primaria': ['robustness assessment',\n",
       "   'skewness',\n",
       "   'sparseness',\n",
       "   'asynchronous advantage actor-critic',\n",
       "   'reinforcement learning'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 2'],\n",
       "  'citado': ['/scholar?cites=16266676036675648691&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c54'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['A Prioritized objective actor-critic method for deep reinforcement learning'],\n",
       "  'autores': ['Ngoc Duy Nguyen', 'Thanh Thi Nguyen', 'Saeid Nahavandi'],\n",
       "  'abstract': ['An increasing number of complex problems have naturally posed significant challenges in decision-making theory and reinforcement learning practices. These problems often involve multiple conflicting reward signals that inherently cause agents’ poor exploration in seeking a specific goal. In extreme cases, the agent gets stuck in a sub-optimal solution and starts behaving harmfully. To overcome such obstacles, we introduce two actor-critic deep reinforcement learning methods, namely ',\n",
       "   ' (MCSP) and ',\n",
       "   ' (SCMP), which can adjust agent behaviors to efficiently achieve a designated goal by adopting a weighted-sum scalarization of different objective functions. In particular, MCSP creates a human-centric policy that corresponds to a predefined priority weight of different objectives. Whereas, SCMP is capable of generating a mixed policy based on a set of priority weights, ',\n",
       "   ', the generated policy uses the knowledge of different policies (each policy corresponds to a priority weight) to dynamically prioritize objectives in real time. We examine our methods by using the ',\n",
       "   ' (A3C) algorithm to utilize the multithreading mechanism for dynamically balancing training intensity of different policies into a single network. Finally, simulation results show that MCSP and SCMP significantly outperform A3C with respect to the mean of total rewards in two complex problems: Food Collector and Seaquest.'],\n",
       "  'clase_primaria': ['Deep learning',\n",
       "   'Reinforcement learning',\n",
       "   'Learning systems',\n",
       "   'Multi-objective optimization',\n",
       "   'Actor-critic architecture'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 9'],\n",
       "  'citado': ['/scholar?cites=5780159474260208219&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c55'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Deep Reinforcement Learning in VizDoom via DQN and Actor-Critic Agents'],\n",
       "  'autores': ['Maria Bakhanova', 'Ilya Makarov'],\n",
       "  'abstract': ['In this work, we study the problem of learning reinforcement learning-based agents in a first-person shooter environment VizDoom. We compare several well-known architectures, such as DQN, DDQN, A3C, and Curiosity-driven model, while highlighting the main differences in learned policies of agents trained via these models.\\n'],\n",
       "  'clase_primaria': ['Deep reinforcement learning',\n",
       "   'DQN',\n",
       "   'A3C',\n",
       "   'A2C',\n",
       "   'VizDoom'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 6'],\n",
       "  'citado': ['/scholar?cites=7478719658550434890&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c56'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Application of the asynchronous advantage actor–critic machine learning algorithm to real-time accelerator tuning'],\n",
       "  'autores': ['Yun Zou', 'Qing-Zi Xing', 'Xue-Wu Wang'],\n",
       "  'abstract': ['This paper describes a real-time beam tuning method with an improved asynchronous advantage actor–critic (A3C) algorithm for accelerator systems. The operating parameters of devices are usually inconsistent with the predictions of physical designs because of errors in mechanical matching and installation. Therefore, parameter optimization methods such as pointwise scanning, evolutionary algorithms (EAs), and robust conjugate direction search are widely used in beam tuning to compensate for this inconsistency. However, it is difficult for them to deal with a large number of discrete local optima. The A3C algorithm, which has been applied in the automated control field, provides an approach for improving multi-dimensional optimization. The A3C algorithm is introduced and improved for the real-time beam tuning code for accelerators. Experiments in which optimization is achieved by using pointwise scanning, the genetic algorithm (one kind of EAs), and the A3C-algorithm are conducted and compared to optimize the currents of four steering magnets and two solenoids in the low-energy beam transport section (LEBT) of the Xi’an Proton Application Facility. Optimal currents are determined when the highest transmission of a radio frequency quadrupole (RFQ) accelerator downstream of the LEBT is achieved. The optimal work points of the tuned accelerator were obtained with currents of 0 A, 0 A, 0 A, and 0.1 A, for the four steering magnets, and 107 A and 96 A for the two solenoids. Furthermore, the highest transmission of the RFQ was 91.2%. Meanwhile, the lower time required for the optimization with the A3C algorithm was successfully verified. Optimization with the A3C algorithm consumed 42% and 78% less time than pointwise scanning with random initialization and pre-trained initialization of weights, respectively.'],\n",
       "  'clase_primaria': ['Real-time beam tuning',\n",
       "   'Parameter optimization',\n",
       "   'Asynchronous advantage actor–critic algorithm',\n",
       "   'Low-energy beam transport'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 5'],\n",
       "  'citado': ['/scholar?cites=51620255185177627&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f116d67832e75895c57'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Natural Actor-Critic'],\n",
       "  'autores': ['Jan Peters', 'Sethu Vijayakumar', 'Stefan Schaal'],\n",
       "  'abstract': ['This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.'],\n",
       "  'clase_primaria': ['Reinforcement Learning',\n",
       "   'Fisher Information Matrix',\n",
       "   'Natural Gradient',\n",
       "   'Imitation Learning',\n",
       "   'Motor Primitive'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 290'],\n",
       "  'citado': ['/scholar?cites=9477490257951164486&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f126d67832e75895c58'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Optimal fractional-order PID controller based on fractional-order actor-critic algorithm'],\n",
       "  'autores': ['Raafat Shalaby', 'Mohammad El-Hossainy', 'Tarek A. Mahmoud'],\n",
       "  'abstract': ['In this paper, an online optimization approach of a fractional-order PID controller based on a fractional-order actor-critic algorithm (FOPID-FOAC) is proposed. The proposed FOPID-FOAC scheme exploits the advantages of the FOPID controller and FOAC approaches to improve the performance of nonlinear systems. The proposed FOAC is built by developing a FO-based learning approach for the actor-critic neural network with adaptive learning rates. Moreover, a FO rectified linear unit (RLU) is introduced to enable the AC neural network to define and optimize its own activation function. By the means of the Lyapunov theorem, the convergence and the stability analysis of the proposed algorithm are investigated. The FO operators for the FOAC learning algorithm are obtained using the gray wolf optimization (GWO) algorithm. The effectiveness of the proposed approach is proven by extensive simulations based on the tracking problem of the two degrees of freedom (2-DOF) helicopter system and the stabilization issue of the inverted pendulum (IP) system. Moreover, the performance of the proposed algorithm is compared against optimized FOPID control approaches in different system conditions, namely when the system is subjected to parameter uncertainties and external disturbances. The performance comparison is conducted in terms of two types of performance indices, the error performance indices, and the time response performance indices. The first one includes the integral absolute error (IAE), and the integral squared error (ISE), whereas the second type involves the rising time, the maximum overshoot (Max. OS), and the settling time. The simulation results explicitly indicate the high effectiveness of the proposed FOPID-FOAC controller in terms of the two types of performance measurements under different scenarios compared with the other control algorithms.'],\n",
       "  'clase_primaria': ['Fractional-order PID controller',\n",
       "   'Reinforcement learning',\n",
       "   'Actor-critic algorithm',\n",
       "   'Gray wolf optimization',\n",
       "   'Lyapunov theorem'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 1'],\n",
       "  'citado': ['/scholar?cites=4924380534535514723&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f126d67832e75895c59'),\n",
       "  'fuente': 'springer',\n",
       "  'titulo': ['Evaluate, explain, and explore the state more exactly: an improved Actor-Critic algorithm for complex environment'],\n",
       "  'autores': ['ZhongYi Zha', 'Bo Wang', 'XueSong Tang'],\n",
       "  'abstract': ['This paper proposes an Advanced Actor-Critic algorithm, which is improved based on the conventional Actor-Critic algorithm, to train the agent to play the complex strategy game StarCraft II. A series of advanced features have been incorporated, including the distributional advantage estimation, information entropy-based uncertainty estimation, self-confidence-based exploration, and normal constraint-based update strategy.\\n A case study including seven StarCraft II mini-games is investigated to identify the effectiveness of the proposed approach, where the famous A3C algorithm is adopted as the comparative baseline. The results verify the superiority of the improved algorithm in accuracy and training efficacy, in complex environment with high-dimensional and hybrid state and action space.\\n'],\n",
       "  'clase_primaria': ['Deep reinforcement learning',\n",
       "   'Video game AI',\n",
       "   'Advanced actor-critic',\n",
       "   'Distributional advantage estimation',\n",
       "   'Normal constraint'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3Aspringer&oq=',\n",
       "  'n_citaciones': ['Citado por 2'],\n",
       "  'citado': ['/scholar?cites=2846085149721688035&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f126d67832e75895c5a'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Cooperative Traffic Signal Control Through AÂ\\xa0 Counterfactual Multi-Agent Deep Actor Critic Approach'],\n",
       "  'autores': ['Xiang Song', 'Bin Zhou', 'Dongfang Ma'],\n",
       "  'abstract': ['Signal control has been effective to alleviate urban traffic congestion. Massive related works about signal timing optimization have been proposed and led to many signal control methods and systems. In recent years, reinforcement learning (RL) algorithms have attracted the increasing attention of researchers in the area of signal control optimization, since they can learn the optimal timing policy themselves by analyzing changing patterns between the traffic condition and signal timing plans. Multi-intersection traffic signal control problems face more challenges than classical single intersection problems such as the dimensionality issue as the joint action space of multiple agents grows exponentially with the number of intersections. The existing state-of-art multi-agent reinforcement learning algorithms developed for other areas may not suit well with traffic signal control given the complex spatial-temporal nature of traffic flows. In this paper, we propose a novel multi-agent counterfactual actor-critic with scheduler (MACS) framework for multiple interactions. In this method, decentralized actors control the traffic signals and the centralized critic combines recurrent policies with feed-forward critics. Additionally, a scheduler module that exchanges information among agents helps the individual agent better understand the entire environment. The proposed method was evaluated using simulation experiments based on a real-world urban street network in Shenzhen, China. Results showed that the proposed method outperforms the classic model-based method and several existing RL-based methods according to different measures such as queue length, average delay time, and throughput. '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f126d67832e75895c5b'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Deep Reinforcement Learning - Masterclass 6 - Actor Critic Methods'],\n",
       "  'autores': ['Eric Benhamou', 'David Saltiel'],\n",
       "  'abstract': [\"Episode 6 of 24 lectures on Deep Reinforcement Learning, that is part of the Syllabus of Dauphine PSL's Master Programm IASD, this lecture presents Actor-Critic Algorithms and their implementations. \"],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f126d67832e75895c5c'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Efficient Multi-Agent Exploration with Mutual-Guided Actor-Critic'],\n",
       "  'autores': ['Renlong Chen', 'Ying Tan'],\n",
       "  'abstract': ['Multi-agent Reinforcement Learning (MARL) has drawn wide attention as a bunch of real-world complex scenes can be abstracted as Multi-Agent Systems. In order to solve the non-local training objective problem in shared reward environments, value-decomposition-based methods have been proposed. Most of them impose priori Individual-Global-Max (IGM) and value-decomposition constraints. Some attempts tune the value-decomposition constraints to achieve a better performance. However, IGM constraint, as the fundamental assumption of value-decomposition methods, is adopted in most value-decomposition methods, which may leads to poor exploration in certain situations. To deal with this problem, a novel algorithm called Mutual-guided Multi-agent Actor-Critic (MugAC) is proposed in this paper. MugAC imposes a joint-action pool, generated by individual action distribution, from which a joint-action is selected by the critic to interact with the environment and as a training objective of the actor. The training paradigm of MugAC provides an off-policy training for actor-critic, which leads to a higher sample efficiency than traditional actor-critic methods in MARL. We evaluate our method against the current state-of-the-art methods in StarCraft micromanagement, which is one of the most challenging and representative tasks. Experimental results show that MugAC outperforms other methods in various scenarios of widely adopted StarCraft Multi-Agent Challenge (SMAC). '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f136d67832e75895c5d'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Stock Market Trading Agent Using On-Policy Reinforcement Learning Algorithms'],\n",
       "  'autores': ['Shreyas Lele',\n",
       "   'Kavit Gangar',\n",
       "   'Harshal Daftary',\n",
       "   'Dewashish Dharkar'],\n",
       "  'abstract': ['Stock market has been a complex system which has been difficult to predict for humans, thereby, making the trading decisions difficult to take. It will be useful for traders if there is a model agent which can learn the stock market trends and suggest trading decisions which in turn maximizes the profits. Inorder to develop this agent we have formulated the problem as a Markov Decision Process (MDP) and created a stock trading environment which serves as a platform for this agent to trade the stocks. In this paper, we introduce a Reinforcement Learning based approach to develop a trading agent which performs trading actions on the environment and learns according to the rewards in terms of profit or loss it receives. We have applied different On-policy Reinforcement Learning Algorithms such as Vanilla Policy Gradient (VPG), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) on the environment to obtain the profits while trading stocks for 3 companies viz. Apple, Microsoft and Nike. The performance of these algorithms in order to maximize the profits have been evaluated and the results and conclusions have been elaborated. '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': ['Citado por 4'],\n",
       "  'citado': ['/scholar?cites=14792290705101612990&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f136d67832e75895c5e'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Evaluation and Potential Improvements of a Deep Reinforcement Learning Model for Automated Stock Trading'],\n",
       "  'autores': ['Rainer Andreas Jager'],\n",
       "  'abstract': ['The basis of this analysis is a model presented at the ACM International Conference in New York on AI in Finance in October 2020. ',\n",
       "   'The authors claim that the introduced deep reinforcement learning ensemble model outperforms the Dow Jones Industrial Average Index, and the three individual algorithms that form the ensemble in terms of the risk-adjusted returns measured by the Sharpe ratio. Furthermore, it is claimed that the ensemble is more robust and reliable than the individual agents. ',\n",
       "   'We evaluate these claims for statistical significance. As some weaknesses of the model become evident, we suggest a work-around and show the results with the suggested alteration. Finally, we combine all the findings and present an alternative model. '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f146d67832e75895c5f'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Fleet Planning Under Demand and Fuel Price Uncertainty'],\n",
       "  'autores': ['Izaak  L. Geursen', 'Bruno F. Santos', 'Neil Yorke-Smith'],\n",
       "  'abstract': ['Current state-of-the-art airline planning models face computational limitations, restricting the operational applicability to problems of representative sizes. This is particular the case when considering the uncertainty necessarily associated with the long-term plan of an aircraft fleet. Considering the growing interest in the application of machine learning techniques to operations research problems, this article investigates the applicability of these techniques for airline planning. Specifically, an Advantage Actorâ\\x80\\x93Critic (A2C) reinforcementÂ\\xa0learning algorithm is developed for the airline fleet planning problem. The increased computational efficiencyÂ\\xa0of using an A2C agent allows us to consider real size problems and account for highly-volatile uncertainty in demand and fuel price. The A2C algorithm is found to outperform a deterministic model and a deep Q-network algorithm. The relative performance of the A2C increases as more complexity is added to the problem. Further, the A2C algorithm can compute multi-stage fleet planning solution within few seconds. '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f146d67832e75895c60'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Resource Allocation Based on Deep Reinforcement Learning with High-Dimensional Matrix Diagram in Multi-Modal Optical Networks'],\n",
       "  'autores': ['Zipiao Zhao',\n",
       "   'Yikai Liu',\n",
       "   'Yongli Zhao',\n",
       "   'Yajie Li',\n",
       "   'Sabidur Rahman',\n",
       "   'Dahai Han',\n",
       "   'Jie Zhang'],\n",
       "  'abstract': [\"Optical networks have many multi-dimensional properties (e.g., nodes, links, wavelength, bandwidth, etc.), leading to the emerging research topic on multi-modal optical networks. In this study, we propose a representation method of multi-modal optical networks using high-dimensional matrix diagram. Our study designs an advantage actor critic (A2C) learning framework named deep reinforcement learning based routing and wavelength allocation (DRL-RWA). DRL-RWA learns the correct online routing and wavelength allocation policy by parameterizing the convolutional neural network (CNN) that can sense the state of complex optical networks. We further improve DRL-RWA and propose DRL-RWA with the multi-thread-based training mechanism (DRL-RWA-MT) algorithm using novel A2C and CNNâ\\x80\\x99s training mechanism. DRL-RWA-MT converts dynamic network resources and request routing to graphs by using the representation method of the high-dimensional matrix diagram. These graphs are input to the CNN for parsing the information in the graphs, and CNN's output is transmitted to A2C for learning the correct online RWA policy. The optimization goal of DRL-RWA-MT in each step of the request is to maximize the cumulative reward in the remaining training. Results with multiple topologies show that the multi-modal representation method can improve the learning efficiency, and the proposed ACRA algorithm can optimize the resource allocation. Compared to the baseline (Dijkstra shortest path and First Fit (DSP-FF)), DRL-RWA-MT can optimize resource allocation while effectively reducing blocking probability in the 9-node and NSFNET topologies. \"],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f146d67832e75895c61'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['Empirical Demonstration of Stock Paper Trading for Financial Reinforcement Learning'],\n",
       "  'autores': ['Xiao-Yang Liu', 'Ziyi Xia'],\n",
       "  'abstract': ['Deep reinforcement learning (DRL) has shown great potential in financial tasks. However, existing works optimistically reported profitable results through backtesting, suffering the \\\\textit{look-ahead bias} and \\\\textit{overfitting} issues. Therefore, these promising research results do not necessarily lead to a good performance in real-world markets. In this paper, we provide the first empirical demonstration of the stock paper trading task using deep reinforcement learning. First, we employ a ``training-validation-trading\" pipeline where an agent always learns the latest market daily data; meanwhile, the pipeline helps avoid information leakage. Then, we show that the trained DRL agent outperforms a conventional machine learning method (random forest) and the Dow Jones Industrial Average (DJIA) index during stock paper trading. Our codes are available online at: \\\\url{https://github.com/AI4Finance-Foundation/FinRL-Meta} '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': [],\n",
       "  'citado': []},\n",
       " {'_id': ObjectId('639d9f146d67832e75895c62'),\n",
       "  'fuente': 'SSRN',\n",
       "  'titulo': ['The Use of Deep Reinforcement Learning in Tactical Asset Allocation'],\n",
       "  'autores': ['Musonda Katongo', 'Ritabrata Bhattacharyya'],\n",
       "  'abstract': ['The Tactical Asset Allocation (TAA) problem is a problem to accurately capture short to medium term market trends and anomalies in order to allocate the assets in a portfolio so as to optimize its performance by increasing the risk adjusted returns. This project seeks to address the Tactical Asset Allocation problem by employing Deep Reinforcement Learning (DRL) Algorithms in a Machine Learning Environment as well as employing Neural Network Autoencoders for selection of portfolio assets. This paper presents the implementation of this proposed methodology applied to 30 stocks of the Dow Jones Industrial Average (DJIA). In (1), the Introduction to the project objectives is done with the Problem Description presented in (2). Part (3) presents the literature review of similar studies in the subject area. The methodology used for our implementation is presented in (4) whilst (5) and (6) presents the benchmark portfolios and the DRL portfolios development respectively. The evaluation of the performance of the models is presented in (7) and we present our conclusions and the future works in (8). '],\n",
       "  'clase_primaria': [],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=advantage+actor+critic+source%3ASSRN&oq=',\n",
       "  'n_citaciones': ['Citado por 3'],\n",
       "  'citado': ['/scholar?cites=7313009570388408153&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']},\n",
       " {'_id': ObjectId('639d9f146d67832e75895c63'),\n",
       "  'fuente': 'cambridge',\n",
       "  'titulo': ['Machine learning in mental health: a scoping review of methods and applications'],\n",
       "  'autores': ['Adrian B. R. Shatte',\n",
       "   'Delyse M. Hutchinson',\n",
       "   'Samantha J. Teague'],\n",
       "  'abstract': ['This paper aims to synthesise the literature on machine learning (ML) and big data applications for mental health, highlighting current research and applications in practice.',\n",
       "   \"We employed a scoping review methodology to rapidly map the field of ML in mental health. Eight health and information technology research databases were searched for papers covering this domain. Articles were assessed by two reviewers, and data were extracted on the article's mental health application, ML technique, data type, and study results. Articles were then synthesised via narrative review.\",\n",
       "   \"Three hundred papers focusing on the application of ML to mental health were identified. Four main application domains emerged in the literature, including: (i) detection and diagnosis; (ii) prognosis, treatment and support; (iii) public health, and; (iv) research and clinical administration. The most common mental health conditions addressed included depression, schizophrenia, and Alzheimer's disease. ML techniques used included support vector machines, decision trees, neural networks, latent Dirichlet allocation, and clustering.\",\n",
       "   'Overall, the application of ML to mental health has demonstrated a range of benefits across the areas of diagnosis, treatment and support, research, and clinical administration. With the majority of studies identified focusing on the detection and diagnosis of mental health conditions, it is evident that there is significant room for the application of ML to other areas of psychology and mental health. The challenges of using ML techniques are discussed, as well as opportunities to improve and advance the field.'],\n",
       "  'clase_primaria': ['Big data',\n",
       "   'health informatics',\n",
       "   'machine learning',\n",
       "   'mental health'],\n",
       "  'pag_espec': 'https://scholar.google.com/scholar?q=machine+learning+source%3Acambridge&oq=',\n",
       "  'n_citaciones': ['Citado por 388'],\n",
       "  'citado': ['/scholar?cites=11270166376113115069&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8de1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7417b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909a871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"fuente\":\"arxiv\",\"titulo\":\"Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU\",\"autores\":[\"Mohammad Babaeizadeh\",\"Iuri Frosio\",\"Stephen Tyree\",\"Jason Clemons\",\"Jan Kautz\"],\"abstract\":\"\\\\n         We introduce a hybrid CPU\\\\/GPU version of the Asynchronous Advantage\\\\nActor-Critic (A3C) algorithm, currently the state-of-the-art method in\\\\nreinforcement learning for various gaming tasks. We analyze its computational\\\\ntraits and concentrate on aspects critical to leveraging the GPU\\'s\\\\ncomputational power. We introduce a system of queues and a dynamic scheduling\\\\nstrategy, potentially helpful for other asynchronous algorithms as well. Our\\\\nhybrid CPU\\\\/GPU version of A3C, based on TensorFlow, achieves a significant\\\\nspeed up compared to a CPU implementation; we make it publicly available to\\\\nother researchers at   .\\\\n\\\\n    \",\"clase_primaria\":[\"Machine Learning (cs.LG)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 222\",\"citado\":[\"\\\\/scholar?cites=8757115672331028243&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic\",\"autores\":[\"Behrad Toghi\",\"Rodolfo Valiente\",\"Dorsa Sadigh\",\"Ramtin Pedarsani\",\"Yaser P. Fallah\"],\"abstract\":\"\\\\n         With the adoption of autonomous vehicles on our roads, we will witness a\\\\nmixed-autonomy environment where autonomous and human-driven vehicles must\\\\nlearn to co-exist by sharing the same road infrastructure. To attain\\\\nsocially-desirable behaviors, autonomous vehicles must be instructed to\\\\nconsider the utility of other vehicles around them in their decision-making\\\\nprocess. Particularly, we study the maneuver planning problem for autonomous\\\\nvehicles and investigate how a decentralized reward structure can induce\\\\naltruism in their behavior and incentivize them to account for the interest of\\\\nother autonomous and human-driven vehicles. This is a challenging problem due\\\\nto the ambiguity of a human driver\\'s willingness to cooperate with an\\\\nautonomous vehicle. Thus, in contrast with the existing works which rely on\\\\nbehavior models of human drivers, we take an end-to-end approach and let the\\\\nautonomous agents to implicitly learn the decision-making process of human\\\\ndrivers only from experience. We introduce a multi-agent variant of the\\\\nsynchronous Advantage Actor-Critic (A2C) algorithm and train agents that\\\\ncoordinate with each other and can affect the behavior of human drivers to\\\\nimprove traffic flow and safety.\\\\n\\\\n    \",\"clase_primaria\":[\"Robotics (cs.RO)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 11\",\"citado\":[\"\\\\/scholar?cites=1543039800876013962&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup\",\"autores\":[\"Han Shen\",\"Kaiqing Zhang\",\"Mingyi Hong\",\"Tianyi Chen\"],\"abstract\":\"\\\\n         Asynchronous and parallel implementation of standard reinforcement learning\\\\n(RL) algorithms is a key enabler of the tremendous success of modern RL. Among\\\\nmany asynchronous RL algorithms, arguably the most popular and effective one is\\\\nthe asynchronous advantage actor-critic (A3C) algorithm. Although A3C is\\\\nbecoming the workhorse of RL, its theoretical properties are still not\\\\nwell-understood, including its non-asymptotic analysis and the performance gain\\\\nof parallelism (a.k.a. linear speedup). This paper revisits the A3C algorithm\\\\nand establishes its non-asymptotic convergence guarantees. Under both i.i.d.\\\\nand Markovian sampling, we establish the local convergence guarantee for A3C in\\\\nthe general policy approximation case and the global convergence guarantee in\\\\nsoftmax policy parameterization. Under i.i.d. sampling, A3C obtains sample\\\\ncomplexity of $\\\\\\\\mathcal{O}(\\\\\\\\epsilon^{-2.5}\\\\/N)$ per worker to achieve $\\\\\\\\epsilon$\\\\naccuracy, where $N$ is the number of workers. Compared to the best-known sample\\\\ncomplexity of $\\\\\\\\mathcal{O}(\\\\\\\\epsilon^{-2.5})$ for two-timescale AC, A3C achieves\\\\n\\\\\\\\emph{linear speedup}, which justifies the advantage of parallelism and\\\\nasynchrony in AC algorithms theoretically for the first time. Numerical tests\\\\non synthetic environment, OpenAI Gym environments and Atari games have been\\\\nprovided to verify our theoretical analysis.\\\\n\\\\n    \",\"clase_primaria\":[\"Machine Learning (cs.LG)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 10\",\"citado\":[\"\\\\/scholar?cites=17816434878891582694&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"The Advantage Regret-Matching Actor-Critic\",\"autores\":[\"Audr\\\\u016bnas Gruslys\",\"Marc Lanctot\",\"R\\\\u00e9mi Munos\",\"Finbarr Timbers\",\"Martin Schmid\",\"Julien Perolat\",\"Dustin Morrill\",\"Vinicius Zambaldi\",\"Jean-Baptiste Lespiau\",\"John Schultz\",\"Mohammad Gheshlaghi Azar\",\"Michael Bowling\",\"Karl Tuyls\"],\"abstract\":\"\\\\n         Regret minimization has played a key role in online learning, equilibrium\\\\ncomputation in games, and reinforcement learning (RL). In this paper, we\\\\ndescribe a general model-free RL method for no-regret learning based on\\\\nrepeated reconsideration of past behavior. We propose a model-free RL\\\\nalgorithm, the AdvantageRegret-Matching Actor-Critic (ARMAC): rather than\\\\nsaving past state-action data, ARMAC saves a buffer of past policies, replaying\\\\nthrough them to reconstruct hindsight assessments of past behavior. These\\\\nretrospective value estimates are used to predict conditional advantages which,\\\\ncombined with regret matching, produces a new policy. In particular, ARMAC\\\\nlearns from sampled trajectories in a centralized training setting, without\\\\nrequiring the application of importance sampling commonly used in Monte Carlo\\\\ncounterfactual regret (CFR) minimization; hence, it does not suffer from\\\\nexcessive variance in large environments. In the single-agent setting, ARMAC\\\\nshows an interesting form of exploration by keeping past policies intact. In\\\\nthe multiagent setting, ARMAC in self-play approaches Nash equilibria on some\\\\npartially-observable zero-sum benchmarks. We provide exploitability estimates\\\\nin the significantly larger game of betting-abstracted no-limit Texas Hold\\'em.\\\\n\\\\n    \",\"clase_primaria\":[\"Artificial Intelligence (cs.AI)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 14\",\"citado\":[\"\\\\/scholar?cites=13622395690856603026&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Actor-Critic Sequence Training for Image Captioning\",\"autores\":[\"Li Zhang\",\"Flood Sung\",\"Feng Liu\",\"Tao Xiang\",\"Shaogang Gong\",\"Yongxin Yang\",\"Timothy M. Hospedales\"],\"abstract\":\"\\\\n         Generating natural language descriptions of images is an important capability\\\\nfor a robot or other visual-intelligence driven AI agent that may need to\\\\ncommunicate with human users about what it is seeing. Such image captioning\\\\nmethods are typically trained by maximising the likelihood of ground-truth\\\\nannotated caption given the image. While simple and easy to implement, this\\\\napproach does not directly maximise the language quality metrics we care about\\\\nsuch as CIDEr. In this paper we investigate training image captioning methods\\\\nbased on actor-critic reinforcement learning in order to directly optimise\\\\nnon-differentiable quality metrics of interest. By formulating a per-token\\\\nadvantage and value computation strategy in this novel reinforcement learning\\\\nbased captioning model, we show that it is possible to achieve the state of the\\\\nart performance on the widely used MSCOCO benchmark.\\\\n\\\\n    \",\"clase_primaria\":[\"Computer Vision and Pattern Recognition (cs.CV)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 111\",\"citado\":[\"\\\\/scholar?cites=16998376404483706528&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Sample Efficient Actor-Critic with Experience Replay\",\"autores\":[\"Ziyu Wang\",\"Victor Bapst\",\"Nicolas Heess\",\"Volodymyr Mnih\",\"Remi Munos\",\"Koray Kavukcuoglu\",\"Nando de Freitas\"],\"abstract\":\"\\\\n         This paper presents an actor-critic deep reinforcement learning agent with\\\\nexperience replay that is stable, sample efficient, and performs remarkably\\\\nwell on challenging environments, including the discrete 57-game Atari domain\\\\nand several continuous control problems. To achieve this, the paper introduces\\\\nseveral innovations, including truncated importance sampling with bias\\\\ncorrection, stochastic dueling network architectures, and a new trust region\\\\npolicy optimization method.\\\\n\\\\n    \",\"clase_primaria\":[\"Machine Learning (cs.LG)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 786\",\"citado\":[\"\\\\/scholar?cites=8369222693188103740&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)\",\"autores\":[\"Zhimin Hou\",\"Kuangen Zhang\",\"Yi Wan\",\"Dongyu Li\",\"Chenglong Fu\",\"Haoyong Yu\"],\"abstract\":\"\\\\n         The optimal policy of a reinforcement learning problem is often discontinuous\\\\nand non-smooth. I.e., for two states with similar representations, their\\\\noptimal policies can be significantly different. In this case, representing the\\\\nentire policy with a function approximator (FA) with shared parameters for all\\\\nstates maybe not desirable, as the generalization ability of parameters sharing\\\\nmakes representing discontinuous, non-smooth policies difficult. A common way\\\\nto solve this problem, known as Mixture-of-Experts, is to represent the policy\\\\nas the weighted sum of multiple components, where different components perform\\\\nwell on different parts of the state space. Following this idea and inspired by\\\\na recent work called advantage-weighted information maximization, we propose to\\\\nlearn for each state weights of these components, so that they entail the\\\\ninformation of the state itself and also the preferred action learned so far\\\\nfor the state. The action preference is characterized via the advantage\\\\nfunction. In this case, the weight of each component would only be large for\\\\ncertain groups of states whose representations are similar and preferred action\\\\nrepresentations are also similar. Therefore each component is easy to be\\\\nrepresented. We call a policy parameterized in this way an Advantage Weighted\\\\nMixture Policy (AWMP) and apply this idea to improve soft-actor-critic (SAC),\\\\none of the most competitive continuous control algorithm. Experimental results\\\\ndemonstrate that SAC with AWMP clearly outperforms SAC in four commonly used\\\\ncontinuous control tasks and achieve stable performance across different random\\\\nseeds.\\\\n\\\\n    \",\"clase_primaria\":[\"Machine Learning (cs.LG)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 10\",\"citado\":[\"\\\\/scholar?cites=9691472150620828666&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Asynchronous Advantage Actor-Critic Agent for Starcraft II\",\"autores\":[\"Basel Alghanem\",\"Keerthana P G\"],\"abstract\":\"\\\\n         Deep reinforcement learning, and especially the Asynchronous Advantage\\\\nActor-Critic algorithm, has been successfully used to achieve super-human\\\\nperformance in a variety of video games. Starcraft II is a new challenge for\\\\nthe reinforcement learning community with the release of pysc2 learning\\\\nenvironment proposed by Google Deepmind and Blizzard Entertainment. Despite\\\\nbeing a target for several AI developers, few have achieved human level\\\\nperformance. In this project we explain the complexities of this environment\\\\nand discuss the results from our experiments on the environment. We have\\\\ncompared various architectures and have proved that transfer learning can be an\\\\neffective paradigm in reinforcement learning research for complex scenarios\\\\nrequiring skill transfer.\\\\n\\\\n    \",\"clase_primaria\":[\"Artificial Intelligence (cs.AI)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 4\",\"citado\":[\"\\\\/scholar?cites=11859479095225713207&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"An advantage actor-critic algorithm for robotic motion planning in dense and dynamic scenarios\",\"autores\":[\"Chengmin Zhou\",\"Bingding Huang\",\"Pasi Fr\\\\u00e4nti\"],\"abstract\":\"\\\\n         Intelligent robots provide a new insight into efficiency improvement in\\\\nindustrial and service scenarios to replace human labor. However, these\\\\nscenarios include dense and dynamic obstacles that make motion planning of\\\\nrobots challenging. Traditional algorithms like A* can plan collision-free\\\\ntrajectories in static environment, but their performance degrades and\\\\ncomputational cost increases steeply in dense and dynamic scenarios.\\\\nOptimal-value reinforcement learning algorithms (RL) can address these problems\\\\nbut suffer slow speed and instability in network convergence. Network of policy\\\\ngradient RL converge fast in Atari games where action is discrete and finite,\\\\nbut few works have been done to address problems where continuous actions and\\\\nlarge action space are required. In this paper, we modify existing advantage\\\\nactor-critic algorithm and suit it to complex motion planning, therefore\\\\noptimal speeds and directions of robot are generated. Experimental results\\\\ndemonstrate that our algorithm converges faster and stable than optimal-value\\\\nRL. It achieves higher success rate in motion planning with lesser processing\\\\ntime for robot to reach its goal.\\\\n\\\\n    \",\"clase_primaria\":[\"Robotics (cs.RO)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 2\",\"citado\":[\"\\\\/scholar?cites=16945176442469715803&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]},{\"fuente\":\"arxiv\",\"titulo\":\"Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space\",\"autores\":[\"Zhou Fan\",\"Rui Su\",\"Weinan Zhang\",\"Yong Yu\"],\"abstract\":\"\\\\n         In this paper we propose a hybrid architecture of actor-critic algorithms for\\\\nreinforcement learning in parameterized action space, which consists of\\\\nmultiple parallel sub-actor networks to decompose the structured action space\\\\ninto simpler action spaces along with a critic network to guide the training of\\\\nall sub-actor networks. While this paper is mainly focused on parameterized\\\\naction space, the proposed architecture, which we call hybrid actor-critic, can\\\\nbe extended for more general action spaces which has a hierarchical structure.\\\\nWe present an instance of the hybrid actor-critic architecture based on\\\\nproximal policy optimization (PPO), which we refer to as hybrid proximal policy\\\\noptimization (H-PPO). Our experiments test H-PPO on a collection of tasks with\\\\nparameterized action space, where H-PPO demonstrates superior performance over\\\\nprevious methods of parameterized action reinforcement learning.\\\\n\\\\n    \",\"clase_primaria\":[\"Machine Learning (cs.LG)\"],\"pag_espec\":\"https:\\\\/\\\\/scholar.google.com\\\\/scholar?q=advantage+actor+critic+source%3Aarxiv&oq=\",\"n_citaciones\":\"Citado por 47\",\"citado\":[\"\\\\/scholar?cites=18098031806703639264&as_sdt=2005&sciodt=0,5&hl=es&oe=ASCII\"]}]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.DataFrame(list(db2.find(limit= 10)))\n",
    "\n",
    "\n",
    "def suma(lista):\n",
    "    lista_tem = \" \".join(lista)\n",
    "    return(lista_tem )\n",
    "def decode_encode(lista):\n",
    "    lista_tem = [x.decode('utf-8','ignore').encode(\"utf-8\") if type(x) == type(\"x\") else x for x in lista   ]\n",
    "    return(lista_tem )\n",
    "\n",
    "def pandas_to_json(df):\n",
    "    df.apply(lambda x: decode_encode(x) if type(x) == type([\"x\"]) else x ).to_json(orient=\"records\")\n",
    "\n",
    "\n",
    "df[\"abstract\"] = list(map(suma, df[\"abstract\"].tolist()))\n",
    "df[\"titulo\"] = list(map(suma, df[\"titulo\"].tolist()))\n",
    "df[\"n_citaciones\"] = list(map(suma, df[\"n_citaciones\"].tolist()))\n",
    "df = df.drop(columns=['_id'])\n",
    "\n",
    "df.apply(lambda x: decode_encode(x) if type(x) == type([\"x\"]) else x ).to_json(orient=\"records\")#.apply(encode(\"utf-8\")).to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70387d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObjectId('639d9f0d6d67832e75895c46'),\n",
       " ObjectId('639d9f0d6d67832e75895c47'),\n",
       " ObjectId('639d9f0e6d67832e75895c48'),\n",
       " ObjectId('639d9f0e6d67832e75895c49'),\n",
       " ObjectId('639d9f0e6d67832e75895c4a'),\n",
       " ObjectId('639d9f0f6d67832e75895c4b'),\n",
       " ObjectId('639d9f0f6d67832e75895c4c'),\n",
       " ObjectId('639d9f0f6d67832e75895c4d'),\n",
       " ObjectId('639d9f0f6d67832e75895c4e'),\n",
       " ObjectId('639d9f106d67832e75895c4f')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.DataFrame(list(db2.find(limit= 10)))\n",
    "df[\"_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6e118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d046af47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>fuente</th>\n",
       "      <th>titulo</th>\n",
       "      <th>autores</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clase_primaria</th>\n",
       "      <th>pag_espec</th>\n",
       "      <th>n_citaciones</th>\n",
       "      <th>citado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639d9f0d6d67832e75895c46</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Reinforcement Learning through Asynchronous A...</td>\n",
       "      <td>[Mohammad Babaeizadeh, Iuri Frosio, Stephen Ty...</td>\n",
       "      <td>[\\n      ,   We introduce a hybrid CPU/GPU ver...</td>\n",
       "      <td>[Machine Learning (cs.LG)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 222]</td>\n",
       "      <td>[/scholar?cites=8757115672331028243&amp;as_sdt=200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>639d9f0d6d67832e75895c47</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Altruistic Maneuver Planning for Cooperative ...</td>\n",
       "      <td>[Behrad Toghi, Rodolfo Valiente, Dorsa Sadigh,...</td>\n",
       "      <td>[\\n      ,   With the adoption of autonomous v...</td>\n",
       "      <td>[Robotics (cs.RO)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 11]</td>\n",
       "      <td>[/scholar?cites=1543039800876013962&amp;as_sdt=200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>639d9f0e6d67832e75895c48</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Towards Understanding Asynchronous Advantage ...</td>\n",
       "      <td>[Han Shen, Kaiqing Zhang, Mingyi Hong, Tianyi ...</td>\n",
       "      <td>[\\n      ,   Asynchronous and parallel impleme...</td>\n",
       "      <td>[Machine Learning (cs.LG)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 10]</td>\n",
       "      <td>[/scholar?cites=17816434878891582694&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639d9f0e6d67832e75895c49</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[The Advantage Regret-Matching Actor-Critic]</td>\n",
       "      <td>[Audrūnas Gruslys, Marc Lanctot, Rémi Munos, F...</td>\n",
       "      <td>[\\n      ,   Regret minimization has played a ...</td>\n",
       "      <td>[Artificial Intelligence (cs.AI)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 14]</td>\n",
       "      <td>[/scholar?cites=13622395690856603026&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>639d9f0e6d67832e75895c4a</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Actor-Critic Sequence Training for Image Capt...</td>\n",
       "      <td>[Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Sh...</td>\n",
       "      <td>[\\n      ,   Generating natural language descr...</td>\n",
       "      <td>[Computer Vision and Pattern Recognition (cs.CV)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 111]</td>\n",
       "      <td>[/scholar?cites=16998376404483706528&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>639d9f0f6d67832e75895c4b</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Sample Efficient Actor-Critic with Experience...</td>\n",
       "      <td>[Ziyu Wang, Victor Bapst, Nicolas Heess, Volod...</td>\n",
       "      <td>[\\n      ,   This paper presents an actor-crit...</td>\n",
       "      <td>[Machine Learning (cs.LG)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 786]</td>\n",
       "      <td>[/scholar?cites=8369222693188103740&amp;as_sdt=200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>639d9f0f6d67832e75895c4c</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Off-policy Maximum Entropy Reinforcement Lear...</td>\n",
       "      <td>[Zhimin Hou, Kuangen Zhang, Yi Wan, Dongyu Li,...</td>\n",
       "      <td>[\\n      ,   The optimal policy of a reinforce...</td>\n",
       "      <td>[Machine Learning (cs.LG)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 10]</td>\n",
       "      <td>[/scholar?cites=9691472150620828666&amp;as_sdt=200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>639d9f0f6d67832e75895c4d</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Asynchronous Advantage Actor-Critic Agent for...</td>\n",
       "      <td>[Basel Alghanem, Keerthana P G]</td>\n",
       "      <td>[\\n      ,   Deep reinforcement learning, and ...</td>\n",
       "      <td>[Artificial Intelligence (cs.AI)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 4]</td>\n",
       "      <td>[/scholar?cites=11859479095225713207&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>639d9f0f6d67832e75895c4e</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[An advantage actor-critic algorithm for robot...</td>\n",
       "      <td>[Chengmin Zhou, Bingding Huang, Pasi Fränti]</td>\n",
       "      <td>[\\n      ,   Intelligent robots provide a new ...</td>\n",
       "      <td>[Robotics (cs.RO)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 2]</td>\n",
       "      <td>[/scholar?cites=16945176442469715803&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>639d9f106d67832e75895c4f</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>[Hybrid Actor-Critic Reinforcement Learning in...</td>\n",
       "      <td>[Zhou Fan, Rui Su, Weinan Zhang, Yong Yu]</td>\n",
       "      <td>[\\n      ,   In this paper we propose a hybrid...</td>\n",
       "      <td>[Machine Learning (cs.LG)]</td>\n",
       "      <td>https://scholar.google.com/scholar?q=advantage...</td>\n",
       "      <td>[Citado por 47]</td>\n",
       "      <td>[/scholar?cites=18098031806703639264&amp;as_sdt=20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id fuente  \\\n",
       "0  639d9f0d6d67832e75895c46  arxiv   \n",
       "1  639d9f0d6d67832e75895c47  arxiv   \n",
       "2  639d9f0e6d67832e75895c48  arxiv   \n",
       "3  639d9f0e6d67832e75895c49  arxiv   \n",
       "4  639d9f0e6d67832e75895c4a  arxiv   \n",
       "5  639d9f0f6d67832e75895c4b  arxiv   \n",
       "6  639d9f0f6d67832e75895c4c  arxiv   \n",
       "7  639d9f0f6d67832e75895c4d  arxiv   \n",
       "8  639d9f0f6d67832e75895c4e  arxiv   \n",
       "9  639d9f106d67832e75895c4f  arxiv   \n",
       "\n",
       "                                              titulo  \\\n",
       "0  [Reinforcement Learning through Asynchronous A...   \n",
       "1  [Altruistic Maneuver Planning for Cooperative ...   \n",
       "2  [Towards Understanding Asynchronous Advantage ...   \n",
       "3       [The Advantage Regret-Matching Actor-Critic]   \n",
       "4  [Actor-Critic Sequence Training for Image Capt...   \n",
       "5  [Sample Efficient Actor-Critic with Experience...   \n",
       "6  [Off-policy Maximum Entropy Reinforcement Lear...   \n",
       "7  [Asynchronous Advantage Actor-Critic Agent for...   \n",
       "8  [An advantage actor-critic algorithm for robot...   \n",
       "9  [Hybrid Actor-Critic Reinforcement Learning in...   \n",
       "\n",
       "                                             autores  \\\n",
       "0  [Mohammad Babaeizadeh, Iuri Frosio, Stephen Ty...   \n",
       "1  [Behrad Toghi, Rodolfo Valiente, Dorsa Sadigh,...   \n",
       "2  [Han Shen, Kaiqing Zhang, Mingyi Hong, Tianyi ...   \n",
       "3  [Audrūnas Gruslys, Marc Lanctot, Rémi Munos, F...   \n",
       "4  [Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Sh...   \n",
       "5  [Ziyu Wang, Victor Bapst, Nicolas Heess, Volod...   \n",
       "6  [Zhimin Hou, Kuangen Zhang, Yi Wan, Dongyu Li,...   \n",
       "7                    [Basel Alghanem, Keerthana P G]   \n",
       "8       [Chengmin Zhou, Bingding Huang, Pasi Fränti]   \n",
       "9          [Zhou Fan, Rui Su, Weinan Zhang, Yong Yu]   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  [\\n      ,   We introduce a hybrid CPU/GPU ver...   \n",
       "1  [\\n      ,   With the adoption of autonomous v...   \n",
       "2  [\\n      ,   Asynchronous and parallel impleme...   \n",
       "3  [\\n      ,   Regret minimization has played a ...   \n",
       "4  [\\n      ,   Generating natural language descr...   \n",
       "5  [\\n      ,   This paper presents an actor-crit...   \n",
       "6  [\\n      ,   The optimal policy of a reinforce...   \n",
       "7  [\\n      ,   Deep reinforcement learning, and ...   \n",
       "8  [\\n      ,   Intelligent robots provide a new ...   \n",
       "9  [\\n      ,   In this paper we propose a hybrid...   \n",
       "\n",
       "                                      clase_primaria  \\\n",
       "0                         [Machine Learning (cs.LG)]   \n",
       "1                                 [Robotics (cs.RO)]   \n",
       "2                         [Machine Learning (cs.LG)]   \n",
       "3                  [Artificial Intelligence (cs.AI)]   \n",
       "4  [Computer Vision and Pattern Recognition (cs.CV)]   \n",
       "5                         [Machine Learning (cs.LG)]   \n",
       "6                         [Machine Learning (cs.LG)]   \n",
       "7                  [Artificial Intelligence (cs.AI)]   \n",
       "8                                 [Robotics (cs.RO)]   \n",
       "9                         [Machine Learning (cs.LG)]   \n",
       "\n",
       "                                           pag_espec      n_citaciones  \\\n",
       "0  https://scholar.google.com/scholar?q=advantage...  [Citado por 222]   \n",
       "1  https://scholar.google.com/scholar?q=advantage...   [Citado por 11]   \n",
       "2  https://scholar.google.com/scholar?q=advantage...   [Citado por 10]   \n",
       "3  https://scholar.google.com/scholar?q=advantage...   [Citado por 14]   \n",
       "4  https://scholar.google.com/scholar?q=advantage...  [Citado por 111]   \n",
       "5  https://scholar.google.com/scholar?q=advantage...  [Citado por 786]   \n",
       "6  https://scholar.google.com/scholar?q=advantage...   [Citado por 10]   \n",
       "7  https://scholar.google.com/scholar?q=advantage...    [Citado por 4]   \n",
       "8  https://scholar.google.com/scholar?q=advantage...    [Citado por 2]   \n",
       "9  https://scholar.google.com/scholar?q=advantage...   [Citado por 47]   \n",
       "\n",
       "                                              citado  \n",
       "0  [/scholar?cites=8757115672331028243&as_sdt=200...  \n",
       "1  [/scholar?cites=1543039800876013962&as_sdt=200...  \n",
       "2  [/scholar?cites=17816434878891582694&as_sdt=20...  \n",
       "3  [/scholar?cites=13622395690856603026&as_sdt=20...  \n",
       "4  [/scholar?cites=16998376404483706528&as_sdt=20...  \n",
       "5  [/scholar?cites=8369222693188103740&as_sdt=200...  \n",
       "6  [/scholar?cites=9691472150620828666&as_sdt=200...  \n",
       "7  [/scholar?cites=11859479095225713207&as_sdt=20...  \n",
       "8  [/scholar?cites=16945176442469715803&as_sdt=20...  \n",
       "9  [/scholar?cites=18098031806703639264&as_sdt=20...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db3 = acceder_una_coleccion(\"papers\")\n",
    "df_s =  pd.DataFrame(list(db3.find(limit= 10)))\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590f061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c3d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ddbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombres colecciones: papers   final_distances  tockenizador  refinded_paper  final_paper   papers_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f78a5c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x21fffe85d40>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2 = acceder_una_coleccion(\"papers_2\")\n",
    "\n",
    "db2.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce881ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creación correcta\n"
     ]
    }
   ],
   "source": [
    "db = acceder_una_coleccion(\"papers\")\n",
    "db2 = acceder_una_coleccion(\"papers_2\")\n",
    "save_mongo(list(db.find()),db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5d041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
