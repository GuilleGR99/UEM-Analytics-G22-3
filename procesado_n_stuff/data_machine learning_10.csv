titulo,abstract,clase_pri,clase_otr
Thumbs up? Sentiment Classification using Machine Learning Techniques,"  We consider the problem of classifying documents not by topic, but by overall
sentiment, e.g., determining whether a review is positive or negative. Using
movie reviews as data, we find that standard machine learning techniques
definitively outperform human-produced baselines. However, the three machine
learning methods we employed (Naive Bayes, maximum entropy classification, and
support vector machines) do not perform as well on sentiment classification as
on traditional topic-based categorization. We conclude by examining factors
that make the sentiment classification problem more challenging.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG)
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"  MXNet is a multi-language machine learning (ML) library to ease the
development of ML algorithms, especially for deep neural networks. Embedded in
the host language, it blends declarative symbolic expression with imperative
tensor computation. It offers auto differentiation to derive gradients. MXNet
is computation and memory efficient and runs on various heterogeneous systems,
ranging from mobile devices to distributed GPU clusters.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG); Mathematical Software (cs.MS); Neural and Evolutionary Computing (cs.NE)
API design for machine learning software: experiences from the scikit-learn project,"  Scikit-learn is an increasingly popular machine learning li- brary. Written
in Python, it is designed to be simple and efficient, accessible to
non-experts, and reusable in various contexts. In this paper, we present and
discuss our design choices for the application programming interface (API) of
the project. In particular, we describe the simple and elegant interface shared
by all learning and processing units in the library and then discuss its
advantages in terms of composition and reusability. The paper also comments on
implementation details specific to the Python ecosystem and analyzes obstacles
faced by users and developers of the library.

    ",Machine Learning (cs.LG),; Mathematical Software (cs.MS)
Foolbox: A Python toolbox to benchmark the robustness of machine learning models,"  Even todays most advanced machine learning models are easily fooled by almost
imperceptible perturbations of their inputs. Foolbox is a new Python package to
generate such adversarial perturbations and to quantify and compare the
robustness of machine learning models. It is build around the idea that the
most comparable robustness measure is the minimum perturbation needed to craft
an adversarial example. To this end, Foolbox provides reference implementations
of most published adversarial attack methods alongside some new ones, all of
which perform internal hyperparameter tuning to find the minimum adversarial
perturbation. Additionally, Foolbox interfaces with most popular deep learning
frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows
different adversarial criteria such as targeted misclassification and top-k
misclassification as well as different distance measures. The code is licensed
under the MIT license and is openly available at
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"  TensorFlow is an interface for expressing machine learning algorithms, and an
implementation for executing such algorithms. A computation expressed using
TensorFlow can be executed with little or no change on a wide variety of
heterogeneous systems, ranging from mobile devices such as phones and tablets
up to large-scale distributed systems of hundreds of machines and thousands of
computational devices such as GPU cards. The system is flexible and can be used
to express a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been used for conducting
research and for deploying machine learning systems into production across more
than a dozen areas of computer science and other fields, including speech
recognition, computer vision, robotics, information retrieval, natural language
processing, geographic information extraction, and computational drug
discovery. This paper describes the TensorFlow interface and an implementation
of that interface that we have built at Google. The TensorFlow API and a
reference implementation were released as an open-source package under the
Apache 2.0 license in November, 2015 and are available at ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG)
Taking Human out of Learning Applications: A Survey on Automated Machine Learning,"  Machine learning techniques have deeply rooted in our everyday life. However,
since it is knowledge- and labor-intensive to pursue good learning performance,
human experts are heavily involved in every aspect of machine learning. In
order to make machine learning techniques easier to apply and reduce the demand
for experienced human experts, automated machine learning (AutoML) has emerged
as a hot topic with both industrial and academic interest. In this paper, we
provide an up to date survey on AutoML. First, we introduce and define the
AutoML problem, with inspiration from both realms of automation and machine
learning. Then, we propose a general AutoML framework that not only covers most
existing approaches to date but also can guide the design for new methods.
Subsequently, we categorize and review the existing works from two aspects,
i.e., the problem setup and the employed techniques. Finally, we provide a
detailed analysis of AutoML approaches and explain the reasons underneath their
successful applications. We hope this survey can serve as not only an
insightful guideline for AutoML beginners but also an inspiration for future
research.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Universal Differential Equations for Scientific Machine Learning,"  In the context of science, the well-known adage ""a picture is worth a
thousand words"" might well be ""a model is worth a thousand datasets."" In this
manuscript we introduce the SciML software ecosystem as a tool for mixing the
information of physical laws and scientific models with data-driven machine
learning approaches. We describe a mathematical object, which we denote
universal differential equations (UDEs), as the unifying framework connecting
the ecosystem. We show how a wide variety of applications, from automatically
discovering biological mechanisms to solving high-dimensional
Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled
through the UDE formalism and its tooling. We demonstrate the generality of the
software tooling to handle stochasticity, delays, and implicit constraints.
This funnels the wide variety of SciML applications into a core set of training
mechanisms which are highly optimized, stabilized for stiff equations, and
compatible with distributed parallelism and GPU accelerators.

    ",Machine Learning (cs.LG),; Dynamical Systems (math.DS); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)
Distributed GraphLab: A Framework for Machine Learning in the Cloud,"  While high-level data parallel frameworks, like MapReduce, simplify the
design and implementation of large-scale data processing systems, they do not
naturally or efficiently support many important data mining and machine
learning algorithms and can lead to inefficient learning systems. To help fill
this critical void, we introduced the GraphLab abstraction which naturally
expresses asynchronous, dynamic, graph-parallel computation while ensuring data
consistency and achieving a high degree of parallel performance in the
shared-memory setting. In this paper, we extend the GraphLab framework to the
substantially more challenging distributed setting while preserving strong data
consistency guarantees. We develop graph based extensions to pipelined locking
and data versioning to reduce network congestion and mitigate the effect of
network latency. We also introduce fault tolerance to the GraphLab abstraction
using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can
be easily implemented by exploiting the GraphLab abstraction itself. Finally,
we evaluate our distributed implementation of the GraphLab abstraction on a
large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains
over Hadoop-based implementations.

    ",Databases (cs.DB),; Machine Learning (cs.LG)
The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning,"  The nascent field of fair machine learning aims to ensure that decisions
guided by algorithms are equitable. Over the last several years, three formal
definitions of fairness have gained prominence: (1) anti-classification,
meaning that protected attributes---like race, gender, and their proxies---are
not explicitly used to make decisions; (2) classification parity, meaning that
common measures of predictive performance (e.g., false positive and false
negative rates) are equal across groups defined by the protected attributes;
and (3) calibration, meaning that conditional on risk estimates, outcomes are
independent of protected attributes. Here we show that all three of these
fairness definitions suffer from significant statistical limitations. Requiring
anti-classification or classification parity can, perversely, harm the very
groups they were designed to protect; and calibration, though generally
desirable, provides little guarantee that decisions are equitable. In contrast
to these formal fairness criteria, we argue that it is often preferable to
treat similarly risky people similarly, based on the most statistically
accurate estimates of risk that one can produce. Such a strategy, while not
universally applicable, often aligns well with policy objectives; notably, this
strategy will typically violate both anti-classification and classification
parity. In practice, it requires significant effort to construct suitable risk
estimates. One must carefully define and measure the targets of prediction to
avoid retrenching biases in the data. But, importantly, one cannot generally
address these difficulties by requiring that algorithms satisfy popular
mathematical formalizations of fairness. By highlighting these challenges in
the foundation of fair machine learning, we hope to help researchers and
practitioners productively advance the area.

    ",Computers and Society (cs.CY),
"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning","  The correct use of model evaluation, model selection, and algorithm selection
techniques is vital in academic machine learning research as well as in many
industrial settings. This article reviews different techniques that can be used
for each of these three subtasks and discusses the main advantages and
disadvantages of each technique with references to theoretical and empirical
studies. Further, recommendations are given to encourage best yet feasible
practices in research and applications of machine learning. Common methods such
as the holdout method for model evaluation and selection are covered, which are
not recommended when working with small datasets. Different flavors of the
bootstrap technique are introduced for estimating the uncertainty of
performance estimates, as an alternative to confidence intervals via normal
approximation if bootstrapping is computationally feasible. Common
cross-validation techniques such as leave-one-out cross-validation and k-fold
cross-validation are reviewed, the bias-variance trade-off for choosing k is
discussed, and practical tips for the optimal choice of k are given based on
empirical evidence. Different statistical tests for algorithm comparisons are
presented, and strategies for dealing with multiple comparisons such as omnibus
tests and multiple-comparison corrections are discussed. Finally, alternative
methods for algorithm selection, such as the combined F-test 5x2
cross-validation and nested cross-validation, are recommended for comparing
machine learning algorithms when datasets are small.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Coronavirus (COVID-19) Classification using CT Images by Machine Learning Methods,"  This study presents early phase detection of Coronavirus (COVID-19), which is
named by World Health Organization (WHO), by machine learning methods. The
detection process was implemented on abdominal Computed Tomography (CT) images.
The expert radiologists detected from CT images that COVID-19 shows different
behaviours from other viral pneumonia. Therefore, the clinical experts specify
that COVÄ°D-19 virus needs to be diagnosed in early phase. For detection of
the COVID-19, four different datasets were formed by taking patches sized as
16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process
was applied to patches to increase the classification performance. Grey Level
Co-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run
Length Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete
Wavelet Transform (DWT) algorithms were used as feature extraction methods.
Support Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold
and 10-fold cross-validations were implemented during the classification
process. Sensitivity, specificity, accuracy, precision, and F-score metrics
were used to evaluate the classification performance. The best classification
accuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature
extraction method.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Machine Learning (stat.ML)
InterpretML: A Unified Framework for Machine Learning Interpretability,"  InterpretML is an open-source Python package which exposes machine learning
interpretability algorithms to practitioners and researchers. InterpretML
exposes two types of interpretability - glassbox models, which are machine
learning models designed for interpretability (ex: linear models, rule lists,
generalized additive models), and blackbox explainability techniques for
explaining existing systems (ex: Partial Dependence, LIME). The package enables
practitioners to easily compare interpretability algorithms by exposing
multiple methods under a unified API, and by having a built-in, extensible
visualization platform. InterpretML also includes the first implementation of
the Explainable Boosting Machine, a powerful, interpretable, glassbox model
that can be as accurate as many blackbox models. The MIT licensed source code
can be downloaded from ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
GraphLab: A New Framework For Parallel Machine Learning,"  Designing and implementing efficient, provably correct parallel machine
learning (ML) algorithms is challenging. Existing high-level parallel
abstractions like MapReduce are insufficiently expressive while low-level tools
like MPI and Pthreads leave ML experts repeatedly solving the same design
challenges. By targeting common patterns in ML, we developed GraphLab, which
improves upon abstractions like MapReduce by compactly expressing asynchronous
iterative algorithms with sparse computational dependencies while ensuring data
consistency and achieving a high degree of parallel performance. We demonstrate
the expressiveness of the GraphLab framework by designing and implementing
parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and
Compressed Sensing. We show that using GraphLab we can achieve excellent
parallel performance on large scale real-world problems.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)"
Adversarial Machine Learning at Scale,"  Adversarial examples are malicious inputs designed to fool machine learning
models. They often transfer from one model to another, allowing attackers to
mount black box attacks without knowledge of the target model's parameters.
Adversarial training is the process of explicitly training a model on
adversarial examples, in order to make it more robust to attack or to reduce
its test error on clean inputs. So far, adversarial training has primarily been
applied to small problems. In this research, we apply adversarial training to
ImageNet. Our contributions include: (1) recommendations for how to succesfully
scale adversarial training to large models and datasets, (2) the observation
that adversarial training confers robustness to single-step attack methods, (3)
the finding that multi-step attack methods are somewhat less transferable than
single-step attack methods, so single-step attacks are the best for mounting
black-box attacks, and (4) resolution of a ""label leaking"" effect that causes
adversarially trained models to perform better on adversarial examples than on
clean examples, because the adversarial example construction process uses the
true label and the model can learn to exploit regularities in the construction
process.

    ",Computer Vision and Pattern Recognition (cs.CV),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)
"Interpretable machine learning: definitions, methods, and applications","  Machine-learning models have demonstrated great success in learning complex
patterns that enable them to make predictions about unobserved data. In
addition to using models for prediction, the ability to interpret what a model
has learned is receiving an increasing amount of attention. However, this
increased focus has led to considerable confusion about the notion of
interpretability. In particular, it is unclear how the wide array of proposed
interpretation methods are related, and what common concepts can be used to
evaluate them.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Applications (stat.AP)
Pylearn2: a machine learning research library,"  Pylearn2 is a machine learning research library. This does not just mean that
it is a collection of machine learning algorithms that share a common API; it
means that it has been designed for flexibility and extensibility in order to
facilitate research projects that involve new or unusual use cases. In this
paper we give a brief history of the library, an overview of its basic
philosophy, a summary of the library's architecture, and a description of how
the Pylearn2 community functions socially.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Mathematical Software (cs.MS)
The Frontiers of Fairness in Machine Learning,"  The last few years have seen an explosion of academic and popular interest in
algorithmic fairness. Despite this interest and the volume and velocity of work
that has been produced recently, the fundamental science of fairness in machine
learning is still in a nascent state. In March 2018, we convened a group of
experts as part of a CCC visioning workshop to assess the state of the field,
and distill the most promising research directions going forward. This report
summarizes the findings of that workshop. Along the way, it surveys recent
theoretical work in the field and points towards promising directions for
research.

    ",Machine Learning (cs.LG),; Data Structures and Algorithms (cs.DS); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)
Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data,"  On-device machine learning (ML) enables the training process to exploit a
massive amount of user-generated private data samples. To enjoy this benefit,
inter-device communication overhead should be minimized. With this end, we
propose federated distillation (FD), a distributed model training algorithm
whose communication payload size is much smaller than a benchmark scheme,
federated learning (FL), particularly when the model size is large. Moreover,
user-generated data samples are likely to become non-IID across devices, which
commonly degrades the performance compared to the case with an IID dataset. To
cope with this, we propose federated augmentation (FAug), where each device
collectively trains a generative model, and thereby augments its local data
towards yielding an IID dataset. Empirical studies demonstrate that FD with
FAug yields around 26x less communication overhead while achieving 95-98% test
accuracy compared to FL.

    ",Machine Learning (cs.LG),; Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML)
ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models,"  Machine learning (ML) has become a core component of many real-world
applications and training data is a key factor that drives current progress.
This huge success has led Internet companies to deploy machine learning as a
service (MLaaS). Recently, the first membership inference attack has shown that
extraction of information on the training set is possible in such MLaaS
settings, which has severe security and privacy implications.
",Cryptography and Security (cs.CR),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain,"  Deep learning-based techniques have achieved state-of-the-art performance on
a wide variety of recognition and classification tasks. However, these networks
are typically computationally expensive to train, requiring weeks of
computation on many GPUs; as a result, many users outsource the training
procedure to the cloud or rely on pre-trained models that are then fine-tuned
for a specific task. In this paper we show that outsourced training introduces
new security risks: an adversary can create a maliciously trained network (a
backdoored neural network, or a \emph{BadNet}) that has state-of-the-art
performance on the user's training and validation samples, but behaves badly on
specific attacker-chosen inputs. We first explore the properties of BadNets in
a toy example, by creating a backdoored handwritten digit classifier. Next, we
demonstrate backdoors in a more realistic scenario by creating a U.S. street
sign classifier that identifies stop signs as speed limits when a special
sticker is added to the stop sign; we then show in addition that the backdoor
in our US street sign detector can persist even if the network is later
retrained for another task and cause a drop in accuracy of {25}\% on average
when the backdoor trigger is present. These results demonstrate that backdoors
in neural networks are both powerful and---because the behavior of neural
networks is difficult to explicate---stealthy. This work provides motivation
for further research into techniques for verifying and inspecting neural
networks, just as we have developed tools for verifying and debugging software.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"  We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Federated Optimization: Distributed Machine Learning for On-Device Intelligence,"  We introduce a new and increasingly relevant setting for distributed
optimization in machine learning, where the data defining the optimization are
unevenly distributed over an extremely large number of nodes. The goal is to
train a high-quality centralized model. We refer to this setting as Federated
Optimization. In this setting, communication efficiency is of the utmost
importance and minimizing the number of rounds of communication is the
principal goal.
",Machine Learning (cs.LG),
Machine Learning that Matters,"  Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution,"  Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
"Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge","  Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic
core, active and non-enhancing core. This intrinsic heterogeneity is also
portrayed in their radio-phenotype, as their sub-regions are depicted by
varying intensity profiles disseminated across multi-parametric magnetic
resonance imaging (mpMRI) scans, reflecting varying biological properties.
Their heterogeneous shape, extent, and location are some of the factors that
make these tumors difficult to resect, and in some cases inoperable. The amount
of resected tumor is a factor also considered in longitudinal scans, when
evaluating the apparent tumor for potential diagnosis of progression.
Furthermore, there is mounting evidence that accurate segmentation of the
various tumor sub-regions can offer the basis for quantitative image analysis
towards prediction of patient overall survival. This study assesses the
state-of-the-art machine learning (ML) methods used for brain tumor image
analysis in mpMRI scans, during the last seven instances of the International
Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we
focus on i) evaluating segmentations of the various glioma sub-regions in
pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue
of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO
criteria, and iii) predicting the overall survival from pre-operative mpMRI
scans of patients that underwent gross total resection. Finally, we investigate
the challenge of identifying the best ML algorithms for each of these tasks,
considering that apart from being diverse on each instance of the challenge,
the multi-institutional mpMRI BraTS dataset has also been a continuously
evolving/growing dataset.

    ",Computer Vision and Pattern Recognition (cs.CV),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)
Quantifying the Carbon Emissions of Machine Learning,"  From an environmental standpoint, there are a few crucial aspects of training
a neural network that have a major impact on the quantity of carbon that it
emits. These factors include: the location of the server used for training and
the energy grid that it uses, the length of the training procedure, and even
the make and model of hardware on which the training takes place. In order to
approximate these emissions, we present our Machine Learning Emissions
Calculator, a tool for our community to better understand the environmental
impact of training ML models. We accompany this tool with an explanation of the
factors cited above, as well as concrete actions that individual practitioners
and organizations can take to mitigate their carbon emissions.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG)
Fairness in Machine Learning: A Survey,"  As Machine Learning technologies become increasingly used in contexts that
affect citizens, companies as well as researchers need to be confident that
their application of these methods will not have unexpected social
implications, such as bias towards gender, ethnicity, and/or people with
disabilities. There is significant literature on approaches to mitigate bias
and promote fairness, yet the area is complex and hard to penetrate for
newcomers to the domain. This article seeks to provide an overview of the
different schools of thought and approaches to mitigating (social) biases and
increase fairness in the Machine Learning literature. It organises approaches
into the widely accepted framework of pre-processing, in-processing, and
post-processing methods, subcategorizing into a further 11 method areas.
Although much of the literature emphasizes binary classification, a discussion
of fairness in regression, recommender systems, unsupervised learning, and
natural language processing is also provided along with a selection of
currently available open source libraries. The article concludes by summarising
open challenges articulated as four dilemmas for fairness research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning,"  Current advances in Artificial Intelligence and machine learning in general,
and deep learning in particular have reached unprecedented impact not only
across research communities, but also over popular media channels. However,
concerns about interpretability and accountability of AI have been raised by
influential thinkers. In spite of the recent impact of AI, several works have
identified the need for principled knowledge representation and reasoning
mechanisms integrated with deep learning-based systems to provide sound and
explainable models for such systems. Neural-symbolic computing aims at
integrating, as foreseen by Valiant, two most fundamental cognitive abilities:
the ability to learn from the environment, and the ability to reason from what
has been learned. Neural-symbolic computing has been an active topic of
research for many years, reconciling the advantages of robust learning in
neural networks and reasoning and interpretability of symbolic representation.
In this paper, we survey recent accomplishments of neural-symbolic computing as
a principled methodology for integrated machine learning and reasoning. We
illustrate the effectiveness of the approach by outlining the main
characteristics of the methodology: principled integration of neural learning
with symbolic knowledge representation and reasoning allowing for the
construction of explainable AI systems. The insights provided by
neural-symbolic computing shed new light on the increasingly prominent need for
interpretable and accountable AI systems.

    ",Artificial Intelligence (cs.AI),
Towards the Science of Security and Privacy in Machine Learning,"  Advances in machine learning (ML) in recent years have enabled a dizzying
array of applications such as data analytics, autonomous systems, and security
diagnostics. ML is now pervasive---new systems and models are being deployed in
every domain imaginable, leading to rapid and widespread deployment of software
based inference and decision making. There is growing recognition that ML
exposes new vulnerabilities in software systems, yet the technical community's
understanding of the nature and extent of these vulnerabilities remains
limited. We systematize recent findings on ML security and privacy, focusing on
attacks identified on these systems and defenses crafted to date. We articulate
a comprehensive threat model for ML, and categorize attacks and defenses within
an adversarial framework. Key insights resulting from works both in the ML and
security communities are identified and the effectiveness of approaches are
related to structural elements of ML algorithms and the data used to train
them. We conclude by formally exploring the opposing relationship between model
accuracy and resilience to adversarial manipulation. Through these
explorations, we show that there are (possibly unavoidable) tensions between
model complexity, accuracy, and resilience that must be calibrated for the
environments in which they will be used.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
"Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology","  Performance metrics (error measures) are vital components of the evaluation
frameworks in various fields. The intention of this study was to overview of a
variety of performance metrics and approaches to their classification. The main
goal of the study was to develop a typology that will help to improve our
knowledge and understanding of metrics and facilitate their selection in
machine learning regression, forecasting and prognostics. Based on the analysis
of the structure of numerous performance metrics, we propose a framework of
metrics which includes four (4) categories: primary metrics, extended metrics,
composite metrics, and hybrid sets of metrics. The paper identified three (3)
key components (dimensions) that determine the structure and properties of
primary metrics: method of determining point distance, method of normalization,
method of aggregation of point distances over a data set.

    ",Methodology (stat.ME),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Opportunities and Challenges for Machine Learning in Materials Science,"  Advances in machine learning have impacted myriad areas of materials science,
ranging from the discovery of novel materials to the improvement of molecular
simulations, with likely many more important developments to come. Given the
rapid changes in this field, it is challenging to understand both the breadth
of opportunities as well as best practices for their use. In this review, we
address aspects of both problems by providing an overview of the areas where
machine learning has recently had significant impact in materials science, and
then provide a more detailed discussion on determining the accuracy and domain
of applicability of some common types of machine learning models. Finally, we
discuss some opportunities and challenges for the materials community to fully
utilize the capabilities of machine learning.

    ",Materials Science (cond-mat.mtrl-sci),; Computational Physics (physics.comp-ph)
TensorFlow Quantum: A Software Framework for Quantum Machine Learning,"  We introduce TensorFlow Quantum (TFQ), an open source library for the rapid
prototyping of hybrid quantum-classical models for classical or quantum data.
This framework offers high-level abstractions for the design and training of
both discriminative and generative quantum models under TensorFlow and supports
high-performance quantum circuit simulators. We provide an overview of the
software architecture and building blocks through several examples and review
the theory of hybrid quantum-classical neural networks. We illustrate TFQ
functionalities via several basic applications including supervised learning
for quantum classification, quantum control, simulating noisy quantum circuits,
and quantum approximate optimization. Moreover, we demonstrate how one can
apply TFQ to tackle advanced quantum learning tasks including meta-learning,
layerwise learning, Hamiltonian learning, sampling thermal states, variational
quantum eigensolvers, classification of quantum phase transitions, generative
adversarial networks, and reinforcement learning. We hope this framework
provides the necessary tools for the quantum computing and machine learning
research communities to explore models of both natural and artificial quantum
systems, and ultimately discover new quantum algorithms which could potentially
yield a quantum advantage.

    ",Quantum Physics (quant-ph),; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Programming Languages (cs.PL)
Augmentor: An Image Augmentation Library for Machine Learning,"  The generation of artificial data based on existing observations, known as
data augmentation, is a technique used in machine learning to improve model
accuracy, generalisation, and to control overfitting. Augmentor is a software
package, available in both Python and Julia versions, that provides a high
level API for the expansion of image data using a stochastic, pipeline-based
approach which effectively allows for images to be sampled from a distribution
of augmented images at runtime. Augmentor provides methods for most standard
augmentation practices as well as several advanced features such as
label-preserving, randomised elastic distortions, and provides many helper
functions for typical augmentation tasks used in machine learning.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML)
FedML: A Research Library and Benchmark for Federated Machine Learning,"  Federated learning (FL) is a rapidly growing research field in machine
learning. However, existing FL libraries cannot adequately support diverse
algorithmic development; inconsistent dataset and model usage make fair
algorithm comparison challenging. In this work, we introduce FedML, an open
research library and benchmark to facilitate FL algorithm development and fair
performance comparison. FedML supports three computing paradigms: on-device
training for edge devices, distributed computing, and single-machine
simulation. FedML also promotes diverse algorithmic research with flexible and
generic API design and comprehensive reference baseline implementations
(optimizer, models, and datasets). We hope FedML could provide an efficient and
reproducible means for developing and evaluating FL algorithms that would
benefit the FL research community. We maintain the source code, documents, and
user community at ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models,"  This paper describes EMBER: a labeled benchmark dataset for training machine
learning models to statically detect malicious Windows portable executable
files. The dataset includes features extracted from 1.1M binary files: 900K
training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test
samples (100K malicious, 100K benign). To accompany the dataset, we also
release open source code for extracting features from additional binaries so
that additional sample features can be appended to the dataset. This dataset
fills a void in the information security machine learning community: a
benign/malicious dataset that is large, open and general enough to cover
several interesting use cases. We enumerate several use cases that we
considered when structuring the dataset. Additionally, we demonstrate one use
case wherein we compare a baseline gradient boosted decision tree model trained
using LightGBM with default settings to MalConv, a recently published
end-to-end (featureless) deep learning model for malware detection. Results
show that even without hyper-parameter optimization, the baseline EMBER model
outperforms MalConv. The authors hope that the dataset, code and baseline model
provided by EMBER will help invigorate machine learning research for malware
detection, in much the same way that benchmark datasets have advanced computer
vision research.

    ",Cryptography and Security (cs.CR),
Differential Privacy and Machine Learning: a Survey and Review,"  The objective of machine learning is to extract useful information from data,
while privacy is preserved by concealing information. Thus it seems hard to
reconcile these competing interests. However, they frequently must be balanced
when mining sensitive data. For example, medical research represents an
important application where it is necessary both to extract useful information
and protect patient privacy. One way to resolve the conflict is to extract
general characteristics of whole populations without disclosing the private
information of individuals.
",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Databases (cs.DB)
Unsupervised Machine Learning on a Hybrid Quantum Computer,"  Machine learning techniques have led to broad adoption of a statistical model
of computing. The statistical distributions natively available on quantum
processors are a superset of those available classically. Harnessing this
attribute has the potential to accelerate or otherwise improve machine learning
relative to purely classical performance. A key challenge toward that goal is
learning to hybridize classical computing resources and traditional learning
techniques with the emerging capabilities of general purpose quantum
processors. Here, we demonstrate such hybridization by training a 19-qubit gate
model processor to solve a clustering problem, a foundational challenge in
unsupervised learning. We use the quantum approximate optimization algorithm in
conjunction with a gradient-free Bayesian optimization to train the quantum
machine. This quantum/classical hybrid algorithm shows robustness to realistic
noise, and we find evidence that classical optimization can be used to train
around both coherent and incoherent imperfections.

    ",Quantum Physics (quant-ph),
Malicious URL Detection using Machine Learning: A Survey,"  Malicious URL, a.k.a. malicious website, is a common and serious threat to
cybersecurity. Malicious URLs host unsolicited content (spam, phishing,
drive-by exploits, etc.) and lure unsuspecting users to become victims of scams
(monetary loss, theft of private information, and malware installation), and
cause losses of billions of dollars every year. It is imperative to detect and
act on such threats in a timely manner. Traditionally, this detection is done
mostly through the usage of blacklists. However, blacklists cannot be
exhaustive, and lack the ability to detect newly generated malicious URLs. To
improve the generality of malicious URL detectors, machine learning techniques
have been explored with increasing attention in recent years. This article aims
to provide a comprehensive survey and a structural understanding of Malicious
URL Detection techniques using machine learning. We present the formal
formulation of Malicious URL Detection as a machine learning task, and
categorize and review the contributions of literature studies that addresses
different dimensions of this problem (feature representation, algorithm design,
etc.). Further, this article provides a timely and comprehensive survey for a
range of different audiences, not only for machine learning researchers and
engineers in academia, but also for professionals and practitioners in
cybersecurity industry, to help them understand the state of the art and
facilitate their own research and practical applications. We also discuss
practical issues in system design, open research challenges, and point out some
important directions for future research.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR)
Quantum embeddings for machine learning,"  Quantum classifiers are trainable quantum circuits used as machine learning
models. The first part of the circuit implements a quantum feature map that
encodes classical inputs into quantum states, embedding the data in a
high-dimensional Hilbert space; the second part of the circuit executes a
quantum measurement interpreted as the output of the model. Usually, the
measurement is trained to distinguish quantum-embedded data. We propose to
instead train the first part of the circuit -- the embedding -- with the
objective of maximally separating data classes in Hilbert space, a strategy we
call quantum metric learning. As a result, the measurement minimizing a linear
classification loss is already known and depends on the metric used: for
embeddings separating data using the l1 or trace distance, this is the Helstrom
measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple
overlap measurement. This approach provides a powerful analytic framework for
quantum machine learning and eliminates a major component in current models,
freeing up more precious resources to best leverage the capabilities of
near-term quantum information processors.

    ",Quantum Physics (quant-ph),
"A machine learning methodology for real-time forecasting of the 2019-2020 COVID-19 outbreak using Internet searches, news alerts, and estimates from mechanistic models","  We present a timely and novel methodology that combines disease estimates
from mechanistic models with digital traces, via interpretable machine-learning
methodologies, to reliably forecast COVID-19 activity in Chinese provinces in
real-time. Specifically, our method is able to produce stable and accurate
forecasts 2 days ahead of current time, and uses as inputs (a) official health
reports from Chinese Center Disease for Control and Prevention (China CDC), (b)
COVID-19-related internet search activity from Baidu, (c) news media activity
reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from
GLEAM, an agent-based mechanistic model. Our machine-learning methodology uses
a clustering technique that enables the exploitation of geo-spatial
synchronicities of COVID-19 activity across Chinese provinces, and a data
augmentation technique to deal with the small number of historical disease
activity observations, characteristic of emerging outbreaks. Our model's
predictive power outperforms a collection of baseline models in 27 out of the
32 Chinese provinces, and could be easily extended to other geographies
currently affected by the COVID-19 outbreak to help decision makers.

    ",Other Statistics (stat.OT),; Machine Learning (cs.LG); Populations and Evolution (q-bio.PE); Machine Learning (stat.ML)
A Differentiable Programming System to Bridge Machine Learning and Scientific Computing,"  Scientific computing is increasingly incorporating the advancements in
machine learning and the ability to work with large amounts of data. At the
same time, machine learning models are becoming increasingly sophisticated and
exhibit many features often seen in scientific computing, stressing the
capabilities of machine learning frameworks. Just as the disciplines of
scientific computing and machine learning have shared common underlying
infrastructure in the form of numerical linear algebra, we now have the
opportunity to further share new computational infrastructure, and thus ideas,
in the form of Differentiable Programming. We describe Zygote, a Differentiable
Programming system that is able to take gradients of general program
structures. We implement this system in the Julia programming language. Our
system supports almost all language constructs (control flow, recursion,
mutation, etc.) and compiles high-performance code without requiring any user
intervention or refactoring to stage computations. This enables an expressive
programming model for deep learning, but more importantly, it enables us to
incorporate a large ecosystem of libraries in our models in a straightforward
way. We discuss our approach to automatic differentiation, including its
support for advanced techniques such as mixed-mode, complex and checkpointed
differentiation, and present several examples of differentiating programs.

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG)
Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,"  Deep learning models with convolutional and recurrent networks are now
ubiquitous and analyze massive amounts of audio, image, video, text and graph
data, with applications in automatic translation, speech-to-text, scene
understanding, ranking user preferences, ad placement, etc. Competing
frameworks for building these networks such as TensorFlow, Chainer, CNTK,
Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between
usability and expressiveness, research or production orientation and supported
hardware. They operate on a DAG of computational operators, wrapping
high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for
various CPUs), and automate memory allocation, synchronization, distribution.
Custom operators are needed where the computation does not fit existing
high-performance library calls, usually at a high engineering cost. This is
frequently required when new operators are invented by researchers: such
operators suffer a severe performance penalty, which limits the pace of
innovation. Furthermore, even if there is an existing runtime call these
frameworks can use, it often doesn't offer optimal performance for a user's
particular network architecture and dataset, missing optimizations between
operators as well as optimizations that can be done knowing the size and shape
of data. Our contributions include (1) a language close to the mathematics of
deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time
compiler to convert a mathematical description of a deep learning DAG into a
CUDA kernel with delegated memory management and synchronization, also
providing optimizations such as operator fusion and specialization for specific
sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]

    ",Programming Languages (cs.PL),; Machine Learning (cs.LG)
6G White Paper on Machine Learning in Wireless Communication Networks,"  The focus of this white paper is on machine learning (ML) in wireless
communications. 6G wireless communication networks will be the backbone of the
digital transformation of societies by providing ubiquitous, reliable, and
near-instant wireless connectivity for humans and machines. Recent advances in
ML research has led enable a wide range of novel technologies such as
self-driving vehicles and voice assistants. Such innovation is possible as a
result of the availability of advanced ML models, large datasets, and high
computational power. On the other hand, the ever-increasing demand for
connectivity will require a lot of innovation in 6G wireless networks, and ML
tools will play a major role in solving problems in the wireless domain. In
this paper, we provide an overview of the vision of how ML will impact the
wireless communication systems. We first give an overview of the ML methods
that have the highest potential to be used in wireless networks. Then, we
discuss the problems that can be solved by using ML in various layers of the
network such as the physical layer, medium access layer, and application layer.
Zero-touch optimization of wireless networks using ML is another interesting
aspect that is discussed in this paper. Finally, at the end of each section,
important research questions that the section aims to answer are presented.

    ",Information Theory (cs.IT),; Signal Processing (eess.SP)
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"  Benchmark datasets have a significant impact on accelerating research in
programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark
dataset to foster machine learning research for program understanding and
generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and
a platform for model evaluation and comparison. CodeXGLUE also features three
baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder
models, to make it easy for researchers to use the platform. The availability
of such data and baselines can help the development and validation of new
methods that can be applied to various program understanding and generation
problems.

    ",Software Engineering (cs.SE),; Computation and Language (cs.CL)
"Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination","  This paper covers the two approaches for sentiment analysis: i) lexicon based
method; ii) machine learning method. We describe several techniques to
implement these approaches and discuss how they can be adopted for sentiment
classification of Twitter messages. We present a comparative study of different
lexicon combinations and show that enhancing sentiment lexicons with emoticons,
abbreviations and social-media slang expressions increases the accuracy of
lexicon-based classification for Twitter. We discuss the importance of feature
generation and feature selection processes for machine learning sentiment
classification. To quantify the performance of the main sentiment analysis
methods over Twitter we run these algorithms on a benchmark Twitter dataset
from the SemEval-2013 competition, task 2-B. The results show that machine
learning method based on SVM and Naive Bayes classifiers outperforms the
lexicon method. We present a new ensemble method that uses a lexicon based
sentiment score as input feature for the machine learning approach. The
combined method proved to produce more precise classifications. We also show
that employing a cost-sensitive classifier for highly unbalanced datasets
yields an improvement of sentiment classification performance up to 7%.

    ",Computation and Language (cs.CL),; Information Retrieval (cs.IR); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)
sktime: A Unified Interface for Machine Learning with Time Series,"  We present sktime -- a new scikit-learn compatible Python library with a
unified interface for machine learning with time series. Time series data gives
rise to various distinct but closely related learning tasks, such as
forecasting and time series classification, many of which can be solved by
reducing them to related simpler tasks. We discuss the main rationale for
creating a unified interface, including reduction, as well as the design of
sktime's core API, supported by a clear overview of common time series tasks
and reduction approaches.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Troubling Trends in Machine Learning Scholarship,"  Collectively, machine learning (ML) researchers are engaged in the creation
and dissemination of knowledge about data-driven algorithms. In a given paper,
researchers might aspire to any subset of the following goals, among others: to
theoretically characterize what is learnable, to obtain understanding through
empirically rigorous experiments, or to build a working system that has high
predictive accuracy. While determining which knowledge warrants inquiry may be
subjective, once the topic is fixed, papers are most valuable to the community
when they act in service of the reader, creating foundational knowledge and
communicating as clearly as possible.
",Machine Learning (stat.ML),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Introduction to Tensor Decompositions and their Applications in Machine Learning,"  Tensors are multidimensional arrays of numerical values and therefore
generalize matrices to multiple dimensions. While tensors first emerged in the
psychometrics community in the $20^{\text{th}}$ century, they have since then
spread to numerous other disciplines, including machine learning. Tensors and
their decompositions are especially beneficial in unsupervised learning
settings, but are gaining popularity in other sub-disciplines like temporal and
multi-relational data analysis, too.
",Machine Learning (stat.ML),; Machine Learning (cs.LG)
Comparing BERT against traditional machine learning text classification,"  The BERT model has arisen as a popular state-of-the-art machine learning
model in the recent years that is able to cope with multiple NLP tasks such as
supervised text classification without human supervision. Its flexibility to
cope with any type of corpus delivering great results has make this approach
very popular not only in academia but also in the industry. Although, there are
lots of different approaches that have been used throughout the years with
success. In this work, we first present BERT and include a little review on
classical NLP approaches. Then, we empirically test with a suite of experiments
dealing different scenarios the behaviour of BERT against the traditional
TF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work
is to add empirical evidence to support or refuse the use of BERT as a default
on NLP tasks. Experiments show the superiority of BERT and its independence of
features of the NLP problem such as the language of the text adding empirical
evidence to use BERT as a default technique to be used in NLP problems.

    ",Computation and Language (cs.CL),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Machine Teaching: A New Paradigm for Building Machine Learning Systems,"  The current processes for building machine learning systems require
practitioners with deep knowledge of machine learning. This significantly
limits the number of machine learning systems that can be created and has led
to a mismatch between the demand for machine learning systems and the ability
for organizations to build them. We believe that in order to meet this growing
demand for machine learning systems we must significantly increase the number
of individuals that can teach machines. We postulate that we can achieve this
goal by making the process of teaching machines easy, fast and above all,
universally accessible.
",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE); Machine Learning (stat.ML)
Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning,"  Machine learning has started to be deployed in fields such as healthcare and
finance, which propelled the need for and growth of privacy-preserving machine
learning (PPML). We propose an actively secure four-party protocol (4PC), and a
framework for PPML, showcasing its applications on four of the most
widely-known machine learning algorithms -- Linear Regression, Logistic
Regression, Neural Networks, and Convolutional Neural Networks. Our 4PC
protocol tolerating at most one malicious corruption is practically efficient
as compared to the existing works. We use the protocol to build an efficient
mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and
Garbled worlds. Our framework operates in the offline-online paradigm over
rings and is instantiated in an outsourced setting for machine learning. Also,
we propose conversions especially relevant to privacy-preserving machine
learning. The highlights of our framework include using a minimal number of
expensive circuits overall as compared to ABY3. This can be seen in our
technique for truncation, which does not affect the online cost of
multiplication and removes the need for any circuits in the offline phase. Our
B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and
$\mathbf{18} \times$ in the communication complexity. The practicality of our
framework is argued through improvements in the benchmarking of the
aforementioned algorithms when compared with ABY3. All the protocols are
implemented over a 64-bit ring in both LAN and WAN settings. Our improvements
go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$
for the prediction phase when observed over LAN and WAN.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML)
Detection of Unauthorized IoT Devices Using Machine Learning Techniques,"  Security experts have demonstrated numerous risks imposed by Internet of
Things (IoT) devices on organizations. Due to the widespread adoption of such
devices, their diversity, standardization obstacles, and inherent mobility,
organizations require an intelligent mechanism capable of automatically
detecting suspicious IoT devices connected to their networks. In particular,
devices not included in a white list of trustworthy IoT device types (allowed
to be used within the organizational premises) should be detected. In this
research, Random Forest, a supervised machine learning algorithm, was applied
to features extracted from network traffic data with the aim of accurately
identifying IoT device types from the white list. To train and evaluate
multi-class classifiers, we collected and manually labeled network traffic data
from 17 distinct IoT devices, representing nine types of IoT devices. Based on
the classification of 20 consecutive sessions and the use of majority rule, IoT
device types that are not on the white list were correctly detected as unknown
in 96% of test cases (on average), and white listed device types were correctly
classified by their actual types in 99% of cases. Some IoT device types were
identified quicker than others (e.g., sockets and thermostats were successfully
detected within five TCP sessions of connecting to the network). Perfect
detection of unauthorized IoT device types was achieved upon analyzing 110
consecutive sessions; perfect classification of white listed types required 346
consecutive sessions, 110 of which resulted in 99.49% accuracy. Further
experiments demonstrated the successful applicability of classifiers trained in
one location and tested on another. In addition, a discussion is provided
regarding the resilience of our machine learning-based IoT white listing method
to adversarial attacks.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV)
OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"  Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at ",Machine Learning (cs.LG),
Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems,"  Several researchers have argued that a machine learning system's
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent's role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.

    ",Artificial Intelligence (cs.AI),
On Formalizing Fairness in Prediction with Machine Learning,"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
bartMachine: Machine Learning with Bayesian Additive Regression Trees,"  We present a new package in R implementing Bayesian additive regression trees
(BART). The package introduces many new features for data analysis using BART
such as variable selection, interaction detection, model diagnostic plots,
incorporation of missing data and the ability to save trees for future
prediction. It is significantly faster than the current R implementation,
parallelized, and capable of handling both large sample sizes and
high-dimensional data.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG)
Machine Learning for Precipitation Nowcasting from Radar Images,"  High-resolution nowcasting is an essential tool needed for effective
adaptation to climate change, particularly for extreme weather. As Deep
Learning (DL) techniques have shown dramatic promise in many domains, including
the geosciences, we present an application of DL to the problem of
precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1
hour) predictions of precipitation. We treat forecasting as an image-to-image
translation problem and leverage the power of the ubiquitous UNET convolutional
neural network. We find this performs favorably when compared to three commonly
used models: optical flow, persistence and NOAA's numerical one-hour HRRR
nowcasting prediction.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Supervised quantum machine learning models are kernel methods,"  With near-term quantum devices available and the race for fault-tolerant
quantum computers in full swing, researchers became interested in the question
of what happens if we replace a supervised machine learning model with a
quantum circuit. While such ""quantum models"" are sometimes called ""quantum
neural networks"", it has been repeatedly noted that their mathematical
structure is actually much more closely related to kernel methods: they analyse
data in high-dimensional Hilbert spaces to which we only have access through
inner products revealed by measurements. This technical manuscript summarises
and extends the idea of systematically rephrasing supervised quantum models as
a kernel method. With this, a lot of near-term and fault-tolerant quantum
models can be replaced by a general support vector machine whose kernel
computes distances between data-encoding quantum states. Kernel-based training
is then guaranteed to find better or equally good quantum models than
variational circuit training. Overall, the kernel perspective of quantum
machine learning tells us that the way that data is encoded into quantum states
is the main ingredient that can potentially set quantum models apart from
classical machine learning models.

    ",Quantum Physics (quant-ph),; Machine Learning (stat.ML)
Unsupervised machine learning and band topology,"  The study of topological bandstructures is an active area of research in
condensed matter physics and beyond. Here, we combine recent progress in this
field with developments in machine-learning, another rising topic of interest.
Specifically, we introduce an unsupervised machine-learning approach that
searches for and retrieves paths of adiabatic deformations between
Hamiltonians, thereby clustering them according to their topological
properties. The algorithm is general as it does not rely on a specific
parameterization of the Hamiltonian and is readily applicable to any symmetry
class. We demonstrate the approach using several different models in both one
and two spatial dimensions and for different symmetry classes with and without
crystalline symmetries. Accordingly, it is also shown how trivial and
topological phases can be diagnosed upon comparing with a generally designated
set of trivial atomic insulators.

    ",Mesoscale and Nanoscale Physics (cond-mat.mes-hall),; Strongly Correlated Electrons (cond-mat.str-el); Computational Physics (physics.comp-ph)
Certified Data Removal from Machine Learning Models,"  Good data stewardship requires removal of data at the request of the data's
owner. This raises the question if and how a trained machine-learning model,
which implicitly stores information about its training data, should be affected
by such a removal request. Is it possible to ""remove"" data from a
machine-learning model? We study this problem by defining certified removal: a
very strong theoretical guarantee that a model from which data is removed
cannot be distinguished from a model that never observed the data to begin
with. We develop a certified-removal mechanism for linear classifiers and
empirically study learning settings in which this mechanism is practical.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation,"  Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable in the specific context
of verification. Suppose we have a machine learning system that predicts X, and
we provide rationale for this prediction X. Given an input, an explanation, and
an output, is the output consistent with the input and the supposed rationale?
Via a series of user-studies, we identify what kinds of increases in complexity
have the greatest effect on the time it takes for humans to verify the
rationale, and which seem relatively insensitive.

    ",Artificial Intelligence (cs.AI),
Automated Machine Learning: State-of-The-Art and Open Challenges,"  With the continuous and vast increase in the amount of data in our digital
world, it has been acknowledged that the number of knowledgeable data
scientists can not scale to address these challenges. Thus, there was a crucial
need for automating the process of building good machine learning models. In
the last few years, several techniques and frameworks have been introduced to
tackle the challenge of automating the process of Combined Algorithm Selection
and Hyper-parameter tuning (CASH) in the machine learning domain. The main aim
of these techniques is to reduce the role of the human in the loop and fill the
gap for non-expert machine learning users by playing the role of the domain
expert.
",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Chiron: Privacy-preserving Machine Learning as a Service,"  Major cloud operators offer machine learning (ML) as a service, enabling
customers who have the data but not ML expertise or infrastructure to train
predictive models on this data. Existing ML-as-a-service platforms require
users to reveal all training data to the service operator. We design,
implement, and evaluate Chiron, a system for privacy-preserving machine
learning as a service. First, Chiron conceals the training data from the
service operator. Second, in keeping with how many existing ML-as-a-service
platforms work, Chiron reveals neither the training algorithm nor the model
structure to the user, providing only black-box access to the trained model.
Chiron is implemented using SGX enclaves, but SGX alone does not achieve the
dual goals of data privacy and model confidentiality. Chiron runs the standard
ML training toolchain (including the popular Theano framework and C compiler)
in an enclave, but the untrusted model-creation code from the service operator
is further confined in a Ryoan sandbox to prevent it from leaking the training
data outside the enclave. To support distributed training, Chiron executes
multiple concurrent enclaves that exchange model parameters via a parameter
server. We evaluate Chiron on popular deep learning models, focusing on
benchmark image classification tasks such as CIFAR and ImageNet, and show that
its training performance and accuracy of the resulting models are practical for
common uses of ML-as-a-service.

    ",Cryptography and Security (cs.CR),
An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning,"  The incidence of malignant melanoma continues to increase worldwide. This
cancer can strike at any age; it is one of the leading causes of loss of life
in young persons. Since this cancer is visible on the skin, it is potentially
detectable at a very early stage when it is curable. New developments have
converged to make fully automatic early melanoma detection a real possibility.
First, the advent of dermoscopy has enabled a dramatic boost in clinical
diagnostic ability to the point that melanoma can be detected in the clinic at
the very earliest stages. The global adoption of this technology has allowed
accumulation of large collections of dermoscopy images of melanomas and benign
lesions validated by histopathology. The development of advanced technologies
in the areas of image processing and machine learning have given us the ability
to allow distinction of malignant melanoma from the many benign mimics that
require no biopsy. These new technologies should allow not only earlier
detection of melanoma, but also reduction of the large number of needless and
costly biopsy procedures. Although some of the new systems reported for these
technologies have shown promise in preliminary trials, widespread
implementation must await further technical progress in accuracy and
reproducibility. In this paper, we provide an overview of computerized
detection of melanoma in dermoscopy images. First, we discuss the various
aspects of lesion segmentation. Then, we provide a brief overview of clinical
feature segmentation. Finally, we discuss the classification stage where
machine learning algorithms are applied to the attributes generated from the
segmented features to predict the existence of melanoma.

    ",Computer Vision and Pattern Recognition (cs.CV),; Machine Learning (stat.ML)
Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning,"  In this paper, we propose an automated computer platform for the purpose of
classifying Electroencephalography (EEG) signals associated with left and right
hand movements using a hybrid system that uses advanced feature extraction
techniques and machine learning algorithms. It is known that EEG represents the
brain activity by the electrical voltage fluctuations along the scalp, and
Brain-Computer Interface (BCI) is a device that enables the use of the brain
neural activity to communicate with others or to control machines, artificial
limbs, or robots without direct physical movements. In our research work, we
aspired to find the best feature extraction method that enables the
differentiation between left and right executed fist movements through various
classification algorithms. The EEG dataset used in this research was created
and contributed to PhysioNet by the developers of the BCI2000 instrumentation
system. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts
removal was done using AAR. Data was epoched on the basis of Event-Related (De)
Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)
features. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta
rhythms were isolated for the MRCP analysis. The Independent Component Analysis
(ICA) spatial filter was applied on related channels for noise reduction and
isolation of both artifactually and neutrally generated EEG sources. The final
feature vector included the ERD, ERS, and MRCP features in addition to the
mean, power and energy of the activations of the resulting independent
components of the epoched feature datasets. The datasets were inputted into two
machine-learning algorithms: Neural Networks (NNs) and Support Vector Machines
(SVMs). Intensive experiments were carried out and optimum classification
performances of 89.8 and 97.1 were obtained using NN and SVM, respectively.

    ",Neural and Evolutionary Computing (cs.NE),; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)
Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning,"  Machine learning is a popular approach to signatureless malware detection
because it can generalize to never-before-seen malware families and polymorphic
strains. This has resulted in its practical use for either primary detection
engines or for supplementary heuristic detection by anti-malware vendors.
Recent work in adversarial machine learning has shown that deep learning models
are susceptible to gradient-based attacks, whereas non-differentiable models
that report a score can be attacked by genetic algorithms that aim to
systematically reduce the score. We propose a more general framework based on
reinforcement learning (RL) for attacking static portable executable (PE)
anti-malware engines. The general framework does not require a differentiable
model nor does it require the engine to produce a score. Instead, an RL agent
is equipped with a set of functionality-preserving operations that it may
perform on the PE file. Through a series of games played against the
anti-malware engine, it learns which sequences of operations are likely to
result in evading the detector for any given malware sample. This enables
completely black-box attacks against static PE anti-malware, and produces
functional evasive malware samples as a direct result. We show in experiments
that our method can attack a gradient-boosted machine learning model with
evasion rates that are substantial and appear to be strongly dependent on the
dataset. We demonstrate that attacks against this model appear to also evade
components of publicly hosted antivirus engines. Adversarial training results
are also presented: by retraining the model on evasive ransomware samples, a
subsequent attack is 33% less effective. However, there are overfitting dangers
when adversarial training, which we note. We release code to allow researchers
to reproduce and improve this approach.

    ",Cryptography and Security (cs.CR),
BLAZE: Blazing Fast Privacy-Preserving Machine Learning,"  Machine learning tools have illustrated their potential in many significant
sectors such as healthcare and finance, to aide in deriving useful inferences.
The sensitive and confidential nature of the data, in such sectors, raise
natural concerns for the privacy of data. This motivated the area of
Privacy-preserving Machine Learning (PPML) where privacy of the data is
guaranteed. Typically, ML techniques require large computing power, which leads
clients with limited infrastructure to rely on the method of Secure Outsourced
Computation (SOC). In SOC setting, the computation is outsourced to a set of
specialized and powerful cloud servers and the service is availed on a
pay-per-use basis. In this work, we explore PPML techniques in the SOC setting
for widely used ML algorithms-- Linear Regression, Logistic Regression, and
Neural Networks.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
Bringing the People Back In: Contesting Benchmark Machine Learning Datasets,"  In response to algorithmic unfairness embedded in sociotechnical systems,
significant attention has been focused on the contents of machine learning
datasets which have revealed biases towards white, cisgender, male, and Western
data subjects. In contrast, comparatively less attention has been paid to the
histories, values, and norms embedded in such datasets. In this work, we
outline a research program - a genealogy of machine learning data - for
investigating how and why these datasets have been created, what and whose
values influence the choices of data to collect, the contextual and contingent
conditions of their creation. We describe the ways in which benchmark datasets
in machine learning operate as infrastructure and pose four research questions
for these datasets. This interrogation forces us to ""bring the people back in""
by aiding us in understanding the labor embedded in dataset construction, and
thereby presenting new avenues of contestation for other researchers
encountering the data.

    ",Computers and Society (cs.CY),
Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach,"  Toxic online content has become a major issue in today's world due to an
exponential increase in the use of internet by people of different cultures and
educational background. Differentiating hate speech and offensive language is a
key challenge in automatic detection of toxic text content. In this paper, we
propose an approach to automatically classify tweets on Twitter into three
classes: hateful, offensive and clean. Using Twitter dataset, we perform
experiments considering n-grams as features and passing their term
frequency-inverse document frequency (TFIDF) values to multiple machine
learning models. We perform comparative analysis of the models considering
several values of n in n-grams and TFIDF normalization methods. After tuning
the model giving the best results, we achieve 95.6% accuracy upon evaluating it
on test data. We also create a module which serves as an intermediate between
user and Twitter.

    ",Computation and Language (cs.CL),
Participation is not a Design Fix for Machine Learning,"  This paper critically examines existing modes of participation in design
practice and machine learning. Cautioning against 'participation-washing', it
suggests that the ML community must become attuned to possibly exploitative and
extractive forms of community involvement and shift away from the prerogatives
of context-independent scalability.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG)
A Time Series Analysis-Based Stock Price Prediction Using Machine Learning and Deep Learning Models,"  Prediction of future movement of stock prices has always been a challenging
task for the researchers. While the advocates of the efficient market
hypothesis (EMH) believe that it is impossible to design any predictive
framework that can accurately predict the movement of stock prices, there are
seminal work in the literature that have clearly demonstrated that the
seemingly random movement patterns in the time series of a stock price can be
predicted with a high level of accuracy. Design of such predictive models
requires choice of appropriate variables, right transformation methods of the
variables, and tuning of the parameters of the models. In this work, we present
a very robust and accurate framework of stock price prediction that consists of
an agglomeration of statistical, machine learning and deep learning models. We
use the daily stock price data, collected at five minutes interval of time, of
a very well known company that is listed in the National Stock Exchange (NSE)
of India. The granular data is aggregated into three slots in a day, and the
aggregated data is used for building and training the forecasting models. We
contend that the agglomerative approach of model building that uses a
combination of statistical, machine learning, and deep learning approaches, can
very effectively learn from the volatile and random movement patterns in a
stock price data. We build eight classification and eight regression models
based on statistical and machine learning approaches. In addition to these
models, a deep learning regression model using a long-and-short-term memory
(LSTM) network is also built. Extensive results have been presented on the
performance of these models, and the results are critically analyzed.

    ",Statistical Finance (q-fin.ST),; Machine Learning (cs.LG); Machine Learning (stat.ML)
TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games,"  We present TorchCraft, a library that enables deep learning research on
Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it
easier to control these games from a machine learning framework, here Torch.
This white paper argues for using RTS games as a benchmark for AI research, and
describes the design and components of TorchCraft.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI)
Analysis of the COVID-19 pandemic by SIR model and machine learning technics for forecasting,"  This work is a trial in which we propose SIR model and machine learning tools
to analyze the coronavirus pandemic in the real world. Based on the public data
from \cite{datahub}, we estimate main key pandemic parameters and make
predictions on the inflection point and possible ending time for the real world
and specifically for Senegal. The coronavirus disease 2019, by World Health
Organization, rapidly spread out in the whole China and then in the whole
world. Under optimistic estimation, the pandemic in some countries will end
soon, while for most part of countries in the world (US, Italy, etc.), the hit
of anti-pandemic will be no later than the end of April.

    ",Populations and Evolution (q-bio.PE),; Optimization and Control (math.OC); Machine Learning (stat.ML)
Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development,"  Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
",Machine Learning (cs.LG),; Computers and Society (cs.CY); Biomolecules (q-bio.BM); Quantitative Methods (q-bio.QM)
A survey on measuring indirect discrimination in machine learning,"  Nowadays, many decisions are made using predictive models built on historical
data.Predictive models may systematically discriminate groups of people even if
the computing process is fair and well-intentioned. Discrimination-aware data
mining studies how to make predictive models free from discrimination, when
historical data, on which they are built, may be biased, incomplete, or even
contain past discriminatory decisions. Discrimination refers to disadvantageous
treatment of a person based on belonging to a category rather than on
individual merit. In this survey we review and organize various discrimination
measures that have been used for measuring discrimination in data, as well as
in evaluating performance of discrimination-aware predictive models. We also
discuss related measures from other disciplines, which have not been used for
measuring discrimination, but potentially could be suitable for this purpose.
We computationally analyze properties of selected measures. We also review and
discuss measuring procedures, and present recommendations for practitioners.
The primary target audience is data mining, machine learning, pattern
recognition, statistical modeling researchers developing new methods for
non-discriminatory predictive modeling. In addition, practitioners and policy
makers would use the survey for diagnosing potential discrimination by
predictive models.

    ",Computers and Society (cs.CY),; Applications (stat.AP)
Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers,"  To construct interpretable explanations that are consistent with the original
ML model, counterfactual examples---showing how the model's output changes with
small perturbations to the input---have been proposed. This paper extends the
work in counterfactual explanations by addressing the challenge of feasibility
of such examples. For explanations of ML models in critical domains such as
healthcare and finance, counterfactual examples are useful for an end-user only
to the extent that perturbation of feature inputs is feasible in the real
world. We formulate the problem of feasibility as preserving causal
relationships among input features and present a method that uses (partial)
structural causal models to generate actionable counterfactuals. When
feasibility constraints cannot be easily expressed, we consider an alternative
mechanism where people can label generated CF examples on feasibility: whether
it is feasible to intervene and realize the candidate CF example from the
original input. To learn from this labelled feasibility data, we propose a
modified variational auto encoder loss for generating CF examples that
optimizes for feasibility as people interact with its output. Our experiments
on Bayesian networks and the widely used ''Adult-Income'' dataset show that our
proposed methods can generate counterfactual explanations that better satisfy
feasibility constraints than existing methods.. Code repository can be accessed
here: \textit{",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
A Survey of Privacy Attacks in Machine Learning,"  As machine learning becomes more widely used, the need to study its
implications in security and privacy becomes more urgent. Although the body of
work in privacy has been steadily growing over the past few years, research on
the privacy aspects of machine learning has received less focus than the
security aspects. Our contribution in this research is an analysis of more than
40 papers related to privacy attacks against machine learning that have been
published during the past seven years. We propose an attack taxonomy, together
with a threat model that allows the categorization of different attacks based
on the adversarial knowledge, and the assets under attack. An initial
exploration of the causes of privacy leaks is presented, as well as a detailed
analysis of the different attacks. Finally, we present an overview of the most
commonly proposed defenses and a discussion of the open problems and future
directions identified during our analysis.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"  Bioinformatics research is characterized by voluminous and incremental
datasets and complex data analytics methods. The machine learning methods used
in bioinformatics are iterative and parallel. These methods can be scaled to
handle big data using the distributed and parallel computing technologies.
","Computational Engineering, Finance, and Science (cs.CE)",; Machine Learning (cs.LG)
Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning,"  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.

    ",Computation and Language (cs.CL),
A Living Review of Machine Learning for Particle Physics,"  Modern machine learning techniques, including deep learning, are rapidly
being applied, adapted, and developed for high energy physics. Given the fast
pace of this research, we have created a living review with the goal of
providing a nearly comprehensive list of citations for those developing and
applying these approaches to experimental, phenomenological, or theoretical
analyses. As a living document, it will be updated as often as possible to
incorporate the latest developments. A list of proper (unchanging) reviews can
be found within. Papers are grouped into a small set of topics to be as useful
as possible. Suggestions and contributions are most welcome, and we provide
instructions for participating.

    ",High Energy Physics - Phenomenology (hep-ph),"; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)"
A Primer on the Signature Method in Machine Learning,"  In these notes, we wish to provide an introduction to the signature method,
focusing on its basic theoretical properties and recent numerical applications.
",Machine Learning (stat.ML),; Machine Learning (cs.LG); Methodology (stat.ME)
Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"  Autonomous vehicles rely on machine learning to solve challenging tasks in
perception and motion planning. However, automotive software safety standards
have not fully evolved to address the challenges of machine learning safety
such as interpretability, verification, and performance limitations. In this
paper, we review and organize practical machine learning safety techniques that
can complement engineering safety for machine learning based software in
autonomous vehicles. Our organization maps safety strategies to
state-of-the-art machine learning techniques in order to enhance dependability
and safety of machine learning algorithms. We also discuss security limitations
and user experience aspects of machine learning components in autonomous
vehicles.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Unified Representation of Molecules and Crystals for Machine Learning,"  Accurate simulations of atomistic systems from first principles are limited
by computational cost. In high-throughput settings, machine learning can reduce
these costs significantly by accurately interpolating between reference
calculations. For this, kernel learning approaches crucially require a
representation that accommodates arbitrary atomistic systems. We introduce a
many-body tensor representation that is invariant to translations, rotations,
and nuclear permutations of same elements, unique, differentiable, can
represent molecules and crystals, and is fast to compute. Empirical evidence
for competitive energy and force prediction errors is presented for changes in
molecular structure, crystal chemistry, and molecular dynamics using kernel
regression and symmetric gradient-domain machine learning as models.
Applicability is demonstrated for phase diagrams of Pt-group/transition-metal
binary systems.

    ",Chemical Physics (physics.chem-ph),; Materials Science (cond-mat.mtrl-sci)
AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling,"  Interpretability of the underlying AI representations is a key raison
d'Ãªtre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of
learners' cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.

    ",Artificial Intelligence (cs.AI),; Computers and Society (cs.CY)
Safely Entering the Deep: A Review of Verification and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry,"  Deep Neural Networks (DNN) will emerge as a cornerstone in automotive
software engineering. However, developing systems with DNNs introduces novel
challenges for safety assessments. This paper reviews the state-of-the-art in
verification and validation of safety-critical systems that rely on machine
learning. Furthermore, we report from a workshop series on DNNs for perception
with automotive experts in Sweden, confirming that ISO 26262 largely
contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge
transfer and systems-based safety approaches, e.g., safety cage architectures
and simulated system test cases.

    ",Software Engineering (cs.SE),
Machine Learning Techniques for Intrusion Detection,"  An Intrusion Detection System (IDS) is a software that monitors a single or a
network of computers for malicious activities (attacks) that are aimed at
stealing or censoring information or corrupting network protocols. Most
techniques used in today's IDS are not able to deal with the dynamic and
complex nature of cyber attacks on computer networks. Hence, efficient adaptive
methods like various techniques of machine learning can result in higher
detection rates, lower false alarm rates and reasonable computation and
communication costs. In this paper, we study several such schemes and compare
their performance. We divide the schemes into methods based on classical
artificial intelligence (AI) and methods based on computational intelligence
(CI). We explain how various characteristics of CI techniques can be used to
build efficient IDS.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)
Trustless Machine Learning Contracts; Evaluating and Exchanging Machine Learning Models on the Ethereum Blockchain,"  Using blockchain technology, it is possible to create contracts that offer a
reward in exchange for a trained machine learning model for a particular data
set. This would allow users to train machine learning models for a reward in a
trustless manner. The smart contract will use the blockchain to automatically
validate the solution, so there would be no debate about whether the solution
was correct or not. Users who submit the solutions won't have counterparty risk
that they won't get paid for their work. Contracts can be created easily by
anyone with a dataset, even programmatically by software agents. This creates a
market where parties who are good at solving machine learning problems can
directly monetize their skillset, and where any organization or software agent
that has a problem to solve with AI can solicit solutions from all over the
world. This will incentivize the creation of better machine learning models,
and make AI more accessible to companies and software agents.

    ",Cryptography and Security (cs.CR),
Machine learning based hyperspectral image analysis: A survey,"  Hyperspectral sensors enable the study of the chemical properties of scene
materials remotely for the purpose of identification, detection, and chemical
composition analysis of objects in the environment. Hence, hyperspectral images
captured from earth observing satellites and aircraft have been increasingly
important in agriculture, environmental monitoring, urban planning, mining, and
defense. Machine learning algorithms due to their outstanding predictive power
have become a key tool for modern hyperspectral image analysis. Therefore, a
solid understanding of machine learning techniques have become essential for
remote sensing researchers and practitioners. This paper reviews and compares
recent machine learning-based hyperspectral image analysis methods published in
literature. We organize the methods by the image analysis task and by the type
of machine learning algorithm, and present a two-way mapping between the image
analysis tasks and the types of machine learning algorithms that can be applied
to them. The paper is comprehensive in coverage of both hyperspectral image
analysis tasks and machine learning algorithms. The image analysis tasks
considered are land cover classification, target detection, unmixing, and
physical parameter estimation. The machine learning algorithms covered are
Gaussian models, linear regression, logistic regression, support vector
machines, Gaussian mixture model, latent linear models, sparse linear models,
Gaussian mixture models, ensemble learning, directed graphical models,
undirected graphical models, clustering, Gaussian processes, Dirichlet
processes, and deep learning. We also discuss the open challenges in the field
of hyperspectral image analysis and explore possible future directions.

    ",Computer Vision and Pattern Recognition (cs.CV),; Image and Video Processing (eess.IV)
Automated software vulnerability detection with machine learning,"  Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.

    ",Software Engineering (cs.SE),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Machine Learning vs Statistical Methods for Time Series Forecasting: Size Matters,"  Time series forecasting is one of the most active research topics. Machine
learning methods have been increasingly adopted to solve these predictive
tasks. However, in a recent work, these were shown to systematically present a
lower predictive performance relative to simple statistical methods. In this
work, we counter these results. We show that these are only valid under an
extremely low sample size. Using a learning curve method, our results suggest
that machine learning methods improve their relative predictive performance as
the sample size grows. The code to reproduce the experiments is available at
",Machine Learning (stat.ML),; Machine Learning (cs.LG)
An Analysis of ISO 26262: Using Machine Learning Safely in Automotive Software,"  Machine learning (ML) plays an ever-increasing role in advanced automotive
functionality for driver assistance and autonomous operation; however, its
adequacy from the perspective of safety certification remains controversial. In
this paper, we analyze the impacts that the use of ML as an implementation
approach has on ISO 26262 safety lifecycle and ask what could be done to
address them. We then provide a set of recommendations on how to adapt the
standard to accommodate ML.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG); Software Engineering (cs.SE); Systems and Control (eess.SY)
Quantifying Interpretability and Trust in Machine Learning Systems,"  Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Adversarial Examples in Modern Machine Learning: A Review,"  Recent research has found that many families of machine learning models are
vulnerable to adversarial examples: inputs that are specifically designed to
cause the target model to produce erroneous outputs. In this survey, we focus
on machine learning models in the visual domain, where methods for generating
and detecting such examples have been most extensively studied. We explore a
variety of adversarial attack methods that apply to image-space content, real
world adversarial attacks, adversarial defenses, and the transferability
property of adversarial examples. We also discuss strengths and weaknesses of
various methods of adversarial attack and defense. Our aim is to provide an
extensive coverage of the field, furnishing the reader with an intuitive
understanding of the mechanics of adversarial attack and defense mechanisms and
enlarging the community of researchers studying this fundamental set of
problems.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML)
A glass-box interactive machine learning approach for solving NP-hard problems with the human-in-the-loop,"  The goal of Machine Learning to automatically learn from data, extract
knowledge and to make decisions without any human intervention. Such automatic
(aML) approaches show impressive success. Recent results even demonstrate
intriguingly that deep learning applied for automatic classification of skin
lesions is on par with the performance of dermatologists, yet outperforms the
average. As human perception is inherently limited, such approaches can
discover patterns, e.g. that two objects are similar, in arbitrarily
high-dimensional spaces what no human is able to do. Humans can deal only with
limited amounts of data, whilst big data is beneficial for aML; however, in
health informatics, we are often confronted with a small number of data sets,
where aML suffer of insufficient training samples and many problems are
computationally hard. Here, interactive machine learning (iML) may be of help,
where a human-in-the-loop contributes to reduce the complexity of NP-hard
problems. A further motivation for iML is that standard black-box approaches
lack transparency, hence do not foster trust and acceptance of ML among
end-users. Rising legal and privacy aspects, e.g. with the new European General
Data Protection Regulations, make black-box approaches difficult to use,
because they often are not able to explain why a decision has been made. In
this paper, we present some experiments to demonstrate the effectiveness of the
human-in-the-loop approach, particularly in opening the black-box to a
glass-box and thus enabling a human directly to interact with an learning
algorithm. We selected the Ant Colony Optimization framework, and applied it on
the Traveling Salesman Problem, which is a good example, due to its relevance
for health informatics, e.g. for the study of protein folding. From studies of
how humans extract so much from so little data, fundamental ML-research also
may benefit.

    ",Artificial Intelligence (cs.AI),; Machine Learning (stat.ML)
Practical Coreset Constructions for Machine Learning,"  We investigate coresets - succinct, small summaries of large data sets - so
that solutions found on the summary are provably competitive with solution
found on the full data set. We provide an overview over the state-of-the-art in
coreset construction for machine learning. In Section 2, we present both the
intuition behind and a theoretically sound framework to construct coresets for
general problems and apply it to $k$-means clustering. In Section 3 we
summarize existing coreset construction algorithms for a variety of machine
learning problems such as maximum likelihood estimation of mixture models,
Bayesian non-parametric models, principal component analysis, regression and
general empirical risk minimization.

    ",Machine Learning (stat.ML),
Machine Learning with Multi-Site Imaging Data: An Empirical Study on the Impact of Scanner Effects,"  This is an empirical study to investigate the impact of scanner effects when
using machine learning on multi-site neuroimaging data. We utilize structural
T1-weighted brain MRI obtained from two different studies, Cam-CAN and UK
Biobank. For the purpose of our investigation, we construct a dataset
consisting of brain scans from 592 age- and sex-matched individuals, 296
subjects from each original study. Our results demonstrate that even after
careful pre-processing with state-of-the-art neuroimaging pipelines a
classifier can easily distinguish between the origin of the data with very high
accuracy. Our analysis on the example application of sex classification
suggests that current approaches to harmonize data are unable to remove
scanner-specific bias leading to overly optimistic performance estimates and
poor generalization. We conclude that multi-site data harmonization remains an
open challenge and particular care needs to be taken when using such data with
advanced machine learning methods for predictive modelling.

    ",Image and Video Processing (eess.IV),; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)
Machine Learning for Spatiotemporal Sequence Forecasting: A Survey,"  Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
mlpy: Machine Learning Python,"  mlpy is a Python Open Source Machine Learning library built on top of
NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of
state-of-the-art machine learning methods for supervised and unsupervised
problems and it is aimed at finding a reasonable compromise among modularity,
maintainability, reproducibility, usability and efficiency. mlpy is
multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at
the website ",Mathematical Software (cs.MS),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Interpretability and Explainability: A Machine Learning Zoo Mini-tour,"  In this review, we examine the problem of designing interpretable and
explainable machine learning models. Interpretability and explainability lie at
the core of many machine learning and statistical applications in medicine,
economics, law, and natural sciences. Although interpretability and
explainability have escaped a clear universal definition, many techniques
motivated by these properties have been developed over the recent 30 years with
the focus currently shifting towards deep learning methods. In this review, we
emphasise the divide between interpretability and explainability and illustrate
these two different research directions with concrete examples of the
state-of-the-art. The review is intended for a general machine learning
audience with interest in exploring the problems of interpretation and
explanation beyond logistic regression or random forest variable importance.
This work is not an exhaustive literature survey, but rather a primer focusing
selectively on certain lines of research which the authors found interesting or
informative.

    ",Machine Learning (cs.LG),
AlphaD3M: Machine Learning Pipeline Synthesis,"  We introduce AlphaD3M, an automatic machine learning (AutoML) system based on
meta reinforcement learning using sequence models with self play. AlphaD3M is
based on edit operations performed over machine learning pipeline primitives
providing explainability. We compare AlphaD3M with state-of-the-art AutoML
systems: Autosklearn, Autostacker, and TPOT, on OpenML datasets. AlphaD3M
achieves competitive performance while being an order of magnitude faster,
reducing computation time from hours to minutes, and is explainable by design.

    ",Machine Learning (cs.LG),
Tunability: Importance of Hyperparameters of Machine Learning Algorithms,"  Modern supervised machine learning algorithms involve hyperparameters that
have to be set before running them. Options for setting hyperparameters are
default values from the software package, manual configuration by the user or
configuring them for optimal predictive performance by a tuning procedure. The
goal of this paper is two-fold. Firstly, we formalize the problem of tuning
from a statistical point of view, define data-based defaults and suggest
general measures quantifying the tunability of hyperparameters of algorithms.
Secondly, we conduct a large-scale benchmarking study based on 38 datasets from
the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield
default values for hyperparameters and enable users to decide whether it is
worth conducting a possibly time consuming tuning strategy, to focus on the
most important hyperparameters and to chose adequate hyperparameter spaces for
tuning.

    ",Machine Learning (stat.ML),
Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions,"  Low-dimensional embedding, manifold learning, clustering, classification, and
anomaly detection are among the most important problems in machine learning.
The existing methods usually consider the case when each instance has a fixed,
finite-dimensional feature representation. Here we consider a different
setting. We assume that each instance corresponds to a continuous probability
distribution. These distributions are unknown, but we are given some i.i.d.
samples from each distribution. Our goal is to estimate the distances between
these distributions and use these distances to perform low-dimensional
embedding, clustering/classification, or anomaly detection for the
distributions. We present estimation algorithms, describe how to apply them for
machine learning tasks on distributions, and show empirical results on
synthetic data, real word images, and astronomical data sets.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Rafiki: Machine Learning as an Analytics Service System,"  Big data analytics is gaining massive momentum in the last few years.
Applying machine learning models to big data has become an implicit requirement
or an expectation for most analysis tasks, especially on high-stakes
applications.Typical applications include sentiment analysis against reviews
for analyzing on-line products, image classification in food logging
applications for monitoring user's daily intake and stock movement prediction.
Extending traditional database systems to support the above analysis is
intriguing but challenging. First, it is almost impossible to implement all
machine learning models in the database engines. Second, expertise knowledge is
required to optimize the training and inference procedures in terms of
efficiency and effectiveness, which imposes heavy burden on the system users.
In this paper, we develop and present a system, called Rafiki, to provide the
training and inference service of machine learning models, and facilitate
complex analytics on top of cloud platforms. Rafiki provides distributed
hyper-parameter tuning for the training service, and online ensemble modeling
for the inference service which trades off between latency and accuracy.
Experimental results confirm the efficiency, effectiveness, scalability and
usability of Rafiki.

    ",Databases (cs.DB),"; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)"
Automated Machine Learning on Graphs: A Survey,"  Machine learning on graphs has been extensively studied in both academic and
industry. However, as the literature on graph learning booms with a vast number
of emerging methods and techniques, it becomes increasingly difficult to
manually design the optimal machine learning algorithm for different
graph-related tasks. To solve this critical challenge, automated machine
learning (AutoML) on graphs which combines the strength of graph machine
learning and AutoML together, is gaining attention from the research community.
Therefore, we comprehensively survey AutoML on graphs in this paper, primarily
focusing on hyper-parameter optimization (HPO) and neural architecture search
(NAS) for graph machine learning. We further overview libraries related to
automated graph machine learning and in-depth discuss AutoGL, the first
dedicated open-source library for AutoML on graphs. In the end, we share our
insights on future research directions for automated graph machine learning.
This paper is the first systematic and comprehensive review of automated
machine learning on graphs to the best of our knowledge.

    ",Machine Learning (cs.LG),
Enhancing Computational Fluid Dynamics with Machine Learning,"  Machine learning is rapidly becoming a core technology for scientific
computing, with numerous opportunities to advance the field of computational
fluid dynamics. In this Perspective, we highlight some of the areas of highest
potential impact, including to accelerate direct numerical simulations, to
improve turbulence closure modeling, and to develop enhanced reduced-order
models. We also discuss emerging areas of machine learning that are promising
for computational fluid dynamics, as well as some potential limitations that
should be taken into account.

    ",Fluid Dynamics (physics.flu-dyn),; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)
Private Machine Learning in TensorFlow using Secure Computation,"  We present a framework for experimenting with secure multi-party computation
directly in TensorFlow. By doing so we benefit from several properties valuable
to both researchers and practitioners, including tight integration with
ordinary machine learning processes, existing optimizations for distributed
computation in TensorFlow, high-level abstractions for expressing complex
algorithms and protocols, and an expanded set of familiar tooling. We give an
open source implementation of a state-of-the-art protocol and report on
concrete benchmarks using typical models from private machine learning.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
The Values Encoded in Machine Learning Research,"  Machine learning currently exerts an outsized influence on the world,
increasingly affecting institutional practices and impacted communities. It is
therefore critical that we question vague conceptions of the field as
value-neutral or universally beneficial, and investigate what specific values
the field is advancing. In this paper, we first introduce a method and
annotation scheme for studying the values encoded in documents such as research
papers. Applying the scheme, we analyze 100 highly cited machine learning
papers published at premier machine learning conferences, ICML and NeurIPS. We
annotate key features of papers which reveal their values: their justification
for their choice of project, which attributes of their project they uplift,
their consideration of potential negative consequences, and their institutional
affiliations and funding sources. We find that few of the papers justify how
their project connects to a societal need (15\%) and far fewer discuss negative
potential (1\%). Through line-by-line content analysis, we identify 59 values
that are uplifted in ML research, and, of these, we find that the papers most
frequently justify and assess themselves based on Performance, Generalization,
Quantitative evidence, Efficiency, Building on past work, and Novelty. We
present extensive textual evidence and identify key themes in the definitions
and operationalization of these values. Notably, we find systematic textual
evidence that these top values are being defined and applied with assumptions
and implications generally supporting the centralization of power.Finally, we
find increasingly close ties between these highly cited papers and tech
companies and elite universities.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)
"Verification for Machine Learning, Autonomy, and Neural Networks Survey","  This survey presents an overview of verification techniques for autonomous
systems, with a focus on safety-critical autonomous cyber-physical systems
(CPS) and subcomponents thereof. Autonomy in CPS is enabling by recent advances
in artificial intelligence (AI) and machine learning (ML) through approaches
such as deep neural networks (DNNs), embedded in so-called learning enabled
components (LECs) that accomplish tasks from classification to control.
Recently, the formal methods and formal verification community has developed
methods to characterize behaviors in these LECs with eventual goals of formally
verifying specifications for LECs, and this article presents a survey of many
of these recent approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG)
Catching Zika Fever: Application of Crowdsourcing and Machine Learning for Tracking Health Misinformation on Twitter,"  In February 2016, World Health Organization declared the Zika outbreak a
Public Health Emergency of International Concern. With developing evidence it
can cause birth defects, and the Summer Olympics coming up in the worst
affected country, Brazil, the virus caught fire on social media. In this work,
use Zika as a case study in building a tool for tracking the misinformation
around health concerns on Twitter. We collect more than 13 million tweets --
spanning the initial reports in February 2016 and the Summer Olympics --
regarding the Zika outbreak and track rumors outlined by the World Health
Organization and Snopes fact checking website. The tool pipeline, which
incorporates health professionals, crowdsourcing, and machine learning, allows
us to capture health-related rumors around the world, as well as clarification
campaigns by reputable health organizations. In the case of Zika, we discover
an extremely bursty behavior of rumor-related topics, and show that, once the
questionable topic is detected, it is possible to identify rumor-bearing tweets
using automated techniques. Thus, we illustrate insights the proposed tools
provide into potentially harmful information on social media, allowing public
health researchers and practitioners to respond with a targeted and timely
action.

    ",Social and Information Networks (cs.SI),; Computers and Society (cs.CY)
Quantum Neuron: an elementary building block for machine learning on quantum computers,"  Even the most sophisticated artificial neural networks are built by
aggregating substantially identical units called neurons. A neuron receives
multiple signals, internally combines them, and applies a non-linear function
to the resulting weighted sum. Several attempts to generalize neurons to the
quantum regime have been proposed, but all proposals collided with the
difficulty of implementing non-linear activation functions, which is essential
for classical neurons, due to the linear nature of quantum mechanics. Here we
propose a solution to this roadblock in the form of a small quantum circuit
that naturally simulates neurons with threshold activation. Our quantum circuit
defines a building block, the ""quantum neuron"", that can reproduce a variety of
classical neural network constructions while maintaining the ability to process
superpositions of inputs and preserve quantum coherence and entanglement. In
the construction of feedforward networks of quantum neurons, we provide
numerical evidence that the network not only can learn a function when trained
with superposition of inputs and the corresponding output, but that this
training suffices to learn the function on all individual inputs separately.
When arranged to mimic Hopfield networks, quantum neural networks exhibit
properties of associative memory. Patterns are encoded using the simple Hebbian
rule for the weights and we demonstrate attractor dynamics from corrupted
inputs. Finally, the fact that our quantum model closely captures (traditional)
neural network dynamics implies that the vast body of literature and results on
neural networks becomes directly relevant in the context of quantum machine
learning.

    ",Quantum Physics (quant-ph),; Neural and Evolutionary Computing (cs.NE)
"Bayesian inference of chaotic dynamics by merging data assimilation, machine learning and expectation-maximization","  The reconstruction from observations of high-dimensional chaotic dynamics
such as geophysical flows is hampered by (i) the partial and noisy observations
that can realistically be obtained, (ii) the need to learn from long time
series of data, and (iii) the unstable nature of the dynamics. To achieve such
inference from the observations over long time series, it has been suggested to
combine data assimilation and machine learning in several ways. We show how to
unify these approaches from a Bayesian perspective using
expectation-maximization and coordinate descents. In doing so, the model, the
state trajectory and model error statistics are estimated all together.
Implementations and approximations of these methods are discussed. Finally, we
numerically and successfully test the approach on two relevant low-order
chaotic models with distinct identifiability.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)
Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't,"  The purpose of this article is to review the achievements made in the last
few years towards the understanding of the reasons behind the success and
subtleties of neural network-based machine learning. In the tradition of good
old applied mathematics, we will not only give attention to rigorous
mathematical results, but also the insight we have gained from careful
numerical experiments as well as the analysis of simplified models. Along the
way, we also list the open problems which we believe to be the most important
topics for further study. This is not a complete overview over this quickly
moving field, but we hope to provide a perspective which may be helpful
especially to new researchers in the area.

    ",Machine Learning (cs.LG),; Numerical Analysis (math.NA); Machine Learning (stat.ML)
Personalized explanation in machine learning: A conceptualization,"  Explanation in machine learning and related fields such as artificial
intelligence aims at making machine learning models and their decisions
understandable to humans. Existing work suggests that personalizing
explanations might help to improve understandability. In this work, we derive a
conceptualization of personalized explanation by defining and structuring the
problem based on prior work on machine learning explanation, personalization
(in machine learning) and concepts and techniques from other domains such as
privacy and knowledge elicitation. We perform a categorization of explainee
data used in the process of personalization as well as describing means to
collect this data. We also identify three key explanation properties that are
amendable to personalization: complexity, decision information and
presentation. We also enhance existing work on explanation by introducing
additional desiderata and measures to quantify the quality of personalized
explanations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
TensorNetwork: A Library for Physics and Machine Learning,"  TensorNetwork is an open source library for implementing tensor network
algorithms. Tensor networks are sparse data structures originally designed for
simulating quantum many-body physics, but are currently also applied in a
number of other research areas, including machine learning. We demonstrate the
use of the API with applications both physics and machine learning, with
details appearing in companion papers.

    ",Computational Physics (physics.comp-ph),; Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); High Energy Physics - Theory (hep-th); Machine Learning (stat.ML)
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving,"  Large computer-understandable proofs consist of millions of intermediate
logical steps. The vast majority of such steps originate from manually selected
and manually guided heuristics applied to intermediate goals. So far, machine
learning has generally not been used to filter or generate these steps. In this
paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for
the purpose of developing new machine learning-based theorem-proving
strategies. We make this dataset publicly available under the BSD license. We
propose various machine learning tasks that can be performed on this dataset,
and discuss their significance for theorem proving. We also benchmark a set of
simple baseline machine learning models suited for the tasks (including
logistic regression, convolutional neural networks and recurrent neural
networks). The results of our baseline models show the promise of applying
machine learning to HOL theorem proving.

    ",Artificial Intelligence (cs.AI),
Convergence Analysis of Machine Learning Algorithms for the Numerical Solution of Mean Field Control and Games: II -- The Finite Horizon Case,"  We propose two numerical methods for the optimal control of McKean-Vlasov
dynamics in finite time horizon. Both methods are based on the introduction of
a suitable loss function defined over the parameters of a neural network. This
allows the use of machine learning tools, and efficient implementations of
stochastic gradient descent in order to perform the optimization. In the first
method, the loss function stems directly from the optimal control problem. The
second method tackles a generic forward-backward stochastic differential
equation system (FBSDE) of McKean-Vlasov type, and relies on suitable
reformulation as a mean field control problem. To provide a guarantee on how
our numerical schemes approximate the solution of the original mean field
control problem, we introduce a new optimization problem, directly amenable to
numerical computation, and for which we rigorously provide an error rate.
Several numerical examples are provided. Both methods can easily be applied to
certain problems with common noise, which is not the case with the existing
technology. Furthermore, although the first approach is designed for mean field
control problems, the second is more general and can also be applied to the
FBSDE arising in the theory of mean field games.

    ",Optimization and Control (math.OC),; Machine Learning (cs.LG); Numerical Analysis (math.NA)
Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules,"  Predicting the relationship between a molecule's structure and its odor
remains a difficult, decades-old task. This problem, termed quantitative
structure-odor relationship (QSOR) modeling, is an important challenge in
chemistry, impacting human nutrition, manufacture of synthetic fragrance, the
environment, and sensory neuroscience. We propose the use of graph neural
networks for QSOR, and show they significantly out-perform prior methods on a
novel data set labeled by olfactory experts. Additional analysis shows that the
learned embeddings from graph neural networks capture a meaningful odor space
representation of the underlying relationship between structure and odor, as
demonstrated by strong performance on two challenging transfer learning tasks.
Machine learning has already had a large impact on the senses of sight and
sound. Based on these early results with graph neural networks for molecular
properties, we hope machine learning can eventually do for olfaction what it
has already done for vision and hearing.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)
A Machine Learning Approach For Opinion Holder Extraction In Arabic Language,"  Opinion mining aims at extracting useful subjective information from reliable
amounts of text. Opinion mining holder recognition is a task that has not been
considered yet in Arabic Language. This task essentially requires deep
understanding of clauses structures. Unfortunately, the lack of a robust,
publicly available, Arabic parser further complicates the research. This paper
presents a leading research for the opinion holder extraction in Arabic news
independent from any lexical parsers. We investigate constructing a
comprehensive feature set to compensate the lack of parsing structural
outcomes. The proposed feature set is tuned from English previous works coupled
with our proposed semantic field and named entities features. Our feature
analysis is based on Conditional Random Fields (CRF) and semi-supervised
pattern recognition techniques. Different research models are evaluated via
cross-validation experiments achieving 54.03 F-measure. We publicly release our
own research outcome corpus and lexicon for opinion mining community to
encourage further research.

    ",Information Retrieval (cs.IR),; Machine Learning (cs.LG)
An Overview of Privacy in Machine Learning,"  Over the past few years, providers such as Google, Microsoft, and Amazon have
started to provide customers with access to software interfaces allowing them
to easily embed machine learning tasks into their applications. Overall,
organizations can now use Machine Learning as a Service (MLaaS) engines to
outsource complex tasks, e.g., training classifiers, performing predictions,
clustering, etc. They can also let others query models trained on their data.
Naturally, this approach can also be used (and is often advocated) in other
contexts, including government collaborations, citizen science projects, and
business-to-business partnerships. However, if malicious users were able to
recover data used to train these models, the resulting information leakage
would create serious issues. Likewise, if the inner parameters of the model are
considered proprietary information, then access to the model should not allow
an adversary to learn such parameters. In this document, we set to review
privacy challenges in this space, providing a systematic review of the relevant
research literature, also exploring possible countermeasures. More
specifically, we provide ample background information on relevant concepts
around machine learning and privacy. Then, we discuss possible adversarial
models and settings, cover a wide range of attacks that relate to private
and/or sensitive information leakage, and review recent results attempting to
defend against such attacks. Finally, we conclude with a list of open problems
that require more work, including the need for better evaluations, more
targeted defenses, and the study of the relation to policy and data protection
efforts.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computers and Society (cs.CY); Machine Learning (stat.ML)
Privacy-preserving Machine Learning through Data Obfuscation,"  As machine learning becomes a practice and commodity, numerous cloud-based
services and frameworks are provided to help customers develop and deploy
machine learning applications. While it is prevalent to outsource model
training and serving tasks in the cloud, it is important to protect the privacy
of sensitive samples in the training dataset and prevent information leakage to
untrusted third parties. Past work have shown that a malicious machine learning
service provider or end user can easily extract critical information about the
training samples, from the model parameters or even just model outputs.
",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning,"  We present Placeto, a reinforcement learning (RL) approach to efficiently
find device placements for distributed neural network training. Unlike prior
approaches that only find a device placement for a specific computation graph,
Placeto can learn generalizable device placement policies that can be applied
to any graph. We propose two key ideas in our approach: (1) we represent the
policy as performing iterative placement improvements, rather than outputting a
placement in one shot; (2) we use graph embeddings to capture relevant
information about the structure of the computation graph, without relying on
node labels for indexing. These ideas allow Placeto to train efficiently and
generalize to unseen graphs. Our experiments show that Placeto requires up to
6.1x fewer training steps to find placements that are on par with or better
than the best placements found by prior approaches. Moreover, Placeto is able
to learn a generalizable placement policy for any given family of graphs, which
can then be used without any retraining to predict optimized placements for
unseen graphs from the same family. This eliminates the large overhead incurred
by prior RL approaches whose lack of generalizability necessitates re-training
from scratch every time a new graph is to be placed.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)"
hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices,"  Accessible machine learning algorithms, software, and diagnostic tools for
energy-efficient devices and systems are extremely valuable across a broad
range of application domains. In scientific domains, real-time near-sensor
processing can drastically improve experimental design and accelerate
scientific discoveries. To support domain scientists, we have developed hls4ml,
an open-source software-hardware codesign workflow to interpret and translate
machine learning algorithms for implementation with both FPGA and ASIC
technologies. We expand on previous hls4ml work by extending capabilities and
techniques towards low-power implementations and increased usability: new
Python APIs, quantization-aware pruning, end-to-end FPGA workflows, long
pipeline kernels for low power, and new device backends include an ASIC
workflow. Taken together, these and continued efforts in hls4ml will arm a new
generation of domain scientists with accessible, efficient, and powerful tools
for machine-learning-accelerated discovery.

    ",Machine Learning (cs.LG),; Hardware Architecture (cs.AR); Instrumentation and Detectors (physics.ins-det)
Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems,"  Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.

    ",Cryptography and Security (cs.CR),; Computer Vision and Pattern Recognition (cs.CV)
TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning,"  TF.Learn is a high-level Python module for distributed machine learning
inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to
simplify the process of creating, configuring, training, evaluating, and
experimenting a machine learning model. TF.Learn integrates a wide range of
state-of-art machine learning algorithms built on top of TensorFlow's low level
APIs for small to large-scale supervised and unsupervised problems. This module
focuses on bringing machine learning to non-specialists using a general-purpose
high-level language as well as researchers who want to implement, benchmark,
and compare their new methods in a structured environment. Emphasis is put on
ease of use, performance, documentation, and API consistency.

    ","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG)
Benchmarking Automatic Machine Learning Frameworks,"  AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
Benchmarking Machine Learning Technologies for Software Defect Detection,"  Machine Learning approaches are good in solving problems that have less
information. In most cases, the software domain problems characterize as a
process of learning that depend on the various circumstances and changes
accordingly. A predictive model is constructed by using machine learning
approaches and classified them into defective and non-defective modules.
Machine learning techniques help developers to retrieve useful information
after the classification and enable them to analyse data from different
perspectives. Machine learning techniques are proven to be useful in terms of
software bug prediction. This study used public available data sets of software
modules and provides comparative performance analysis of different machine
learning techniques for software bug prediction. Results showed most of the
machine learning methods performed well on software bug datasets.

    ",Software Engineering (cs.SE),
Co-Creative Level Design via Machine Learning,"  Procedural Level Generation via Machine Learning (PLGML), the study of
generating game levels with machine learning, has received a large amount of
recent academic attention. For certain measures these approaches have shown
success at replicating the quality of existing game levels. However, it is
unclear the extent to which they might benefit human designers. In this paper
we present a framework for co-creative level design with a PLGML agent. In
support of this framework we present results from a user study and results from
a comparative study of PLGML approaches.

    ",Artificial Intelligence (cs.AI),; Machine Learning (cs.LG)
The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey,"  Machine learning models have made many decision support systems to be faster,
more accurate and more efficient. However, applications of machine learning in
network security face more disproportionate threat of active adversarial
attacks compared to other domains. This is because machine learning
applications in network security such as malware detection, intrusion
detection, and spam filtering are by themselves adversarial in nature. In what
could be considered an arms race between attackers and defenders, adversaries
constantly probe machine learning systems with inputs which are explicitly
designed to bypass the system and induce a wrong prediction. In this survey, we
first provide a taxonomy of machine learning techniques, styles, and
algorithms. We then introduce a classification of machine learning in network
security applications. Next, we examine various adversarial attacks against
machine learning in network security and introduce two classification
approaches for adversarial attacks in network security. First, we classify
adversarial attacks in network security based on a taxonomy of network security
applications. Secondly, we categorize adversarial attacks in network security
into a problem space vs. feature space dimensional classification model. We
then analyze the various defenses against adversarial attacks on machine
learning-based network security applications. We conclude by introducing an
adversarial risk model and evaluate several existing adversarial attacks
against machine learning in network security using the risk model. We also
identify where each attack classification resides within the adversarial risk
model

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)
A Hierarchy of Limitations in Machine Learning,"  ""All models are wrong, but some are useful"", wrote George E. P. Box (1979).
Machine learning has focused on the usefulness of probability models for
prediction in social systems, but is only now coming to grips with the ways in
which these models are wrong---and the consequences of those shortcomings. This
paper attempts a comprehensive, structured overview of the specific conceptual,
procedural, and statistical limitations of models in machine learning when
applied to society. Machine learning modelers themselves can use the described
hierarchy to identify possible failure points and think through how to address
them, and consumers of machine learning models can know what to question when
confronted with the decision about if, where, and how to apply machine
learning. The limitations go from commitments inherent in quantification
itself, through to showing how unmodeled dependencies can lead to
cross-validation being overly optimistic as a way of assessing model
performance.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)
Detecting Fake News Using Machine Learning : A Systematic Literature Review,"  Internet is one of the important inventions and a large number of persons are
its users. These persons use this for different purposes. There are different
social media platforms that are accessible to these users. Any user can make a
post or spread the news through the online platforms. These platforms do not
verify the users or their posts. So some of the users try to spread fake news
through these platforms. These news can be propaganda against an individual,
society, organization or political party. A human being is unable to detect all
these fake news. So there is a need for machine learning classifiers that can
detect these fake news automatically. Use of machine learning classifiers for
detecting fake news is described in this systematic literature review.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG)
ARDA: Automatic Relational Data Augmentation for Machine Learning,"  Automatic machine learning (\AML) is a family of techniques to automate the
process of training predictive models, aiming to both improve performance and
make machine learning more accessible. While many recent works have focused on
aspects of the machine learning pipeline like model selection, hyperparameter
tuning, and feature selection, relatively few works have focused on automatic
data augmentation. Automatic data augmentation involves finding new features
relevant to the user's predictive task with minimal ``human-in-the-loop''
involvement.
",Machine Learning (cs.LG),; Databases (cs.DB); Machine Learning (stat.ML)
Advances in quantum machine learning,"  Here we discuss advances in the field of quantum machine learning. The
following document offers a hybrid discussion; both reviewing the field as it
is currently, and suggesting directions for further research. We include both
algorithms and experimental implementations in the discussion. The field's
outlook is generally positive, showing significant promise. However, we believe
there are appreciable hurdles to overcome before one can claim that it is a
primary application of quantum computation.

    ",Quantum Physics (quant-ph),
AGL: a Scalable System for Industrial-purpose Graph Machine Learning,"  Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
",Social and Information Networks (cs.SI),
Secure Computation for Machine Learning With SPDZ,"  Secure Multi-Party Computation (MPC) is an area of cryptography that enables
computation on sensitive data from multiple sources while maintaining privacy
guarantees. However, theoretical MPC protocols often do not scale efficiently
to real-world data. This project investigates the efficiency of the SPDZ
framework, which provides an implementation of an MPC protocol with malicious
security, in the context of popular machine learning (ML) algorithms. In
particular, we chose applications such as linear regression and logistic
regression, which have been implemented and evaluated using semi-honest MPC
techniques. We demonstrate that the SPDZ framework outperforms these previous
implementations while providing stronger security.

    ",Cryptography and Security (cs.CR),
A review of homomorphic encryption and software tools for encrypted statistical machine learning,"  Recent advances in cryptography promise to enable secure statistical
computation on encrypted data, whereby a limited set of operations can be
carried out without the need to first decrypt. We review these homomorphic
encryption schemes in a manner accessible to statisticians and machine
learners, focusing on pertinent limitations inherent in the current state of
the art. These limitations restrict the kind of statistics and machine learning
algorithms which can be implemented and we review those which have been
successfully applied in the literature. Finally, we document a high performance
R package implementing a recent homomorphic scheme in a general framework.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG)
Learning perturbation sets for robust machine learning,"  Although much progress has been made towards robust deep learning, a
significant gap in robustness remains between real-world perturbations and more
narrowly defined sets typically studied in adversarial defenses. In this paper,
we aim to bridge this gap by learning perturbation sets from data, in order to
characterize real-world effects for robust training and evaluation.
Specifically, we use a conditional generator that defines the perturbation set
over a constrained region of the latent space. We formulate desirable
properties that measure the quality of a learned perturbation set, and
theoretically prove that a conditional variational autoencoder naturally
satisfies these criteria. Using this framework, our approach can generate a
variety of perturbations at different complexities and scales, ranging from
baseline spatial transformations, through common image corruptions, to lighting
variations. We measure the quality of our learned perturbation sets both
quantitatively and qualitatively, finding that our models are capable of
producing a diverse set of meaningful perturbations beyond the limited data
seen during training. Finally, we leverage our learned perturbation sets to
train models which are empirically and certifiably robust to adversarial image
corruptions and adversarial lighting variations, while improving generalization
on non-adversarial data. All code and configuration files for reproducing the
experiments as well as pretrained model weights can be found at
",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Music Genre Classification using Machine Learning Techniques,"  Categorizing music files according to their genre is a challenging task in
the area of music information retrieval (MIR). In this study, we compare the
performance of two classes of models. The first is a deep learning approach
wherein a CNN model is trained end-to-end, to predict the genre label of an
audio signal, solely using its spectrogram. The second approach utilizes
hand-crafted features, both from the time domain and the frequency domain. We
train four traditional machine learning classifiers with these features and
compare their performance. The features that contribute the most towards this
multi-class classification task are identified. The experiments are conducted
on the Audio set data set and we report an AUC value of 0.894 for an ensemble
classifier which combines the two proposed approaches.

    ",Sound (cs.SD),; Audio and Speech Processing (eess.AS)
The Bach Doodle: Approachable music composition with machine learning at scale,"  To make music composition more approachable, we designed the first AI-powered
Google Doodle, the Bach Doodle, where users can create their own melody and
have it harmonized by a machine learning model Coconet (Huang et al., 2017) in
the style of Bach. For users to input melodies, we designed a simplified
sheet-music based interface. To support an interactive experience at scale, we
re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the
browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise
separable convolutions and fusing operations. We also reduced the model
download size to approximately 400KB through post-training weight quantization.
We calibrated a speed test based on partial model evaluation time to determine
if the harmonization request should be performed locally or sent to remote TPU
servers. In three days, people spent 350 years worth of time playing with the
Bach Doodle, and Coconet received more than 55 million queries. Users could
choose to rate their compositions and contribute them to a public dataset,
which we are releasing with this paper. We hope that the community finds this
dataset useful for applications ranging from ethnomusicological studies, to
music education, to improving machine learning models.

    ",Sound (cs.SD),; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)
Development of a Machine-Learning System to Classify Lung CT Scan Images into Normal/COVID-19 Class,"  Recently, the lung infection due to Coronavirus Disease (COVID-19) affected a
large human group worldwide and the assessment of the infection rate in the
lung is essential for treatment planning. This research aims to propose a
Machine-Learning-System (MLS) to detect the COVID-19 infection using the CT
scan Slices (CTS). This MLS implements a sequence of methods, such as
multi-thresholding, image separation using threshold filter,
feature-extraction, feature-selection, feature-fusion and classification. The
initial part implements the Chaotic-Bat-Algorithm and Kapur's Entropy (CBA+KE)
thresholding to enhance the CTS. The threshold filter separates the image into
two segments based on a chosen threshold 'Th'. The texture features of these
images are extracted, refined and selected using the chosen procedures.
Finally, a two-class classifier system is implemented to categorize the chosen
CTS (n=500 with a pixel dimension of 512x512x1) into normal/COVID-19 group. In
this work, the classifiers, such as Naive Bayes (NB), k-Nearest Neighbors
(KNN), Decision Tree (DT), Random Forest (RF) and Support Vector Machine with
linear kernel (SVM) are implemented and the classification task is performed
using various feature vectors. The experimental outcome of the SVM with
Fused-Feature-Vector (FFV) helped to attain a detection accuracy of 89.80%.

    ",Image and Video Processing (eess.IV),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Importance of Tuning Hyperparameters of Machine Learning Algorithms,"  The performance of many machine learning algorithms depends on their
hyperparameter settings. The goal of this study is to determine whether it is
important to tune a hyperparameter or whether it can be safely set to a default
value. We present a methodology to determine the importance of tuning a
hyperparameter based on a non-inferiority test and tuning risk: the performance
loss that is incurred when a hyperparameter is not tuned, but set to a default
value. Because our methods require the notion of a default parameter, we
present a simple procedure that can be used to determine reasonable default
parameters. We apply our methods in a benchmark study using 59 datasets from
OpenML. Our results show that leaving particular hyperparameters at their
default value is non-inferior to tuning these hyperparameters. In some cases,
leaving the hyperparameter at its default value even outperforms tuning it
using a search procedure with a limited number of iterations.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
What's Sex Got To Do With Fair Machine Learning?,"  Debate about fairness in machine learning has largely centered around
competing definitions of what fairness or nondiscrimination between groups
requires. However, little attention has been paid to what precisely a group is.
Many recent approaches to ""fairness"" require one to specify a causal model of
the data generating process. These exercises make an implicit ontological
assumption that a racial or sex group is simply a collection of individuals who
share a given trait. We show this by exploring the formal assumption of
modularity in causal models, which holds that the dependencies captured by one
causal pathway are invariant to interventions on any other pathways. Causal
models of sex propose two substantive claims: 1) There exists a feature,
sex-on-its-own, that is an inherent trait of an individual that causally brings
about social phenomena external to it in the world; and 2) the relations
between sex and its effects can be modified in whichever ways and the former
feature would still retain the meaning that sex has in our world. We argue that
this ontological picture is false. Many of the ""effects"" that sex purportedly
""causes"" are in fact constitutive features of sex as a social status. They give
the social meaning of sex features, meanings that are precisely what make sex
discrimination a distinctively morally problematic type of action. Correcting
this conceptual error has a number of implications for how models can be used
to detect discrimination. Formal diagrams of constitutive relations present an
entirely different path toward reasoning about discrimination. Whereas causal
diagrams guide the construction of sophisticated modular counterfactuals,
constitutive diagrams identify a different kind of counterfactual as central to
an inquiry on discrimination: one that asks how the social meaning of a group
would be changed if its non-modular features were altered.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Machine Learning in Astronomy: a practical overview,"  Astronomy is experiencing a rapid growth in data size and complexity. This
change fosters the development of data-driven science as a useful companion to
the common model-driven data analysis paradigm, where astronomers develop
automatic tools to mine datasets and extract novel information from them. In
recent years, machine learning algorithms have become increasingly popular
among astronomers, and are now used for a wide variety of tasks. In light of
these developments, and the promise and challenges associated with them, the
IAC Winter School 2018 focused on big data in Astronomy, with a particular
emphasis on machine learning and deep learning techniques. This document
summarizes the topics of supervised and unsupervised learning algorithms
presented during the school, and provides practical information on the
application of such tools to astronomical datasets. In this document I cover
basic topics in supervised machine learning, including selection and
preprocessing of the input dataset, evaluation methods, and three popular
supervised learning algorithms, Support Vector Machines, Random Forests, and
shallow Artificial Neural Networks. My main focus is on unsupervised machine
learning algorithms, that are used to perform cluster analysis, dimensionality
reduction, visualization, and outlier detection. Unsupervised learning
algorithms are of particular importance to scientific research, since they can
be used to extract new knowledge from existing datasets, and can facilitate new
discoveries.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),
Cascade Adversarial Machine Learning Regularized with a Unified Embedding,"  Injecting adversarial examples during training, known as adversarial
training, can improve robustness against one-step attacks, but not for unknown
iterative attacks. To address this challenge, we first show iteratively
generated adversarial images easily transfer between networks trained with the
same strategy. Inspired by this observation, we propose cascade adversarial
training, which transfers the knowledge of the end results of adversarial
training. We train a network from scratch by injecting iteratively generated
adversarial images crafted from already defended networks in addition to
one-step adversarial images from the network being trained. We also propose to
utilize embedding space for both classification and low-level (pixel-level)
similarity learning to ignore unknown pixel level perturbation. During
training, we inject adversarial images without replacing their corresponding
clean images and penalize the distance between the two embeddings (clean and
adversarial). Experimental results show that cascade adversarial training
together with our proposed low-level similarity learning efficiently enhances
the robustness against iterative attacks, but at the expense of decreased
robustness against one-step attacks. We show that combining those two
techniques can also improve robustness under the worst case black box attack
scenario.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG)
Persistent-Homology-based Machine Learning and its Applications -- A Survey,"  A suitable feature representation that can both preserve the data intrinsic
information and reduce data complexity and dimensionality is key to the
performance of machine learning models. Deeply rooted in algebraic topology,
persistent homology (PH) provides a delicate balance between data
simplification and intrinsic structure characterization, and has been applied
to various areas successfully. However, the combination of PH and machine
learning has been hindered greatly by three challenges, namely topological
representation of data, PH-based distance measurements or metrics, and PH-based
feature representation. With the development of topological data analysis,
progresses have been made on all these three problems, but widely scattered in
different literatures. In this paper, we provide a systematical review of PH
and PH-based supervised and unsupervised models from a computational
perspective. Our emphasizes are the recent development of mathematical models
and tools, including PH softwares and PH-based functions, feature
representations, kernels, and similarity models. Essentially, this paper can
work as a roadmap for the practical application of PH-based machine learning
tools. Further, we consider different topological feature representations in
different machine learning models, and investigate their impacts on the protein
secondary structure classification.

    ",Algebraic Topology (math.AT),
"Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus","  Recent literature has underscored the importance of dataset documentation
work for machine learning, and part of this work involves addressing
""documentation debt"" for datasets that have been used widely but documented
sparsely. This paper aims to help address documentation debt for BookCorpus, a
popular text dataset for training large language models. Notably, researchers
have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models,
even though little to no documentation exists about the dataset's motivation,
composition, collection process, etc. We offer a preliminary datasheet that
provides key context and information about BookCorpus, highlighting several
notable deficiencies. In particular, we find evidence that (1) BookCorpus
likely violates copyright restrictions for many books, (2) BookCorpus contains
thousands of duplicated books, and (3) BookCorpus exhibits significant skews in
genre representation. We also find hints of other potential deficiencies that
call for future research, including problematic content, potential skews in
religious representation, and lopsided author contributions. While more work
remains, this initial effort to provide a datasheet for BookCorpus adds to
growing literature that urges more careful and systematic documentation for
machine learning datasets.

    ",Computation and Language (cs.CL),; Computers and Society (cs.CY); Machine Learning (cs.LG)
"DALI: a large Dataset of synchronized Audio, LyrIcs and notes, automatically created using teacher-student machine learning paradigm","  The goal of this paper is twofold. First, we introduce DALI, a large and rich
multimodal dataset containing 5358 audio tracks with their time-aligned vocal
melody notes and lyrics at four levels of granularity. The second goal is to
explain our methodology where dataset creation and learning models interact
using a teacher-student machine learning paradigm that benefits each other. We
start with a set of manual annotations of draft time-aligned lyrics and notes
made by non-expert users of Karaoke games. This set comes without audio.
Therefore, we need to find the corresponding audio and adapt the annotations to
it. To that end, we retrieve audio candidates from the Web. Each candidate is
then turned into a singing-voice probability over time using a teacher, a deep
convolutional neural network singing-voice detection system (SVD), trained on
cleaned data. Comparing the time-aligned lyrics and the singing-voice
probability, we detect matches and update the time-alignment lyrics
accordingly. From this, we obtain new audio sets. They are then used to train
new SVD students used to perform again the above comparison. The process could
be repeated iteratively. We show that this allows to progressively improve the
performances of our SVD and get better audio-matching and alignment.

    ",Audio and Speech Processing (eess.AS),; Databases (cs.DB); Machine Learning (cs.LG); Sound (cs.SD)
Machine Learning Based Student Grade Prediction: A Case Study,"  In higher educational institutes, many students have to struggle hard to
complete different courses since there is no dedicated support offered to
students who need special attention in the registered courses. Machine learning
techniques can be utilized for students' grades prediction in different
courses. Such techniques would help students to improve their performance based
on predicted grades and would enable instructors to identify such individuals
who might need assistance in the courses. In this paper, we use Collaborative
Filtering (CF), Matrix Factorization (MF), and Restricted Boltzmann Machines
(RBM) techniques to systematically analyze a real-world data collected from
Information Technology University (ITU), Lahore, Pakistan. We evaluate the
academic performance of ITU students who got admission in the bachelor's degree
program in ITU's Electrical Engineering department. The RBM technique is found
to be better than the other techniques used in predicting the students'
performance in the particular course.

    ",Computers and Society (cs.CY),
Equity forecast: Predicting long term stock price movement using machine learning,"  Long term investment is one of the major investment strategies. However,
calculating intrinsic value of some company and evaluating shares for long term
investment is not easy, since analyst have to care about a large number of
financial indicators and evaluate them in a right manner. So far, little help
in predicting the direction of the company value over the longer period of time
has been provided from the machines. In this paper we present a machine
learning aided approach to evaluate the equity's future price over the long
time. Our method is able to correctly predict whether some company's value will
be 10% higher or not over the period of one year in 76.5% of cases.

    ",Machine Learning (cs.LG),; General Finance (q-fin.GN)
Insights into Performance Fitness and Error Metrics for Machine Learning,"  Machine learning (ML) is the field of training machines to achieve high level
of cognition and perform human-like analysis. Since ML is a data-driven
approach, it seemingly fits into our daily lives and operations as well as
complex and interdisciplinary fields. With the rise of commercial, open-source
and user-catered ML tools, a key question often arises whenever ML is applied
to explore a phenomenon or a scenario: what constitutes a good ML model?
Keeping in mind that a proper answer to this question depends on a variety of
factors, this work presumes that a good ML model is one that optimally performs
and best describes the phenomenon on hand. From this perspective, identifying
proper assessment metrics to evaluate performance of ML models is not only
necessary but is also warranted. As such, this paper examines a number of the
most commonly-used performance fitness and error metrics for regression and
classification algorithms, with emphasis on engineering applications.

    ",Machine Learning (cs.LG),"; Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)"
NSML: A Machine Learning Platform That Enables You to Focus on Your Models,"  Machine learning libraries such as TensorFlow and PyTorch simplify model
implementation. However, researchers are still required to perform a
non-trivial amount of manual tasks such as GPU allocation, training status
tracking, and comparison of models with different hyperparameter settings. We
propose a system to handle these tasks and help researchers focus on models. We
present the requirements of the system based on a collection of discussions
from an online study group comprising 25k members. These include automatic GPU
allocation, learning status visualization, handling model parameter snapshots
as well as hyperparameter modification during learning, and comparison of
performance metrics between models via a leaderboard. We describe the system
architecture that fulfills these requirements and present a proof-of-concept
implementation, NAVER Smart Machine Learning (NSML). We test the system and
confirm substantial efficiency improvements for model development.

    ",Machine Learning (cs.LG),"; Distributed, Parallel, and Cluster Computing (cs.DC)"
CausalML: Python Package for Causal Machine Learning,"  CausalML is a Python implementation of algorithms related to causal inference
and machine learning. Algorithms combining causal inference and machine
learning have been a trending topic in recent years. This package tries to
bridge the gap between theoretical work on methodology and practical
applications by making a collection of methods in this field available in
Python. This paper introduces the key concepts, scope, and use cases of this
package.

    ",Computers and Society (cs.CY),; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)
Double Debiased Machine Learning Nonparametric Inference with Continuous Treatments,"  We propose a nonparametric inference method for causal effects of continuous
treatment variables, under unconfoundedness and nonparametric or
high-dimensional nuisance parameters. Our double debiased machine learning
(DML) estimators for the average dose-response function (or the average
structural function) and the partial effects are asymptotically normal with
nonparametric convergence rates. The nuisance estimators for the conditional
expectation function and the conditional density can be nonparametric or ML
methods. Utilizing a kernel-based doubly robust moment function and
cross-fitting, we give high-level conditions under which the nuisance
estimators do not affect the first-order large sample distribution of the DML
estimators. We further provide sufficient low-level conditions for kernel,
series, and deep neural networks. We propose a data-driven bandwidth to
consistently estimate the optimal bandwidth that minimizes the asymptotic mean
squared error. We justify the use of kernel to localize the continuous
treatment at a given value by the Gateaux derivative. We implement various ML
methods in Monte Carlo simulations and an empirical application on a job
training program evaluation.

    ",Econometrics (econ.EM),
Using Visual Analytics to Interpret Predictive Machine Learning Models,"  It is commonly believed that increasing the interpretability of a machine
learning model may decrease its predictive power. However, inspecting
input-output relationships of those models using visual analytics, while
treating them as black-box, can help to understand the reasoning behind
outcomes without sacrificing predictive quality. We identify a space of
possible solutions and provide two examples of where such techniques have been
successfully used in practice.

    ",Machine Learning (stat.ML),; Machine Learning (cs.LG)
A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,"  The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.
",Machine Learning (stat.ML),; Machine Learning (cs.LG)
A Topology Layer for Machine Learning,"  Topology applied to real world data using persistent homology has started to
find applications within machine learning, including deep learning. We present
a differentiable topology layer that computes persistent homology based on
level set filtrations and edge-based filtrations. We present three novel
applications: the topological layer can (i) regularize data reconstruction or
the weights of machine learning models, (ii) construct a loss on the output of
a deep generative network to incorporate topological priors, and (iii) perform
topological adversarial attacks on deep networks trained with persistence
features. The code (",Machine Learning (cs.LG),; Algebraic Topology (math.AT); Machine Learning (stat.ML)
Machine Learning Explainability for External Stakeholders,"  As machine learning is increasingly deployed in high-stakes contexts
affecting people's livelihoods, there have been growing calls to open the black
box and to make machine learning algorithms more explainable. Providing useful
explanations requires careful consideration of the needs of stakeholders,
including end-users, regulators, and domain experts. Despite this need, little
work has been done to facilitate inter-stakeholder conversation around
explainable machine learning. To help address this gap, we conducted a
closed-door, day-long workshop between academics, industry experts, legal
scholars, and policymakers to develop a shared language around explainability
and to understand the current shortcomings of and potential solutions for
deploying explainable machine learning in service of transparency goals. We
also asked participants to share case studies in deploying explainable machine
learning at scale. In this paper, we provide a short summary of various case
studies of explainable machine learning, lessons from those studies, and
discuss open challenges.

    ",Computers and Society (cs.CY),; Artificial Intelligence (cs.AI)
On the Existence of Simpler Machine Learning Models,"  It is almost always easier to find an accurate-but-complex model than an
accurate-yet-simple model. Finding optimal, sparse, accurate models of various
forms (linear models with integer coefficients, decision sets, rule lists,
decision trees) is generally NP-hard. We often do not know whether the search
for a simpler model will be worthwhile, and thus we do not go to the trouble of
searching for one. In this work, we ask an important practical question: can
accurate-yet-simple models be proven to exist, or shown likely to exist, before
explicitly searching for them? We hypothesize that there is an important reason
that simple-yet-accurate models often do exist. This hypothesis is that the
size of the Rashomon set is often large, where the Rashomon set is the set of
almost-equally-accurate models from a function class. If the Rashomon set is
large, it contains numerous accurate models, and perhaps at least one of them
is the simple model we desire. In this work, we formally present the Rashomon
ratio as a new gauge of simplicity for a learning problem, depending on a
function class and a data set. The Rashomon ratio is the ratio of the volume of
the set of accurate models to the volume of the hypothesis space, and it is
different from standard complexity measures from statistical learning theory.
Insight from studying the Rashomon ratio provides an easy way to check whether
a simpler model might exist for a problem before finding it, namely whether
several different machine learning methods achieve similar performance on the
data. In that sense, the Rashomon ratio is a powerful tool for understanding
why and when an accurate-yet-simple model might exist. If, as we hypothesize in
this work, many real-world data sets admit large Rashomon sets, the
implications are vast: it means that simple or interpretable models may often
be used for high-stakes decisions without losing accuracy.

    ",Machine Learning (cs.LG),; Machine Learning (stat.ML)
Revealing the Autonomous System Taxonomy: The Machine Learning Approach,"  Although the Internet AS-level topology has been extensively studied over the
past few years, little is known about the details of the AS taxonomy. An AS
""node"" can represent a wide variety of organizations, e.g., large ISP, or small
private business, university, with vastly different network characteristics,
external connectivity patterns, network growth tendencies, and other properties
that we can hardly neglect while working on veracious Internet representations
in simulation environments. In this paper, we introduce a radically new
approach based on machine learning techniques to map all the ASes in the
Internet into a natural AS taxonomy. We successfully classify 95.3% of ASes
with expected accuracy of 78.1%. We release to the community the AS-level
topology dataset augmented with: 1) the AS taxonomy information and 2) the set
of AS attributes we used to classify ASes. We believe that this dataset will
serve as an invaluable addition to further understanding of the structure and
evolution of the Internet.

    ",Networking and Internet Architecture (cs.NI),; Machine Learning (cs.LG)
Encrypted statistical machine learning: new privacy preserving methods,"  We present two new statistical machine learning methods designed to learn on
fully homomorphic encrypted (FHE) data. The introduction of FHE schemes
following Gentry (2009) opens up the prospect of privacy preserving statistical
machine learning analysis and modelling of encrypted data without compromising
security constraints. We propose tailored algorithms for applying extremely
random forests, involving a new cryptographic stochastic fraction estimator,
and naÃ¯ve Bayes, involving a semi-parametric model for the class decision
boundary, and show how they can be used to learn and predict from encrypted
data. We demonstrate that these techniques perform competitively on a variety
of classification data sets and provide detailed information about the
computational practicalities of these and other FHE methods.

    ",Machine Learning (stat.ML),; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME)
Machine Learning in Artificial Intelligence: Towards a Common Understanding,"  The application of ""machine learning"" and ""artificial intelligence"" has
become popular within the last decade. Both terms are frequently used in
science and media, sometimes interchangeably, sometimes with different
meanings. In this work, we aim to clarify the relationship between these terms
and, in particular, to specify the contribution of machine learning to
artificial intelligence. We review relevant literature and present a conceptual
framework which clarifies the role of machine learning to build (artificial)
intelligent agents. Hence, we seek to provide more terminological clarity and a
starting point for (interdisciplinary) discussions and future research.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI)
Motion Planning and Control for Mobile Robot Navigation Using Machine Learning: a Survey,"  Moving in complex environments is an essential capability of intelligent
mobile robots. Decades of research and engineering have been dedicated to
developing sophisticated navigation systems to move mobile robots from one
point to another. Despite their overall success, a recently emerging research
thrust is devoted to developing machine learning techniques to address the same
problem, based in large part on the success of deep learning. However, to date,
there has not been much direct comparison between the classical and emerging
paradigms to this problem. In this article, we survey recent works that apply
machine learning for motion planning and control in mobile robot navigation,
within the context of classical navigation systems. The surveyed works are
classified into different categories, which delineate the relationship of the
learning approaches to classical methods. Based on this classification, we
identify common challenges and promising future directions.

    ",Robotics (cs.RO),
"Declarative Recursive Computation on an RDBMS, or, Why You Should Use a Database For Distributed Machine Learning","  A number of popular systems, most notably Google's TensorFlow, have been
implemented from the ground up to support machine learning tasks. We consider
how to make a very small set of changes to a modern relational database
management system (RDBMS) to make it suitable for distributed learning
computations. Changes include adding better support for recursion, and
optimization and execution of very large compute plans. We also show that there
are key advantages to using an RDBMS as a machine learning platform. In
particular, learning based on a database management system allows for trivial
scaling to large data sets and especially large models, where different
computational units operate on different parts of a model that may be too large
to fit into RAM.

    ",Databases (cs.DB),; Machine Learning (cs.LG)
A Review of Machine Learning based Anomaly Detection Techniques,"  Intrusion detection is so much popular since the last two decades where
intrusion is attempted to break into or misuse the system. It is mainly of two
types based on the intrusions, first is Misuse or signature based detection and
the other is Anomaly detection. In this paper Machine learning based methods
which are one of the types of Anomaly detection techniques is discussed.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR)
Value-laden Disciplinary Shifts in Machine Learning,"  As machine learning models are increasingly used for high-stakes decision
making, scholars have sought to intervene to ensure that such models do not
encode undesirable social and political values. However, little attention thus
far has been given to how values influence the machine learning discipline as a
whole. How do values influence what the discipline focuses on and the way it
develops? If undesirable values are at play at the level of the discipline,
then intervening on particular models will not suffice to address the problem.
Instead, interventions at the disciplinary-level are required. This paper
analyzes the discipline of machine learning through the lens of philosophy of
science. We develop a conceptual framework to evaluate the process through
which types of machine learning models (e.g. neural networks, support vector
machines, graphical models) become predominant. The rise and fall of
model-types is often framed as objective progress. However, such disciplinary
shifts are more nuanced. First, we argue that the rise of a model-type is
self-reinforcing--it influences the way model-types are evaluated. For example,
the rise of deep learning was entangled with a greater focus on evaluations in
compute-rich and data-rich environments. Second, the way model-types are
evaluated encodes loaded social and political values. For example, a greater
focus on evaluations in compute-rich and data-rich environments encodes values
about centralization of power, privacy, and environmental concerns.

    ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning,"  SpiNNaker is an ARM-based processor platform optimized for the simulation of
spiking neural networks. This brief describes the roadmap in going from the
current SPINNaker1 system, a 1 Million core machine in 130nm CMOS, to
SpiNNaker2, a 10 Million core machine in 22nm FDSOI. Apart from pure scaling,
we will take advantage of specific technology features, such as runtime
adaptive body biasing, to deliver cutting-edge power consumption. Power
management of the cores allows a wide range of workload adaptivity, i.e.
processor power scales with the complexity and activity of the spiking network.
Additional numerical accelerators will enhance the utility of SpiNNaker2 for
simulation of spiking neural networks as well as for executing conventional
deep neural networks. These measures should increase the simulation capacity of
the machine by a factor $>$50. The interplay between the two domains, i.e.
spiking and rate based, will provide an interesting field for algorithm
exploration on SpiNNaker2. Apart from the platforms' traditional usage as a
neuroscience exploration tool, the extended functionality opens up new
application areas such as automotive AI, tactile internet, industry 4.0 and
biomedical processing.

    ",Emerging Technologies (cs.ET),
Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients,"  The nature of clinical data makes it difficult to quickly select, tune and
apply machine learning algorithms to clinical prognosis. As a result, a lot of
time is spent searching for the most appropriate machine learning algorithms
applicable in clinical prognosis that contains either binary-valued or
multi-valued attributes. The study set out to identify and evaluate the
performance of machine learning classification schemes applied in clinical
prognosis of post-operative life expectancy in the lung cancer patients.
Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train
and test models on Thoracic Surgery datasets obtained from the University of
California Irvine machine learning repository. Stratified 10-fold
cross-validation was used to evaluate baseline performance accuracy of the
classifiers. The comparative analysis shows that multilayer perceptron
performed best with classification accuracy of 82.3%, J48 came out second with
classification accuracy of 81.8%, and Naive Bayes came out the worst with
classification accuracy of 74.4%. The quality and outcome of the chosen machine
learning algorithms depends on the ingenuity of the clinical miner.

    ",Machine Learning (cs.LG),
Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning,"  Federated Learning is the current state of the art in supporting secure
multi-party machine learning (ML): data is maintained on the owner's device and
the updates to the model are aggregated through a secure protocol. However,
this process assumes a trusted centralized infrastructure for coordination, and
clients must trust that the central service does not use the byproducts of
client data. In addition to this, a group of malicious clients could also harm
the performance of the model by carrying out a poisoning attack.
",Machine Learning (cs.LG),"; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)"
Generalization in quantum machine learning from few training data,"  Modern quantum machine learning (QML) methods involve variationally
optimizing a parameterized quantum circuit on a training data set, and
subsequently making predictions on a testing data set (i.e., generalizing). In
this work, we provide a comprehensive study of generalization performance in
QML after training on a limited number $N$ of training data points. We show
that the generalization error of a quantum machine learning model with $T$
trainable gates scales at worst as $\sqrt{T/N}$. When only $K \ll T$ gates have
undergone substantial change in the optimization process, we prove that the
generalization error improves to $\sqrt{K / N}$. Our results imply that the
compiling of unitaries into a polynomial number of native gates, a crucial
application for the quantum computing industry that typically uses
exponential-size training data, can be sped up significantly. We also show that
classification of quantum states across a phase transition with a quantum
convolutional neural network requires only a very small training data set.
Other potential applications include learning quantum error correcting codes or
quantum dynamical simulation. Our work injects new hope into the field of QML,
as good generalization is guaranteed from few training data.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML)
BoostClean: Automated Error Detection and Repair for Machine Learning,"  Predictive models based on machine learning can be highly sensitive to data
error. Training data are often combined with a variety of different sources,
each susceptible to different types of inconsistencies, and new data streams
during prediction time, the model may encounter previously unseen
inconsistencies. An important class of such inconsistencies is domain value
violations that occur when an attribute value is outside of an allowed domain.
We explore automatically detecting and repairing such violations by leveraging
the often available clean test labels to determine whether a given detection
and repair combination will improve model accuracy. We present BoostClean which
automatically selects an ensemble of error detection and repair combinations
using statistical boosting. BoostClean selects this ensemble from an extensible
library that is pre-populated general detection functions, including a novel
detector based on the Word2Vec deep learning model, which detects errors across
a diverse set of domains. Our evaluation on a collection of 12 datasets from
Kaggle, the UCI repository, real-world data analyses, and production datasets
that show that Boost- Clean can increase absolute prediction accuracy by up to
9% over the best non-ensembled alternatives. Our optimizations including
parallelism, materialization, and indexing techniques show a 22.2x end-to-end
speedup on a 16-core machine.

    ",Databases (cs.DB),
The Role of Machine Learning in the Next Decade of Cosmology,"  In recent years, machine learning (ML) methods have remarkably improved how
cosmologists can interpret data. The next decade will bring new opportunities
for data-driven cosmological discovery, but will also present new challenges
for adopting ML methodologies and understanding the results. ML could transform
our field, but this transformation will require the astronomy community to both
foster and promote interdisciplinary research endeavors.

    ",Instrumentation and Methods for Astrophysics (astro-ph.IM),; Cosmology and Nongalactic Astrophysics (astro-ph.CO)
Machine Learning for AC Optimal Power Flow,"  We explore machine learning methods for AC Optimal Powerflow (ACOPF) - the
task of optimizing power generation in a transmission network according while
respecting physical and engineering constraints. We present two formulations of
ACOPF as a machine learning problem: 1) an end-to-end prediction task where we
directly predict the optimal generator settings, and 2) a constraint prediction
task where we predict the set of active constraints in the optimal solution. We
validate these approaches on two benchmark grids.

    ",Machine Learning (cs.LG),; Signal Processing (eess.SP); Machine Learning (stat.ML)
Scaling Datalog for Machine Learning on Big Data,"  In this paper, we present the case for a declarative foundation for
data-intensive machine learning systems. Instead of creating a new system for
each specific flavor of machine learning task, or hardcoding new optimizations,
we argue for the use of recursive queries to program a variety of machine
learning systems. By taking this approach, database query optimization
techniques can be utilized to identify effective execution plans, and the
resulting runtime plans can be executed on a single unified data-parallel query
processing engine. As a proof of concept, we consider two programming
models--Pregel and Iterative Map-Reduce-Update---from the machine learning
domain, and show how they can be captured in Datalog, tuned for a specific
task, and then compiled into an optimized physical plan. Experiments performed
on a large computing cluster with real data demonstrate that this declarative
approach can provide very good performance while offering both increased
generality and programming ease.

    ",Databases (cs.DB),; Machine Learning (cs.LG); Performance (cs.PF)
tf.data: A Machine Learning Data Processing Framework,"  Training machine learning models requires feeding input data for models to
ingest. Input pipelines for machine learning jobs are often challenging to
implement efficiently as they require reading large volumes of data, applying
complex transformations, and transferring data to hardware accelerators while
overlapping computation and communication to achieve optimal performance. We
present tf.data, a framework for building and executing efficient input
pipelines for machine learning jobs. The tf.data API provides operators which
can be parameterized with user-defined computation, composed, and reused across
different machine learning domains. These abstractions allow users to focus on
the application logic of data processing, while tf.data's runtime ensures that
pipelines run efficiently.
",Machine Learning (cs.LG),; Mathematical Software (cs.MS)
Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions,"  Applying popular machine learning algorithms to large amounts of data raised
new challenges for the ML practitioners. Traditional ML libraries does not
support well processing of huge datasets, so that new approaches were needed.
Parallelization using modern parallel computing frameworks, such as MapReduce,
CUDA, or Dryad gained in popularity and acceptance, resulting in new ML
libraries developed on top of these frameworks. We will briefly introduce the
most prominent industrial and academic outcomes, such as Apache Mahout,
GraphLab or Jubatus.
","Distributed, Parallel, and Cluster Computing (cs.DC)",; Machine Learning (cs.LG)
Opportunities in Machine Learning for Particle Accelerators,"  Machine learning (ML) is a subfield of artificial intelligence. The term
applies broadly to a collection of computational algorithms and techniques that
train systems from raw data rather than a priori models. ML techniques are now
technologically mature enough to be applied to particle accelerators, and we
expect that ML will become an increasingly valuable tool to meet new demands
for beam energy, brightness, and stability. The intent of this white paper is
to provide a high-level introduction to problems in accelerator science and
operation where incorporating ML-based approaches may provide significant
benefit. We review ML techniques currently being investigated at particle
accelerator facilities, and we place specific emphasis on active research
efforts and promising exploratory results. We also identify new applications
and discuss their feasibility, along with the required data and infrastructure
strategies. We conclude with a set of guidelines and recommendations for
laboratory managers and administrators, emphasizing the logistical and
technological requirements for successfully adopting this technology. This
white paper also serves as a summary of the discussion from a recent workshop
held at SLAC on ML for particle accelerators.

    ",Accelerator Physics (physics.acc-ph),
Predicting human decisions with behavioral theories and machine learning,"  Behavioral decision theories aim to explain human behavior. Can they help
predict it? An open tournament for prediction of human choices in fundamental
economic decision tasks is presented. The results suggest that integration of
certain behavioral theories as features in machine learning systems provides
the best predictions. Surprisingly, the most useful theories for prediction
build on basic properties of human and animal learning and are very different
from mainstream decision theories that focus on deviations from rational
choice. Moreover, we find that theoretical features should be based not only on
qualitative behavioral insights (e.g. loss aversion), but also on quantitative
behavioral foresights generated by functional descriptive models (e.g. Prospect
Theory). Our analysis prescribes a recipe for derivation of explainable, useful
predictions of human decisions.

    ",Artificial Intelligence (cs.AI),; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)
Tutorial: Safe and Reliable Machine Learning,"  This document serves as a brief overview of the ""Safe and Reliable Machine
Learning"" tutorial given at the 2019 ACM Conference on Fairness,
Accountability, and Transparency (FAT* 2019). The talk slides can be found
here: ",Machine Learning (cs.LG),; Artificial Intelligence (cs.AI)
TensorNetwork for Machine Learning,"  We demonstrate the use of tensor networks for image classification with the
TensorNetwork open source library. We explain in detail the encoding of image
data into a matrix product state form, and describe how to contract the network
in a way that is parallelizable and well-suited to automatic gradients for
optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
find out-of-the-box performance of 98% and 88% accuracy, respectively, using
the same tensor network architecture. The TensorNetwork library allows us to
seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
improvement in computational speed using a GPU.

    ",Machine Learning (cs.LG),; Strongly Correlated Electrons (cond-mat.str-el); Computer Vision and Pattern Recognition (cs.CV); Computational Physics (physics.comp-ph); Machine Learning (stat.ML)
A semi-agnostic ansatz with variable structure for quantum machine learning,"  Quantum machine learning (QML) offers a powerful, flexible paradigm for
programming near-term quantum computers, with applications in chemistry,
metrology, materials science, data science, and mathematics. Here, one trains
an ansatz, in the form of a parameterized quantum circuit, to accomplish a task
of interest. However, challenges have recently emerged suggesting that deep
ansatzes are difficult to train, due to flat training landscapes caused by
randomness or by hardware noise. This motivates our work, where we present a
variable structure approach to build ansatzes for QML. Our approach, called
VAns (Variable Ansatz), applies a set of rules to both grow and (crucially)
remove quantum gates in an informed manner during the optimization.
Consequently, VAns is ideally suited to mitigate trainability and noise-related
issues by keeping the ansatz shallow. We employ VAns in the variational quantum
eigensolver for condensed matter and quantum chemistry applications and also in
the quantum autoencoder for data compression, showing successful results in all
cases.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG); Machine Learning (stat.ML)
Classification with Quantum Machine Learning: A Survey,"  Due to the superiority and noteworthy progress of Quantum Computing (QC) in a
lot of applications such as cryptography, chemistry, Big data, machine
learning, optimization, Internet of Things (IoT), Blockchain, communication,
and many more. Fully towards to combine classical machine learning (ML) with
Quantum Information Processing (QIP) to build a new field in the quantum world
is called Quantum Machine Learning (QML) to solve and improve problems that
displayed in classical machine learning (e.g. time and energy consumption,
kernel estimation). The aim of this paper presents and summarizes a
comprehensive survey of the state-of-the-art advances in Quantum Machine
Learning (QML). Especially, recent QML classification works. Also, we cover
about 30 publications that are published lately in Quantum Machine Learning
(QML). we propose a classification scheme in the quantum world and discuss
encoding methods for mapping classical data to quantum data. Then, we provide
quantum subroutines and some methods of Quantum Computing (QC) in improving
performance and speed up of classical Machine Learning (ML). And also some of
QML applications in various fields, challenges, and future vision will be
presented.

    ",Quantum Physics (quant-ph),; Machine Learning (cs.LG)
Helix: Holistic Optimization for Accelerating Iterative Machine Learning,"  Machine learning workflow development is a process of trial-and-error:
developers iterate on workflows by testing out small modifications until the
desired accuracy is achieved. Unfortunately, existing machine learning systems
focus narrowly on model training---a small fraction of the overall development
time---and neglect to address iterative development. We propose Helix, a
machine learning system that optimizes the execution across
iterations---intelligently caching and reusing, or recomputing intermediates as
appropriate. Helix captures a wide variety of application needs within its
Scala DSL, with succinct syntax defining unified processes for data
preprocessing, model specification, and learning. We demonstrate that the reuse
problem can be cast as a Max-Flow problem, while the caching problem is
NP-Hard. We develop effective lightweight heuristics for the latter. Empirical
evaluation shows that Helix is not only able to handle a wide variety of use
cases in one unified workflow but also much faster, providing run time
reductions of up to 19x over state-of-the-art systems, such as DeepDive or
KeystoneML, on four real-world applications in natural language processing,
computer vision, social and natural sciences.

    ",Databases (cs.DB),; Machine Learning (cs.LG)
A Comprehensive Physics-Informed Machine Learning Framework for Predictive Turbulence Modeling,"  Although an increased availability of computational resources has enabled
high-fidelity simulations of turbulent flows, the RANS models are still the
dominant tools for industrial applications. However, the predictive
capabilities of RANS models are limited by potential inaccuracy driven by
hypotheses in the Reynolds stress closure. Recently, a Physics-Informed Machine
Learning (PIML) approach has been proposed to learn the functional form of
Reynolds stress discrepancy in RANS simulations based on available data. It has
been demonstrated that the learned discrepancy function can be used to improve
Reynolds stresses in different flows where data are not available. However,
owing to a number of challenges, the improvements have been demonstrated only
in the Reynolds stress prediction but not in the corresponding propagated
quantities of interest. In this work, we introduce the procedures toward a
complete PIML framework for predictive turbulence modeling, including learning
Reynolds stress discrepancy function, predicting Reynolds stresses in different
flows, and propagating to mean flow fields. The process of Reynolds stress
propagation and predictive accuracy of the propagated velocity field are
investigated. To improve the learning-prediction performance, the input
features are enriched based on an integrity basis of invariants. The fully
developed turbulent flow in a square duct is used as the test case. The
discrepancy model is trained on flow fields obtained from several Reynolds
numbers and evaluated on a duct flow at a Reynolds number higher than any of
the training cases. The predicted Reynolds stresses are propagated to velocity
field through RANS equations. Numerical results show excellent predictive
performances in both Reynolds stresses and their propagated velocities,
demonstrating the merits of the PIML approach in predictive turbulence
modeling.

    ",Fluid Dynamics (physics.flu-dyn),
A machine learning framework for data driven acceleration of computations of differential equations,"  We propose a machine learning framework to accelerate numerical computations
of time-dependent ODEs and PDEs. Our method is based on recasting
(generalizations of) existing numerical methods as artificial neural networks,
with a set of trainable parameters. These parameters are determined in an
offline training process by (approximately) minimizing suitable (possibly
non-convex) loss functions by (stochastic) gradient descent methods. The
proposed algorithm is designed to be always consistent with the underlying
differential equation. Numerical experiments involving both linear and
non-linear ODE and PDE model problems demonstrate a significant gain in
computational efficiency over standard numerical methods.

    ",Numerical Analysis (math.NA),; Machine Learning (cs.LG)
Boosting Combinatorial Problem Modeling with Machine Learning,"  In the past few years, the area of Machine Learning (ML) has witnessed
tremendous advancements, becoming a pervasive technology in a wide range of
applications. One area that can significantly benefit from the use of ML is
Combinatorial Optimization. The three pillars of constraint satisfaction and
optimization problem solving, i.e., modeling, search, and optimization, can
exploit ML techniques to boost their accuracy, efficiency and effectiveness. In
this survey we focus on the modeling component, whose effectiveness is crucial
for solving the problem. The modeling activity has been traditionally shaped by
optimization and domain experts, interacting to provide realistic results.
Machine Learning techniques can tremendously ease the process, and exploit the
available data to either create models or refine expert-designed ones. In this
survey we cover approaches that have been recently proposed to enhance the
modeling process by learning either single constraints, objective functions, or
the whole model. We highlight common themes to multiple approaches and draw
connections with related fields of research.

    ",Artificial Intelligence (cs.AI),
Malware Detection Module using Machine Learning Algorithms to Assist in Centralized Security in Enterprise Networks,"  Malicious software is abundant in a world of innumerable computer users, who
are constantly faced with these threats from various sources like the internet,
local networks and portable drives. Malware is potentially low to high risk and
can cause systems to function incorrectly, steal data and even crash. Malware
may be executable or system library files in the form of viruses, worms,
Trojans, all aimed at breaching the security of the system and compromising
user privacy. Typically, anti-virus software is based on a signature definition
system which keeps updating from the internet and thus keeping track of known
viruses. While this may be sufficient for home-users, a security risk from a
new virus could threaten an entire enterprise network. This paper proposes a
new and more sophisticated antivirus engine that can not only scan files, but
also build knowledge and detect files as potential viruses. This is done by
extracting system API calls made by various normal and harmful executable, and
using machine learning algorithms to classify and hence, rank files on a scale
of security risk. While such a system is processor heavy, it is very effective
when used centrally to protect an enterprise network which maybe more prone to
such threats.

    ",Cryptography and Security (cs.CR),; Machine Learning (cs.LG)
Enhanced Membership Inference Attacks against Machine Learning Models,"  How much does a machine learning algorithm leak about its training data, and
why? Membership inference attacks are used as an auditing tool to quantify this
leakage. In this paper, we present a comprehensive \textit{hypothesis testing
framework} that enables us not only to formally express the prior work in a
consistent way, but also to design new membership inference attacks that use
reference models to achieve a significantly higher power (true positive rate)
for any (false positive rate) error. More importantly, we explain \textit{why}
different attacks perform differently. We present a template for
indistinguishability games, and provide an interpretation of attack success
rate across different instances of the game. We discuss various uncertainties
of attackers that arise from the formulation of the problem, and show how our
approach tries to minimize the attack uncertainty to the one bit secret about
the presence or absence of a data point in the training set. We perform a
\textit{differential analysis} between all types of attacks, explain the gap
between them, and show what causes data points to be vulnerable to an attack
(as the reasons vary due to different granularities of memorization, from
overfitting to conditional memorization). Our auditing framework is openly
accessible as part of the \textit{Privacy Meter} software tool.

    ",Machine Learning (cs.LG),; Cryptography and Security (cs.CR); Machine Learning (stat.ML)
Survey on Causal-based Machine Learning Fairness Notions,"  Addressing the problem of fairness is crucial to safely use machine learning
algorithms to support decisions with a critical impact on people's lives such
as job hiring, child maltreatment, disease diagnosis, loan granting, etc.
Several notions of fairness have been defined and examined in the past decade,
such as statistical parity and equalized odds. The most recent fairness
notions, however, are causal-based and reflect the now widely accepted idea
that using causality is necessary to appropriately address the problem of
fairness. This paper examines an exhaustive list of causal-based fairness
notions and study their applicability in real-world scenarios. As the majority
of causal-based fairness notions are defined in terms of non-observable
quantities (e.g., interventions and counterfactuals), their deployment in
practice requires to compute or estimate those quantities using observational
data. This paper offers a comprehensive report of the different approaches to
infer causal quantities from observational data including identifiability
(Pearl's SCM framework) and estimation (potential outcome framework). The main
contributions of this survey paper are (1) a guideline to help selecting a
suitable fairness notion given a specific real-world scenario, and (2) a
ranking of the fairness notions according to Pearl's causation ladder
indicating how difficult it is to deploy each notion in practice.

    ",Machine Learning (cs.LG),
